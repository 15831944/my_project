[{"content":"转自http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html 在做分类时常常需要估算不同样本之间的相似性度量(Similarity Measurement)，这时通常采用的方法就是计算样本间的“距离”(Distance)。采用什么样的方法计算距离是很讲究，甚至关系到分类的正确与否。 　　本文的目的就是对常用的相似性度量作一个总结。 本文目录： 1. 欧氏距离 2. 曼哈顿距离 3. 切比雪夫距离 4. 闵可夫斯基距离 5. 标准化欧氏距离 6. 马氏距离 7. 夹角余弦 8. 汉明距离 9. 杰卡德距离 & 杰卡德相似系数 10. 相关系数 & 相关距离 11. 信息熵 1. 欧氏距离(Euclidean Distance)        欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。 (1)二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：   (2)三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：   (3)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：   　　也可以用表示成向量运算的形式：   (4)Matlab计算欧氏距离 Matlab计算距离主要使用pdist函数。若X是一个M×N的矩阵，则pdist(X)将X矩阵M行的每一行作为一个N维向量，然后计算这M个向量两两间的距离。 例子：计算向量(0,0)、(1,0)、(0,2)两两间的欧式距离 X = [0 0 ; 1 0 ; 0 2] D = pdist(X,'euclidean') 结果： D =     1.0000    2.0000    2.2361   2. 曼哈顿距离(Manhattan Distance)        从名字就可以猜出这种距离的计算方法了。想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源， 曼哈顿距离也称为城市街区距离(City Block distance)。 (1)二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离   (2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离   (3) Matlab计算曼哈顿距离 例子：计算向量(0,0)、(1,0)、(0,2)两两间的曼哈顿距离 X = [0 0 ; 1 0 ; 0 2] D = pdist(X, 'cityblock') 结果： D =      1     2     3 3. 切比雪夫距离 ( Chebyshev Distance )        国际象棋玩过么？国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？自己走走试试。你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。 (1)二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离   (2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离   　　这个公式的另一种等价形式是          看不出两个公式是等价的？提示一下：试试用放缩法和夹逼法则来证明。 (3)Matlab计算切比雪夫距离 例子：计算向量(0,0)、(1,0)、(0,2)两两间的切比雪夫距离 X = [0 0 ; 1 0 ; 0 2] D = pdist(X, 'chebychev') 结果： D =      1     2     2   4. 闵可夫斯基距离(Minkowski Distance) 闵氏距离不是一种距离，而是一组距离的定义。 (1) 闵氏距离的定义        两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：   其中p是一个变参数。 当p=1时，就是曼哈顿距离 当p=2时，就是欧氏距离 当p→∞时，就是切比雪夫距离        根据变参数的不同，闵氏距离可以表示一类的距离。 (2)闵氏距离的缺点 　　闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。 　　举个例子：二维样本(身高,体重)，其中身高范围是150~190，体重范围是50~60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。        简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。 (3)Matlab计算闵氏距离 例子：计算向量(0,0)、(1,0)、(0,2)两两间的闵氏距离（以变参数为2的欧氏距离为例） X = [0 0 ; 1 0 ; 0 2] D = pdist(X,'minkowski',2) 结果： D =     1.0000    2.0000    2.2361 5. 标准化欧氏距离 (Standardized Euclidean distance ) (1)标准欧氏距离的定义 　　标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为： 　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是： 　　标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差 　　经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式： 　　如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。 (2)Matlab计算标准化欧氏距离 例子：计算向量(0,0)、(1,0)、(0,2)两两间的标准化欧氏距离 (假设两个分量的标准差分别为0.5和1) X = [0 0 ; 1 0 ; 0 2] D = pdist(X, 'seuclidean',[0.5,1]) 结果： D =     2.0000    2.0000    2.8284   6. 马氏距离(Mahalanobis Distance) （1）马氏距离定义        有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：          而其中向量Xi与Xj之间的马氏距离定义为：        若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：        也就是欧氏距离了。 　　若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。 (2)马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。 (3) Matlab计算(1 2)，( 1 3)，( 2 2)，( 3 1)两两之间的马氏距离 X = [1 2; 1 3; 2 2; 3 1] Y = pdist(X,'mahalanobis')   结果： Y =     2.3452    2.0000    2.3452    1.2247    2.4495    1.2247   7. 夹角余弦(Cosine)        有没有搞错，又不是学几何，怎么扯到夹角余弦了？各位看官稍安勿躁。几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。 (1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式： (2) 两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦        类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。 　　即：        夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。        夹角余弦的具体应用可以参阅参考文献[1]。 (3)Matlab计算夹角余弦 例子：计算(1,0)、( 1,1.732)、( -1,0)两两间的夹角余弦 X = [1 0 ; 1 1.732 ; -1 0] D = 1- pdist(X, 'cosine')  % Matlab中的pdist(X, 'cosine')得到的是1减夹角余弦的值 结果： D =     0.5000   -1.0000   -0.5000   8. 汉明距离(Hamming distance) (1)汉明距离的定义        两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。        应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。 (2)Matlab计算汉明距离 　　Matlab中2个向量之间的汉明距离的定义为2个向量不同的分量所占的百分比。        例子：计算向量(0,0)、(1,0)、(0,2)两两间的汉明距离 X = [0 0 ; 1 0 ; 0 2]; D = PDIST(X, 'hamming') 结果： D =     0.5000    0.5000    1.0000   9. 杰卡德相似系数(Jaccard similarity coefficient) (1) 杰卡德相似系数        两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。 　　杰卡德相似系数是衡量两个集合的相似度一种指标。 (2) 杰卡德距离        与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示： 　　杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。 (3) 杰卡德相似系数与杰卡德距离的应用        可将杰卡德相似系数用在衡量样本的相似度上。 　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。 p ：样本A与B都是1的维度的个数 q ：样本A是1，样本B是0的维度的个数 r ：样本A是0，样本B是1的维度的个数 s ：样本A与B都是0的维度的个数 那么样本A与B的杰卡德相似系数可以表示为： 这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。 而样本A与B的杰卡德距离表示为： (4)Matlab 计算杰卡德距离 Matlab的pdist函数定义的杰卡德距离跟我这里的定义有一些差别，Matlab中将其定义为不同的维度的个数占“非全零维度”的比例。 例子：计算(1,1,0)、(1,-1,0)、(-1,1,0)两两之间的杰卡德距离 X = [1 1 0; 1 -1 0; -1 1 0] D = pdist( X , 'jaccard') 结果 D = 0.5000    0.5000    1.0000   10. 相关系数 ( Correlation coefficient )与相关距离(Correlation distance) (1) 相关系数的定义 相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。 (2)相关距离的定义   (3)Matlab计算(1, 2 ,3 ,4 )与( 3 ,8 ,7 ,6 )之间的相关系数与相关距离 X = [1 2 3 4 ; 3 8 7 6] C = corrcoef( X' )   %将返回相关系数矩阵 D = pdist( X , 'correlation') 结果： C =     1.0000    0.4781     0.4781    1.0000 D = 0.5219       其中0.4781就是相关系数，0.5219是相关距离。 11. 信息熵(Information Entropy)        信息熵并不属于一种相似性度量。那为什么放在这篇文章中啊？这个。。。我也不知道。 (╯▽╰) 信息熵是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。        计算给定的样本集X的信息熵的公式： 参数的含义： n：样本集X的分类数 pi：X中第i类元素出现的概率        信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0 参考资料：  [1]吴军. 数学之美 系列 12 - 余弦定理和新闻的分类. http://www.google.com.hk/ggblog/googlechinablog/2006/07/12_4010.html [2] Wikipedia. Jaccard index. http://en.wikipedia.org/wiki/Jaccard_index [3] Wikipedia. Hamming distance http://en.wikipedia.org/wiki/Hamming_distance [4] 求马氏距离（Mahalanobis distance ）matlab版 http://junjun0595.blog.163.com/blog/static/969561420100633351210/ [5] Pearson product-moment correlation coefficient http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient","title":"相似性度量"},{"content":"相关文章: 基于baseline和stochastic gradient descent的个性化推荐系统 基于baseline、svd和stochastic gradient descent的个性化推荐系统 转载请注明：转自 zh's note    http://blog.csdn.net/wuzh670/ 文章主要介绍的是koren 08年发的论文[1],  2.2neighborhood models部分内容（其余部分会陆续补充上来）。 koren论文中用到netflix 数据集， 过于大， 在普通的pc机上运行时间很长很长。考虑到写文章目地主要是已介绍总结方法为主，所以采用Movielens 数据集。 变量介绍(涉及到的其他变量可以参看上面提到的相关文章)： 利用pearson相关系数，求i,j之间的相关性。 文章中提到shrunk correlation coefficient(收缩的相关系数)，收缩后pearson相关系数作为i,j相似性，后面会通过实践证明收缩的效果会更好。 预测值： 系统评判标准：RMSE, MAE 系统采用5-fold cross-validation(movielens数据集中已经默认划分好了) 注: 用SGD来训练出最优的用户和项的偏置值，后续会补充完整。 详细代码实现： '''Created on Dec 16, 2012@Author: Dennis Wu@E-mail: hansel.zh@gmail.com@Homepage: http://blog.csdn.net/wuzh670@Weibo: http://weibo.com/hanselData set download from : http://www.grouplens.org/system/files/ml-100k.zip'''from operator import itemgetter, attrgetterfrom math import sqrt,fabs,logimport randomdef load_data(filename_train, filename_test):    train = {}    test = {}        for line in open(filename_train):        (userId, itemId, rating, timestamp) = line.strip().split('\\t')        train.setdefault(userId,{})        train[userId][itemId] = float(rating)    for line in open(filename_test):        (userId, itemId, rating, timestamp) = line.strip().split('\\t')        test.setdefault(userId,{})        test[userId][itemId] = float(rating)    return train, testdef initialBias(train, userNum, movieNum, mean):    bu = {}    bi = {}    biNum = {}    buNum = {}        u = 1    while u < (userNum+1):        su = str(u)        for i in train[su].keys():            bi.setdefault(i,0)            biNum.setdefault(i,0)            bi[i] += (train[su][i] - mean)            biNum[i] += 1        u += 1            i = 1    while i < (movieNum+1):        si = str(i)        biNum.setdefault(si,0)        if biNum[si] >= 1:            bi[si] = bi[si]*1.0/(biNum[si]+25)        else:            bi[si] = 0.0        i += 1    u = 1    while u < (userNum+1):        su = str(u)        for i in train[su].keys():            bu.setdefault(su,0)            buNum.setdefault(su,0)            bu[su] += (train[su][i] - mean - bi[i])            buNum[su] += 1        u += 1            u = 1    while u < (userNum+1):        su = str(u)        buNum.setdefault(su,0)        if buNum[su] >= 1:            bu[su] = bu[su]*1.0/(buNum[su]+10)        else:            bu[su] = 0.0        u += 1    return bu, bidef initial(train, userNum, movieNum):    average = {}    Sij = {}    mean = 0    num = 0    N = {}    for u in train.keys():        for i in train[u].keys():            mean += train[u][i]            num += 1            average.setdefault(i,0)            average[i] += train[u][i]            N.setdefault(i,0)            N[i] += 1            Sij.setdefault(i,{})            for j in train[u].keys():                if i == j:                    continue                Sij[i].setdefault(j,[])                Sij[i][j].append(u)    mean = mean / num    for i in average.keys():        average[i] = average[i] / N[i]            pearson = {}    itemSim = {}    for i in Sij.keys():        pearson.setdefault(i,{})        itemSim.setdefault(i,{})        for j in Sij[i].keys():            pearson[i][j] = 1            part1 = 0            part2 = 0            part3 = 0            for u in Sij[i][j]:                part1 += (train[u][i] - average[i]) * (train[u][j] - average[j])                part2 += pow(train[u][i] - average[i], 2)                part3 += pow(train[u][j] - average[j], 2)            if part1 != 0:                pearson[i][j] = part1 / sqrt(part2 * part3)            itemSim[i][j] = fabs(pearson[i][j] * len(Sij[i][j]) / (len(Sij[i][j]) + 100))    # initial user and item Bias, respectly    bu, bi = initialBias(train, userNum, movieNum, mean)    return itemSim, mean, average, bu, bi      def neighborhoodModels(train, test, itemSim, mean, average, bu, bi):        pui = {}    rmse = 0.0    mae = 0.0    num = 0    for u in test.keys():        pui.setdefault(u,{})        for i in test[u].keys():            pui[u][i] = mean + bu[u] + bi[i]            stat = 0            stat2 = 0            for j in train[u].keys():                if itemSim.has_key(i) and itemSim[i].has_key(j):                    stat += (train[u][j] - mean - bu[u] - bi[j]) * itemSim[i][j]                    stat2 += itemSim[i][j]            if stat > 0:                pui[u][i] += stat * 1.0 / stat2            rmse += pow((pui[u][i] - test[u][i]), 2)            mae += fabs(pui[u][i] - test[u][i])            num += 1    rmse = sqrt(rmse*1.0 / num)    mae = mae * 1.0 / num        return rmse, maeif __name__ == \"__main__\":    i = 1    sumRmse = 0.0    sumMae = 0.0    while i <= 5:        # load data        filename_train = 'data/u' + str(i) + '.base'        filename_test = 'data/u' + str(i) + '.test'        train, test = load_data(filename_train, filename_test)        # initial variables        itemSim, mean, average, bu, bi = initial(train, 943, 1682)        # neighborhoodModels        rmse, mae = neighborhoodModels(train, test, itemSim, mean, average, bu, bi)        print 'cross-validation %d:  rmse: %s     mae: %s' % (i, rmse, mae)                sumRmse += rmse        sumMae += mae        i += 1            print 'neighborhood models final results:  Rmse: %s      Mae: %s' % (sumRmse/5, sumMae/5) 实验结果：  注：第一个结果是没有使用收缩的pearson相关系数跑出的结果；第二个结果则是使用收缩的相关系数跑出的结果。 从图中容易得出使用收缩的相关系数的必要性和有效性。 REFERENCES 1.Y. Koren. Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model. Proc. 14th ACM SIGKDD Int. Conf. On Knowledge Discovery and Data Mining  (KDD’08), pp. 426–434, 2008. 2. Y.Koren.  The BellKor Solution to the Netflix Grand Prize  2009","title":"[置顶] 基于neighborhood models(item-based) 的个性化推荐系统"},{"content":"最大似然估计与贝叶斯估计（一） 原创文章，转载注明出处 ——————————————————————————————————————— 首先介绍的是最大似然估计： 已知有先验概率，类条件概率，我们可以通过贝叶斯分类器进行分类。可是，条件概率在实际应用中很难得到。 怎么解决？ ——>利用训练样本估计问题中的先验概率以及条件概率密度函数。 有什么方法？ ——>最大似然估计，贝叶斯估计。 有什么区别？ ——>          最大似然估计：将待估计的参数看为已知量，但是实际取值未知，求得使得观测样本中的概率在这个参数下概率最大。          贝叶斯估计    ：将待估计的参数看成符合某种先验概率分布的随机变量。 如何分类？ ——>同前一篇文章提到的，使用后验概率进行分类。 最大似然估计过程以及结论如下： 接下来，则学习下有关贝叶斯估计的问题。","title":"最大似然估计与贝叶斯估计（一）"},{"content":"============================================================================= EM FOR FACTOR ANALYSIS ============================================================================= 通过cs229学习笔记3 (EM alogrithm,Mixture of Gaussians revisited & Factor analysis )最后的推导 我们得到了factor analysis model的似然公式： 下面将通过EM算法最大化似然函数： E-step：       我们知道      通过之前多元高斯分布的条件概率我们可以得到            再将u和Sigma 带回到多元高斯分布的公式里，得到：      M-step： 这里我们需要maximize的部分是：(其中的∫……d(z(i))相当于离散数据中的求和符号) 因为p(x(i),z(i))=p( x(i) | z(i) )*p(z(i)) ,所以上式可以化作：（用log函数的性质） 将Qi(z(i))看作是z(i)的概率密度函数，那么积分符号和Qi合起来就是期望的表达式，即上式为：                            应为p(z(i))和Qi(z(i))是由已经固定了的高斯分布决定的，所以中括号内的后两项与我们关心的三个参数无关，可以直接省略。 再因为factor analysis的定义，，所以p(x(i) | z(i))=N(μ+∧z,Ψ)，这是多元高斯分布的性质，详细请参看多元高斯分布 所以上式等于： 再用log的性质将公式化简，然后直接使用最大似然估计，就可以得到M-step的参数更新公式了： 提醒一句：Ψ是对角矩阵 ======================================================================================== Principal components analysis （PCA） ======================================================================================== 这是一个用于降维的算法，先前在coursera上也提到过，至于有什么用处，我们先给出具体的算法吧 在执行PCA之前一般需要将各个参数scalling，使它们的量级相同 大概就是求出平均值，减去平均值，求出方差，除以方差，这样所有参数都在相同的scale上了 然后找到一个响亮，使得所有样例投影到该向量上的距离最短 也就是说：我们需要使下式最大化： 括号内的内容是x(i)映射在u向量上的大小，得到u之后选取前k个u就可以将x降维到k维。 注意u是与红色线相垂直的向量，所以最大化上式也就是最小化点到直线的垂直距离 展示图片是2D-1D，蓝色为原数据点，红色为映射点。可以看出这种方法对数据是有影响的，但是只要在限定范围内，这种影响是可以接受的。 PCA的应用： 1.visualization   将数据降低到2D或者3D使其可视化 2.compression   压缩数据 3.Learning   在learning的过程中可能因为维度过高使其速度变慢，可以用PCA加速，而且实际效果不错 4.anomaly detection   判断异常，看新点距离子空间是否很大 5.matching/distance calculation   计算两个样例的距离时不一定是用欧氏距离，而是使用低维中得距离，这样更符合实际，特别是在图像识别中 实例：   下面是一些人脸图片：    上面的图片维度可能较高，比如说100*100，经过PCA之后，可能只是25*25，甚至更低，但仍然很好地捕捉了人脸的形状 在使用pca之后，    注意：上面每张图片是单独的一个ui的表示，可以看出还是不错地捕捉了人脸的形状 上面这张图是经过PCA降维后再还原为原维度的图片，可以发现图片虽然有失真，但是失得不多，还是很好的展示了原图像","title":"机器学习 cs229学习笔记4 (EM for factor analysis & PCA（Principal components analysis）)"},{"content":"1：什么是贝叶斯公式？ 而： 其中， 比如： 给定样本：   X W(分类为：a,b) 0 b 0 a 0 b 1 b 1 a 1 a 2 a 2 a 2 a   对于以上样本，我们可以知道： P(0|a)=1/6 P(a)=6/9 P(0)=P(0|a)P(a)+P(0|b)P(b)=2/9 所以有P(a|0)=1/2 2:一般风险分类 上面已经知道贝叶斯公式的基本功能，但是，当假设一个x特征值被分类到Wj,会有一个风险函数，当处于状态Wj时，采取行动ai的风险为  首先贝叶斯公式： 当x采取ai行动的损失是：（选择最小的这个公式） 此时，上述为条件风险，整个系统的风险为： 选择a(x)使每个条件风险最小，则总风险最小。 所以此时应当选择x的分类使做出的行为ai使行动损失最小。 3:最小误差分类 当行为ai与分类wj一一对应时，也就是说采取行为ai，应当是处于wj(i=j)分类时的行为，若不是，则产生误判，这可以简单的认为： 最大时，取wj分类，可以近似认为是最开始的贝叶斯公式基本应用。 所以，一个分类器判别函数可以写成：Gi(x),若Gi(x)>Gj(x),则取i类。 4：正太分布的判别函数 已知， 在几种特殊情况下，例如方差相等 等等情况可以从此得到有不同的简化形式。 例如：已知样本，进行二分类： 以上就是贝叶斯决策论基本原理。 百分之百原创，如需转载请注明出处。","title":"贝叶斯决策论基本原理"},{"content":"本文是Andrew NG先生机器学习公开课第二课的听课笔记，由于csdn中对mathtype公式的显示问题，所以直接以图片格式发表。本系列是由公开课的学习和一些自己的理解组成的。","title":"Andrew NG 机器学习听课笔记（2）——过学习与欠学习，最小二乘的概率意义、logistic回归"},{"content":"会议信息 @ 周晓方Xiaofang：ICDE Workshop: Data-Driven Decision Guidance and Support Systems，截稿日期本月31日，欢迎投稿 http://t.cn/zlux0Us；ICDE PhD Workshop 11月20日截稿， 欢迎博士生投稿！@王敏MinWang http://t.cn/zlmHZy9；ICDE Workshop: Graph Data Management: Techniques and Applications。 11月3日截稿，欢迎投稿！http://t.cn/zlmY8Ic；ICDE Workshop: Privacy-Preserving Data Publication and Analysis. 欢迎投稿！马上就要截稿了。 http://t.cn/zlm711X；ICDE Workshop: Mobile Data Analytics，十一月五日截稿。欢迎投稿！http://t.cn/zlngiLc。 @XueminLin ： Apweb 13 的 deadline 最后延迟到了本月27日因为有些朋友反映时间不够。。。这次是Firm的Deadline了。 @Sean王晓阳 ： 第14届MDM(IEEE International Conference on Mobile Data Management)将于2013年6月3-6日在意大利米兰举行。论文摘要截稿11月12日。我是PC co-chair，请捧场。http://t.cn/zlKwVRj @周晓方Xiaofang @郑宇MSRA @谢幸Xing @于旭jxyu @黄艳YanHuang @瑞melb @孟卫一WeiyiMeng：明 年的DASFAA会议论文摘要提交截止日期延期到10月28日，给错过10月21日截止日期作者再一次投稿机会。DASFAA 2013将于明年4月22－25在武汉召开。DASFAA 2013 也收industry paper and demo paper。希望大家支持，踊跃投稿。谢谢！DASFAA2013网址在此 http://t.cn/zWLv7ZG @ICT山世光 ： ECCV2012共录用文章408篇（40 orals+368 posters)，录取率28.4%。所有文章中来自美国的占36%，…，微软占7%，中国占6%… google占1%…我们组4篇，约占1%. @余凯_西二旗民工 ： 宣传一下，第十届机器学习及其应用研讨会，清华大学，11月3号－4号http://t.cn/zlj0kzJ   学习资料 @唐杰THU：关于专家发现的一个非常好的综述论文，近期研究搜索人的童鞋可以关注一下，全文140页，发表在Foundations and Trends in Information Retrieval http://t.cn/zl1VrzQ @52nlp：正态分布的前世今生(六) by @rickjin | 我爱自然语言处理 http://t.cn/zl3iOQt @龙星计划：对信息检索感兴趣的童鞋，可以看看这个课程http://t.cn/zOyY0Go的PPT @XuYueshen_XDU_ZJU：分享自北武飘风 《信息检索和网络数据挖掘领域论文技术基础》 – 信息检索和网络数据领域（WWW, SIGIR, CIKM, WSDM, ACL, EMNLP等）的论文中常用的模型和技术总结 引子：对于这个领… (来自 @头条博客) – http://t.cn/zluRg7C @陈利人：32个非常重要的算法，你全都知道吗？不全知道没关系，我们也是，今后的一段日子，请关注我们，正好我们一块学习，我们会对每个算法整理出比较好的资料，然后和大家分享，交流。 更多参见链接： http://www.guzili.com/?paged=2","title":"机器学习 数据挖掘 资料包"},{"content":"推荐数据的处理是大规模的，在集群环境下一次要处理的数据可能是数GB，所以Mahout针对推荐数据进行了优化。 Preference 在Mahout中，用户的喜好被抽象为一个Preference，包含了userId，itemId和偏好值（user对item的偏好）。Preference是一个接口，它有一个通用的实现是GenericPreference。 因为用户的喜好数据是大规模的，我们通常会选择把它放入集合或者数组。同时，由于Java的对象的内存消耗机制，在大数据量下使用Collection<Preference>和Preference[]是非常低效的。为什么呢？ 在Java中，一个对象占用的字节数 = 基本的8字节 + 基本数据类型所占的字节 + 对象引用所占的字节 (1) 先说这基本的8字节 在JVM中，每个对象（数组除外）都有一个头，这个头有两个字，第一个字存储对象的一些标志位信息，如：锁标志位、经历了几次gc等信息；第二个字节是一个引用，指向这个类的信息。JVM为这两个字留了8个字节的空间。 这样一来的话，new Object()就占用了8个字节，那怕它是个空对象 (2) 基本类型所占用的字节数 byte/boolean 1bytes char/short 2bytes int/float 4byte double/long 8bytes (3) 对象引用所占用的字节数 reference 4bytes 注：实际中，有数据成员的话，要把数据成员按基本类型和对象引用分开统计。基本类型按(2)进行累加，然后对齐到8个倍数；对象引用按每个4字节进行累加，然后对齐到8的倍数。 占 8(基本) + 16(数据成员——基本类型：8 + 1，对齐到8) + 8(数据成员——对象引用Integer，4，对齐到8) = 32字节   如此一来的话，一个GenericPreference的对象就需要占用28个字节，userId(8bytes) + itemId(8bytes) + preference(4bytes) + 基本的8bytes = 28。如果我们使用了Collection<Preference>和Preference[]，就会浪费很多这基本的8字节。设想如果我们的数据量是上GB或是上TB，这样的开销是很难承受的。   为此Mahout封装了一个PreferenceArray，用于表示喜好数据的集合 我们看到，GenericUserPreferenceArray包含了一个userId，一个itemId的数组long[]，一个用户的喜好评分数据float[]。而不是一个Preference对象的集合。下面我们做个比较，分别创建一个PreferenceArray和Preference数组 在size为5，但只包含一条喜好数据的情况下：PreferenceArray需要20Bytes(userId 8bytes + preference 4bytes + itemId8bytes)，而Preference[]需要48字节(基本8bytes + 一个Preference对象28bytes + 4个空的引用4×3 12Bytes)。如果在有多条喜好数据的情况下，PreferenceArray中将只有一个itemId，这样它所占用的8Bytes微乎其微。所以PreferenceArray用它特殊的实现节省了4倍内存。 用《Mahout in action》一书中的原话“mahout has alreadly reinvented an 'array of Javaobjects'”——\"mahout已经重新改造了Java对象数组\"。PreferenceArray和它的具体实现减少的内存开销远远比它的的复杂性有价值，它减少了近75%的内存开销（相对于Java的集合和对象数组） 除了PreferenceArray，Mahout中还大量使用了像Map和Set这些非常典型的数据结构，但是Mahout没有直接使用像HashMap和TreeSet这些常用的Java集合实现，取而代之的是专门为Mahout推荐的需要实现了两个API，FastByIDMap和FastIDSet，之所以专门封装了这两个数据结构，主要目的是为了减少内存的开销，提升性能。它们之间主要有以下区别： ·   和HashMap一样，FastByIDMap也是基于hash的。不过FastByIDMap使用的是线性探测来解决hash冲突，而不是分割链； ·    FastByIDMap的key和值都是long类型，而不是Object，这是基于节省内存开销和改善性能所作的改良； ·    FastByIDMap类似于一个缓存区，它有一个“maximumsize”的概念，当我们添加一个新元素的时候，如果超过了这个size，那些使用不频繁的元素就会被移除。 FastByIDMap和FastIDSet在存储方面的改进非常显著。FastIDSet的每个元素平均占14字节，而HashSet而需要84字节；FastByIDMap的每个entry占28字节，而HashMap则需要84字节。 DataModel Mahout推荐引擎实际接受的输入是DataModel，它是对用户喜好数据的压缩表示。DataModel的具体实现支持从任意类型的数据源抽取用户喜好信息，可以很容易的返回输入的喜好数据中关联到一个物品的用户ID列表和count计数，以及输入数据中所有用户和物品的数量。具体实现包括内存版的GenericDataModel，支持文件读取的FileDataModel和支持数据库读取的JDBCDataModel。 GenericDataModel是DataModel的内存版实现。适用于在内存中构造推荐数据，它仅只是作为推荐引擎的输入接受用户的喜好数据，保存着一个按照用户ID和物品ID进行散列的PreferenceArray，而PreferenceArray中对应保存着这个用户ID或者物品ID的所有用户喜好数据。 FileDataModel支持文件的读取，Mahout对文件的格式没有太多严格的要求，只要满足一下格式就OK： ·     每一行包含一个用户Id，物品Id，用户喜好 ·     逗号隔开或者Tab隔开 ·     *.zip 和 *.gz 文件会自动解压缩（Mahout 建议在数据量过大时采用压缩的数据存储） FileDataModel从文件中读取数据，然后将数据以GenericDataModel的形式载入内存，具体可以查看FileDataModel中的buildModel方法。   JDBCDataModel支持对数据库的读取操作，Mahout提供了对MySQL的默认支持MySQLJDBCDataModel，它对用户喜好数据的存储有以下要求： ·     用户ID列需要是BIGINT而且非空 ·     物品ID列需要是BIGINT而且非空 ·     用户喜好值列需要是FLOAT ·     建议在用户ID和物品ID上建索引 有的时候，我们会忽略用户的喜好值，仅仅只关心用户和物品之间存不存在关联关系，这种关联关系在Mahout里面叫做“boolean preference”。 之所以会有这类喜好，是因为用户和物品的关联要么存在，要么不存在，记住只是表示关联关系存不存在，不代表喜欢和不喜欢。实际上一条“boolean preference”可有三个状态：喜欢、不喜欢、没有任何关系。 在喜好数据中有大量的噪音数据的情况下，这种特殊的喜好评定方式是有意义的。 同时Mahout为“boolean preference”提供了一个内存版的DataModel——GenericBooleanPrefDataModel 可以看到，GenericBooleanPrefDataModel没有对喜好值进行存储，仅仅只存储了关联的userId和itemId，注意和GenericDataModel的差别，GenericBooleanPrefDataModel采用了FastIDSet，只有关联的Id，没有喜好值。因此它的一些方法（继承自DataModel的）如getItemIDsForUser()有更好的执行速度，而getPreferencesFromUser()的执行速度会更差，因为GenericBooleanPrefDataModel本来就没存储喜好值，它默认用户对物品的喜好值都是1.0","title":"Mahout学习——数据承载"},{"content":"http://blog.pluskid.org/?tag=clustering漫谈clustering 系列。 谱聚类综述：“A tutorial on spectral clustering\"","title":"聚类问题"},{"content":"bagging,boosting,adboost,random forests都属于集成学习范畴.  在boosting算法产生之前，还出现过两种比较重要的算法，即boostrapping方法和bagging方法。首先介绍一下这二个算法思路: 1.     从整体样本集合中，抽样n* < N个样本针对抽样的集合训练分类器Ci ,抽样的方法有很多,例如放回抽样,不放回抽样等. 2.     对于预测样本, 众多分类器进行投票，最终的结果是分类器投票的优胜结果. 以上就是bagging的主要思想.但是，上述这两种方法，都只是将分类器进行简单的组合，实际上，并没有发挥出分类器组合的威力来。到1989年，Yoav Freund与 Robert Schapire提出了一种可行的将弱分类器组合为强分类器的方法。即boosting算法. 还有1995年，Freund and schapire提出了现在的adaboost算法：http://www.cnblogs.com/liqizhou/archive/2012/04/23/2466578.html. 以下参考别人写的boosting方法,加上自己的一点体会. Boosting方法：     Boosting这其实思想相当的简单，大概是，对一份数据，建立M个模型（比如分类），一般这种模型比较简单，称为弱分类器(weak learner)每次分类都将上一次分错的数据权重提高一点再进行分类，这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的成绩。 上图（图片来自prml p660）就是一个Boosting的过程，绿色的线表示目前取得的模型（模型是由前m次得到的模型合并得到的），虚线表示当前这次模型。每次分类的时候，会更关注分错的数据，上图中，红色和蓝色的点就是数据，点越大表示权重越高，看看右下角的图片，当m=150的时候，获取的模型已经几乎能够将红色和蓝色的点区分开了。     Boosting可以用下面的公式来表示： 训练集中一共有n个点，我们可以为里面的每一个点赋上一个权重Wi(0 <= i < n)，表示这个点的重要程度，通过依次训练模型的过程，我们对点的权重进行修正，如果分类正确了，权重降低，如果分类错了，则权重提高，初始的时候，权重都是一样的。上图中绿色的线就是表示依次训练模型，可以想象得到，程序越往后执行，训练出的模型就越会在意那些容易分错（权重高）的点。当全部的程序执行完后，会得到M个模型，分别对应上图的y1(x)…yM(x)，通过加权的方式组合成一个最终的模型YM(x)。     我觉得Boosting更像是一个人学习的过程，开始学一样东西的时候，会去做一些习题，但是常常连一些简单的题目都会弄错，但是越到后面，简单的题目已经难不倒他了，就会去做更复杂的题目，等到他做了很多的题目后，不管是难题还是简单的题都可以解决掉了。   算法的流程: 1.     给每个样本都赋予一个权重,可以使随机,也可以使相等的权重. 2.     按照样本权重从大到小的顺序,选择n个样本训练弱分类器Ci. 3.     在样本预测阶段, 多个分类器进行投票决定样本数据那种分类.   Online Boosting  在线 Boosting 算法重点研究如何替换弱分类器来学习时变的样本集,本文提出了一种新的在线 Boosting 算法,各弱学习器调节自身参数进而适应时变的环境。与以往直接替换学习器的方式不同,该方法随着新的样本集的到来调节弱分类器的参数,使分类正确率高的弱分类器有较高的权重,而分类正确率低的弱分类器的权重也会相应降低。 下面讲一下Poisson重采样 对每一训练样本,按照参数为 λ 的 Poisson分布得到一个随机数,对样本重复训练该随机数次。一个基学习器训练完毕之后将该样本继续传递给下一个基学习器。当一个基学习器误分类一个训练样本,该样本在提交给下一个基学习器时,其对应的 Poisson 分布参数 λ 增加;反之样本对应的 Poisson分布参数减少,并通过这些来更新每个基学习器的误差 ε 。从这个角度,训练样本的权重更新方法和离线 AdaBoost 相同。 其流程如下： 其中：λm 的作用是控制整个基分类器的误差，因为在线进来的不知一个样本。有很多样本。 下面给出online boosting 和online adboosting的思路。 在目标图像的目标物体附近选择正样本，离目标物体远的区域作为负样本。 OAB1 = 在线boosting（搜索半径r=1，每帧1个正样本） OAB5 = 在线boosting（搜索半径r=5，每帧45个正样本）   以上内容是自己在网络上看到的，呵呵，和大家分享一下.....","title":"Boosting, Online Boosting,adaboost"},{"content":"（all is based on stanford's opencourse cs229 lecture 12） 首先介绍的是聚类算法中最简单的K-Means算法 /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 算法的实质就是：（随机初始化聚点）     1、将样例的每个点分配给离它最近的那个聚点     2、统计所有同类点的中点，将聚点移到这个中点     重复1.2两个过程直到中心点不再变化，为了稳健可以多次算法选取最优的分类     大概过程如图所示：   一些值得注意的细节是，初始化的时候可以随机从样例中选择k个点作为初始聚点（k是据点数），k值的选取最好是人工选取，另一种兼容性较差的方法就是测试k值找到拐点（elbow）：下图拐点为3. （以上内容来自我的coursera学习笔记8）下面将是本堂课补充的关于K-Means的内容 ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 可以定义误差：distortion function 迭代过程也就是J减少的过程，但是J不是凸函数，所以不能保证收敛到global minimum，所以给出一个J，可能会有不同的聚类方式。 所以最好多次聚类选择最好的结果 /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 接下来提到了Anomaly Detection，这在coursera上也有提到，同样参看coursera学习笔记8 由此引出了Mixture of Gaussians得概念：数据由多个高斯分布混合而成。 通过最大似然可以求出三个参数的值 其中z(i)是指第i的样本被分为第多少个高斯分布。z(i)=j 也就是说第i个点被分为是由第j个高斯分布产生的点 这是在知道z(i)的情况下，事实上我们不知道，所以可以通过猜测-》修正-》猜测的迭代来求出最佳的z(i) 这就是EM（高斯分布的一个特例，再下面会介绍更加一般的EM算法)算法的思路 其中的w其实就是z，相对z来说要“软”一些，z可以看作对特定组为1，其他为0的w参数。 w表示的是每个点分到每类的概率。 //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 一般的EM算法 给一个定义：Jensen's inequality:对后面的推导有用 这是f为严格的凸函数的时候的不等式，而如果f为凹函数，不等式反向即可。 当X为常数时等号成立 ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 下面是推导过程 这个问题的最大似然函数是： 通过进一步推导可以得到如下结果 （图片7） Q(z(i))可以看作是那个分式的概率密度，则第二个∑的部分可以看作分式的期望 又因为log函数是一个凹函数，借用上面的Jensen's inequality，可以得到 （图片8） 最大似然函数就有了一个下界，而EM算法就是通过每次确定下界，找到相对此下界更优的一个位置再作为下界 最后会得到一个局部最优的值。也就是说一次次放大下界值，最后converge到局部最优值 所以我们要让不等式相等，通过上面我们知道不等式相等只有当E[]中得数为常数时才会相等，而且Q(z(i))的和为1 可以得到： （图片9） 所以可以得到EM算法的过程： (图片10) 整个过程大概就是通过数次调整下界最后得到最大似然值","title":"机器学习 cs229学习笔记2 (k-means,EM & Mixture of Gaussians)"},{"content":"免费开源的软件 参考： 1.五个免费开源的数据挖掘软件：http://www.oschina.net/question/12_14026 Orange Orange 是一个基于组件的数据挖掘和机器学习软件套装，它的功能即友好，又很强大，快速而又多功能的可视化编程前端，以便浏览数据分析和可视化，基绑定了 Python以进行脚本开发。它包含了完整的一系列的组件以进行数据预处理，并提供了数据帐目，过渡，建模，模式评估和勘探的功能。其由C++ 和 Python开发，它的图形库是由跨平台的Qt框架开发。 RapidMiner RapidMiner, 以前叫 YALE (Yet Another Learning Environment), 其是一个给机器学习和数据挖掘和分析的试验环境，同时用于研究了真实世界数据挖掘。它提供的实验由大量的算子组成，而这些算子由详细的XML 文件记录，并被RapidMiner图形化的用户接口表现出来。RapidMiner为主要的机器学习过程提供了超过500算子，并且，其结合了学习方案 和Weka学习环境的属性评估器。它是一个独立的工具可以用来做数据分析，同样也是一个数据挖掘引擎可以用来集成到你的产品中。 Weka Weka 由Java开发的 Weka (Waikato Environment for Knowledge Analysis) 是一个知名机器学习软件，其支持几种经典的数据挖掘任务，显著的数据预处理，集群，分类，回归，虚拟化，以及功能选择。其技术基于假设数据是以一种单个文 件或关联的，在那里，每个数据点都被许多属性标注。 Weka 使用Java的数据库链接能力可以访问SQL数据库，并可以处理一个数据库的查询结果。它主要的用户接品是Explorer，也同样支持相同功能的命令 行，或是一种基于组件的知识流接口。 JHepWork 为科学家，工程师和学生所设计的 jHepWork 是一个免费的开源数据分析框架，其主要是用开源库来创建 一个数据分析环境，并提供了丰富的用户接口，以此来和那些收费的的软件竞争。它主要是为了科学计算用的二维和三维的制图，并包含了用Java实现的数学科 学库，随机数，和其它的数据挖掘算法。 jHepWork 是基于一个高级的编程语言 Jython，当然，Java代码同样可以用来调用 jHepWork 的数学和图形库。 KNIME KNIME (Konstanz Information Miner) 是一个用户友好，智能的，并有丰演的开源的数据集成，数据处理，数据分析和数据勘探平台。它给了用户有能力以可视化的方式创建数据流或数据通道，可选择性 地运行一些或全部的分析步骤，并以后面研究结果，模型 以及 可交互的视图。 KNIME 由Java写成，其基于 Eclipse 并通过插件的方式来提供更多的功能。通过以插件的文件，用户可以为文件，图片，和时间序列加入处理模块，并可以集成到其它各种各样的开源项目中，比如：R 语言，Weka， Chemistry Development Kit, 和 LibSVM.","title":"数据挖掘资料汇总"},{"content":"发信人: zibuyu (得之我幸), 信区: NLP 标 题: 机器学习推荐论文和书籍 发信站: 水木社区 (Thu Oct 30 21:00:39 2008), 站内 我们组内某小神童师弟通读论文，拟了一个机器学习的推荐论文和书籍列表。 经授权发布在这儿，希望对大家有用。:) 基本模型: HMM(Hidden Markov Models): A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.pdf ME(Maximum Entropy): ME_to_NLP.pdf MEMM(Maximum Entropy Markov Models): memm.pdf CRF(Conditional Random Fields): An Introduction to Conditional Random Fields for Relational Learning.pdf Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.pdf SVM(support vector machine): *张学工<<统计学习理论>> LSA(or LSI)(Latent Semantic Analysis): Latent semantic analysis.pdf pLSA(or pLSI)(Probablistic Latent Semantic Analysis): Probabilistic Latent Semantic Analysis.pdf LDA(Latent Dirichlet Allocation): Latent Dirichlet Allocaton.pdf(用variational theory + EM算法解模型) Parameter estimation for text analysis.pdf(using Gibbs Sampling 解模) Neural Networksi(including Hopfield Model& self-organizing maps & Stochastic networks & Boltzmann Machine etc.): Neural Networks - A Systematic Introduction Diffusion Networks: Diffusion Networks, Products of Experts, and Factor Analysis.pdf Markov random fields: Generalized Linear Model(including logistic regression etc.): An introduction to Generalized Linear Models 2nd Chinese Restraunt Model (Dirichlet Processes): Dirichlet Processes, Chinese Restaurant Processes and all that.pdf Estimating a Dirichlet Distribution.pdf ================================================================= Some important algorithms: EM(Expectation Maximization): Expectation Maximization and Posterior Constraints.pdf Maximum Likelihood from Incomplete Data via the EM Algorithm.pdf MCMC(Markov Chain Monte Carlo) & Gibbs Sampling: Markov Chain Monte Carlo and Gibbs Sampling.pdf Explaining the Gibbs Sampler.pdf An introduction to MCMC for Machine Learning.pdf PageRank: 矩阵分解算法: SVD, QR分解, Shur分解, LU分解, 谱分解 Boosting( including Adaboost): *adaboost_talk.pdf Spectral Clustering: Tutorial on spectral clustering.pdf Energy-Based Learning: A tutorial on Energy-based learning.pdf Belief Propagation: Understanding Belief Propagation and its Generalizations.pdf bp.pdf Construction free energy approximation and generalized belief propagation algorithms.pdf Loopy Belief Propagation for Approximate Inference An Empirical Study.pdf Loopy Belief Propagation.pdf AP (affinity Propagation): L-BFGS: <<最优化理论与算法 2nd>> chapter 10 On the limited memory BFGS method for large scale optimization.pdf IIS: IIS.pdf ================================================================= 理论部分： 概率图(probabilistic networks): An introduction to Variational Methods for Graphical Models.pdf Probabilistic Networks Factor Graphs and the Sum-Product Algorithm.pdf Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms.pdf *Graphical Models, exponential families, and variational inference.pdf Variational Theory(变分理论，我们只用概率图上的变分): Tutorial on varational approximation methods.pdf A variational Bayesian framework for graphical models.pdf variational tutorial.pdf Information Theory: Elements of Information Theory 2nd.pdf 测度论： 测度论(Halmos).pdf 测度论讲义(严加安).pdf 概率论: …… <<概率与测度论>> 随机过程: 应用随机过程 林元烈 2002.pdf <<随机数学引论>> Matrix Theory: 矩阵分析与应用.pdf 模式识别： <<模式识别 2nd>> 边肇祺 *Pattern Recognition and Machine Learning.pdf 最优化理论: <<Convex Optimization>> <<最优化理论与算法>> 泛函分析： <<泛函分析导论及应用>> Kernel理论： <<模式分析的核方法>> 统计学: …… <<统计手册>> 综合: semi-supervised learning: <<Semi-supervised Learning>> MIT Press semi-supervised learning based on Graph.pdf Co-training: Self-training:","title":"机器学习推荐论文和书籍"},{"content":"在机器学习(Machine learning)领域，监督学习(Supervised learning)、非监督学习(Unsupervised learning)以及半监督学习(Semi-supervised learning)是三类研究比较多，应用比较广的学习技术，wiki上对这三种学习的简单描述如下： 监督学习：通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如分类。 非监督学习：直接对输入数据集进行建模，例如聚类。 半监督学习：综合利用有类标的数据和没有类标的数据，来生成合适的分类函数。 以上表述是我直接翻译过来的，因为都是一句话，所以说得不是很清楚，下面我用一个例子来具体解释一下。 其实很多机器学习都是在解决类别归属的问题，即给定一些数据，判断每条数据属于哪些类，或者和其他哪些数据属于同一类等等。这样，如果我们上来就对这一堆数据进行某种划分(聚类)，通过数据内在的一些属性和联系，将数据自动整理为某几类，这就属于非监督学习。如果我们一开始就知道了这些数据包含的类别，并且有一部分数据(训练数据)已经标上了类标，我们通过对这些已经标好类标的数据进行归纳总结，得出一个 “数据-->类别” 的映射函数，来对剩余的数据进行分类，这就属于监督学习。而半监督学习指的是在训练数据十分稀少的情况下，通过利用一些没有类标的数据，提高学习准确率的方法。 铺垫了那么多，其实我想说的是，在wiki上对于半监督学习的解释是有一点点歧义的，这跟下面要介绍的主动学习有关。 主动学习(active learning)，指的是这样一种学习方法： 有的时候，有类标的数据比较稀少而没有类标的数据是相当丰富的，但是对数据进行人工标注又非常昂贵，这时候，学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。 这个筛选过程也就是主动学习主要研究的地方了，怎么样筛选数据才能使得请求标注的次数尽量少而最终的结果又尽量好。 主动学习的过程大致是这样的，有一个已经标好类标的数据集K(初始时可能为空)，和还没有标记的数据集U，通过K集合的信息，找出一个U的子集C，提出标注请求，待专家将数据集C标注完成后加入到K集合中，进行下一次迭代。 按wiki上所描述的看，主动学习也属于半监督学习的范畴了，但实际上是不一样的，半监督学习和直推学习(transductive learning)以及主动学习，都属于利用未标记数据的学习技术，但基本思想还是有区别的。 如上所述，主动学习的“主动”，指的是主动提出标注请求，也就是说，还是需要一个外在的能够对其请求进行标注的实体(通常就是相关领域人员)，即主动学习是交互进行的。 而半监督学习，特指的是学习算法不需要人工的干预，基于自身对未标记数据加以利用。 至于直推学习，它与半监督学习一样不需要人工干预，不同的是，直推学习假设未标记的数据就是最终要用来测试的数据，学习的目的就是在这些数据上取得最佳泛化能力。相对应的，半监督学习在学习时并不知道最终的测试用例是什么。 也就是说，直推学习其实类似于半监督学习的一个子问题，或者说是一个特殊化的半监督学习，所以也有人将其归为半监督学习。 而主动学习和半监督学习，其基本思想上就不一样了，所以还是要加以区分的，如果wiki上对半监督学习的解释能特别强调一下“是在不需要人工干预的条件下由算法自行完成对无标记数据的利用”，问题就会更清楚一些了。 http://blog.sina.com.cn/s/blog_627a4f560100xmj1.html","title":"机器学习——监督学习，半监督学习，无监督学习，主动学习"},{"content":"随机搜索：         该方法接受两个参数，Domain和costf，其中Domain是一个由二元组构成的列表，指定了每个变量的最大最小值。而costf胃成本函数，为其他优化问题所重用，它会跟踪最佳猜测（即具有最低成本的题解）并将结果返回。        本质：随机选择出一些题解，并进行尝试，比较得出尝试解中的最佳值，但不一定是问题的全局最优值。   爬山法：以一个随机解开始，然后在其接近的解集中寻找更好的题解。        缺陷：简单的从斜坡滑下不一定会产生全局最优解。有可能最后的解会是一个局部范围内的最小值，它比临近解的表现都好，但却不是全局最优的。        改进方法：随机重复爬山法，即让爬山法以多个随机生成的初始解为起点运行若干次，借此希望其中有一个解能够逼近全局的最小值。   模拟退火算法：以一个问题的随机解开始，多次迭代。每一次迭代会随机选中题解中的某个数字，然后朝某个方向变化。算法的关键部分：如果新的成本值更低，则新的题解就会称为当前题解，这和爬山法非常相似。但是如果成本值更高的话，则新的题解仍将可能成为当前题解——避免局部最小值问题。   遗传算法：先随机生成一组解（称为种群），在优化过程中的每一步，算法会计算整个种群的成本函数，从而得到一个有关题解的有序列表。在对题解进行排序之后，一个新的种群（下一代）被创建出来了。首先，我们将当前种群中位于最顶端的题解加入其所在的新种群中。新种群中的余下部分是由修改最优解后形成的全新解所组成的。修改题解的两种方法： 变异：在一个存在解的基础上进行微小的、简单的、随机的改变； 交叉或者配对：选取最优解中的两个解，然后将他们按某种方式进行结合 文档过滤 过滤垃圾信息：        1.基于规则的分类器：使用事先设计好的一组规则，用以指明某条信息是否属于垃圾信息。典型的规则有：英文大写字母的过度使用、与药品相关的单词、过于花哨的HTML用色等。        缺点：规则的正确性与否受垃圾信息的读者和张贴为止的不同而不同；垃圾制造者知道了规则以后，容易绕开过滤器，其行为变得更加隐蔽。   文档和单词：        利用某些特征来对不同的内容项进行分类。文档分类，特征即为文档中的单词。当将单词作为特征时，其假设为：某些单词相对而言更有可能会出现于垃圾信息中。这一假设是大多数垃圾信息过滤器背后所依赖的基本前提。   分类器训练：        通过读取正确答案的样本进行学习。如果分类器掌握的文档机器正确分类的样本越多，其预测的效果也就越好。分类器作用：从极为不确定的状态开始，随着分类器不断了解到哪些特征对于分类而言更为重要，其确定性也在逐渐地增加。   计算概率： 对文档中的单词在每个分类中出现次数进行了统计，那么接下来的工作就是要将其转换成概率。   朴素分类器：求出了指定单词在一篇属于某个分类的文档中出现的概率，就需要有一种方法将各个单词的概率进行组合，从而得出整篇文档属于该分类的概率。        1.朴素贝叶斯分类器：朴素的意思即将要被组合的各个概率是彼此独立的，即一个单词在属于某个指定分类的文档中出现的概率，与其他单词出现在该分类的概率是不相关的。        使用朴素贝叶斯分类器，首先我们需要确定整篇文档属于给定分类的概率。通过将所有的概率相乘，计算出总的概率值。 即的概率已经算出来，而对文档分类，真正需要的是，即对于一篇给定的文档，它属于某个分类的概率是多少？这个可以借助于贝叶斯公式求得：   构造朴素贝叶斯分类器的最后一步是实际判定某个内容项所属的分类。此处最简单的方法，计算被考查内容在每个不同分类中的概率，然后选择概率最大的分类。 同时，各个分类不能同等看待，而且在一些应用中，对于分类器而言，承认不知道答案，要好过判断答案就是概率值稍大一些的分类。为了解决这个问题，我们可以为每个分类定义一个最小阈值。   2.费舍尔方法：朴素贝叶斯方法的一种替代方案，它可以给出非常精确的结果，尤其适合垃圾信息过滤。该方法为文档中的每个特征都求得了分类的概率，然后又将这些概率组合起来，并判断其是否有可能构成一个随机集合。同时还会返回每个分类的概率，这些概率彼此间可以进行比较。 每个特征的分类概率计算值为： 注意这种计算方式与朴素贝叶斯分类中特征分类的概率计算不同，朴素贝叶斯中的计算公式等价于：。 概率值组合：将所有概率相乘起来，然后取自然对数，再将所得结果乘以（-2）. 计算结果特征：如果概率彼此独立且随机分布，则计算结果将满足对数卡方分布。 通过将菲舍尔方法的计算结果传给倒置对数卡方函数，我们将得到一组随机概率中的最大值。 优点：为分类选择临界值时允许更大的灵活性； 缺点：复杂     对树进行训练：        CART（分类回归树）：首先创建一个根结点，然后通过评估表中的所有观测变量，从中选出最合适的变量对数据进行拆分。   基尼不纯度：        将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。 熵：        集合的无序程度。 区别：熵对于混乱集合的“判罚”往往要更重一些。熵更为普遍。   递归方式构造树：        信息增益：当前熵与两个新群组加权平均后的熵之间的差值。算法会针对每个属性计算相应的信息增益，然后从中选出信息增益最大的属性。算法结束：当熵值最低的一对子集求得的加权平均熵比当前几个的熵要大，针对各种可能结果的计数所得将会被保存下来——拆分结束。   决策树的剪枝：        利用递归的方式来训练决策树存在的问题：决策树变得过度拟合，即它可能变得过于针对训练数据。专门针对训练集所创建出来的分支，其熵值与真实情况相比可能会有所降低，但决策树上的判断条件实际上是完全随意的，因此一颗过度拟合的决策树所给出的答案也许会比实际情况更具特殊性。        措施：        1.当熵减少的数量小雨某个最小值时，我们就停止分支的创建——经常被使用，缺陷是某一次分支的创建并不会令熵降低多少，但是随后创建的分支却会使熵大幅降低；        2.先构造好如前所述的整棵树，然后再尝试消除多余的节点——剪枝，解决措施1出现的问题。 剪枝的过程就是对具有相同父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否会小于某个指定的阈值。如果是，则将这些叶节点合并成一个单一的节点，合并后的新节点包含了所有可能的结果值。 决策树优点：处理缺失数据的能力。如果我们缺失了某些数据，而这些数据是确定分支走向所必需的。那么实际上我们可以选择两个分支都走。不过，此处我们不是平均地统计各分支对应地结果值，而是对其进行加权统计。而如果要走多个分支地话，那么我们可以给每个分支赋予一个权重，其值等于所有位于该分支地其他数据行所占地比重。   处理数值型结果： 对于数值型结果，我们可以使用方差作为评价函数来取代熵或者基尼不纯度。较低地方差代表数字彼此都非常地接近，而偏高地方差则意味着数字分散得很开。当使用方差作为评价函数来构造决策树时，我们选择节点判断条件得就变成了：拆分之后令数字较大者位于树得一侧，而数字较小者位于树得另一侧，这样来降低分支得整体方差。   决策树得使用有一个显著得缺陷：我们必须建立大量得价格数据才行，这是因为住房得价格千差万别，而且为了能够得出有价值得结论，我们需要以某种方式对其进行有效得分组。   利用多种不同属性对数值型数据进行预测时，贝叶斯分类器、决策树，以及支持向量机都不是最佳的方法。   在一个拥有众多买家和卖家的大型市场中，通常对于交易双方而言，商品的价格最终将会达到一个最优值。而对于只有一个卖家和多个买家的情况或者只有一个买家和多个卖家的情况，商品的价格都是一个非最优值。美丽心灵中john nash对于一个美女和多个男子之间的约会结果就是没有一个男子能够约到此美女。","title":"集体编程智慧(2)"},{"content":"       为了对机器学习有一个大体的了解，开始看网易上斯坦福机器学习的公开课，记录一些东西在这里。        机器学习在我们生活中的应用已经很多了，它已经成为了IT界一项很重要的技能，learning algothrim在各种产业中都有广泛的应用。        这门课需要一些高等数学，线性代数，概率统计和数据结构的基础。 什么是机器学习： Arthur Samuel (1959)  Machine Learning : Field of study that gives computers the ability to learn without being explicitly programmed. 非正式的定义：        机器学习就是在不直接针对具体问题编程的情况下赋予计算机学习能力的一个研究领域。 Tom Mitchell (19998)  Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. 更正式的定义：        一个合理的学习问题应该这样定义：对于一个计算机程序来说，给它一个任务T和一个性能测试方法P，如果在经验E的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习。 该课程主要分为四个部分：        第一部分是 Supervised Learning (监督学习) ，有regression (回归问题)和classification(分类问题)。监督学习是通过一些确定结果的数据来学习。        第二部分是 Learning Theory (学习理论)，有关机器学习的一些理论知识。        第三部分是 Unsupervised Learning (无监督学习)，主要有clustering(聚类问题)。无监督学习是通过一些未知结果的数据来进行学习。        第四部分是 Reinforcement Learning (强化学习)，核心reward function(回报函数)。通过结果来不断修正算法。 结束","title":"Stanford Machine Learning - Lecture 01"},{"content":"写在最前： 本专题经 @老师木 同意， 特收录“老湿”对AI/ML的一些独到见解。如果非要问我为什么要特别收录这几篇文章，回答：个人认为，他的大部分见解已经并肩甚至超过了该领域的一般教授。如果你再八卦一下问这个专题为什么叫“褪去华衣 裸视学习”，答曰：这些见解一定程度上褪去了AI/ML的神秘色彩，可以让我们更客观的看待这一学科。 专题分为六个篇章： 1）机器学习 基础篇 褪去华衣 裸视学习 之 高斯分布 褪去华衣 裸视学习 之 sigmod函数 褪去华衣 裸视学习 之 关于‘基’ 褪去华衣 裸视学习 之 随机数学 褪去华衣 裸视学习 之 小赞deep learning 2）机器学习 进阶篇 褪去华衣 裸视学习 之 有监督学习 褪去华衣 裸视学习 之 线性分类器 褪去华衣 裸视学习 之 SVM神话 褪去华衣 裸视学习 之 无监督学习 褪去华衣 裸视学习 之 概率图 3）机器学习 实战篇 褪去华衣 裸视学习 之 人机对话小思路 褪去华衣 裸视学习 之 Marr 视觉计算理论 褪去华衣 裸视学习 之 小议无监督分词 褪去华衣 裸视学习 之 博弈论与广告 4）机器学习 反思篇 褪去华衣 裸视学习 之 《机器学习那点事儿》解读 褪去华衣 裸视学习 之 李国杰院士大数据一文的不同意见 褪去华衣 裸视学习 之 机器学习无用论 褪去华衣 裸视学习 之 机器学习有点用 5）机器学习 方法论 褪去华衣 裸视学习 之 规则与统计 褪去华衣 裸视学习 之 优质数据 褪去华衣 裸视学习 之 跨界的机器学习 褪去华衣 裸视学习 之 概率与统计区别 6）机器学习 番外篇 褪去华衣 裸视学习 之 产品设计 褪去华衣 裸视学习 之 学术界 褪去华衣 裸视学习 之 如何学习 褪去华衣 裸视学习 之 机器学习教材","title":"机器学习 探讨系列 绝对干货"},{"content":"1. 机器学习定义 机器学习（Arthur Samuel，1959）：在确定编程之外给予计算机学习能力的研究领域。 机器学习（Tom Mitchell，1998）：如果计算机程序对于任务T的性能度量P通过经验E得到了提高，则认为此程序对E进行了学习。 2. 机器学习四个主要内容 -监督学习：包括回归（连续性问题）、分类（离散性问题）。需要训练集，训练集给出自变量和因变量（即标签），通过训练集构造模型；之后，对于新的自变量，采用学习到的模型预测因变量的值。 -学习理论（Learning Theory）：证明机器学习算法的有效性。 -无监督学习（UnSupervised Learning）：无训练集。将所有的数据根据某些属性进行聚类，之后对聚类结果进行分析，获取知识。 -强化学习（Reinforcement Learning）：关键是引入了回报函数（reward function）的概念。通过回报函数不断修正学习算法，以使算法更有效。 参考资料： [1] Andrew Ng. Stanford CS229. Machine Learning. Stanford University","title":"Stanford机器学习系列之一：机器学习基本概念"},{"content":"========================================================================== 上周生病再加上课余的一些琐事，这边的进度就慢下来了，本篇笔记基于 斯坦福大学公开课cs229 的 lecture16，lecture 17 ========================================================================== 零：一些认识 涉及到机器人的操控的时候，很多事情可能并不是supervised和unsupervised learning能够解决的，比如说andrew ng之前一直提到的自动控制直升飞机，另一个例子就是下棋，有可能很久之前的一步棋就埋下了后面失败的伏笔，而机器很难去判断一步棋的好坏。这就是增强学习需要解决的问题。 注:这里的Value价值即是很多书上写的Q值，貌似也有点差别，在于Q可能是Q(s,a)的，是给定状态和一个动作之后的V值，但差异不大。 一：马尔科夫决策过程 （Markov decision processes） 马尔科夫决策是一个五元组，，用一个机器人走地图的例子来说明它们各自的作用 S：状态集：就是所有可能出现的状态，在机器人走地图的例子中就是所有机器人可能出现的位置 A：action，也就是所有可能的行动。机器人走地图的例子假设机器人只能朝四个方向走，那么A就是{N，S，E，W}表示四个方向 P：就是机器人在S状态时采取a行动的概率 γ：叫做discount factor，是一个0到1之间的数，这个数决定了动作先后对于结果的影响度，在棋盘上的例子来说就是影响了这一步   棋对于最结果的影响有多大可能说起来比较模糊，通过后面的说明可能会讲得比较清楚。 R：是一个reward function，也就是可能是一个，也可能是，对应来说就是地图上的权值          =============================================================================== 有了这样一个决策过程，那么机器人在地图上活动的过程也可以表现为如下的形式： 也就是从初始位置开始，选择一个action到达另一个状态，直到到达终状态，因此我们这样来定义这个过程的价值： 可以看出越早的决定对价值影响越大，其后则依次因为γ而衰减 其实可以看出，给出一个MDP之后，因为各个元都是定值，所以存在一个最优的策略(ploicy)，策略即是对于每个状态给出一个action，最优 策略就是在这样的策略下从任意一个初始状态能够以最大的价值到达终状态。策略用π表示。用 表示在策略π下以s为初始状态所能取得的价值，而通过Bellman equation，上式又等于： 注意这是一个递归的过程，在知道s的价值函数之前必去知道所有的s'的价值函数。(价值函数指的是Vπ()) 而我们定义最优的策略为π*，最优的价值函数为V*，可以发现这两个东西互为因果，都能互相转化。 二.价值迭代和策略迭代(Value iteration & policy iteration)    ///////////////价值迭代VI：////////////////////    这个过程其实比较简单，因为我们知道R的值，所以通过不断更新V，最后V就是converge到V*，再通过V*就可以得到最优策略π*，通    过V*就可以得到最优策略π*其实就是看所有action中哪个action最后的value值最大即可，此处是通过bellman equation，可以通过解bellman equation得到    所有的V的值，这里有一个动归的方法，注意马尔科夫决策过程中的P其实是指客观存在的概率，比如机器人转弯可能没法精确到一个方向，而不是指在s状态    机器人选择a操作   的概率，刚才没说清楚    在此说明，也就是说：    是一个客观的统计量。     /////////////策略迭代PI/////////////////////       这次就是通过每次最优化π来使π converge到π*，V到V*。但因为每次都要计算π的value值，所以这种算法并不常用    这两个算法的区别就是过程的区别，但我感觉本质上差别不大。(andrew说有不一样，至少看起来不一样……这个待查) 三.连续状态的MDP 之前我们的状态都是离散的，如果状态是连续的，下面将用一个例子来予以说明，这个例子就是inverted pendulum问题 也就是一个铁轨小车上有一个长杆，要用计算机来让它保持平衡(其实就是我们平时玩杆子，放在手上让它一直保持竖直状态) 这个问题需要的状态有：都是real的值 x(在铁轨上的位置) theta(杆的角度) x’(铁轨上的速度) thata'(角速度)   /////////////////离散化///////////////////////////   也就是把连续的值分成多个区间，这是很自然的一个想法，比如一个二维的连续区间可以分成如下的离散值： 但是这样做的效果并不好，因为用一个离散的去表示连续空间毕竟是有限的离散值。 离散值不好的另一个原因是因为curse of dimension(维度诅咒)，因为连续值离散值后会有多个离散值，这样如果维度很大就会造成有非常多状态 从而使需要更多计算，这是随着dimension以指数增长的 //////////////////////simulator方法/////////////////////////////// 也就是说假设我们有一个simulator，输入一个状态s和一个操作a可以输出下一个状态，并且下一个状态是服从MDP中的概率Psa的分布，即： 这样我们就把状态变成连续的了，但是如何得到这样一个simulator呢？ ①：根据客观事实 比如说上面的inverted pendulum问题，action就是作用在小车上的水平力，根据物理上的知识，完全可以解出这个加速度对状态的影响 也就是算出该力对于小车的水平加速度和杆的角加速度，再去一个比较小的时间间隔，就可以得到S(t+1)了 ②：学习一个simulator 这个部分，首先你可以自己尝试控制小车，得到一系列的数据，假设力是线性的或者非线性的，将S(t+1)看作关于S(t)和a(t)的一个函数 得到这些数据之后，你可以通过一个supervised learning来得到这个函数，其实就是得到了simulator了。 比如我们假设这是一个线性的函数： 在inverted pendulum问题中，A就是一个4*4的矩阵，B就是一个4维向量，再加上一点噪音，就变成了：其中噪音服从 我们的任务就是要学习到A和B (这里只是假设线性的，更具体的，如果我们假设是非线性的，比如说加一个feature是速度和角速度的乘积，或者平方，或者其他，上式还可以写作：) 这样就是非线性的了，我们的任务就是得到A和B，用一个supervised learning分别拟合每个参数就可以了 四.连续状态中得Value(Q)函数 这里介绍了一个fitted value(Q) iteration的算法 在之前我们的value iteration算法中，我们有： 这里使用了期望的定义而转化。fitted value(Q) iteration算法的主要思想就是用一个参数去逼近右边的这个式子 也就是说：令 其中是一些基于s的参数，我们需要去得到系数的值，先给出算法步骤再一步步解释吧： 算法步骤其实很简单，最主要的其实就是他的思想： 在对于action的那个循环里，我们尝试得到这个action所对应的，记作q(a) 这里的q(a)都是对应第i个样例的情况 然后i=1……m的那个循环是得到是最优的action对应的Value值，记作y(i)，然后用y(i)拿去做supervised learning，大概就是这样一个思路 至于reward函数就比较简单了，比如说在inverted pendulum问题中，杆子比较直立就是给高reward，这个可以很直观地从状态得到衡量奖励的方法 在有了之上的东西之后，我们就可以去算我们的policy了： 五.确定性的模型 上面讲的连续状态的算法其实是针对一个非确定性的模型，即一个动作可能到达多个状态，有P在影响到达哪个状态 如果在一个确定性模型中，其实是一个简化的问题，得到的样例简化了，计算也简化了 也就是说一个对于一个状态和一个动作，只能到达另一个状态，而不是多个，特例就不细讲了","title":"机器学习 cs229学习笔记6(增强学习 reinforcement learning,MDP)"},{"content":"(all is based on the stanford's open-course CS229 lecture 15) 因为对ICA理解得不是很深刻 在网上东搜西搜搜到一个牛人的bloghttp://www.cnblogs.com/jerrylead/ 他也写了cs229的学习笔记系列，和我的一比较顿时我就自惭形秽了，不过既然都差不多快写完了，我也就继续坚持着吧 毕竟我的本意是给自己留个笔记，不过我也把他写的笔记download下来加深理解，学习了 ========================================================================== 零.知识准备 概率密度函数和累积分布函数(cumulative distribution function, 缩写为 cdf)---这是概率论的基础知识了吧 其中概率密度函数的变换 也就是说如果我们有一个概率密度函数，如果有，怎样求 公式如下：其中|W|时矩阵W的行列式，W时A的逆矩阵 一.引例     这个问题的引例其实就是在cs229的第一节课上讲到的鸡尾酒问题     即两个人在酒会上同时讲话，而在房间不同位置的两个麦克风记录了他们的声音，要把两个声音分离出来     andrew ng当时给了一个svd的公式，但好像和lecture 15的关系不大     其实刚进大一的时候我就很无知地想过用函数去拟合声音，但是后来觉得声音是混杂的，所以也没继续想，想在想来真是===无知者无畏啊     这样的问题不光出现在声音上，也出现在脑电波识别(meg)和图像处理(cv)等领域 二.思路 如果我们拿到一个混杂的声音，其实很难识别出其独立的部分（鸡尾酒问题中的两个独立部分就是两个人分别说话的声音） 而鸡尾酒问题中要两个麦克风的原因也在于此，就像人的双耳效应一样，两个麦克风记录的声音，每个独立部分的大小是不一样的 ica就是通过其中的差异来分离出其独立部分 我们知道矩阵其实是一种变换，而声音的重叠其实就是一种变换（考虑声音的在计算机中的储存方式，是以时间为轴的图像，图像上的高度对应声音的数据），         所以假设我们有两个源声音s1,s2,声音的叠加就是关于s1和s2的线性变换，用一个矩阵A表示这种变换，在鸡尾酒问题中，A是2*2的 所以，也就是我们麦克风记录到的两个混杂的声音         ica就是通过找到A的逆矩阵从而还原源声音s的 同时，这种变换对于高斯分布的数据是无效的，因为变换在旋转时会造成歧义(貌似是这样，再看看) 三.算法       机器学习的东西很难不和概率扯上关系，所以照旧，我们的最大似然函数就是         而通过上面的讲解， 可以推导出： 在此我们需要给出ps的定义，ps是数据的概率密度函数，而我们需要一个非高斯分布的概率密度函数，因此我们需要一个非高斯的概率分布函数 其实可以任意选择，只要符合概率分布函数的性质，这里我们选择了sigmoid函数作为概率分布函数，即：         这样log似然函数就变成了 其中m是训练样例数，n是参数数，对应带鸡尾酒问题中，m是离散的声音点的个数，n是麦克风(人)的个数 然后再通最大化似然函数我们就可以得到随机梯度上升算法的迭代式了（注意这里如果用batch gradient ascent没什么效果） 四.算法的不确定性（ambiguities） 1.源数据的顺序 ica可以分理出不同的独立成分，但是无法指出哪一个独立成分对应一个源数据，这是很好理解的 因为变换矩阵A可能会是一个permutation matrix，也就是说矩阵行于行之间交换顺序这不会影响矩阵的各种性质 幸运的是，这对大多数应用没有影响  2.还原数据的scaling 因为scaling是一维的线性变换，也就是说乘除一个常数，所以因为，我们只知道x，所以不能得到具体的A和s的scaling 这在声音问题上的影响就是声音的大小，也是可以忽略的 五.代码 学习要用代码来加强，找了一些文章，发现基本上都是用fastICA的版本，是一个类似于神经网络的方法，略高端   最后给出一个基于最大似然的梯度上升算法的代码以及实验数据(matlab) [n,m] = size(MixedS);chunk = 10;alpha = 1.5;W = eye(n);sss=MixedS;for iter=1:40\tdisp([num2str(iter)]);\tMixedS= MixedS(:,randperm(m));\tfor i=1:floor(m/chunk)\t\tXc = MixedS(:,(i-1)*chunk+1:i*chunk);\t\tdW = (1 - 2./(1+exp(-W*Xc)))*Xc'+ chunk*inv(W');\t\tW = W + alpha*dW;\tendend;       实验效果（实验用的音频来自网上一个fastICA示范的数据，我就直接拿过来用了） 可以发现较好地还原了源数据，同时也体现了无法得到具体的scaling这个不确定性，音频强度明显减弱了，但是这对实验效果没什么影响 ==========================================================================","title":"机器学习 cs229学习笔记5 (ICA Independent components analysis)"},{"content":"（all is based on the stanford open course cs229.Lecture 11.） regularization（正规化）防止过拟合:保留所有的参数 贝叶斯学习方法：增加先验概率，常用为高斯分布N(0,λ)，使算法更倾向于选择更小的参数 体现在最大似然估计上就是增加惩罚项目，||Θ||^2 //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 在线学习：online learning ,与batch learning相差较大，具体可见coursera里面第八节的内容，讲得比较清楚 //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 接下来讲的是如果算法不起作用，如何分析改进一个算法 首先是bias和variance的分析，这在笔记六里讲得很清楚了 cs229和coursera上不同的是是增加了叫做“fix  optimization algorithm “和”fix optimization objective”的分析 采用的方法有： 1)多做几次迭代，看效果是否有改进 fix optimization algorithm 2)尝试其他算法进行converge fix optimization algorithm 3)改变一些常量参数 fix optimization objective 4)尝试使用其他机器学习算法 fix optimization algorithm 而对于特定问题，我们必须自己去构造分析方法，andrew ng就举了一个关于自动驾驶直升飞机的例子 能正确对算法进行分析，才能避免无谓地浪费时间 //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 接下来是针对一个系统的分析，Error Analysis 一个系统常常由多个机器学习算法组成的，所以如果系统工作不顺利，必须要分析出是哪个部分出问题了 Error Analysis就是对每一个部件，依次用基准值(ground-truth)替换每个部件的输出 看看整体系统的效果改变是否较大，如果较大就说明系统精度的瓶颈就在刚刚替换了的这个部件这里 这在coursera上的课程上也有说明，但我好像没有作笔记 //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 再次就是ablative analysis（销蚀分析） 和上面不同的的是销蚀分析是在最后结果很高的情况下，remove一个组件的输出，来看每个部件对最后结果的影响 视频截图，就看个轮廓吧，99.9%是整个系统的精度，依次remove左边的一个组件的基准值 看看哪个组建值得优化，值得注意的是“依次” //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 最后一点就是the danger of over-theorizing （视频截图），也就是说，不要太过理论化，否则你的系统会变得越来越复杂，以至于无法实现 这就和有一个寓言一样：为了钉一幅画，没有锤子，去找锤子。做锤子要木头，就去伐木 伐木要伐木机，就去找伐木机，伐木机没油………… //////////////////////////////////////////////////////////////////////////////////////////////////////////////","title":"机器学习 cs229学习笔记1"},{"content":"基于统计信息 基于DOM的网页主题信息自动提取 http://www.ccf.org.cn/resources/1190201776262/2010/05/12/h049617026.pdf 总结：与主题无关的块总是含有大量的无关链接和极少非链接文字   [PDF] 使用特征文本密度的网页正文提取 http://www.cqvip.com/qk/95939x/201003/32891243.html 总结：与主题无关的块总是含有大量的“无关词“，如“版权“，”声明“，“搜索“，”首页“，”帮助“。可以计算无关词和总文本的比例。   基于标签密度的自适应正文提取方法 http://wenku.baidu.com/view/773479eb998fcc22bcd10d12.html 总结：除了通过超链接标签，还可以通过其他标签密度来确定正文。但个人认为这种方式可靠性不高。  基于视觉 http://hi.baidu.com/gghgdk/item/9d5d5e0945e3fe96a2df4308 总结：正文节点在网页的位置总是在“中间”的，以及和其中图像元素的数量也有关联。 基于决策树 基于双决策的新闻网页正文精确抽取 http://file.lw23.com/4/4f/4fa/4fa9ed31-f1fa-42c1-abea-51d9f143b4a9.pdf 总结：人类识别正文段通过两个步骤：1.大概判断正文范围。2判读该正文范围内的段落是否属于正文部分。因此，机器识别可以通过全局和局部两个方面进行决策。 想法：对于决策树(暂时不理解其工作方式，求相关书目)的训练数据，可以通过这种方式获得。制作一个浏览器插件，类似于firebug或clipper的节点选择，可以选择页面的DOM元素，通过手工选取正文节点，该插件将数据传回服务器。通过这种方式将url和人工确定的正文节点对应，形成大量的训练数据。    基于包装器 通过为特定站点建立特定的包装器，即特定的正文节点获取模式，可以准确判断特定站点的正文节点。确定是需要手工确定站点。可以借助在”基于决策树“小节提到的训练数据获取方式来简化包装器的构建。 通俗来讲，就是为正文提取建立黑名单和白名单。   对当前某些插件的理解 研究了clearly的源码。源码的获取详见http://blog.csdn.net/cattail2012/article/details/8168025。 从文件js/bulk.js的4320行起，描述的是该插件如何进行网页净化的。我称之为网页净化，因为clearly做的是这样一件事情：它从body 节点开始，对文档所有节点进行遍历处理，处理依据4419行的$R.parsingOptions，对不同的节点进行不同处理，如保留该节点或者删除该节 点，对节点的属性也进行删除或者修改，通过这种方式净化了页面元素。也就是说，clearly并没有做寻找正文节点这个工作，以此推测，对于 readability或pocket等插件，它们也都没有做提取正文节点的工作。而且对于它们的需求，也没有必要进行正文节点的获取。虽然这些插件没有进行正文提取，但是对于非正文节点的删除这个思想，可以使用在正文节点提取的算法中。   可行性分析 理论上，基于统计信息和视觉信息可以创建出可行的正文提取方案。   相关文献(未读) Machine Learning for Information Extraction in Informal Domains http://reports-archive.adm.cs.cmu.edu/anon/1999/CMU-CS-99-104.pdf [PDF]Fact or fiction: Content classification for digital libraries - Ercim http://www.ercim.eu/publication/ws-proceedings/DelNoe02/AidanFinn.pdf Two Approaches to Bringing Internet Services to WAP Devices http://www9.org/w9cdrom/228/228.html Seeing the Whole in Parts: Text Summarization forWeb Browsing on Handheld Devices http://ilpubs.stanford.edu:8090/511/1/2001-45.pdf","title":"正文提取"},{"content":"转载自http://leftnoteasy.cnblogs.com 前言:    上次写过一篇关于贝叶斯概率论的数学，最近时间比较紧，coding的任务比较重，不过还是抽空看了一些机器学习的书和视频，其中很推荐两个：一个是stanford的machine learning公开课，在verycd可下载，可惜没有翻译。不过还是可以看。另外一个是prml-pattern recognition and machine learning, Bishop的一部反响不错的书，而且是2008年的，算是比较新的一本书了。    前几天还准备写一个分布式计算的系列，只写了个开头，又换到写这个系列了。以后看哪边的心得更多，就写哪一个系列吧。最近干的事情比较杂，有跟机器学习相关的，有跟数学相关的，也有跟分布式相关的。    这个系列主要想能够用数学去描述机器学习，想要学好机器学习，首先得去理解其中的数学意义，不一定要到能够轻松自如的推导中间的公式，不过至少得认识这些式子吧，不然看一些相关的论文可就看不懂了，这个系列主要将会着重于去机器学习的数学描述这个部分，将会覆盖但不一定局限于回归、聚类、分类等算法。 回归与梯度下降：    回归在数学上来说是给定一个点集，能够用一条曲线去拟合之，如果这个曲线是一条直线，那就被称为线性回归，如果曲线是一条二次曲线，就被称为二次回归，回归还有很多的变种，如locally weighted回归，logistic回归，等等，这个将在后面去讲。    用一个很简单的例子来说明回归，这个例子来自很多的地方，也在很多的open source的软件中看到，比如说weka。大概就是，做一个房屋价值的评估系统，一个房屋的价值来自很多地方，比如说面积、房间的数量（几室几厅）、地段、朝向等等，这些影响房屋价值的变量被称为特征(feature)，feature在机器学习中是一个很重要的概念，有很多的论文专门探讨这个东西。在此处，为了简单，假设我们的房屋就是一个变量影响的，就是房屋的面积。    假设有一个房屋销售的数据如下：    面积(m^2)  销售价钱（万元）    123            250    150            320    87              160    102            220    …               …    这个表类似于帝都5环左右的房屋价钱，我们可以做出一个图，x轴是房屋的面积。y轴是房屋的售价，如下：        如果来了一个新的面积，假设在销售价钱的记录中没有的，我们怎么办呢？    我们可以用一条曲线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以在将曲线上这个点对应的值返回。如果用一条直线去拟合，可能是下面的样子：         绿色的点就是我们想要预测的点。    首先给出一些概念和常用的符号，在不同的机器学习书籍中可能有一定的差别。    房屋销售记录表 - 训练集(training set)或者训练数据(training data), 是我们流程中的输入数据，一般称为x    房屋销售价钱 - 输出数据，一般称为y    拟合的函数（或者称为假设或者模型），一般写做 y = h(x)    训练数据的条目数(#training set), 一条训练数据是由一对输入数据和输出数据组成的    输入数据的维度(特征的个数，#features)，n    下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。就如同上面的线性回归函数。           我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向，等等，我们可以做出一个估计函数：     θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了：     我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个函数称为损失函数（loss function）或者错误函数(error function)，描述h函数不好的程度，在下面，我们称这个函数为J函数     在这儿我们可以做出下面的一个错误函数：       这个错误估计函数是去对x(i)的估计值与真实值y(i)差的平方和作为错误估计函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。     如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，在stanford机器学习开放课最后的部分会推导最小二乘法的公式的来源，这个来很多的机器学习和数学书上都可以找到，这里就不提最小二乘法，而谈谈梯度下降法。     梯度下降法是按下面的流程进行的：     1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。     2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。     为了更清楚，给出下面的图：     这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低。也就是深蓝色的部分。θ0，θ1表示θ向量的两个维度。     在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。     然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如图所示，算法的结束将是在θ下降到无法继续下降为止。      当然，可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，可能是下面的情况：    上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点      下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：（求导的过程如果不明白，可以温习一下微积分）        下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。      一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。     用更简单的数学语言进行描述步骤2）是这样的：       倒三角形表示梯度，按这种方式来表示，θi就不见了，看看用好向量和矩阵，真的会大大的简化数学的描述啊。 总结与预告：     本文中的内容主要取自stanford的课程第二集，希望我把意思表达清楚了：）本系列的下一篇文章也将会取自stanford课程的第三集，下一次将会深入的讲讲回归、logistic回归、和Newton法，不过本系列并不希望做成stanford课程的笔记版，再往后面就不一定完全与stanford课程保持一致了。","title":"机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)"},{"content":"今天在网上找到转载的《机器学习推荐论文与书籍》，看起来不错，无出处。搜索得知为水木社区某神童编写，可惜找不到原文链接。所以这里把里面的东西整理一下，收集打包至网盘（没有包含的标上了“缺”字），方便爱好研究的朋友~ （因本人才疏学浅，如有查找错误，还请见谅……） 基本模型 HMM (Hidden Markov Models，隐含马尔可夫模型) [1] A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition ME (Maximum Entropy，最大熵) [2] A Maximum Entropy Approach to Natural Language Processing MEMM (Maximum Entropy Markov Models，最大熵马尔可夫模型) [3]  Maximum Entropy Markov Models for Information Extraction and Segmentation CRF (Conditional Random Fields，条件随机场) [4] An Introduction to Conditional Random Fields for Relational Learning [5] Conditional Random Fields – Probabilistic Models for Segmenting and Labeling Sequence Data SVM (Support Vector Machine，支持向量机) [6] 统计学习理论 (张学工) # 缺 LSA/LSI (Latent Sematic Analysis / Indexing) [7] An Introduction to Latent Semantic Analysis pLSA/pLSI (Probablistic Latent Sematic Analysis / Indexing) [8] Probabilistic Latent Semantic Analysis LDA(Latent Dirichlet Allocation) [9] Latent Dirichlet Allocation # 用变分理论和最大化期望算法求解模型 [10] Parameter estimation for text analysis # 用吉布斯采样求解模型 Neural Networks (神经网络，包括霍普菲尔模型、自组织映射、随机网络、玻尔兹曼机等) [11] Neural Networks – A Systematic Introduction Diffusion Networks (扩散网络) [12] Diffusion Networks, Products of Experts, and Factor Analysis Markov Random Fields (马尔可夫随机场) Generalized Linear Model (广义线性模型，包括逻辑回归等) [13] An introduction to Generalized Linear Models (2nd) # 有第三版，不过我没寻到 Chinese Restaurant Model (中餐馆模型？，狄利克雷过程) [14] Dirichlet Processes, Chinese Restaurant Processes and all that [15] Estimating a Dirichlet Distribution 一些重要算法 EM (Expectation Maximization，期望值最大化) [16] Expectation Maximization and Posterior Constraints [17] Maximum Likelihood from Incomplete Data via the EM Algorithm MCMC & Gibbs Sampling (马尔可夫链蒙特卡罗算法与吉布斯采样) [18] Markov Chain Monte Carlo and Gibbs Sampling [19] Explaining the Gibbs Sampler [20] An Introduction to MCMC for Machine Learning PageRank 矩阵分解算法 SVD、QR 分解、Shur 分解、LU 分解、谱分解 Boosting (包括 Adaboost) [21] adaboost_talk Spectral Clustering (谱聚类) [22] A Tutorial on Spectral Clustering Energy-Based Learning [23] A Tutorial on Energy-Based Learning Belief Propagation (置信传播) [24] Understanding Belief Propagation and its Generalizations [25] Construction free energy approximation and generalized belief propagation algorithms [26] Loopy Belief Propagation for Approximate Inference An Empirical Study [27] Loopy Belief Propagation AP (Affinity Propagation，亲缘传播) [28] Affinity Propagation L-BFGS [29] 最优化理论与算法(第二版) 第十章 # 缺 [30] On the Limited Memory BFGS Method for Large Scale Optimization IIS (Improved Iterative Scaling，改进迭代算法) [31] Improved Iterative Scaling Algorithm – Parameter Estimation of Feature-based Model 理论部分 Probabilistic Networks (概率网络) [32] An Introduction to Variational Methods [33] Factor Graphs and the Sum-Product Algorithm [34] Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms [35] Graphical Models, Exponential Families, and Variational Inference Variational Theory (变分理论) # 我们只用概率网络上的变分 [36] Tutorial on variational approximation methods [37] A Variational Bayesian Framework for Graphical Models [38] Tutorial on variational approximation methods (ppt) # 原文为 variational tutorial.pdf，未寻到 Information Theory (信息论) [39] Elements of Information Theory (2nd) 测度论 [40] 测度论(Halmos) # 缺，据说写得好但是有点过时了 [41] 测度论讲义(严加安) 概率论 [42] 概率与测度论 # 缺 随机过程 [43] 应用随机过程(林元烈) # 缺 [44] 随机数学引论(林元烈) # 缺 Matrix Theory (矩阵论) [45] 矩阵分析与应用(张贤达) # 缺 模式识别 [46] 模式识别 (2nd)(边肇祺) # 缺 [47] Pattern Recognition and Machine Learning 最优化理论 [48] Convex Optimization [49] 最优化理论与算法(陈宝林) 泛函分析 [50] 泛函分析导论及应用 核方法 [51] 模式分析的核方法 # 缺 统计学 [52] 统计手册 # 缺 综合部分 Semi-Supervised Learning (半监督学习) [53] Semi-Supervised Learning (MIT Press) # 缺 [54] Graph-Based Semi-Supervised Learning Co-Training (协同训练) Self-Training (自我训练) 网盘打包下载：http://115.com/file/dpu3ribw#机器学习论文与书籍推荐.7z 感谢书单原作者的无私奉献。 Posted in 机器学习.","title":"机器学习论文与书籍推荐"},{"content":"User CF 和 Item CF 都依赖于相似度的计算，因为只有通过衡量用户之间或物品之间的相似度，才能找到用户的“邻居”，才能完成推荐。上文简单的介绍了相似性的计算，但不完全，下面就对常用的相似度计算方法进行详细的介绍： 1. 基于皮尔森相关性的相似度 —— Pearson correlation-based similarity 皮尔森相关系数反应了两个变量之间的线性相关程度，它的取值在[-1, 1]之间。当两个变量的线性关系增强时，相关系数趋于1或-1；当一个变量增大，另一个变量也增大时，表明它们之间是正相关的，相关系数大于0；如果一个变量增大，另一个变量却减小，表明它们之间是负相关的，相关系数小于0；如果相关系数等于0，表明它们之间不存在线性相关关系。 用数学公式表示，皮尔森相关系数等于两个变量的协方差除于两个变量的标准差。 协方差（Covariance）：在概率论和统计学中用于衡量两个变量的总体误差。如果两个变量的变化趋于一致，也就是说如果其中一个大于自身的期望值，另一个也大于自身的期望值，那么两个变量之间的协方差就是正值；如果两个变量的变化趋势相反，则协方差为负值。 其中u表示X的期望E(X), v表示Y的期望E(Y) 标准差（Standard Deviation）：标准差是方差的平方根 方差(Variance)：在概率论和统计学中，一个随机变量的方差表述的是它的离散程度，也就是该变量与期望值的距离 即方差等于误差的平方和的期望 基于皮尔森相关系数的相似度有两个缺点： (1) 没有考虑（take into account）用户间重叠的评分项数量对相似度的影响； (2) 如果两个用户之间只有一个共同的评分项，相似度也不能被计算 上表中，行表示用户（1～5）对项目（101～103）的一些评分值。直观来看，User1和User5用3个共同的评分项，并且给出的评分走差也不大，按理他们之间的相似度应该比User1和User4之间的相似度要高，可是User1和User4有一个更高的相似度1。 同样的场景在现实生活中也经常发生，比如两个用户共同观看了200部电影，虽然不一定给出相同或完全相近的评分，他们之间的相似度也应该比另一位只观看了2部相同电影的相似度高吧！但事实并不如此，如果对这两部电影，两个用户给出的相似度相同或很相近，通过皮尔森相关性计算出的相似度会明显大于观看了相同的200部电影的用户之间的相似度。 Mahout对基于皮尔森相关系数的相似度给出了实现，它依赖一个DataModel作为输入。 同时，Mahout还针对缺点(1)进行了优化，只需要在构造PearsonCorrelationSimilarity时多传入一个Weighting.WEIGHTED参数，就能使有更多相同评分项目的用户之间的相似度更趋近于1或-1。 UserSimilarity similarity1 = new PearsonCorrelationSimilarity(model);double value1 = similarity1.userSimilarity(1, 5);UserSimilarity similarity2 = new PearsonCorrelationSimilarity(model, Weighting.WEIGHTED);double value2 = similarity2.userSimilarity(1, 5); 结果： Similarity of User1 and User5: 0.944911182523068 Similarity of User1 and User5 with weighting: 0.9655694890769175 2. 基于欧几里德距离的相似度 —— Euclidean Distance-based Similarity 欧几里德距离计算相似度是所有相似度计算里面最简单、最易理解的方法。它以经过人们一致评价的物品为坐标轴，然后将参与评价的人绘制到坐标系上，并计算他们彼此之间的直线距离。 图中用户A和用户B分别对项目X、Y进行了评分。用户A对项目X的评分为1.8，对项目Y的评分为4，表示到坐标系中为坐标点A(1.8, 4)；同样用户B对项目X、Y的评分表示为坐标点B(4.5, 2.5)，因此他们之间的欧几里德距离（直线距离）为：sqrt((B.x - A.x)^2 + (A.y - B.y)^2) 计算出来的欧几里德距离是一个大于0的数，为了使其更能体现用户之间的相似度，可以把它规约到(0, 1]之间，具体做法为：1 / (1 + d)。参见上表 只要至少有一个共同评分项，就能用欧几里德距离计算相似度；如果没有共同评分项，那么欧几里德距离也就失去了作用。其实照常理理解，如果没有共同评分项，那么意味着这两个用户或物品根本不相似。 3. 余弦相似度 —— Cosine Similarity 余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。 与欧几里德距离类似，基于余弦相似度的计算方法也是把用户的喜好作为n-维坐标系中的一个点，通过连接这个点与坐标系的原点构成一条直线（向量），两个用户之间的相似度值就是两条直线（向量）间夹角的余弦值。因为连接代表用户评分的点与原点的直线都会相交于原点，夹角越小代表两个用户越相似，夹角越大代表两个用户的相似度越小。同时在三角系数中，角的余弦值是在[-1, 1]之间的，0度角的余弦值是1，180角的余弦值是-1。 借助三维坐标系来看下欧氏距离和余弦相似度的区别： 从图上可以看出距离度量衡量的是空间各点间的绝对距离，跟各个点所在的位置坐标（即个体特征维度的数值）直接相关；而余弦相似度衡量的是空间向量的夹角，更加的是体现在方向上的差异，而不是位置。如果保持A点的位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦相似度cosθ是保持不变的，因为夹角不变，而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦相似度的不同之处。 根据欧氏距离和余弦相似度各自的计算方式和衡量特征，分别适用于不同的数据分析模型：欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异；而余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦相似度对绝对数值不敏感）。 Mahout没有专门给出基于余弦相似度的实现。 4. 调整余弦相似度 —— Adjusted Cosine Similarity 在余弦相似度的介绍中说到：余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感。因此没法衡量每个维数值的差异，会导致这样一个情况：比如用户对内容评分，5分制，X和Y两个用户对两个内容的评分分别为(1,2)和(4,5)，使用余弦相似度得出的结果是0.98，两者极为相似，但从评分上看X似乎不喜欢这2个内容，而Y比较喜欢，余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性，就出现了调整余弦相似度，即所有维度上的数值都减去一个均值，比如X和Y的评分均值都是3，那么调整后为(-2,-1)和(1,2)，再用余弦相似度计算，得到-0.8，相似度为负值并且差异不小，但显然更加符合现实。 5. 斯皮尔曼相关 —— Spearman Correlation 斯皮尔曼相关性可以理解为是排列后（Rank）用户喜好值之间的Pearson相关度。《Mahout in Action》中有这样的解释：假设对于每个用户，我们找到他最不喜欢的物品，重写他的评分值为“1”；然后找到下一个最不喜欢的物品，重写评分值为“2”，依此类推。然后我们对这些转换后的值求Pearson相关系数，这就是Spearman相关系数。 斯皮尔曼相关度的计算舍弃了一些重要信息，即真实的评分值。但它保留了用户喜好值的本质特性——排序（ordering），它是建立在排序（或等级，Rank）的基础上计算的。 回顾前面表中User1～5对Item101～103的喜好（评分）值，通过斯皮尔曼相关系数计算出的相似度为： 我们发现，计算出来的相似度值要么是1，要么是-1，因为这依赖于用户的喜好值和User1的喜好值是否趋于“一致变化”还是呈“相反趋势变化\"。 Mahout对斯皮尔曼相关系数给出了实现，具体可参考SpearmanCorrelationSimilarity，它的执行效率不是非常高，因为斯皮尔曼相关性的计算需要花时间计算并存储喜好值的一个排序（Ranks），具体时间取决于数据的数量级大小。正因为这样，斯皮尔曼相关系数一般用于学术研究或者是小规模的计算。 UserSimilarity similarity1 = new SpearmanCorrelationSimilarity(model); // construct a Spearman Correlation-based Similarity 结果： User1 to User1 : 1.0 User2 to User1 : -1.0 User3 to User1 : NaN User4 to User1 : 1.0 User4 to User1 : 1.0 考虑到Spearman Correlation的效率，可以把SpearmanCorrelationSimilarity包装一层Cache，具体做法为： UserSimilarity similarity2 = new CachingUserSimilarity(new SpearmanCorrelationSimilarity(model), model); 这样，每次计算的结果会直接放入Cache，下一次计算的时候可以立即得到结果，而不是重新再计算一次。 6. 基于谷本系数的相似性度量 —— Tanimoto Coefficient-based Similarity Tanimoto Coefficient和前面的5中相关度计算方式有很大的不同，它不关心用户对物品的具体评分值是多少，它在关心用户与物品之间是否存在关联关系。还记得上一篇文章《Mahout学习笔记——数据承载》里面提到的布尔喜好值（Boolean Preference）吧！Tanimoto Coefficient依赖于用户和物品之间的这种Boolean关系作为输入。 更准确的说法为：Tanimoto Coefficient主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Tanimoto Coefficient只关心个体间共同具有的特征是否一致这个问题。Tanimoto Coefficient又被叫做Jaccard Coefficient，其值等于两个用户共同关联（不管喜欢还是不喜欢）的物品数量除于两个用户分别关联的所有物品数量。 也就是关联的交集除于关联的并集，用公式表示为： 其值介于[0, 1]之间，如果两个用户关联的物品完全相同，交集等于并集，值为1；如果没有任何关联，交集为空，值为0。 注：本块中说到的关联指用户对物品有评分值","title":"Mahout之（三）相似性度量"},{"content":"协同过滤 —— Collaborative Filtering 协同过滤简单来说就是根据目标用户的行为特征，为他发现一个兴趣相投、拥有共同经验的群体，然后根据群体的喜好来为目标用户过滤可能感兴趣的内容。 协同过滤推荐 —— Collaborative Filtering Recommend 协同过滤推荐是基于一组喜好相同的用户进行推荐。它是基于这样的一种假设：为一用户找到他真正感兴趣的内容的最好方法是首先找到与此用户有相似喜好的其他用户，然后将他们所喜好的内容推荐给用户。这与现实生活中的“口碑传播(word-of-mouth)”颇为类似。 协同过滤推荐分为三类： 基于用户的推荐（User-based Recommendation） 基于项目的推荐（Item-based Recommendation） 基于模型的推荐（Model-based Recommendation） 基于用户的协同过滤推荐 —— User CF 原理：基于用户对物品的喜好找到相似邻居用户，然后将邻居用户喜欢的物品推荐给目标用户 上图示意出User CF的基本原理，假设用户A喜欢物品A和物品C，用户B喜欢物品B，用户C喜欢物品A、物品C和物品D；从这些用户的历史喜好信息中，我们可以发现用户A和用户C的口味和偏好是比较类似的，同时用户C还喜欢物品D，那么我们可以推断用户A可能也喜欢物品D，因此可以将物品D推荐给用户A。 实现：将一个用户对所有物品的偏好作为一个向量（Vector）来计算用户之间的相似度，找到K-邻居后，根据邻居的相似度权重以及他们对物品的喜好，为目标用户生成一个排序的物品列表作为推荐，列表里面都是目标用户为涉及的物品。 基于物品的协同过滤推荐 —— Item CF 原理：基于用户对物品的喜好找到相似的物品，然后根据用户的历史喜好，推荐相似的物品给目标用户。与User CF类似，只是关注的视角变成了Item。 假设用户A喜欢物品A和物品C，用户B喜欢物品A、物品B和物品C，用户C喜欢物品A，从这些用户的历史喜好可以分析出物品A和物品C是比较类似的，喜欢物品A的人都喜欢物品C，基于这个数据可以推断用户C 很有可能也喜欢物品C，所以系统会将物品C推荐给用户C。 实现：将所有用户对某一个物品的喜好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的喜好预测目标用户还没有涉及的物品，计算得到一个排序的物品列表作为推荐。 相似度的计算 —— Similarity Metrics Computing 关于相似度的计算，现有的几种基本方法都是基于向量（Vector）的，其实也就是计算两个向量的距离，距离越近相似度越大。在推荐的场景中，在用户 - 物品偏好的二维矩阵中，我们可以将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，或者将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度。下面我们详细介绍几种常用的相似度计算方法：    欧几里德距离（Euclidean Distance）   最初用于计算欧几里德空间中两个点的距离，假设 x，y 是 n 维空间的两个点，它们之间的欧几里德距离是： 可以看出，当 n=2 时，欧几里德距离就是平面上两个点的距离。 当用欧几里德距离表示相似度，一般采用以下公式进行转换：距离越小，相似度越大 皮尔森相关系数（Pearson Correlation Coefficient） 皮尔森相关系数一般用于计算两个定距变量间联系的紧密程度，它的取值在 [-1，+1] 之间。 Cosine 相似度（Cosine Similarity） Cosine 相似度被广泛应用于计算文档数据的相似度： 相似邻居的计算 邻居就是上文说到的“兴趣相投、拥有共同经验的群体”，在协同过滤中，邻居的计算对于推荐数据的生成是至关重要的，常用的划分邻居的方法有两类： 固定数量的邻居：K-neighborhoods 或者 Fix-size neighborhoods 用“最近”的K个用户或物品最为邻居。如下图中的 A，假设要计算点 1 的 5- 邻居，那么根据点之间的距离，我们取最近的 5 个点，分别是点 2，点 3，点 4，点 7 和点 5。但很明显我们可以看出，这种方法对于孤立点的计算效果不好，因为要取固定个数的邻居，当它附近没有足够多比较相似的点，就被迫取一些不太相似的点作为邻居，这样就影响了邻居相似的程度，比如图 1 中，点 1 和点 5 其实并不是很相似。 基于相似度门槛的邻居：Threshold-based neighborhoods 与计算固定数量的邻居的原则不同，基于相似度门槛的邻居计算是对邻居的远近进行最大值的限制，落在以当前点为中心，距离为 K 的区域中的所有点都作为当前点的邻居，这种方法计算得到的邻居个数不确定，但相似度不会出现较大的误差。如下图中的 B，从点 1 出发，计算相似度在 K 内的邻居，得到点 2，点 3，点 4 和点 7，这种方法计算出的邻居的相似度程度比前一种优，尤其是对孤立点的处理。 Threshold-based neighborhoods要表现的就是“宁缺勿滥”，在数据稀疏的情况下效果是非常明显的。Mahout对这两类邻居的计算给出了自己的实现，分别是NearestNUserNeighborhood和ThresholdUserNeighborhood，从名字就可以看出它们的对应关系","title":"Mahout之（二）协同过滤推荐"},{"content":"推荐数据的处理是大规模的，在集群环境下一次要处理的数据可能是数GB，所以Mahout针对推荐数据进行了优化。 Preference 在Mahout中，用户的喜好被抽象为一个Preference，包含了userId，itemId和偏好值（user对item的偏好）。Preference是一个接口，它有一个通用的实现是GenericPreference。 因为用户的喜好数据是大规模的，我们通常会选择把它放入集合或者数组。同时，由于Java的对象的内存消耗机制，在大数据量下使用Collection<Preference>和Preference[]是非常低效的。为什么呢？ 在Java中，一个对象占用的字节数 = 基本的8字节 + 基本数据类型所占的字节 + 对象引用所占的字节 (1) 先说这基本的8字节 在JVM中，每个对象（数组除外）都有一个头，这个头有两个字，第一个字存储对象的一些标志位信息，如：锁标志位、经历了几次gc等信息；第二个字节是一个引用，指向这个类的信息。JVM为这两个字留了8个字节的空间。 这样一来的话，new Object()就占用了8个字节，那怕它是个空对象 (2) 基本类型所占用的字节数 byte/boolean    1bytes char/short         2bytes int/float              4byte double/long     8bytes (3) 对象引用所占用的字节数 reference         4bytes 注：实际中，有数据成员的话，要把数据成员按基本类型和对象引用分开统计。基本类型按(2)进行累加，然后对齐到8个倍数；对象引用按每个4字节进行累加，然后对齐到8的倍数。 class test {  Integer i;  long      l;  byte      b;}占 8(基本) + 16(数据成员——基本类型：8 + 1，对齐到8) + 8(数据成员——对象引用Integer，4，对齐到8) = 32字节 如此一来的话，一个GenericPreference的对象就需要占用28个字节，userId(8bytes) + itemId(8bytes) + preference(4bytes) + 基本的8bytes = 28。如果我们使用了Collection<Preference>和Preference[]，就会浪费很多这基本的8字节。设想如果我们的数据量是上GB或是上TB，这样的开销是很难承受的。 为此Mahout封装了一个PreferenceArray，用于表示喜好数据的集合 我们看到，GenericUserPreferenceArray包含了一个userId，一个itemId的数组long[]，一个用户的喜好评分数据float[]。而不是一个Preference对象的集合。下面我们做个比较，分别创建一个PreferenceArray和Preference数组   在size为5，但只包含一条喜好数据的情况下：PreferenceArray需要20Bytes(userId 8bytes + preference 4bytes + itemId 8bytes)，而Preference[]需要48字节(基本8bytes + 一个Preference对象28bytes + 4个空的引用4×3 12Bytes)。如果在有多条喜好数据的情况下，PreferenceArray中将只有一个itemId，这样它所占用的8Bytes微乎其微。所以PreferenceArray用它特殊的实现节省了4倍内存。 用《Mahout in action》一书中的原话“mahout has alreadly reinvented an 'array of Java objects'”——\"mahout已经重新改造了Java对象数组\"。PreferenceArray和它的具体实现减少的内存开销远远比它的的复杂性有价值，它减少了近75%的内存开销（相对于Java的集合和对象数组） 除了PreferenceArray，Mahout中还大量使用了像Map和Set这些非常典型的数据结构，但是Mahout没有直接使用像HashMap和TreeSet这些常用的Java集合实现，取而代之的是专门为Mahout推荐的需要实现了两个API，FastByIDMap和FastIDSet，之所以专门封装了这两个数据结构，主要目的是为了减少内存的开销，提升性能。它们之间主要有以下区别： 和HashMap一样，FastByIDMap也是基于hash的。不过FastByIDMap使用的是线性探测来解决hash冲突，而不是分割链； FastByIDMap的key和值都是long类型，而不是Object，这是基于节省内存开销和改善性能所作的改良； FastByIDMap类似于一个缓存区，它有一个“maximum size”的概念，当我们添加一个新元素的时候，如果超过了这个size，那些使用不频繁的元素就会被移除。 FastByIDMap和FastIDSet在存储方面的改进非常显著。FastIDSet的每个元素平均占14字节，而HashSet而需要84字节；FastByIDMap的每个entry占28字节，而HashMap则需要84字节。 DataModel Mahout推荐引擎实际接受的输入是DataModel，它是对用户喜好数据的压缩表示。DataModel的具体实现支持从任意类型的数据源抽取用户喜好信息，可以很容易的返回输入的喜好数据中关联到一个物品的用户ID列表和count计数，以及输入数据中所有用户和物品的数量。具体实现包括内存版的GenericDataModel，支持文件读取的FileDataModel和支持数据库读取的JDBCDataModel。 GenericDataModel是DataModel的内存版实现。适用于在内存中构造推荐数据，它仅只是作为推荐引擎的输入接受用户的喜好数据，保存着一个按照用户ID和物品ID进行散列的PreferenceArray，而PreferenceArray中对应保存着这个用户ID或者物品ID的所有用户喜好数据。 FileDataModel支持文件的读取，Mahout对文件的格式没有太多严格的要求，只要满足一下格式就OK： 每一行包含一个用户Id，物品Id，用户喜好 逗号隔开或者Tab隔开 *.zip 和 *.gz 文件会自动解压缩（Mahout 建议在数据量过大时采用压缩的数据存储） FileDataModel从文件中读取数据，然后将数据以GenericDataModel的形式载入内存，具体可以查看FileDataModel中的buildModel方法。 JDBCDataModel支持对数据库的读取操作，Mahout提供了对MySQL的默认支持MySQLJDBCDataModel，它对用户喜好数据的存储有以下要求： 用户ID列需要是BIGINT而且非空 物品ID列需要是BIGINT而且非空 用户喜好值列需要是FLOAT 建议在用户ID和物品ID上建索引 有的时候，我们会忽略用户的喜好值，仅仅只关心用户和物品之间存不存在关联关系，这种关联关系在Mahout里面叫做“boolean preference”。 之所以会有这类喜好，是因为用户和物品的关联要么存在，要么不存在，记住只是表示关联关系存不存在，不代表喜欢和不喜欢。实际上一条“boolean preference”可有三个状态：喜欢、不喜欢、没有任何关系。 在喜好数据中有大量的噪音数据的情况下，这种特殊的喜好评定方式是有意义的。 同时Mahout为“boolean preference”提供了一个内存版的DataModel——GenericBooleanPrefDataModel 可以看到，GenericBooleanPrefDataModel没有对喜好值进行存储，仅仅只存储了关联的userId和itemId，注意和GenericDataModel的差别，GenericBooleanPrefDataModel采用了FastIDSet，只有关联的Id，没有喜好值。因此它的一些方法（继承自DataModel的）如getItemIDsForUser()有更好的执行速度，而getPreferencesFromUser()的执行速度会更差，因为GenericBooleanPrefDataModel本来就没存储喜好值，它默认用户对物品的喜好值都是1.0   @Override  public Float getPreferenceValue(long userID, long itemID) throws NoSuchUserException {    FastIDSet itemIDs = preferenceFromUsers.get(userID);    if (itemIDs == null) {      throw new NoSuchUserException(userID);    }    if (itemIDs.contains(itemID)) {      return 1.0f;    }    return null;  }","title":"Mahout之（一）数据承载"},{"content":"在传统的机器学习的框架下，学习的任务就是在给定充分训练数据的基础上来学习一个分类模型；然后利用这个学习到的模型来对测试文档进行分类与预测。然而，我们看到机器学习算法在当前的Web挖掘研究中存在着一个关键的问题：一些新出现的领域中的大量训练数据非常难得到。我们看到Web应用领域的发展非常快速。大量新的领域不断涌现，从传统的新闻，到网页，到图片,再到博客、播客等等。传统的机器学习需要对每个领域都标定大量训练数据，这将会耗费大量的人力与物力。而没有大量的标注数据，会使得很多与学习相关研究与应用无法开展。其次，传统的机器学习假设训练数据与测试数据服从相同的数据分布。然而，在许多情况下，这种同分布假设并不满足。通常可能发生的情况如训练数据过期。这往往需要我们去重新标注大量的训练数据以满足我们训练的需要，但标注新数据是非常昂贵的，需要大量的人力与物力。从另外一个角度上看，如果我们有了大量的、在不同分布下的训练数据，完全丢弃这些数据也是非常浪费的。如何合理的利用这些数据就是迁移学习主要解决的问题。迁移学习可以从现有的数据中迁移知识，用来帮助将来的学习。迁移学习（Transfer Learning）的目标是将从一个环境中学到的知识用来帮助新环境中的学习任务。因此，迁移学习不会像传统机器学习那样作同分布假设。 我们在迁移学习方面的工作目前可以分为以下三个部分：同构空间下基于实例的迁移学习，同构空间下基于特征的迁移学习与异构空间下的迁移学习。我们的研究指出，基于实例的迁移学习有更强的知识迁移能力，基于特征的迁移学习具有更广泛的知识迁移能力，而异构空间的迁移具有广泛的学习与扩展能力。这几种方法各千秋。 1.同构空间下基于实例的迁移学习 基于实例的迁移学习的基本思想是，尽管辅助训练数据和源训练数据或多或少会有些不同，但是辅助训练数据中应该还是会存在一部分比较适合用来训练一个有效的分类模型，并且适应测试数据。于是，我们的目标就是从辅助训练数据中找出那些适合测试数据的实例，并将这些实例迁移到源训练数据的学习中去。在基于实例的迁移学习方面，我们推广了传统的AdaBoost算法，提出一种具有迁移能力的boosting算法：Tradaboosting [9]，使之具有迁移学习的能力，从而能够最大限度的利用辅助训练数据来帮助目标的分类。我们的关键想法是，利用boosting的技术来过滤掉辅助数据中那些与源训练数据最不像的数据。其中，boosting的作用是建立一种自动调整权重的机制，于是重要的辅助训练数据的权重将会增加，不重要的辅助训练数据的权重将会减小。调整权重之后，这些带权重的辅助训练数据将会作为额外的训练数据，与源训练数据一起从来提高分类模型的可靠度。基于实例的迁移学习只能发生在源数据与辅助数据非常相近的情况下。但是，当源数据和辅助数据差别比较大的时候，基于实例的迁移学习算法往往很难找到可以迁移的知识。但是我们发现，即便有时源数据与目标数据在实例层面上并没有共享一些公共的知识，它们可能会在特征层面上有一些交集。因此我们研究了基于特征的迁移学习，它讨论的是如何利用特征层面上公共的知识进行学习的问题。 2.同构空间下基于特征的迁移学习 在基于特征的迁移学习研究方面，我们提出了多种学习的算法，如CoCC算法[7]，TPLSA算法[4]，谱分析算法[2]与自学习算法[3]等。其中利用互聚类算法产生一个公共的特征表示，从而帮助学习算法。我们的基本思想是使用互聚类算法同时对源数据与辅助数据进行聚类，得到一个共同的特征表示，这个新的特征表示优于只基于源数据的特征表示。通过把源数据表示在这个新的空间里，以实现迁移学习。应用这个思想，我们提出了基于特征的有监督迁移学习与基于特征的无监督迁移学习。 2.1 基于特征的有监督迁移学习 我们在基于特征的有监督迁移学习方面的工作是基于互聚类的跨领域分类[7]，这个工作考虑的问题是：当给定一个新的、不同的领域，标注数据及其稀少时，如何利用原有领域中含有的大量标注数据进行迁移学习的问题。在基于互聚类的跨领域分类这个工作中，我们为跨领域分类问题定义了一个统一的信息论形式化公式，其中基于互聚类的分类问题的转化成对目标函数的最优化问题。在我们提出的模型中，目标函数被定义为源数据实例，公共特征空间与辅助数据实例间互信息的损失。 2.2 基于特征的无监督迁移学习：自学习聚类 我们提出的自学习聚类算法[3]属于基于特征的无监督迁移学习方面的工作。这里我们考虑的问题是：现实中可能有标记的辅助数据都难以得到，在这种情况下如何利用大量无标记数据辅助数据进行迁移学习的问题。自学习聚类的基本思想是通过同时对源数据与辅助数据进行聚类得到一个共同的特征表示，而这个新的特征表示由于基于大量的辅助数据，所以会优于仅基于源数据而产生的特征表示，从而对聚类产生帮助。 上面提出的两种学习策略（基于特征的有监督迁移学习与无监督迁移学习）解决的都是源数据与辅助数据在同一特征空间内的基于特征的迁移学习问题。当源数据与辅助数据所在的特征空间中不同时，我们还研究了跨特征空间的基于特征的迁移学习，它也属于基于特征的迁移学习的一种。 ３　异构空间下的迁移学习：翻译学习 我们提出的翻译学习[1][5]致力于解决源数据与测试数据分别属于两个不同的特征空间下的情况。在[1]中，我们使用大量容易得到的标注过文本数据去帮助仅有少量标注的图像分类的问题，如上图所示。我们的方法基于使用那些用有两个视角的数据来构建沟通两个特征空间的桥梁。虽然这些多视角数据可能不一定能够用来做分类用的训练数据，但是，它们可以用来构建翻译器。通过这个翻译器，我们把近邻算法和特征翻译结合在一起，将辅助数据翻译到源数据特征空间里去，用一个统一的语言模 型进行学习与分类。","title":"transfer learning"},{"content":"算法代码 Github传送门：K-MeansCluster@skyline0623 数据聚类是对于静态数据分析的一门技术，在许多领域内都被广泛地应用，包括机器学习、数据挖掘、模式识别、图像分析、信息检索以及生物信息等。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集，这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。 我们拿2维特征的实例作为例子。我们以这个2维特征向量作为坐标，在一个2维空间中用点标注出这些实例，如图1所示，这里是随机生成的100个实例。图1中带颜色的方框同样是实例，在之后介绍的K-means算法中，这些方框是初始被随机选择出来的聚类中心点。   图1  在二维空间中100个具有两个特征的实例 我们希望聚类算法能够将特征相近的实例聚集成一个集合，最后形成多个由特征相近的实例聚集成的聚类。如图2所示，我们将上面的数据通过聚类算法聚集成了10个类，分别用10种颜色表示，其中点的颜色标示它属于哪一个聚类。这里使用了点间的欧式距离作为评价两个点特征相似程度的度量。   图2 聚成10个类的效果 通常，我们将聚类分析作为一种无监督机器学习算法来看待。与有监督学习不同，如常见的分类问题，我们有标注好分类类别的训练数据，通过这些数据训练出一个模型来对新的数据的分类进行预测。在聚类分析中，我们的数据是没有标注类别的，我们通过数据中实例的特征值相似程度将相似的实例划分到一类中。 在接下来的内容中，我们将介绍一种简单的聚类算法----K-means，第二部分它的原理、过程以及可能存在的问题和解决方案。第三部分介绍K-means算法的具体实现中的流程与细节，以及实验结果的分析。 1  K-Means算法介绍 1.1  算法初探 K-means是一种基于距离的迭代式算法[1]。它将n个观察实例分类到k个聚类中，以使得每个观察实例距离它所在的聚类的中心点比其他的聚类中心点的距离更小。 其中，距离的计算方式可以是欧式距离（2-norm distance），或者是曼哈顿距离（Manhattan distance,1-norm distance）或者其他。这里我们使用欧式距离。 要将每个观测实例都归类到距离它最近的聚类中心点，我们需要找到这些聚类中心点的具体位置。然而，要确定聚类中心点的位置，我们必须知道该聚类中包含了哪些观测实例。这似乎是一个“先有蛋还是先有鸡”的问题。从理论上来说，这是一个NP-Hard问题[3]。 但是，我们可以通过启发式的算法来近似地解决问题。找到一个全局最优的方案是NP-hard问题，但是，降低问题的难度，如果能够找到多个局部最优方案，然后通过一种评价其聚类结果优劣的方式，把其中最优的解决方案作为结果，我们就可以得到一个不错的聚类结果[2]。 这就是为什么我们说K-means算法是一个迭代式的算法。算法[2]的过程如下： 1）所有的观测实例中随机抽取出k个观测点，作为聚类中心点，然后遍历其余的观测点找到距离各自最近的聚类中心点，将其加入到该聚类中。这样，我们就有了一个初始的聚类结果，这是一次迭代的过程。 2）我们每个聚类中心都至少有一个观测实例，这样，我们可以求出每个聚类的中心点（means），作为新的聚类中心，然后再遍历所有的观测点，找到距离其最近的中心点，加入到该聚类中。然后继续运行2）。 3）如此往复2），直到前后两次迭代得到的聚类中心点一模一样。 这样，算法就稳定了，这样得到的k个聚类中心，和距离它们最近的观测点构成k个聚类，就是我们要的结果。 实验证明，算法试可以收敛的[2]。 计算聚类的中心点有三种方法如下： 1）Minkowski Distance 公式 —— λ 可以随意取值，可以是负数，也可以是正数，或是无穷大。                                                       公式（1） 2）Euclidean Distance 公式 —— 也就是第一个公式 λ=2 的情况                                                      公式（2） 3）CityBlock Distance 公式 —— 也就是第一个公式 λ=1 的情况                                                          公式（3） 这三个公式的求中心点有一些不一样的地方，我们看下图（对于第一个 λ 在 0-1之间）。                             （1）Minkowski Distance                                             （2）Euclidean Distance                                                        （3） CityBlock Distance 上面这几个图的大意是他们是怎么个逼近中心的，第一个图以星形的方式，第二个图以同心圆的方式，第三个图以菱形的方式。我们采用第二种方式，在实际实现中，我们并没有开根号。 我们算法十分的简单。下面对100个随机生成的2维特征的观测实例在2维空间中运行这个K-means算法的过程用图表示出来。整个过程用了10次迭代后收敛。假设k=10。 100个观测点的初始分布如图1所示。方块为随机挑出的10个聚类中心点。 图3显示了将各个观测值划分到距离最近的聚类后的结果。   图3 然后根据每个聚类的点的情况，计算出新的中心点，得到新的聚类中心，再以此往复，直到达到收敛的条件。 在第9次迭代和第10次迭代中，聚类中心没有任何变化，所以算法收敛，结束。从图5中可以看出，聚类中心在这10次迭代中位置变化的过程。黑色方块为初始位置，白色为收敛后位置，其余同一颜色的为中间过程中的位置。    图5 聚类中心位置变化的过程 那么，如何评价一个聚类结果呢？我们计算所有观测点距离它对应的聚类中心的距离的平方和即可，我们称这个评价函数为evaluate(C)。它越小，说明聚类越好。 1.2   K-means的问题及解决方案 K-means算法非常简单，然而却也有许多问题。 1）首先，算法只能找到局部最优的聚类，而不是全局最优的聚类。而且算法的结果非常依赖于初始随机选择的聚类中心的位置。我们通过多次运行算法，使用不同的随机生成的聚类中心点运行算法，然后对各自结果C通过evaluate(C)函数进行评估，选择多次结果中evaluate(C)值最小的那一个。 2）关于初始k值选择的问题。首先的想法是，从一个起始值开始，到一个最大值，每一个值运行k-means算法聚类，通过一个评价函数计算出最好的一次聚类结果，这个k就是最优的k。我们首先想到了上面用到的evaluate(C)。然而，k越大，聚类中心越多，显然每个观测点距离其中心的距离的平方和会越小，这在实践中也得到了验证。第四节中的实验结果分析中将详细讨论这个问题。 3）关于性能问题。原始的算法，每一次迭代都要计算每一个观测点与所有聚类中心的距离。有没有方法能够提高效率呢？是有的，可以使用k-d tree或者ball tree这种数据结构来提高算法的效率。特定条件下，对于一定区域内的观测点，无需遍历每一个观测点，就可以把这个区域内所有的点放到距离最近的一个聚类中去。这将在第三节中详细地介绍。        2  实现 2.1  k-d tree 与 ball tree k-d tree[5]或者ball tree[4]这个数据结构在K近邻算法中被用到来提高算法的效率，在K-means中能否使用这些数据结构呢？当然能，而且，它们用在这里更加高效。在这里，我们使用ball tree来优化算法的性能。我们首先介绍k-d tree。 1）k-d tree[5] 把n维特征的观测实例放到n维空间中，k-d tree每次通过某种算法选择一个特征(坐标轴)，以它的某一个值作为分界做超平面，把当前所有观测点分为两部分，然后对每一个部分使用同样的方法，直到达到某个条件为止。 上面的表述中，有几个地方下面将会详细说明：（1）选择特征（坐标轴）的方法  （2）以该特征的哪一个为界 （3）达到什么条件算法结束。 (1)选择特征的方法 计算当前观测点集合中每个特征的方差，选择方差最大的一个特征，然后画一个垂直于这个特征的超平面将所有观测点分为两个集合。 （2)以该特征的哪一个值为界 即垂直选择坐标轴的超平面的具体位置。 第一种是以各个点的方差的中值（median）为界。这样会使建好的树非常地平衡，会均匀地分开一个集合。这样做的问题是，如果点的分布非常不好地偏斜的，选择中值会造成连续相同方向的分割，形成细长的超矩形(hyperrectangles)。 替代的方法是计算这些点该坐标轴的平均值，选择距离这个平均值最近的点作为超平面与这个坐标轴的交点。这样这个树不会完美地平衡，但区域会倾向于正方地被划分，连续的分割更有可能在不同方向上发生。    （3）达到什么条件算法结束 实际中，不用指导叶子结点只包含两个点时才结束算法。你可以设定一个预先设定的最小值，当这个最小值达到时结束算法。       图 6  一个k-d tree划分二维空间 图6中，星号标注的是目标点，我们在k-d tree中找到这个点所处的区域后，依次计算此区域包含的点的距离，找出最近的一个点（黑色点），如果在其他region中还包含更近的点则一定在以这两个点为半径的圆中。假设这个圆如图中所示包含其他区域。先看这个区域兄弟结点对应区域，与圆不重叠；再看其双亲结点的兄弟结点对应区域。从它的子结点对应区域中寻找（图中确实与这个双亲结点的兄弟结点的子结点对应区域重叠了）。在其中找是否有更近的结点。     k-d tree的优势是可以递增更新。新的观测点可以不断地加入进来。找到新观测点应该在的区域，如果它是空的，就把它添加进去，否则，沿着最长的边分割这个区域来保持接近正方形的性质。这样会破坏树的平衡性，同时让区域不利于找最近邻。我们可以当树的深度到达一定值时重建这棵树。 然而，k-d tree也有问题。矩形并不是用到这里最好的方式。偏斜的数据集会造成我们想要保持树的平衡与保持区域的正方形特性的冲突。另外，矩形甚至是正方形并不是用在这里最完美的形状，由于它的角。如果图6中的圆再大一些，即黑点距离目标点点再远一些，圆就会与左上角的矩形相交，需要多检查一个区域的点，而且那个区域是当前区域双亲结点的兄弟结点的子结点。  为了解决上面的问题，我们引入了ball tree。 2）ball tree[4] 解决上面问题的方案就是使用超球面而不是超矩形划分区域。使用球面可能会造成球面间的重叠，但却没有关系。ball tree就是一个k维超球面来覆盖这些观测点，把它们放到树里面。图7（a)显示了一个2维平面包含16个观测实例的图,图7（b）是其对应的ball tree，其中结点中的数字表示包含的观测点数。                        图 7  ball tree对二维平面的划分和ball tree 不同层次的圆被用不同的风格画出。树中的每个结点对应一个圆，结点的数字表示该区域保含的观测点数，但不一定就是图中该区域囊括的点数，因为有重叠的情况，并且一个观测点只能属于一个区域。实际的ball tree的结点保存圆心和半径。叶子结点保存它包含的观测点。     使用ball tree时，先自上而下找到包含target的叶子结点，从此结点中找到离它最近的观测点。这个距离就是最近邻的距离的上界。检查它的兄弟结点中是否包含比这个上界更小的观测点。方法是：如果目标点距离兄弟结点的圆心的距离大于这个圆的圆心加上前面的上界的值，则这个兄弟结点不可能包含所要的观测点。（如图8）否则，检查这个兄弟结点是否包含符合条件的观测点。       图 8 点与超圆     那么，ball tree的分割算法是什么呢？     选择一个距离当前圆心最远的观测点i1，和距离i1最远的观测点 i2，将圆中所有离这两个点最近的观测点都赋给这两个簇的中心，然后计算每一个簇的中心点和包含所有其所属观测点的最小半径。对包含n个观测点的超圆进行分割，只需要线性的时间。     与k-d tree一样，如果结点包含的观测点到达了预先设定的最小值，这个顶点就可以不再分割了。 2.2  在K-means算法中使用ball tree 在k-means算法中使用k-d tree和ball tree效率更高，因为在此所有的观测点都是一起处理，而在k近邻中，测试的观测点单独处理。 首先，根据所有的观测点创建一个包含它们的k-d tree 或者是ball tree。 在k-means算法的每一次迭代中，设定的聚类中心集合为C={Ci，i= 1,...,k},每个簇中心有一个对应其归属它的观测点集合，初始为空。从根结点开始遍历树，直到找到叶子结点为止，判断其中的观测点距离哪一个中心近，然后赋给那个中心。有可能在浏览一个内部结点时，它所包含的点完全包含在一个聚类的区域内。这时只需要直接把其下所有的观测点加入到该聚类中即可。 我们可以在每个簇的中心点保存一个向量，保存聚类中点各个坐标之和和点的个数。在计算其中心点时只需一除即可。 在多维空间中，如何判断一个内部结点包含的观测点全部落在一个聚类中呢？ 我们采用下面描述的方法。Stackoverflow上的讨论见这里：点击这里 如果一个空间容器比如这里的超球面与某一簇中心的最大距离小于其与其它所有簇中心的最小距离，即：    公式（4） 其中，表示聚类的中心点，如果上式成立，则可推出：                                             公式（5） 即此容器中的任意观测点距离这个的距离都小于距离其他聚类中心点的距离。此时，可以断定这个容器中的所有观测点都在这个所在的聚类中。 在我们这个环境即ball tree中，任意点与超球面的最大距离为其与圆心距离加圆的半径，最小距离为其与圆心距离减去圆的半径。在实际的运行中，这个策略可以明显地提高程序运行的性能。经大量的实验得出，在对ball tree中的结点访问时，能够避免继续向下探索的次数占所有访问结点次数平均达到17.8764%。每次避免访问其子树时，就大量减少了访问总结点的时间。 3  实验结果分析 我们从k=2开始，一直调用K-means算法（每次调用10次，选择evaluate最小那一次聚类的结果返回）直到k=14.我们记录下每个k聚类后评估函数evaluate(Ck)的值，然后画出其变化的曲线。理论上，随着k的增大，evaluate(Ck)的值一定是非递增的。因为中心越多，各个观测点距离中心的平均距离越近。所以，我们通过另一种方式来确定k。我们在一个要聚类数据集上我们按照上面的方法运行多次，其evaluate关于k的趋势图如图9所示：                   图9 多次运行上述过程的evaluate关于k的变化图 通过我们的多次试验（上面只给出了3次的evaluate变化过程），多次结果的共同特征是，每次k=9~k=10间，evaluate函数的计算值都有极小的变化，而k<9时变化又极大，在k=9之后明显下降变缓。所以，我们k=9或者k=10作为初始的聚类个数，即k-means中的k比较好。根据MDL原则，这里选择k = 9。 上面的例子中以实际的过程提供了一种选择k的思路，就是根据evaluate(C)值下降的“拐点”和MDL原则来选择k。另外，还有许多更科学的方法选择k，见这里：点击这里 程序采用java实现，源码在Github上，传送门：K-MeansCluster@skyline0623","title":"【机器学习】K-means聚类算法初探"},{"content":"A*寻路初探 GameDev.net http://blog.vckbase.com/panic/archive/2005/03/20/3778.html","title":"关于A*(A-star)算法"},{"content":"转载自：http://blog.csdn.net/lcjpure/article/details/8069704 结合自己的学习经历，总结一下如何学习机器学习。我自己的学习过程其实是非常混乱和痛苦的，一个人瞎搞现在也不知道入没入门。希望能对其他想自学机器学习而找不到方向的人有一点点帮助。 一、可以读读一些科普性的，综述性的东西。 南京大学周志华教授写的科普文章《机器学习和数据挖掘》还不错，对机器学习和数据挖掘的区别说的挺好。另外对机器学习的历史和前景做了说明。文章最后也给出了领域内比较重要的会议和期刊。 吴军写的数学之美（浪潮之巅也很赞）可能确切的说应该是搜索、自然语言处理、机器学习的一个综合性科普，但是机器学习本来就和这些领域有着很大的关联的，所以说对学习机器学习的人来说也是一个不错的入门科普。   二、可以了解下领域里面的牛人。曾经听香港科技大学的杨强教授在一个讲座上讲过这么一件事，他说如果面试一个学生，一个好的方法就是问他这个领域有哪些牛人，每个牛人的代表性工作是什么，这样可以检验一个人是否真正喜欢并关注这个方向。我觉得说得挺有道理。网上有人总结了一些牛人，当然这些总结都是个人观点，随便看看了解一下就行了。（注：下面的链接不一定是原始出处） http://blog.sina.com.cn/s/blog_56f7cc3a0100qktd.html   （国外的） http://blog.sina.com.cn/s/blog_6a6b58ce01017jy3.html  （国内的） http://blog.csdn.net/yihaizhiyan/article/details/6795073 （这个有一些是机器视觉里的）   三、可以了解下领域的重要会议，前面提到的周的文章有提到，网上也有各种版本的分析，列两个 http://blog.csdn.net/blow_jj/article/details/2415305 （人工智能和机器学习的） http://taoo.iteye.com/blog/1052495               （数据库和数据挖掘的）   四、可以系统的上一下机器学习的课程，Standford的Andrew Ng的机器学习课程很赞，网上有他的公开课视频，上他的课真是如沐春风，浑身通畅。 课程主页： http://cs229.stanford.edu/  （里面有讲义，最好打印出来对照视频看） 网易公开课有中文字幕翻译的视频： http://v.163.com/special/opencourse/machinelearning.html 这是在coursera项目中的版本，好像比网易的内容要多，起码多的有推荐算法的部分 https://class.coursera.org/ml/lecture/preview/index   五、可以系统的读一本机器学习的教材，系统的学习很重要。口碑最好的，我自己感觉写的最好的是 Christopher M. Bishop写的《pattern Recognition and Machine Learning》   六、对一些重要的经典的文章可以拜读一下，下面的这个资料列表可以参考 http://www.newsmth.net/bbsanc.php?path=%2Fgroups%2Fsci.faq%2FNLP%2F1%2FM.1225371502.h0  在学习的过程中，手头有一本统计的参考书是必要的，当然还有利器google，不懂的概念随时可以查。   七、其实学习机器学习最重要的就是实践了，实践出真知。                                                                                                                   ==================                                                                                                                    by lcj ,2012 -10 -14                                  ","title":"如何学习机器学习的一点心得"},{"content":"转载自：http://www.cnblogs.com/jerrylead 1 摘要       本报告是在学习斯坦福大学机器学习课程前四节加上配套的讲义后的总结与认识。前四节主要讲述了回归问题，回归属于有监督学习中的一种方法。该方法的核心思想是从连续型统计数据中得到数学模型，然后将该数学模型用于预测或者分类。该方法处理的数据可以是多维的。      讲义最初介绍了一个基本问题，然后引出了线性回归的解决方法，然后针对误差问题做了概率解释。之后介绍了logistic回归。最后上升到理论层次，提出了一般回归。 2 问题引入      这个例子来自http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html      假设有一个房屋销售的数据如下： 面积(m^2) 销售价钱（万元） 123 250 150 320 87 160 102 220 … …      这个表类似于北京5环左右的房屋价钱，我们可以做出一个图，x轴是房屋的面积。y轴是房屋的售价，如下：            如果来了一个新的面积，假设在销售价钱的记录中没有的，我们怎么办呢？      我们可以用一条曲线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以在将曲线上这个点对应的值返回。如果用一条直线去拟合，可能是下面的样子：            绿色的点就是我们想要预测的点。      首先给出一些概念和常用的符号。      房屋销售记录表：训练集(training set)或者训练数据(training data), 是我们流程中的输入数据，一般称为x      房屋销售价钱：输出数据，一般称为y      拟合的函数（或者称为假设或者模型）：一般写做 y = h(x)      训练数据的条目数(#training set),：一条训练数据是由一对输入数据和输出数据组成的输入数据的维度n (特征的个数，#features)      这个例子的特征是两维的，结果是一维的。然而回归方法能够解决特征多维，结果是一维多离散值或一维连续值的问题。 3 学习过程      下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。就如同上面的线性回归函数。       4 线性回归      线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。      我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向，等等，我们可以做出一个估计函数：            θ在这儿称为参数，在这的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了：            我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个函数称为损失函数（loss function）或者错误函数(error function)，描述h函数不好的程度，在下面，我们称这个函数为J函数      在这儿我们可以认为错误函数如下：            这个错误估计函数是去对x(i)的估计值与真实值y(i)差的平方和作为错误估计函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。      至于为何选择平方和作为错误估计函数，讲义后面从概率分布的角度讲解了该公式的来源。      如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，和梯度下降法。 5 梯度下降法      在选定线性回归模型后，只需要确定参数θ，就可以将模型用来预测。然而θ需要在J(θ)最小的情况下才能确定。因此问题归结为求极小值问题，使用梯度下降法。梯度下降法最大的问题是求得有可能是全局极小值，这与初始点的选取有关。      梯度下降法是按下面的流程进行的：      1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。      2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。      梯度方向由J(θ)对θ的偏导数确定，由于求的是极小值，因此梯度方向是偏导数的反方向。结果为                 迭代更新的方式有两种，一种是批梯度下降，也就是对全部的训练数据求得误差后再对θ进行更新，另外一种是增量梯度下降，每扫描一步都要对θ进行更新。前一种方法能够不断收敛，后一种方法结果可能不断在收敛处徘徊。      一般来说，梯度下降法收敛速度还是比较慢的。      另一种直接计算结果的方法是最小二乘法。 6 最小二乘法      将训练特征表示为X矩阵，结果表示成y向量，仍然是线性回归模型，误差函数不变。那么θ可以直接由下面公式得出      但此方法要求X是列满秩的，而且求矩阵的逆比较慢。 7 选用误差函数为平方和的概率解释      假设根据特征的预测结果与实际结果有误差，那么预测结果和真实结果满足下式：      一般来讲，误差满足平均值为0的高斯分布，也就是正态分布。那么x和y的条件概率也就是      这样就估计了一条样本的结果概率，然而我们期待的是模型能够在全部样本上预测最准，也就是概率积最大。注意这里的概率积是概率密度函数积，连续函数的概率密度函数与离散值的概率函数不同。这个概率积成为最大似然估计。我们希望在最大似然估计得到最大值时确定θ。那么需要对最大似然估计公式求导，求导结果既是                 这就解释了为何误差函数要使用平方和。      当然推导过程中也做了一些假定，但这个假定符合客观规律。 8 带权重的线性回归      上面提到的线性回归的误差函数里系统都是1，没有权重。带权重的线性回归加入了权重信息。      基本假设是                 其中假设符合公式                      其中x是要预测的特征，这样假设的道理是离x越近的样本权重越大，越远的影响越小。这个公式与高斯分布类似，但不一样，因为不是随机变量。      此方法成为非参数学习算法，因为误差函数随着预测值的不同而不同，这样θ无法事先确定，预测一次需要临时计算，感觉类似KNN。 9 分类和logistic回归      一般来说，回归不用在分类问题上，因为回归是连续型模型，而且受噪声影响比较大。如果非要应用进入，可以使用logistic回归。      logistic回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)将最为假设函数来预测。g(z)可以将连续值映射到0和1上。      logistic回归的假设函数如下，线性回归假设函数只是。      logistic回归用来分类0/1问题，也就是预测结果属于0或者1的二值分类问题。这里假设了二值满足伯努利分布，也就是      当然假设它满足泊松分布、指数分布等等也可以，只是比较复杂，后面会提到线性回归的一般形式。      与第7节一样，仍然求的是最大似然估计，然后求导，得到迭代公式结果为            可以看到与线性回归类似，只是换成了，而实际上就是经过g(z)映射过来的。 10 牛顿法来解最大似然估计      第7和第9节使用的解最大似然估计的方法都是求导迭代的方法，这里介绍了牛顿下降法，使结果能够快速的收敛。      当要求解时，如果f可导，那么可以通过迭代公式      来迭代求解最小值。      当应用于求解最大似然估计的最大值时，变成求解最大似然估计概率导数的问题。      那么迭代公式写作            当θ是向量时，牛顿法可以使用下面式子表示             其中是n×n的Hessian矩阵。      牛顿法收敛速度虽然很快，但求Hessian矩阵的逆的时候比较耗费时间。      当初始点X0靠近极小值X时，牛顿法的收敛速度是最快的。但是当X0远离极小值时，牛顿法可能不收敛，甚至连下降都保证不了。原因是迭代点Xk+1不一定是目标函数f在牛顿方向上的极小点。 11 一般线性模型      之所以在logistic回归时使用            的公式是由一套理论作支持的。      这个理论便是一般线性模型。      首先，如果一个概率分布可以表示成            时，那么这个概率分布可以称作是指数分布。      伯努利分布，高斯分布，泊松分布，贝塔分布，狄特里特分布都属于指数分布。      在logistic回归时采用的是伯努利分布，伯努利分布的概率可以表示成            其中            得到            这就解释了logistic回归时为了要用这个函数。      一般线性模型的要点是      1）  满足一个以为参数的指数分布，那么可以求得的表达式。      2） 给定x，我们的目标是要确定，大多数情况下，那么我们实际上要确定的是，而。（在logistic回归中期望值是，因此h是；在线性回归中期望值是，而高斯分布中，因此线性回归中h=）。      3）  12 Softmax回归      最后举了一个利用一般线性模型的例子。      假设预测值y有k种可能，即y∈{1,2,…,k}      比如k=3时，可以看作是要将一封未知邮件分为垃圾邮件、个人邮件还是工作邮件这三类。      定义            那么            这样            即式子左边可以有其他的概率表示，因此可以当作是k-1维的问题。      为了表示多项式分布表述成指数分布，我们引入T(y)，它是一组k-1维的向量，这里的T(y)不是y，T(y)i表示T(y)的第i个分量。            应用于一般线性模型，结果y必然是k中的一种。1{y=k}表示当y=k的时候，1{y=k}=1。那么p(y)可以表示为            其实很好理解，就是当y是一个值m（m从1到k）的时候，p(y)=，然后形式化了一下。      那么            最后求得            而y=i时            求得期望值      那么就建立了假设函数，最后就获得了最大似然估计      对该公式可以使用梯度下降或者牛顿法迭代求解。      解决了多值模型建立与预测问题。     学习总结      该讲义组织结构清晰，思路独特，讲原因，也讲推导。可贵的是讲出了问题的基本解决思路和扩展思路，更重要的是讲出了为什么要使用相关方法以及问题根源。在看似具体的解题思路中能引出更为抽象的一般解题思路，理论化水平很高。      该方法可以用在对数据多维分析和多值预测上，更适用于数据背后蕴含某种概率模型的情景。 几个问题      一是采用迭代法的时候，步长怎么确定比较好      而是最小二乘法的矩阵形式是否一般都可用","title":"对线性回归，logistic回归和一般回归的认识"},{"content":"转载自：http://leftnoteasy.cnblogs.com 前言：     第二篇的文章中谈到，和部门老大一宁出去outing的时候，他给了我相当多的机器学习的建议，里面涉及到很多的算法的意义、学习方法等等。一宁上次给我提到，如果学习分类算法，最好从线性的入手，线性分类器最简单的就是LDA，它可以看做是简化版的SVM，如果想理解SVM这种分类器，那理解LDA就是很有必要的了。    谈到LDA，就不得不谈谈PCA，PCA是一个和LDA非常相关的算法，从推导、求解、到算法最终的结果，都有着相当的相似。    本次的内容主要是以推导数学公式为主，都是从算法的物理意义出发，然后一步一步最终推导到最终的式子，LDA和PCA最终的表现都是解一个矩阵特征值的问题，但是理解了如何推导，才能更深刻的理解其中的含义。本次内容要求读者有一些基本的线性代数基础，比如说特征值、特征向量的概念，空间投影，点乘等的一些基本知识等。除此之外的其他公式、我都尽量讲得更简单清楚。 LDA：     LDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。有些资料上也称为是Fisher’s Linear Discriminant，因为它被Ronald Fisher发明自1936年，Discriminant这次词我个人的理解是，一个模型，不需要去通过概率的方法来训练、预测数据，比如说各种贝叶斯方法，就需要获取数据的先验、后验概率等等。LDA是在目前机器学习、数据挖掘领域经典且热门的一个算法，据我所知，百度的商务搜索部里面就用了不少这方面的算法。     LDA的原理是，将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(Linear Classifier)：因为LDA是一种线性分类器。对于K-分类的一个分类问题，会有K个线性函数：      当满足条件：对于所有的j，都有Yk > Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的，就是所属的分类了。     上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：      红色的方形的点为0类的原始点、蓝色的方形点为1类的原始点，经过原点的那条线就是投影的直线，从图上可以清楚的看到，红色的点和蓝色的点被原点明显的分开了，这个数据只是随便画的，如果在高维的情况下，看起来会更好一点。下面我来推导一下二分类LDA问题的公式：      假设用来区分二分类的直线（投影函数)为：     LDA分类的一个目标是使得不同类别之间的距离越远越好，同一类别之中的距离越近越好，所以我们需要定义几个关键的值。     类别i的原始中心点为：（Di表示属于类别i的点)     类别i投影后的中心点为：     衡量类别i投影后，类别点之间的分散程度（方差）为：     最终我们可以得到一个下面的公式，表示LDA投影到w后的损失函数：    我们分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。    我们定义一个投影前的各类别分散程度的矩阵，这个矩阵看起来有一点麻烦，其实意思是，如果某一个分类的输入点集Di里面的点距离这个分类的中心店mi越近，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.    带入Si，将J(w)分母化为：    同样的将J(w)分子化为：    这样损失函数可以化成下面的形式：      这样就可以用最喜欢的拉格朗日乘子法了，但是还有一个问题，如果分子、分母是都可以取任意值的，那就会使得有无穷解，我们将分母限制为长度为1（这是用拉格朗日乘子法一个很重要的技巧，在下面将说的PCA里面也会用到，如果忘记了，请复习一下高数），并作为拉格朗日乘子法的限制条件，带入得到：    这样的式子就是一个求特征值的问题了。    对于N(N>2)分类的问题，我就直接写出下面的结论了：    这同样是一个求特征值的问题，我们求出的第i大的特征向量，就是对应的Wi了。    这里想多谈谈特征值，特征值在纯数学、量子力学、固体力学、计算机等等领域都有广泛的应用，特征值表示的是矩阵的性质，当我们取到矩阵的前N个最大的特征值的时候，我们可以说提取到的矩阵主要的成分（这个和之后的PCA相关，但是不是完全一样的概念）。在机器学习领域，不少的地方都要用到特征值的计算，比如说图像识别、pagerank、LDA、还有之后将会提到的PCA等等。    下图是图像识别中广泛用到的特征脸（eigen face），提取出特征脸有两个目的，首先是为了压缩数据，对于一张图片，只需要保存其最重要的部分就是了，然后是为了使得程序更容易处理，在提取主要特征的时候，很多的噪声都被过滤掉了。跟下面将谈到的PCA的作用非常相关。     特征值的求法有很多，求一个D * D的矩阵的时间复杂度是O(D^3), 也有一些求Top M的方法，比如说power method，它的时间复杂度是O(D^2 * M), 总体来说，求特征值是一个很费时间的操作，如果是单机环境下，是很局限的。 PCA：     主成分分析（PCA）与LDA有着非常近似的意思，LDA的输入数据是带标签的，而PCA的输入数据是不带标签的，所以PCA是一种unsupervised learning。LDA通常来说是作为一个独立的算法存在，给定了训练数据后，将会得到一系列的判别函数（discriminate function），之后对于新的输入，就可以进行预测了。而PCA更像是一个预处理的方法，它可以将原本的数据降低维度，而使得降低了维度的数据之间的方差最大（也可以说投影误差最小，具体在之后的推导里面会谈到）。     方差这个东西是个很有趣的，有些时候我们会考虑减少方差（比如说训练模型的时候，我们会考虑到方差-偏差的均衡），有的时候我们会尽量的增大方差。方差就像是一种信仰（强哥的话），不一定会有很严密的证明，从实践来说，通过尽量增大投影方差的PCA算法，确实可以提高我们的算法质量。     说了这么多，推推公式可以帮助我们理解。我下面将用两种思路来推导出一个同样的表达式。首先是最大化投影后的方差，其次是最小化投影后的损失（投影产生的损失最小）。     最大化方差法：     假设我们还是将一个空间中的点投影到一个向量中去。首先，给出原空间的中心点：     假设u1为投影向量，投影之后的方差为：     上面这个式子如果看懂了之前推导LDA的过程，应该比较容易理解，如果线性代数里面的内容忘记了，可以再温习一下，优化上式等号右边的内容，还是用拉格朗日乘子法：     将上式求导，使之为0，得到：     这是一个标准的特征值表达式了，λ对应的特征值，u对应的特征向量。上式的左边取得最大值的条件就是λ1最大，也就是取得最大的特征值的时候。假设我们是要将一个D维的数据空间投影到M维的数据空间中（M < D)， 那我们取前M个特征向量构成的投影矩阵就是能够使得方差最大的矩阵了。     最小化损失法：     假设输入数据x是在D维空间中的点，那么，我们可以用D个正交的D维向量去完全的表示这个空间（这个空间中所有的向量都可以用这D个向量的线性组合得到）。在D维空间中，有无穷多种可能找这D个正交的D维向量，哪个组合是最合适的呢？     假设我们已经找到了这D个向量，可以得到：     我们可以用近似法来表示投影后的点：     上式表示，得到的新的x是由前M 个基的线性组合加上后D - M个基的线性组合，注意这里的z是对于每个x都不同的，而b对于每个x是相同的，这样我们就可以用M个数来表示空间中的一个点，也就是使得数据降维了。但是这样降维后的数据，必然会产生一些扭曲，我们用J描述这种扭曲，我们的目标是，使得J最小：     上式的意思很直观，就是对于每一个点，将降维后的点与原始的点之间的距离的平方和加起来，求平均值，我们就要使得这个平均值最小。我们令：     将上面得到的z与b带入降维的表达式：     将上式带入J的表达式得到：      再用上拉普拉斯乘子法（此处略），可以得到，取得我们想要的投影基的表达式为：     这里又是一个特征值的表达式，我们想要的前M个向量其实就是这里最大的M个特征值所对应的特征向量。证明这个还可以看看，我们J可以化为：     也就是当误差J是由最小的D - M个特征值组成的时候，J取得最小值。跟上面的意思相同。     下图是PCA的投影的一个表示，黑色的点是原始的点，带箭头的虚线是投影的向量，Pc1表示特征值最大的特征向量，pc2表示特征值次大的特征向量，两者是彼此正交的，因为这原本是一个2维的空间，所以最多有两个投影的向量，如果空间维度更高，则投影的向量会更多。   总结：     本次主要讲了两种方法，PCA与LDA，两者的思想和计算方法非常类似，但是一个是作为独立的算法存在，另一个更多的用于数据的预处理的工作。另外对于PCA和LDA还有核方法，本次的篇幅比较大了，先不说了，以后有时间再谈：   参考资料：     prml bishop，introduce to LDA（对不起，这个真没有查到出处）","title":"机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)"},{"content":"转载自：http://leftnoteasy.cnblogs.com 前言：     本来上一章的结尾提到，准备写写线性分类的问题，文章都已经写得差不多了，但是突然听说最近Team准备做一套分布式的分类器，可能会使用Random Forest来做，下了几篇论文看了看，简单的random forest还比较容易弄懂，复杂一点的还会与boosting等算法结合（参见iccv09），对于boosting也不甚了解，所以临时抱佛脚的看了看。说起boosting，强哥之前实现过一套Gradient Boosting Decision Tree（GBDT)算法，正好参考一下。     最近看的一些论文中发现了模型组合的好处，比如GBDT或者rf，都是将简单的模型组合起来，效果比单个更复杂的模型好。组合的方式很多，随机化（比如random forest），Boosting（比如GBDT）都是其中典型的方法，今天主要谈谈Gradient Boosting方法（这个与传统的Boosting还有一些不同）的一些数学基础，有了这个数学基础，上面的应用可以看Freidman的Gradient Boosting Machine。     本文要求读者学过基本的大学数学，另外对分类、回归等基本的机器学习概念了解。     本文主要参考资料是prml与Gradient Boosting Machine。   Boosting方法：     Boosting这其实思想相当的简单，大概是，对一份数据，建立M个模型（比如分类），一般这种模型比较简单，称为弱分类器(weak learner)每次分类都将上一次分错的数据权重提高一点再进行分类，这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的成绩。         上图（图片来自prml p660）就是一个Boosting的过程，绿色的线表示目前取得的模型（模型是由前m次得到的模型合并得到的），虚线表示当前这次模型。每次分类的时候，会更关注分错的数据，上图中，红色和蓝色的点就是数据，点越大表示权重越高，看看右下角的图片，当m=150的时候，获取的模型已经几乎能够将红色和蓝色的点区分开了。     Boosting可以用下面的公式来表示：     训练集中一共有n个点，我们可以为里面的每一个点赋上一个权重Wi(0 <= i < n)，表示这个点的重要程度，通过依次训练模型的过程，我们对点的权重进行修正，如果分类正确了，权重降低，如果分类错了，则权重提高，初始的时候，权重都是一样的。上图中绿色的线就是表示依次训练模型，可以想象得到，程序越往后执行，训练出的模型就越会在意那些容易分错（权重高）的点。当全部的程序执行完后，会得到M个模型，分别对应上图的y1(x)…yM(x)，通过加权的方式组合成一个最终的模型YM(x)。     我觉得Boosting更像是一个人学习的过程，开始学一样东西的时候，会去做一些习题，但是常常连一些简单的题目都会弄错，但是越到后面，简单的题目已经难不倒他了，就会去做更复杂的题目，等到他做了很多的题目后，不管是难题还是简单的题都可以解决掉了。   Gradient Boosting方法：     其实Boosting更像是一种思想，Gradient Boosting是一种Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。这句话有一点拗口，损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错（其实这里有一个方差、偏差均衡的问题，但是这里就假设损失函数越大，模型越容易出错）。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。     下面的内容就是用数学的方式来描述Gradient Boosting，数学上不算太复杂，只要潜下心来看就能看懂：）     可加的参数的梯度表示：     假设我们的模型能够用下面的函数来表示，P表示参数，可能有多个参数组成，P = {p0,p1,p2….}，F(x;P)表示以P为参数的x的函数，也就是我们的预测函数。我们的模型是由多个模型加起来的，β表示每个模型的权重，α表示模型里面的参数。为了优化F，我们就可以优化{β,α}也就是P。     我们还是用P来表示模型的参数，可以得到，Φ(P)表示P的likelihood函数，也就是模型F(x;P)的loss函数，Φ(P)=…后面的一块看起来很复杂，只要理解成是一个损失函数就行了，不要被吓跑了。    既然模型(F(x;P))是可加的，对于参数P，我们也可以得到下面的式子：   这样优化P的过程，就可以是一个梯度下降的过程了，假设当前已经得到了m-1个模型，想要得到第m个模型的时候，我们首先对前m-1个模型求梯度。得到最快下降的方向，gm就是最快下降的方向。     这里有一个很重要的假设，对于求出的前m-1个模型，我们认为是已知的了，不要去改变它，而我们的目标是放在之后的模型建立上。就像做事情的时候，之前做错的事就没有后悔药吃了，只有努力在之后的事情上别犯错：     我们得到的新的模型就是，它就在P似然函数的梯度方向。ρ是在梯度方向上下降的距离。     我们最终可以通过优化下面的式子来得到最优的ρ：     可加的函数的梯度表示：     上面通过参数P的可加性，得到了参数P的似然函数的梯度下降的方法。我们可以将参数P的可加性推广到函数空间，我们可以得到下面的函数，此处的fi(x)类似于上面的h(x;α)，因为作者的文献中这样使用，我这里就用作者的表达方法：     同样，我们可以得到函数F(x)的梯度下降方向g(x)     最终可以得到第m个模型fm(x)的表达式:       通用的Gradient Descent Boosting的框架：    下面我将推导一下Gradient Descent方法的通用形式，之前讨论过的：     对于模型的参数{β,α}，我们可以用下面的式子来进行表示，这个式子的意思是，对于N个样本点(xi,yi)计算其在模型F(x;α,β)下的损失函数，最优的{α,β}就是能够使得这个损失函数最小的{α,β}。 表示两个m维的参数：     写成梯度下降的方式就是下面的形式，也就是我们将要得到的模型fm(x)的参数{αm,βm}能够使得fm的方向是之前得到的模型Fm-1(x)的损失函数下降最快的方向：     对于每一个数据点xi都可以得到一个gm(xi)，最终我们可以得到一个完整梯度下降方向     为了使得fm(x)能够在gm(x)的方向上，我们可以优化下面的式子得到，可以使用最小二乘法：     得到了α的基础上，然后可以得到βm。      最终合并到模型中：     算法的流程图如下      之后，作者还说了这个算法在其他的地方的推广，其中，Multi-class logistic regression and classification就是GBDT的一种实现，可以看看，流程图跟上面的算法类似的。这里不打算继续写下去，再写下去就成论文翻译了，请参考文章：Greedy function Approximation – A Gradient Boosting Machine，作者Freidman。   总结：     本文主要谈了谈Boosting与Gradient Boosting的方法，Boosting主要是一种思想，表示“知错就改”。而Gradient Boosting是在这个思想下的一种函数（也可以说是模型）的优化的方法，首先将函数分解为可加的形式（其实所有的函数都是可加的，只是是否好放在这个框架中，以及最终的效果如何）。然后进行m次迭代，通过使得损失函数在梯度方向上减少，最终得到一个优秀的模型。值得一提的是，每次模型在梯度方向上的减少的部分，可以认为是一个“小”的或者“弱”的模型，最终我们会通过加权(也就是每次在梯度方向上下降的距离）的方式将这些“弱”的模型合并起来，形成一个更好的模型。     有了这个Gradient Descent这个基础，还可以做很多的事情。也在机器学习的道路上更进一步了：）","title":"机器学习中的数学(3)-模型组合(Model Combining)之Boosting与Gradient Boosting"},{"content":"转载自：http://leftnoteasy.cnblogs.com 前言：     距离上次发文章，也快有半个月的时间了，这半个月的时间里又在学习机器学习的道路上摸索着前进，积累了一点心得，以后会慢慢的写写这些心得。写文章是促进自己对知识认识的一个好方法，看书的时候往往不是非常细，所以有些公式、知识点什么的就一带而过，里面的一些具体意义就不容易理解了。而写文章，特别是写科普性的文章，需要对里面的具体意义弄明白，甚至还要能举出更生动的例子，这是一个挑战。为了写文章，往往需要把之前自己认为看明白的内容重新理解一下。     机器学习可不是一个完全的技术性的东西，之前和部门老大在outing的时候一直在聊这个问题，机器学习绝对不是一个一个孤立的算法堆砌起来的，想要像看《算法导论》这样看机器学习是个不可取的方法，机器学习里面有几个东西一直贯穿全书，比如说数据的分布、最大似然（以及求极值的几个方法，不过这个比较数学了），偏差、方差的权衡，还有特征选择，模型选择，混合模型等等知识，这些知识像砖头、水泥一样构成了机器学习里面的一个个的算法。想要真正学好这些算法，一定要静下心来将这些基础知识弄清楚，才能够真正理解、实现好各种机器学习算法。     今天的主题是线性回归，也会提一下偏差、方差的均衡这个主题。 线性回归定义：     在上一个主题中，也是一个与回归相关的，不过上一节更侧重于梯度这个概念，这一节更侧重于回归本身与偏差和方差的概念。     回归最简单的定义是，给出一个点集D，用一个函数去拟合这个点集，并且使得点集与拟合函数间的误差最小。        上图所示，给出一个点集(x,y), 需要用一个函数去拟合这个点集，蓝色的点是点集中的点，而红色的曲线是函数的曲线，第一张图是一个最简单的模型，对应的函数为y = f(x) = ax + b，这个就是一个线性函数，     第二张图是二次曲线，对应的函数是y = f(x) = ax^2 + b。     第三张图我也不知道是什么函数，瞎画的。     第四张图可以认为是一个N次曲线，N = M - 1，M是点集中点的个数，有一个定理是，对于给定的M个点，我们可以用一个M - 1次的函数去完美的经过这个点集。     真正的线性回归，不仅会考虑使得曲线与给定点集的拟合程度最好，还会考虑模型最简单，这个话题我们将在本章后面的偏差、方差的权衡中深入的说，另外这个话题还可以参考我之前的一篇文章：贝叶斯、概率分布与机器学习，里面对模型复杂度的问题也进行了一些讨论。     线性回归(linear regression)，并非是指的线性函数，也就是  （为了方便起见，以后向量我就不在上面加箭头了）     x0,x1…表示一个点不同的维度，比如说上一节中提到的，房子的价钱是由包括面积、房间的个数、房屋的朝向等等因素去决定的。而是用广义的线性函数：      wj是系数，w就是这个系数组成的向量，它影响着不同维度的Φj(x)在回归函数中的影响度，比如说对于房屋的售价来说，房间朝向的w一定比房间面积的w更小。Φ(x)是可以换成不同的函数，不一定要求Φ(x)=x，这样的模型我们认为是广义线性模型。   最小二乘法与最大似然：     这个话题在此处有一个很详细的讨论，我这里主要谈谈这个问题的理解。最小二乘法是线性回归中一个最简单的方法，它的推导有一个假设，就是回归函数的估计值与真实值间的误差假设是一个高斯分布。这个用公式来表示是下面的样子： ，y(x,w)就是给定了w系数向量下的回归函数的估计值，而t就是真实值了，ε表示误差。我们可以接下来推出下面的式子：      这是一个简单的条件概率表达式，表示在给定了x，w，β的情况下，得到真实值t的概率，由于ε服从高斯分布，则从估计值到真实值间的概率也是高斯分布的，看起来像下面的样子：          贝叶斯、概率分布与机器学习这篇文章中对分布影响结果这个话题讨论比较多，可以回过头去看看，由于最小二乘法有这样一个假设，则会导致，如果我们给出的估计函数y(x,w)与真实值t不是高斯分布的，甚至是一个差距很大的分布，那么算出来的模型一定是不正确的，当给定一个新的点x’想要求出一个估计值y’，与真实值t’可能就非常的远了。      概率分布是一个可爱又可恨的东西，当我们能够准确的预知某些数据的分布时，那我们可以做出一个非常精确的模型去预测它，但是在大多数真实的应用场景中，数据的分布是不可知的，我们也很难去用一个分布、甚至多个分布的混合去表示数据的真实分布，比如说给定了1亿篇网页，希望用一个现有的分布（比如说混合高斯分布）去匹配里面词频的分布，是不可能的。在这种情况下，我们只能得到词的出现概率，比如p(的)的概率是0.5，也就是一个网页有1/2的概率出现“的”。如果一个算法，是对里面的分布进行了某些假设，那么可能这个算法在真实的应用中就会表现欠佳。最小二乘法对于类似的一个复杂问题，就很无力了   偏差、方差的权衡(trade-off)：     偏差(bias)和方差(variance)是统计学的概念，刚进公司的时候，看到每个人的嘴里随时蹦出这两个词，觉得很可怕。首先得明确的，方差是多个模型间的比较，而非对一个模型而言的，对于单独的一个模型，比如说:     这样的一个给定了具体系数的估计函数，是不能说f(x)的方差是多少。而偏差可以是单个数据集中的，也可以是多个数据集中的，这个得看具体的定义。     方差和偏差一般来说，是从同一个数据集中，用科学的采样方法得到几个不同的子数据集，用这些子数据集得到的模型，就可以谈他们的方差和偏差的情况了。方差和偏差的变化一般是和模型的复杂程度成正比的，就像本文一开始那四张小图片一样，当我们一味的追求模型精确匹配，则可能会导致同一组数据训练出不同的模型，它们之间的差异非常大。这就叫做方差，不过他们的偏差就很小了，如下图所示：      上图的蓝色和绿色的点是表示一个数据集中采样得到的不同的子数据集，我们有两个N次的曲线去拟合这些点集，则可以得到两条曲线（蓝色和深绿色），它们的差异就很大，但是他们本是由同一个数据集生成的，这个就是模型复杂造成的方差大。模型越复杂，偏差就越小，而模型越简单，偏差就越大，方差和偏差是按下面的方式进行变化的:      当方差和偏差加起来最优的点，就是我们最佳的模型复杂度。      用一个很通俗的例子来说，现在咱们国家一味的追求GDP，GDP就像是模型的偏差，国家希望现有的GDP和目标的GDP差异尽量的小，但是其中使用了很多复杂的手段，比如说倒卖土地、强拆等等，这个增加了模型的复杂度，也会使得偏差（居民的收入分配）变大，穷的人越穷(被赶出城市的人与进入城市买不起房的人），富的人越富（倒卖土地的人与卖房子的人）。其实本来模型不需要这么复杂，能够让居民的收入分配与国家的发展取得一个平衡的模型是最好的模型。     最后还是用数学的语言来描述一下偏差和方差：     E(L)是损失函数，h(x)表示真实值的平均，第一部分是与y（模型的估计函数）有关的，这个部分是由于我们选择不同的估计函数（模型）带来的差异，而第二部分是与y无关的，这个部分可以认为是模型的固有噪声。     对于上面公式的第一部分，我们可以化成下面的形式：     这个部分在PRML的1.5.5推导，前一半是表示偏差，而后一半表示方差，我们可以得出：损失函数=偏差^2+方差+固有噪音。     下图也来自PRML：     这是一个曲线拟合的问题，对同分布的不同的数据集进行了多次的曲线拟合，左边表示方差，右边表示偏差，绿色是真实值函数。ln lambda表示模型的复杂程度，这个值越小，表示模型的复杂程度越高，在第一行，大家的复杂度都很低（每个人都很穷）的时候，方差是很小的，但是偏差同样很小（国家也很穷），但是到了最后一幅图，我们可以得到，每个人的复杂程度都很高的情况下，不同的函数就有着天壤之别了（贫富差异大），但是偏差就很小了（国家很富有）。","title":"机器学习中的数学(2)-线性回归，偏差、方差权衡"},{"content":"SCI或SCIE收录的本学科刊物清单请登陆 http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=K 和http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=D 下面列一些CS的顶级会议和期刊    有些是网上查到的，有些是某些人用SCI的IF排序做出来的： Computer Vision Conf.:   Best:     ICCV, Inter. Conf. on Computer Vision     CVPR, Inter. Conf. on Computer Vision and Pattern Recognition   Good:     ECCV, Euro. Conf. on Comp. Vision     ICIP, Inter. Conf. on Image Processing     ICPR, Inter. Conf. on Pattern Recognition     ACCV, Asia Conf. on Comp. Vision Computer Vision  Jour.:   Best:     PAMI, IEEE Trans. on Patt. Analysis and Machine Intelligence     IJCV, Inter. Jour. on Comp. Vision   Good:     CVIU, Computer Vision and Image Understanding PR, Pattern Reco. Network Conf.:     ACM/SigCOMM     ACM Special Interest Group of Communication     ACM/SigMetric Info Com Globe Com Network Jour.:     ToN (ACM/IEEE Transaction on Network) A.I.Conf.:     AAAI: American Association for Artificial Intelligence     ACM/SigIR IJCAI: International Joint Conference on Artificial Intelligence     NIPS: Neural Information Processing Systems     ICML: International Conference on Machine Learning A.I.Jour.:     Machine Learning     NEURAL COMPUTATION     ARTIFICIAL INTELLIGENCE PAMI     IEEE TRANSACTIONS ON FUZZY SYSTEMS     IEEE TRANSACTIONS ON NEURAL NETWORKS AI MAGAZINE     NEURAL NETWORKS     PATTERN RECOGNITION     IMAGE AND VISION COMPUTING     IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING     APPLIED INTELLIGENCE OS,System Conf.:     SOSP: The ACM Symposium on Operating Systems Principles     OSDI: USENIX Symposium on Operating Systems Design and Implementation Database Conf.:     ACM SIGMOD     VLDB:International Conference on Very Large Data Bases     ICDE:International Conference on Data Engineering Security Conf.:     IEEE Security and Privacy     CCS: ACM Computer and Communications Security NDSS (Network and Distributed Systems Security) Web Conf.:     WWW(International World Wide Web Conference) Theory Conf.:     STOC FOCS EDA Conf.: Best:     DAC: IEEE/ACM Design Automation Conference     ICCAD: IEEE International Conference on Computer Aided Design Good:     ISCAS: IEEE International Symposium on Circuits And Systems     ISPD: IEEE International Symposium on Physical Design     ICCD: IEEE International Conference on Computer Design     ASP-DAC: European Design Automation Conference     E-DAC: Asia and South Pacific Design Automation Conference Graphics Conf.:   Best:     Siggraph: ACM SigGraph   Good:     Euro Graph Jour.: IEEE(ACM) Trans. on Graphics     IEEE Trans. on Visualization and Computer Graphics CAD    Jour.: CAD CAGD Softe Engineering: conf.:     ICSE The International Conference on Software Engineering     FSE The Foundations of Software Engineering Conferences     ICASE IEEE International Conference on Automated Software Engineering     COMPSAC International Computer Software and Applications Conferences     ESEC The European Software Engineering Conferences Jour.:     SEN ACM SIGSOFT Software Engineering Notes     TSE IEEE Transactions on Software Engineering     ASE Automated Software Engineering SPE Software-Practice and Experience   EI收录的中国期刊：    来自http://www.ei.org.cn/twice/coverage.jsp ISSN 期 刊 名 相关链接 0567-7718 Acta Mechanica Sinica   1006-7191 Acta Metallurgica Sinica (English Letters)   0253-4827 Applied Mathematics and Mechanics (English Edition)   0890-5487 China Ocean Engineering   1004-5341 China Welding   1004-9541 Chinese Journal of Chemical Engineering   1022-4653 Chinese Journal of Electronics   1000-9345 Chinese Journal of Mechanical Engineering (English Edition) 学报网站 1671-7694 Chinese Optics Letters 学报网站 1673-7350 Frontiers of Computer Science in China 期刊网址 1006-6748 High Technology Letters   1674-4799 International Journal of Minerals, Metallurgy and Materials   1004-0579 Journal of Beijing Institute of Technology (English Edition) 学报编辑部 1005-9784 Journal of Central South University of Technology 学报网站 1672-5220 Journal of Donghua University (English Edition)   1005-9113 Journal of Harbin Institute of Technology (New Series)   1001-6058 Journal of Hydrodynamics   1005-0302 Journal of Materials Science and Technology   1002-0721 Journal of Rare Earths   1674-4926 Journal of Semiconductors 学报编辑部 1007-1172 Journal of Shanghai Jiaotong University (Science)   1003-7985 Journal of Southeast University (English Edition)   1004-4132 Journal of Systems Engineering and Electronics   1009-6124 Journal of Systems Science and Complexity   1003-2169 Journal of Thermal Science   1000-2413 Journal of Wuhan University of Technology -Materials Science Edition   1673-565X Journal of Zhejiang University SCIENCE A   1674-5264 Mining Science and Technology   1001-0521 Rare Metals   1006-9291 Science in China, Series B: Chemistry   1672-1799 Science in China, Series G: Physics, Astronomy   1005-8885 The Journal of China Universities of Posts and Telecommunications   1005-1120 Transactions of Nanjing University of Aeronautics and Astronautics   1003-6326 Transactions of Nonferrous Metals Society of China   1006-4982 Transactions of Tianjin University   1007-0214 Tsinghua Science and Technology Editor Information 1001-1455 爆炸与冲击   0254-0037 北京工业大学学报   1001-5965 北京航空航天大学学报 学报编辑部 1001-053X 北京科技大学学报 学报编辑部 1001-0645 北京理工大学学报 学报编辑部 1007-5321 北京邮电大学学报 学报编辑部 1000-1093 兵工学报   1001-4381 材料工程   1005-0299 材料科学与工艺   1009-6264 材料热处理学报 学报网站 1005-3093 材料研究学报   1001-1595 测绘学报 学报编辑部 1007-7294 船舶力学   1000-8608 大连理工大学学报   1004-499X 弹道学报   1000-2383 地球科学 学报网站 1005-0388 电波科学学报   1000-6753 电工技术学报   1007-449X 电机与控制学报   1000-1026 电力系统自动化 学报网站 1006-6047 电力自动化设备   1001-0548 电子科技大学学报   0372-2112 电子学报   1009-5896 电子与信息学报   1005-3026 东北大学学报　(自然科学版)   1001-0505 东南大学学报 (自然科学版)   1000-3851 复合材料学报   1003-6520 高电压技术   1000-7555 高分子材料科学与工程   1002-0470 高技术通讯   1003-9015 高校化学工程学报   1000-5773 高压物理学报   1000-4750 工程力学   0253-231X 工程热物理学报   1001-9731 功能材料 学报网站 1006-2793 固体火箭技术   0254-7805 固体力学学报   1005-0086 光电子.激光   1000-0593 光谱学与光谱分析   1004-924X 光学精密工程 学报网站 0253-2239 光学学报 学报网站 0454-5648 硅酸盐学报   1001-2486 国防科技大学学报   1006-7043 哈尔滨工程大学学报 学报网站 0367-6234 哈尔滨工业大学学报   0253-360X 焊接学报   1005-5053 航空材料学报   1000-8055 航空动力学报 编辑部网站 1000-6893 航空学报 学报网站 0258-0926 核动力工程   1001-9014 红外与毫米波学报   1000-2472 湖南大学学报 (自然科学版)   1000-565X 华南理工大学学报(自然科学版) 编辑部网站 1671-4512 华中科技大学学报(自然科学版)   0438-1157 化工学报   1002-0446 机器人 学报网站 0577-6686 机械工程学报 学报网站 1671-5497 吉林大学学报(工学版) 学报编辑部 1003-9775 计算机辅助设计与图形学学报   1006-5911 计算机集成制造系统 编辑部网站 0254-4164 计算机学报   1000-1239 计算机研究与发展 学报网站 1007-4708 计算力学学报   1001-246X 计算物理   1007-9629 建筑材料学报   1000-6869 建筑结构学报   1671-7775 江苏大学学报（自然科学版）   1009-3443 解放军理工大学学报（自然科学版）   0412-1961 金属学报   0258-1825 空气动力学学报   1000-8152 控制理论与应用 学报网站 1001-0920 控制与决策   0459-1879 力学学报 学报网站 0253-9993 煤炭学报 学报网站 1003-6059 模式识别与人工智能   1004-0595 摩擦学学报   1672-6030 纳米技术与精密工程   1005-2615 南京航空航天大学学报   1005-9830 南京理工大学学报 (自然科学版)   1000-0925 内燃机工程   1000-0909 内燃机学报   1002-6819 农业工程学报 学报编辑部 1000-1298 农业机械学报 学报编辑部 1001-4322 强激光与粒子束 学报编辑部 1000-0054 清华大学学报 (自然科学版)   0253-2409 燃料化学学报   1006-8740 燃烧科学与技术   1000-985X 人工晶体学报 无机材料期刊网 1000-9825 软件学报 学报编辑部 1006-2467 上海交通大学学报   1000-2618 深圳大学学报（理工版）   0371-0025 声学学报   1000-7210 石油地球物理勘探   1000-0747 石油勘探与开发   0253-2697 石油学报   1001-8719 石油学报:石油加工 学报网站 1672-9897 实验流体力学 学报网站 1001-6791 水科学进展   0559-9350 水利学报 学报编辑部 1003-1243 水力发电学报   1009-3087 四川大学学报(工程科学版) 学报编辑部 0254-0096 太阳能学报 学报编辑部 0493-2137 天津大学学报 学报编辑部 1001-8360 铁道学报   1000-436X 通信学报   0253-374X 同济大学学报 (自然科学版)   1000-131X 土木工程学报 学报网站 1674-4764 土木建筑与环境工程 学报编辑部 1001-4055 推进技术   1000-324X 无机材料学报   1671-8860 武汉大学学报(信息科学版)   1001-2400 西安电子科技大学学报 学报网站 0253-987X 西安交通大学学报   1000-2758 西北工业大学学报   0258-2724 西南交通大学学报 学报网站 1002-185X 稀有金属材料与工程   1000-6788 系统工程理论与实践   1001-506X 系统工程与电子技术   1007-8827 新型炭材料   1000-6915 岩石力学与工程学报 学报网站 1000-4548 岩土工程学报   1000-7598 岩土力学 期刊编辑部 0254-3087 仪器仪表学报   1005-0930 应用基础与工程科学学报 学报网站 1000-6931 原子能科学技术   1008-973X 浙江大学学报 (工学版)   1672-7126 真空科学与技术学报   1004-6801 振动测试与诊断   1004-4523 振动工程学报   1000-3835 振动与冲击 学报网站 0258-8013 中国电机工程学报   1001-7372 中国公路学报   0258-7025 中国激光 学报网站 1000-1964 中国矿业大学学报   1673-5005 中国石油大学学报 (自然科学版)   1001-4632 中国铁道科学   1004-0609 中国有色金属学报 学报网站 1672-7207 中南大学学报（自然科学版） 学报网站 0254-4156 自动化学报 学报网站     中科院计算所推荐国际会议   序号 会议名称 会议介绍 代表领域 1 ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左 右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录 用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。< /p> 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。 GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千 人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会 议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左 右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一 次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一 次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu.  http://hpdc13.cs.ucsb.edu 高性能计算 42  NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/  高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications  高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括 technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing  该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems  由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing  This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下 半年排名。 高性能计算 48 ACM International Conference on Supercomputing  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing  IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 　 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57  FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶 尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59  SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60  IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62  IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等 概念。 自主计算 63  Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64  International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65  [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66  IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67  USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68  IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69  International Symposium on Computer Architecture（ISCA ） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很 难 系统结构 70  International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72  IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73  Annual ACM International Conference on Supercomputing（ICS）  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 74  Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其 困难 操作系统 75  ACM Symposium on Operating Systems Principles （SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要 中极其困难 操作系统 76  Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困 难 操作系统，程序语言 77  Workshop on Hot Topics in Operating Systems （HOTOS） 操作系统最好的会议之一 操作系统 78  Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79  Annual IEEE Conference on Local Computer Networks（LCN） 　 网络 80  International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影 响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收 率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会 议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影 响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收 率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。< /p> 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统 等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影 响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术 121 ECOOP - European Conference on Object-Oriented Programming 关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。 编译技术 122 ESOP - European Symposium on Programming ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。< /p> 编译技术 123 Euro-Par - European Conference on Parallel Computing 关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。 　 124 SAS - International Static Analysis Symposium 关注程序的静态分析的权威会议。 编译技术 125 CAV - Computer Aided Verification Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。 编译技术 126 FASE - Fundamental Approaches to Software Engineering ETAPS的member conference，主要关注Software Science，影响因子0.91。 编译技术 127 TACAS - Tools and Algorithms for the Construction and Analysis of Systems ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24 编译技术 128 VMCAI - Verification, Model Checking and Abstract Interpretation Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。 编译技术 129  ACL: The Association for Computational Linguistics 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。 人工智能 计算语言学 130 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索方面最好的会议, ACM 主办, 每年开。19％左右 信息检索技术 131 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining 数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右 　 132 WWW: The ACM International World Wide Web Conference 应用和媒体领域顶级国际会议 万维网 133 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库领域顶级国际 数据管理 134 CIKM: The ACM Conference on Information and Knowledge Management 数据库领域知名国际会议 数据管理 135 COLING: International Conference on Computational Linguistics 计算语言学知名国际会议 计算语言学 136 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 137 IEEE ICDM: International Conference on Data Mining 数据挖掘领域顶级国际会议 　 138 IJCAI: International Joint Conference on Artificial Intelligence 人工智能领域顶级国际会议，论文接受率18％左右 人工智能 139 VLDB: The ACM International Conference on Very Large Data Bases 数据库领域顶级国际 数据库 140 SIGGRAPH: ACM SIGGRAPH Conference 计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％ 计算机图形学 141 EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics 欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20% 计算机图形学 142 AAAI: American Association for Artificial Intelligence 美国人工智能学会AAAI的年会，使该领域的顶级会议 人工智能 143 ACM Conference　on　Computer and　Communications Security ACM通信和计算健全领域顶级学术会议 信息安全 144 ACM SIGCOMM: Special Interest Group on Data Communications 　 数据通信 145 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索领域的重要会议 信息检索 146 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。 通信与网络 147 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems ACM性能建模与评价领域顶级学术会议 通信与网络 148 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。 数据管理 149 ASPLOS: Architectural Support for Programming Languages and Operating Systems 编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。 体系结构 150 CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid 网格计算国际会议，网格平台、中间件 　 151 CIKM: The ACM Conference on Information and Knowledge Management 信息检索领域的会议，录用率为15% 信息检索 152 CLUSTER - IEEE International Conference on Cluster Computing 集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。 集群计算 153 CPM: Combinatorial Pattern Matching Symposium 组合模式匹配年会，是字符串匹配、模式匹配较好的会议。 模式匹配 154 FAST4: Third USENIX Conference on File and Storage Technologies, USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。 文件与存储 155 Grid : IEEE/ACM International Workshop on Grid Computing 网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。 网格计算 156 HPC: IEEE International Conference for High Performance Computing 　 　 157 HPDC: International Symposium on High Performance Distributed Computing 高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。< /p> 高性能计算 158 ICDCS: IEEE International Conference on Distributed Computing Systems IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS 举办了26届，录用率为15%。 分布式计算 159 ICML: International Conference on Machine Learning 机器学习领域中的顶级会议 机器学习 160 ICWS: IEEE International Conference on Web Services Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。 Web服务 161 IEEE CSB: Computer Society Bioinformatics 　 　 162 IEEE ICDM: International Conference on Data Mining 数据挖掘领域的著名会议，率用率为14%。 数据挖掘 163 IEEE ICNP: International Conference on Network Protocols IEEE 网络通信领域顶级学术会议，录用率在10%左右。 网络 164 IEEE ICON: IEEE International Conference on Networks 　 　 165 IEEE INFOCOM: conference on computer communications IEEE网络通信领域著名会议，领域广泛。 网络 166 IEEE IPCCC: International Performance Computing and Communications Conference IEEE性能领域著名学术会议，主要关注性能评价。 网络性能 167 IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval 字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。 字符串处理信息检索 168 IJCAI: International Joint Conference on AI 人工智能领域的顶级会议。 人工智能 169 IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。 170 International Symposium on High Performance Computer Architecture(HPCA) 　 服务计算 171 IPDPS: IEEE International Parallel and Distributed Processing Symposium IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%， 会议的优秀论文可能发表于JDPC杂志上。 并行计算分布式计算 172 ISMB: International conference on Intelligent Systems for Molecular Biology 　 无线 173 MobiCom: ACM/IEEE Conference on Mobile Computing and Networking 始于1995，无线、移动计算方面比较有历史和重要的会议。 安全 174 MobiSys: The International Conference on Mobile Systems, Applications, and Services 无线方面，2006年第4名。 无线 175 OSDI: USENIX Symposium on Operating Systems Design and Implementation USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。 操作系统 176 PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining 　 　 177 PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies 关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。 并行计算分布式计算 178 PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases 数据挖掘领域的重要会议，录用率为14%。 数据挖掘 179 SCC: IEEE International Conference on Services Computing 服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。< /p> 服务计算 180 SDM: SIAM International Conference on Data Mining 数据挖掘领域的重要会议，录用率为14% 数据挖据 181 SOSE: IEEE International Workshop on Service-Oriented System Engineering 　 　 182 USENIX Sec: USENIX Security Symposium USENIX安全领域重要会议，侧重安全技术。 安全 183 USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI) USENIX网络领域重要会议，设计网络涉及各方面内容。 网络 184 VLDB: The ACM International Conference on Very Large Data Bases 　 数据管理 185 WWW: The ACM International World Wide Web Conference ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。 Internet 186 RAID International Symposium on Recent Advances in Intrusion Detection 数据库顶级国际会议 　 187 IJCAI: International Joint Conference on Artificial Intelligence 人工智能顶级国际会议 人工智能 188 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据库 189 ICML: International Conference on Machine Learning 机器学习顶级国际会议 机器学习 190 PRICAI: Pacific Rim International Conference on Artificial Intelligence 亚太人工智能国际会议 人工智能 191 IFIP ICIIP: IFIP International Conference on Intelligent Information Processing IFIP智能信息处理国际会议 智能信息处理 192 NIPS: Neural Information Processing Systems 神经信息处理领域顶级国际会议 神经计算，机器学习 193 ISCA: International Symposium on Computer Architecture   体系结构领域的顶级会议 微处理器设计 194 International Symposium on Microarchitecture 体系结构领域的顶级会议 微处理器设计 195 HPCA：International Symposium on High Performance Computer Architecture 体系结构领域的顶级会议 微处理器设计 196 APCSAC: Asia-Pacific Computer Systems Architecture Conference  体系结构方面的重要会议 微处理器设计 197 ISLPED: International Symposium on Low Power Electronics and Design 低功耗设计的重要会议 微处理器设计 198 OSDI: Operation System Design & Implementation  操作系统方面的重要会议 微处理器设计 199 ASPLOS: Architecture Support for Programming Languages and Operation  体系结构方面的顶尖会议 微处理器设计 200 ICCD: IEEE International Conference on Computer Design 体系结构方面的顶尖会议 微处理器设计 201 DAC: Design Automation Conference  设计自动化领域的顶级会议 微处理器设计 202 IEEE/ACM International Conference on Computer Aided Design(ICCAD) 集成电路设计自动化方面的顶尖会议 微处理器设计 203 ASP-DAC: Asia and South Pacific Design Automation Conference  设计自动化领域的重要会议 微处理器设计 204 ISSCC: IEEE International Solid-State Circuits Conference 设计自动化领域的重要会议 微处理器设计 205 CICC: Custom Integrated Circuits Conference 集成电路设计方面的顶尖会议（公认排名第二） 微处理器设计 206 ESSCIRC: European Conference on Solid-State Circuits 集成电路设计方面的顶尖会议 微处理器设计 207 Symposium on VLSI Circuits 集成电路设计方面的顶尖会议 微处理器设计 208 IEEE International ASIC/SOC Conference 集成电路设计方面的重要会议 微处理器设计 209 Symposium on VLSI Technology 集成电路设计方面的重要会议 微处理器设计 210 ASSCC: Asian Conference on Solid-State Circuits 集成电路领域重要会议 微处理器设计 211 MWSCAS: Midwest Symposium on Circuits and Systems 集成电路领域重要会议 微处理器设计 212 ICECS: IEEE International Conference on Electronics, Circuits and Systems： 集成电路领域重要会议 微处理器设计 213 ISCAS: International Symposium Circuit and System  电路与系统方面的重要会议 微处理器设计 214 RFIC: IEEE Symposium on Radio Frequency Integrated Circuits  射频集成电路领域顶尖会议 微处理器设计 215 ACM RECOMB: Int. Conference on Research in Computational Molecular Biology RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在 20%左右。 计算生物学 216 IEEE CSB: Computer Society Bioinformatics CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办。 计算生物学 217 International Conference of the IEEE Engineering in Medicine and Biology Society （EMBC） 生物信息学与计算生物学国际研讨会 计算生物学 218 PSB: Pacific Symposium on Biocomputing  PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举 办。 计算生物学 219 WABI:Workshop on Algorithms in Bioinformatics WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方 面。 计算生物学 220 CSB: IEEE Computational Systems Bioinformatics Conference  CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办 计算生物学 221 ISMB：Annual International Conference on Intelligent Systems for Molecular Biology ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为 Bioinformatics杂志的专刊发表，近年来录取率为15%左右。 生物信息 222 ECCB：European Conference on Computational Biology ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为 Bioinformatics杂志的专刊发表，近年录取率在20%左右。 生物信息 223 APBC：Asia Pacific Bioinformatics Conference APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。< /p> 生物信息 224 COCOON：Annual International Computing and Combinatorics Conference COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左 右。 生物信息 225 CPM：Annual Symposium on Combinatorial Pattern Matching CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左 右。 生物信息 226 EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千 计。 生物信息 227 Geospatial Information and Technology Association（GITA） Annual Conference 地球空间信息与技术协会年会 遥感与空间信息处理 228 International Geoscience and Remote Sensing Symposium （IGRSS） 地球科学与遥感国际会议 遥感与空间信息处理 229 International Society for Photogrammetry and Remote Sensing （ISPRS）Technical Commission Symposium 国际摄影测量与遥感学会专业委员会会议 遥感与空间信息处理 230 International Conference on Geoinformatics 地球信息国际会议 遥感与空间信息处理 231 IEEE SKG (Semantics, Knowledge and Grid) 由计算所发起的IEEE国际会议，每年有100人参加。 知识网格 232 WWW: The ACM International World Wide Web Conference Internet领域顶级国际会议 Internet 233 International Semantic Web Conference Semantic Web领域顶级会议，录用率17% Semantic Web 234 ACM SIGMOD: ACM SIGMOD Conf on Management of Data ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。 数据管理 235 ACM PODS Conference ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。 数据管理 236 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据管理 237 IEEE ICDE - International Conference on Data Engineering 数据库顶级国际会议 数据管理","title":"期刊会议"},{"content":"首先什么是中文分词stop word？  英文是以词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。例如，英文句子I am a student，用中文则为：“我是一个学生”。计算机可以很简单通过空格知道student是一个单词，但是不能很容易明白“学”、“生”两个字合起来才表示一个词。把中文的汉字序列切分成有意义的词，就是中文分词，有些人也称为切词。我是一个学生，分词的结果是：我是 一个 学生。 其次中文分词和搜索引擎关系与影响！ 中文分词到底对搜索引擎有多大影响？对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。笔者最近替朋友找一些关于日本和服的资料，在搜索引擎上输入“和服”，得到的结果就发现了很多问题。 小谈：中文分词技术 中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。 现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 1、基于字符串匹配的分词方法 这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下： 1）正向最大匹配法（由左到右的方向）； 2）逆向最大匹配法（由右到左的方向）； 3）最少切分（使每一句中切出的词数最小）。 还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。 一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。 对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。 2、基于理解的分词方法 这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。 3、基于统计的分词方法 从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。笔者了解，海量科技的分词算法就采用“复方分词法”，所谓复方，相当于用中药中的复方概念，即用不同的药才综合起来去医治疾病，同样，对于中文词的识别，需要多种算法来处理不同的问题。 分词中的难题 有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。 1、歧义识别 歧义是指同样的一句话，可能有两种或者更多的切分方法。例如：表面的，因为“表面”和“面的”都是词，那么这个短语就可以分成“表面的”和“表面的”。这种称为交叉歧义。像这种交叉歧义十分常见，前面举的“和服”的例子，其实就是因为交叉歧义引起的错误。“化妆和服装”可以分成“化妆和 服装”或者“化妆 和服 装”。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。 交叉歧义相对组合歧义来说是还算比较容易处理，组合歧义就必需根据整个句子来判断了。例如，在句子“这个门把手坏了”中，“把手”是个词，但在句子“请把手拿开”中，“把手”就不是一个词；在句子“将军任命了一名中将”中，“中将”是个词，但在句子“产量三年中将增长两倍”中，“中将”就不再是词。这些词计算机又如何去识别? 如果交叉歧义和组合歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：“乒乓球拍卖完了”，可以切分成“乒乓球拍 卖 完 了”、也可切分成“乒乓球 拍卖 完 了”，如果没有上下文其他的句子，恐怕谁也不知道“拍卖”在这里算不算一个词。 2、新词识别 新词，专业术语称为未登录词。也就是那些在字典中都没有收录过，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解句子“王军虎去广州了”中， “王军虎”是个词，因为是一个人的名字，但要是让计算机去识别就困难了。如果把“王军虎”做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子“王军虎头虎脑的”中，“王军虎”还能不能算词？ 新词中除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。 中文分词的应用 目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。 分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有了。科研院校研究的技术，大部分不能很快产品化，而一个专业公司的力量毕竟有限，看来中文分词技术要想更好的服务于更多的产品，还有很长一段路要走。。。 一、什么是停用词？ 停用词(Stop Words) ，词典译为“电脑检索中的虚字、非检索用字”。在SEO中，为节省存储空间和提高搜索效率，搜索引擎在索引页面或处理搜索请求时会自动忽略某些字或词，这些字或词即被称为Stop Words(停用词)。 停用词一定程度上相当于过滤词(Filter Words)，不过过滤词的范围更大一些，包含黄色、政治等敏感信息的关键词都会被视做过滤词加以处理，停用词本身则没有这个限制。通常意义上，停用词(Stop Words)大致可分为如下两类： 1、使用十分广泛，甚至是过于频繁的一些单词。比如英文的“i”、“is”、“what”，中文的“我”、“就”之类词几乎在每个文档上均会出现，查询这样的词搜索引擎就无法保证能够给出真正相关的搜索结果，难于缩小搜索范围提高搜索结果的准确性，同时还会降低搜索的效率。因此，在真正的工作中，Google和百度等搜索引擎会忽略掉特定的常用词，在搜索的时候，如果我们使用了太多的停用词，也同样有可能无法得到非常精确的结果，甚至是可能大量毫不相关的搜索结果。 2、文本中出现频率很高，但实际意义又不大的词。这一类主要包括了语气助词、副词、介词、连词等，通常自身并无明确意义，只有将其放入一个完整的句子中才有一定作用的词语。如常见的“的”、“在”、“和”、“接着”之类，比如“SEO研究院是原创的SEO博客”这句话中的“是”、“的”就是两个停用词。 二、Stop Words对SEO的影响 文档中如果大量使用Stop words容易对页面中的有效信息造成噪音干扰，所以搜索引擎在运算之前都要对所索引的信息进行消除噪音的处理。了解了Stop Words，在网页内容中适当地减少停用词出现的频率，可以有效地帮助我们提高关键词密度，在网页标题标签中避免出现停用词能够让所优化的关键词更集中、更突出。","title":"中文分词与停用词的作用"},{"content":"  将聚类结果展示在网页上 分类： 机器学习与数据挖掘2011-08-31 22:17 282人阅读 评论(2) 收藏 举报 Nutch中自带对搜索结果的聚类，使用开源的Carrot2，以插件形式被调用，大概看了一下nutch关于clustering这一块的搜索源码，它会显示出URL和title，可是用mahout做文本聚类的话，最后的聚类结果中，只有向量， 当然自己可以将URL加进去，但如何显示标题呢？title不是存放在parse_text中的，标题是存放在parse_data中的，难不成再反过来根据URL去查parse_data，可以调nutch的命令根据URL查，返回来的估计是个String，这样还要在这个String里去匹配title字段，这个过程中是不是复杂了点呢?parse_data还存放每个URL解析出的外部连接和元数据Metadata 看了一下抓取网页和抓取pdf文件后生成的parse_data，抓取网页后的parse_data中有标题，即网页源码中的<title>标签包含的字段，而抓取pdf文件后生成的parse_data中，title字段为空，而在其对应的parse_text中，发现pdf文档的标题被当作正文放在parse_text中了 看nutch对搜索结果的聚类，每个簇都有个标签，一个簇下有属于这个簇的点，在对应簇下显示出相应点的标题及URL。如果是用mahout聚类后，取不到这个值，这个可以仔细看一下nutch搜索源码，看它是如何做到的，然后看能不能用mahout实现一下。同时建议看一下topic model，在机器学习和自然语言处理领域，topic model指一种统计模型，用来从一批文档的集合中发现抽象的主题/论题。 在nutch对结果搜索聚类中是用HitsCluster.getDescriptionLabels() 来获得聚类的标签的，文档标题是通过HitsCluster.getHits().getValue(\"title\")来获得，这个只是粗浅的看，具体实现细节还不知道，还需要时间继续向下挖掘，看它是如何利用Carrot2来做搜索结果聚类的 好了，上述提的问题，等后面有时间再看，现在言归正传，如何将mahout的聚类结果通过网页来展示呢？如果能在eclipse将聚类结果成功打在控制台中，那么把它移到web容器中，如tomcat，这个就不是难事了 现假设调用mahout kmens命令生成的聚类结果在本地，那么问题就转化成如何读取这些聚类结果文件，前面文章里有对聚类结果文件介绍，有兴趣的可以去找找。读取聚类结果，大家可以参照mahout clusterdump这个命令的源文件是如何读取的，本人主要是在读取过程中遇到了少包的问题，现记录下来，以便后面查看 需要在tomcat的web项目下的WEB-INF/lib中，因为页面中引用了一些类，根据页面提示，导入hadoop-core-0.20.2.jar，mahout-core-0.4.jar，mahout-math-0.4.jar，mahout-utils-0.4.jar，此时页面没有错误提示，但一运行会报错，依次根据报的提示，导入commons-logging-1.1.1.jar，mahout-collections-1.0.jar，gson-1.3.jar，google-collections-1.0-rc2.jar，commons-cli-2.0-mahout.jar，slf4j-api-1.6.0.jar，slf-jcl-1.6.0.jar，完成后，终于在网页上看到了聚类结果   Nutch中自带对搜索结果的聚类，使用开源的Carrot2，以插件形式被调用，大概看了一下nutch关于clustering这一块的搜索源码，它会显示出URL和title，可是用mahout做文本聚类的话，最后的聚类结果中，只有向量， 当然自己可以将URL加进去，但如何显示标题呢？title不是存放在parse_text中的，标题是存放在parse_data中的，难不成再反过来根据URL去查parse_data，可以调nutch的命令根据URL查，返回来的估计是个String，这样还要在这个String里去匹配title字段，这个过程中是不是复杂了点呢?parse_data还存放每个URL解析出的外部连接和元数据Metadata 看了一下抓取网页和抓取pdf文件后生成的parse_data，抓取网页后的parse_data中有标题，即网页源码中的<title>标签包含的字段，而抓取pdf文件后生成的parse_data中，title字段为空，而在其对应的parse_text中，发现pdf文档的标题被当作正文放在parse_text中了 看nutch对搜索结果的聚类，每个簇都有个标签，一个簇下有属于这个簇的点，在对应簇下显示出相应点的标题及URL。如果是用mahout聚类后，取不到这个值，这个可以仔细看一下nutch搜索源码，看它是如何做到的，然后看能不能用mahout实现一下。同时建议看一下topic model，在机器学习和自然语言处理领域，topic model指一种统计模型，用来从一批文档的集合中发现抽象的主题/论题。 在nutch对结果搜索聚类中是用HitsCluster.getDescriptionLabels() 来获得聚类的标签的，文档标题是通过HitsCluster.getHits().getValue(\"title\")来获得，这个只是粗浅的看，具体实现细节还不知道，还需要时间继续向下挖掘，看它是如何利用Carrot2来做搜索结果聚类的 好了，上述提的问题，等后面有时间再看，现在言归正传，如何将mahout的聚类结果通过网页来展示呢？如果能在eclipse将聚类结果成功打在控制台中，那么把它移到web容器中，如tomcat，这个就不是难事了 现假设调用mahout kmens命令生成的聚类结果在本地，那么问题就转化成如何读取这些聚类结果文件，前面文章里有对聚类结果文件介绍，有兴趣的可以去找找。读取聚类结果，大家可以参照mahout clusterdump这个命令的源文件是如何读取的，本人主要是在读取过程中遇到了少包的问题，现记录下来，以便后面查看 需要在tomcat的web项目下的WEB-INF/lib中，因为页面中引用了一些类，根据页面提示，导入hadoop-core-0.20.2.jar，mahout-core-0.4.jar，mahout-math-0.4.jar，mahout-utils-0.4.jar，此时页面没有错误提示，但一运行会报错，依次根据报的提示，导入commons-logging-1.1.1.jar，mahout-collections-1.0.jar，gson-1.3.jar，google-collections-1.0-rc2.jar，commons-cli-2.0-mahout.jar，slf4j-api-1.6.0.jar，slf-jcl-1.6.0.jar，完成后，终于在网页上看到了聚类结果  ","title":"mahout 将聚类结果展示在网页上 将聚类结果展示在网页上"},{"content":" http://blog.csdn.net/kauu/article/details/1867677 分类： 我的体验 lucene/nutch 搜索引擎2007-11-05 14:58 1623人阅读 评论(5) 收藏 举报     参考：http://lotusroots.bokee.com/6106980.html     了解nutch的人基本上对这个开源的系统都是比较欣赏的，起码在国内是这样的，也很有多搜索网站是基于这个系统修改过来的，不过要做得好，做得真正是一个商业化的搜索，这个修改就不是一朝一夕的事情，也不是修修剪剪那么简单了。     作为一个通用的全网级别的搜索引擎架构，nutch(lucene)确实为广大人民群众提供了一块大大的蛋糕，为进入搜索这个行业大大降低了门槛。那么它距商业的搜索到底有多远呢？以我的个人观点来谈一下。 一、总体功能      一个专业的网络搜索引擎至少包含3部分即抓取、处理和搜索。下面是它们的一般功能： 抓取：抓取（蜘蛛、爬虫、crawler、spider等）程序负责爬行特定网络（也可能是整个网络），把网络上的页面和其它需要的文件下载到本地来。目前的难点是web2.0的普及导致的js分析和身份认证等问题。 处理：处理（分类、信息抽取、数据挖掘、classify、information extraction、data mining等）程序对抓回来的页面进行分析，比如，对网站的内容进行分类、对新闻页面的新闻信息进行提取、页面模版生成、对各个网站之间的关系进行计算等等。 搜索：搜索（information retrieve）程序负责把文档填充到数据库，然后根据查询字符串到数据库中寻找最相关的文档展现给用户。    二、信息抓取     网络信息抓取包含了见面抓取、文本文件抓取和其它文件地抓取。普通的信息抓取利用基本的html页面分析器(htmlParser、NeckoHtml、 JTidy等)来解析页面，得到其中的信息。基本上两点，一个是抓取，一个是分析。抓取这一步要处理身份论证、要支持多种协议等等的，nutch在这里默 认的插件使用的是nekohtml，效果还可以。但是nutch对html分析的结果的文本是把页面里所有的文本都合在一起(其中有一个开关来控制内层锚文本是否加上)作为总文本输出，所以这样页面上所有的噪音都没有去除。另一个是分析，分析一个html，最强的要数ie、firefox等浏览器了，在这一步上，nutch默认的htmlParser的处理能力是不可同日而语的。现在ajax盛行，对于js的处理也是一个重大的问题，现在nutch对js是视而不见的。 三、信息处理     对于信息的处理是nutch最薄弱的环节了，同时也是这个行业里的“宝地”，胜败决定就在这里。这里包括分类、信息抽取、数据挖掘、classify、information extraction、data mining等等，在默认的nutch组件里有cluster这个包，是用来为搜索结果进行聚类用的,还有ontology (本体),是人工智能范畴内的概念。nutch为这方面准备了最最基本的接口，其它的就得自己搞定了，比如机器学习(ML)、自然语言处理(NLP)、数据分析(DA)。nutch默认的聚类是用开源Carrot2 的后缀树算法做Web文本聚类。而Ontology研究热点的出现与Semantic Web的提出和发展直接相关，借助Ontology中的推理规则，使应用系统具有一定的推理能力。默认的nutch也带了一个简单的ontology的应用系统--HP的jena。但是对于一个商业应用来说这些仅仅是一个模具性质的。 四、搜索     nutch其实从功能上来讲是由爬虫和搜索两大部分组成的，搜索是lucene来挑梁的。所以这部分的局限其实就是lucene的局限了。lucene也 可能从功能上分为两大部分，一个是索引，一个是查询。对于这部分的研究已经很久了，就是把用户最想要的文档返回给用户，对于搜索引擎而言，速度是非常重要 的。索引，专业点说，包含2种：前向索引和反向索引（倒排索引，inverted index）。前者表示的是某个文档里面的所有词语，后者表示的是包含某个词语的所有文档。对应到Lucene上面，它的前向索引可以认为是Term Vectors（词语向量）相关文件，包含.tvx、.tvd和.tvf这3种文件。前向索引没有什么好评论的，它一般只是做为重组原始数据时候的依据，其构建十分简单明了。反向索引对应到Lucene上就是index（索引）。Lucene把索引划分成一个一个的segment（块，其实是一个小索引），直观的说，当有一批新数据到达的时候，我们一般给其构建成一个新的segment，这是因为修改原来的segment的代价很高（并不是说一定很高，只是lucene采用的文件结构无法简单的加入新的文档）。当一个index包含的segment太多的时候，查找性能就很差了（因为一次查询需要查询多个segment），需要进行segment的合并。在搜索方面nutch对lucene作了外部的处理，一是可以进行分布式搜索，每个节点只返回最高分值的结果，最近再合并；另一方面是对查询进行缓冲，不过只有一级缓冲--LRU(nutch的cache策略及cache策略研究)。 五、结论       仅仅从搜索引擎的构架来看，Lucene(Nutch)缺失的一环是信息的处理。信息的处理恰恰是整个搜索引擎中最核心的技术。所以说对于现在这个行业化、垂直化的搜索时代，nutch 的先天不足就已经是致命的了，但是这并不是不可挽回的，nutch的插件式架构，开放的系统逻辑等等特点，已经为开发者打开了窗户。nutch比较通用的 处理逻辑，加上灵活的插件式架构，给我们定制它插上了翅膀。但是它也仅仅是一个框架，里面的任何一个细节都会让你头痛不已(比如ML，NLP)!所以真正 的难点也就是这些让人头痛的地方。     参考：http://lotusroots.bokee.com/6106980.html     了解nutch的人基本上对这个开源的系统都是比较欣赏的，起码在国内是这样的，也很有多搜索网站是基于这个系统修改过来的，不过要做得好，做得真正是一个商业化的搜索，这个修改就不是一朝一夕的事情，也不是修修剪剪那么简单了。     作为一个通用的全网级别的搜索引擎架构，nutch(lucene)确实为广大人民群众提供了一块大大的蛋糕，为进入搜索这个行业大大降低了门槛。那么它距商业的搜索到底有多远呢？以我的个人观点来谈一下。 一、总体功能      一个专业的网络搜索引擎至少包含3部分即抓取、处理和搜索。下面是它们的一般功能： 抓取：抓取（蜘蛛、爬虫、crawler、spider等）程序负责爬行特定网络（也可能是整个网络），把网络上的页面和其它需要的文件下载到本地来。目前的难点是web2.0的普及导致的js分析和身份认证等问题。 处理：处理（分类、信息抽取、数据挖掘、classify、information extraction、data mining等）程序对抓回来的页面进行分析，比如，对网站的内容进行分类、对新闻页面的新闻信息进行提取、页面模版生成、对各个网站之间的关系进行计算等等。 搜索：搜索（information retrieve）程序负责把文档填充到数据库，然后根据查询字符串到数据库中寻找最相关的文档展现给用户。    二、信息抓取     网络信息抓取包含了见面抓取、文本文件抓取和其它文件地抓取。普通的信息抓取利用基本的html页面分析器(htmlParser、NeckoHtml、 JTidy等)来解析页面，得到其中的信息。基本上两点，一个是抓取，一个是分析。抓取这一步要处理身份论证、要支持多种协议等等的，nutch在这里默 认的插件使用的是nekohtml，效果还可以。但是nutch对html分析的结果的文本是把页面里所有的文本都合在一起(其中有一个开关来控制内层锚文本是否加上)作为总文本输出，所以这样页面上所有的噪音都没有去除。另一个是分析，分析一个html，最强的要数ie、firefox等浏览器了，在这一步上，nutch默认的htmlParser的处理能力是不可同日而语的。现在ajax盛行，对于js的处理也是一个重大的问题，现在nutch对js是视而不见的。 三、信息处理     对于信息的处理是nutch最薄弱的环节了，同时也是这个行业里的“宝地”，胜败决定就在这里。这里包括分类、信息抽取、数据挖掘、classify、information extraction、data mining等等，在默认的nutch组件里有cluster这个包，是用来为搜索结果进行聚类用的,还有ontology (本体),是人工智能范畴内的概念。nutch为这方面准备了最最基本的接口，其它的就得自己搞定了，比如机器学习(ML)、自然语言处理(NLP)、数据分析(DA)。nutch默认的聚类是用开源Carrot2 的后缀树算法做Web文本聚类。而Ontology研究热点的出现与Semantic Web的提出和发展直接相关，借助Ontology中的推理规则，使应用系统具有一定的推理能力。默认的nutch也带了一个简单的ontology的应用系统--HP的jena。但是对于一个商业应用来说这些仅仅是一个模具性质的。 四、搜索     nutch其实从功能上来讲是由爬虫和搜索两大部分组成的，搜索是lucene来挑梁的。所以这部分的局限其实就是lucene的局限了。lucene也 可能从功能上分为两大部分，一个是索引，一个是查询。对于这部分的研究已经很久了，就是把用户最想要的文档返回给用户，对于搜索引擎而言，速度是非常重要 的。索引，专业点说，包含2种：前向索引和反向索引（倒排索引，inverted index）。前者表示的是某个文档里面的所有词语，后者表示的是包含某个词语的所有文档。对应到Lucene上面，它的前向索引可以认为是Term Vectors（词语向量）相关文件，包含.tvx、.tvd和.tvf这3种文件。前向索引没有什么好评论的，它一般只是做为重组原始数据时候的依据，其构建十分简单明了。反向索引对应到Lucene上就是index（索引）。Lucene把索引划分成一个一个的segment（块，其实是一个小索引），直观的说，当有一批新数据到达的时候，我们一般给其构建成一个新的segment，这是因为修改原来的segment的代价很高（并不是说一定很高，只是lucene采用的文件结构无法简单的加入新的文档）。当一个index包含的segment太多的时候，查找性能就很差了（因为一次查询需要查询多个segment），需要进行segment的合并。在搜索方面nutch对lucene作了外部的处理，一是可以进行分布式搜索，每个节点只返回最高分值的结果，最近再合并；另一方面是对查询进行缓冲，不过只有一级缓冲--LRU(nutch的cache策略及cache策略研究)。 五、结论       仅仅从搜索引擎的构架来看，Lucene(Nutch)缺失的一环是信息的处理。信息的处理恰恰是整个搜索引擎中最核心的技术。所以说对于现在这个行业化、垂直化的搜索时代，nutch 的先天不足就已经是致命的了，但是这并不是不可挽回的，nutch的插件式架构，开放的系统逻辑等等特点，已经为开发者打开了窗户。nutch比较通用的 处理逻辑，加上灵活的插件式架构，给我们定制它插上了翅膀。但是它也仅仅是一个框架，里面的任何一个细节都会让你头痛不已(比如ML，NLP)!所以真正 的难点也就是这些让人头痛的地方。","title":"Nutch距离一个商业应用的搜索引擎还有多远"},{"content":"1，两个字符串比较得操作： delete ：cost 1；insert cost 1；substutition cost 2； 2，计算字符串最短距离 动态规划算法：D(i,j) = min(D(i-1, j)+1, D(i, j-1)+1, D(i-1, j-1){if S1(i) == S2(j) then 0 else 2})； 3，BackTrace，可以计算出具体每个字符得操作是什么； 4,   MInimum Edit computational biology: F(i, j) = max(0, F(i-1,j) - d, F(i,j-1) -d , F(i-1,j-1)+S(xi,yj)); And BackTrace, you will look up for longest sun string;so it's best result;","title":"自然语言处理之字符串距离"},{"content":"将OpenCV2.3中的HOG抽取出来 http://www.zhizhihu.com/html/y2011/3451.html CVPR 2012 papers http://www.zhizhihu.com/html/y2012/3765.html 2011年度图灵奖依旧人工智能 刘江CE ： 最新一届图灵奖（2011年）颁发给了UCLA的Judea Pearl教授（75岁）。奖励他在人工智能领域的基础性贡献，他提出概率和因果性推理演算法，彻底改变了人工智能最初基于规则和逻辑的方向。 solidot:美国计算机协会宣布2011年度图灵奖得主为加州洛杉矶分校（UCLA）的计算机科学家Judea Pearl教授。 Judea Pearl出生于以色 列，本科毕业于以色列理工学院，1965年在美国罗格斯大学获物理学硕士学位，同年在布鲁克林理工学院获得电机工程博士学位。他的儿子是《华尔街日报》记 者，2002年被巴基斯坦激进分子绑架杀害。Pearl教授的研究领域是人工智能，他是最早将贝叶斯网络和概率方法引入人工智能的先锋之一，也是在经验科 学中数学化因果模型的先锋。他的研究为iPhone的Siri语音识别和Google的无人驾驶汽车奠定了基础。 Judea Pearl United States – 2011 CITATIONFor fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning.记得2010年度的图灵奖得主Leslie Valiant是ML方向，也算人工智能方向相关吧：瓦里安是哈佛大学计算机科学和应用数学教授，一直是围绕一些当今尖端技术进行开发的主要参与者。他在人工智能、自然语言处理和手写识别等大量革新技术上发挥了重要影响。特别是他发表的题为“学习能力理论”的论文，对计算学习理论起到巨大影响。 关于“SVD奇异值分解（Singular value decomposition）”的几篇博文 ICCV 2011 一篇文章：Ensemble of Exemplar-SVMs Ensemble of Exemplar-SVMs for Object Detection and Beyond，还没有细读，但是看了题目我就吸引住了，因为和自己正在思考的一个idea类似，也是用模板集成的方法，只是搜索模板的方法可能不同，集成方法可能不同，但是我要继续思考下要不要做，还有没有真是的以及和价值。 TimeHandle's Blog：hog+svm_行人检测matlab程序 http://www.zhizhihu.com/html/y2012/3729.html 用String Kernel-字符串核函数 String Kernel是这样一种Kernel方法，它根据两个字符串的所有公共子串计算它们的相似度，最简单的方法是用Dynamic Programming的办法，但复杂度较高，是N的平方。通过使用Suffix Tree或Suffix Array可以成功地把复杂度降为线性的，参见论文：eprints.pascal-network.org/archive/00002056/01/VisSmo04.pdf . http://www.zhizhihu.com/html/y2011/2993.html LeftNotEasy写的理解SVM的博文 http://www.zhizhihu.com/html/y2011/2964.html 在新的划时代的机器学习框架诞生之前，关注下“集成学习(Ensemble Learning)” http://www.zhizhihu.com/html/y2009/482.html ICML2007上的两篇文章：Boosting for transfer learning和Self-taught learning http://www.zhizhihu.com/html/y2011/3573.html [技术前瞻] 未来人机交互方式 http://www.zhizhihu.com/html/y2010/1464.html 神奇的图像处理算法 http://www.zhizhihu.com/html/y2011/3287.html 关于MINDS研讨会的讨论及报告下载 http://www.zhizhihu.com/html/y2011/3394.html 一个用BoW|Pyramid BoW+SVM进行图像分类的Matlab Demo http://www.zhizhihu.com/html/y2011/3536.html 差不多了，剩下的自己看了","title":"一个网友收集的科研资源--内容很多，转载麻烦"},{"content":"第一个是“人工智能的历史”（History of Artificial Intelligence），我在讨论组上写道： 而今天看到的这篇文章是我在 wikipedia 浏览至今觉得最好的。文章名为《人工智能的历史》，顺着 AI 发展时间线娓娓道来，中间穿插无数牛人故事，且一波三折大气磅礴，可谓\"事实比想象更令人惊讶\"。人工智能始于哲学思辨，中间经历了一个没有心理学（尤其是认知神经科学的）的帮助的阶段，仅通过牛人对人类思维的外在表现的归纳、内省，以及数学工具进行探索，其间最令人激动的是 Herbert Simon （决策理论之父，诺奖，跨领域牛人）写的一个自动证明机，证明了罗素的数学原理中的二十几个定理，其中有一个定理比原书中的还要优雅，Simon 的程序用的是启发式搜索，因为公理系统中的证明可以简化为从条件到结论的树状搜索（但由于组合爆炸，所以必须使用启发式剪枝）。后来 Simon 又写了 GPS （General Problem Solver），据说能解决一些能良好形式化的问题，如汉诺塔。但说到底 Simon 的研究毕竟只触及了人类思维的一个很小很小的方面 —— Formal Logic，甚至更狭义一点 Deductive Reasoning （即不包含 Inductive Reasoning , Transductive Reasoning (俗称 analogic thinking）。还有诸多比如 Common Sense、Vision、尤其是最为复杂的 Language 、Consciousness 都还谜团未解。还有一个比较有趣的就是有人认为 AI 问题必须要以一个物理的 Body 为支撑，一个能够感受这个世界的物理规则的身体本身就是一个强大的信息来源，基于这个信息来源，人类能够自身与时俱进地总结所谓的 Common-Sense Knowledge （这个就是所谓的 Emboddied  Mind 理论。 ），否则像一些老兄直接手动构建 Common-Sense Knowledge Base ，就很傻很天真了，须知人根据感知系统从自然界获取知识是一个动态的自动更新的系统，而手动构建常识库则无异于古老的 Expert System 的做法。当然，以上只总结了很小一部分我个人觉得比较有趣或新颖的，每个人看到的有趣的地方不一样，比如里面相当详细地介绍了神经网络理论的兴衰。所以我强烈建议你看自己一遍，别忘了里面链接到其他地方的链接。 顺便一说，徐宥同学打算找时间把这个条目翻译出来，这是一个相当长的条目，看不动 E 文的等着看翻译吧:) 第二个则是“人工智能”（Artificial Intelligence）。当然，还有机器学习等等。从这些条目出发能够找到许多非常有用和靠谱的深入参考资料。   然后是一些书籍 书籍： 1. 《Programming Collective Intelligence》，近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的:P 2. Peter Norvig 的《AI, Modern Approach 2nd》（无争议的领域经典）。 3. 《The Elements of Statistical Learning》，数学性比较强，可以做参考了。 4. 《Foundations of Statistical Natural Language Processing》，自然语言处理领域公认经典。 5. 《Data Mining, Concepts and Techniques》，华裔科学家写的书，相当深入浅出。 6. 《Managing Gigabytes》，信息检索好书。 7. 《Information Theory：Inference and Learning Algorithms》，参考书吧，比较深。 相关数学基础（参考书，不适合拿来通读）： 1. 线性代数：这个参考书就不列了，很多。 2. 矩阵数学：《矩阵分析》，Roger Horn。矩阵分析领域无争议的经典。 3. 概率论与统计：《概率论及其应用》，威廉·费勒。也是极牛的书，可数学味道太重，不适合做机器学习的。于是讨论组里的 Du Lei 同学推荐了《All Of Statistics》并说到 机器学习这个方向，统计学也一样非常重要。推荐All of statistics，这是CMU的一本很简洁的教科书，注重概念，简化计算，简化与Machine Learning无关的概念和统计内容，可以说是很好的快速入门材料。 4. 最优化方法：《Nonlinear Programming, 2nd》非线性规划的参考书。《Convex Optimization》凸优化的参考书。此外还有一些书可以参考 wikipedia 上的最优化方法条目。要深入理解机器学习方法的技术细节很多时候（如SVM）需要最优化方法作为铺垫。   王宁同学推荐了好几本书： 《Machine Learning, Tom Michell》, 1997.  老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能\"新\"到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。 《Modern Information Retrieval, Ricardo Baeza-Yates et al》. 1999  老书，牛人。貌似第一本完整讲述IR的书。可惜IR这些年进展迅猛，这本书略有些过时了。翻翻做参考还是不错的。另外，Ricardo同学现在是Yahoo Research for Europe and Latin Ameria的头头。 《Pattern Classification (2ed)》, Richard O. Duda, Peter E. Hart, David G. Stork  大约也是01年左右的大块头，有影印版，彩色。没读完，但如果想深入学习ML和IR，前三章（介绍，贝叶斯学习，线性分类器）必修。 还有些经典与我只有一面之缘，没有资格评价。另外还有两本小册子，论文集性质的，倒是讲到了了不少前沿和细节，诸如索引如何压缩之类。可惜忘了名字，又被我压在箱底，下次搬家前怕是难见天日了。 （呵呵，想起来一本：《Mining the Web – Discovering Knowledge from Hypertext Data》） 说一本名气很大的书：《Data Mining: Practical Machine Learning Tools and Techniques》。Weka 的作者写的。可惜内容一般。理论部分太单薄，而实践部分也很脱离实际。DM的入门书已经不少，这一本应该可以不看了。如果要学习了解 Weka ，看文档就好。第二版已经出了，没读过，不清楚。   信息检索方面，Du Lei 同学再次推荐： 信息检索方面的书现在建议看Stanford的那本《Introduction to Information Retrieval》，这书刚刚正式出版，内容当然up to date。另外信息检索第一大牛Croft老爷也正在写教科书，应该很快就要面世了。据说是非常pratical的一本书。 对信息检索有兴趣的同学，强烈推荐翟成祥博士在北大的暑期学校课程，这里有全slides和阅读材料：http://net.pku.edu.cn/~course/cs410/schedule.html maximzhao 同学推荐了一本机器学习： 加一本书：Bishop, 《Pattern Recognition and Machine Learning》. 没有影印的，但是网上能下到。经典中的经典。Pattern Classification 和这本书是两本必读之书。《Pattern Recognition and Machine Learning》是很新（07年），深入浅出，手不释卷。   最后，关于人工智能方面（特别地，决策与判断），再推荐两本有意思的书， 一本是《Simple Heuristics that Makes Us Smart》 另一本是《Bounded Rationality: The Adaptive Toolbox》 不同于计算机学界所采用的统计机器学习方法，这两本书更多地着眼于人类实际上所采用的认知方式，以下是我在讨论组上写的简介： 这两本都是德国ABC研究小组（一个由计算机科学家、认知科学家、神经科学家、经济学家、数学家、统计学家等组成的跨学科研究团体）集体写的，都是引起领域内广泛关注的书，尤其是前一本，後一本则是对 Herbert Simon （决策科学之父，诺奖获得者）提出的人类理性模型的扩充研究），可以说是把什么是真正的人类智能这个问题提上了台面。核心思想是，我们的大脑根本不能做大量的统计计算，使用fancy的数学手法去解释和预测这个世界，而是通过简单而鲁棒的启发法来面对不确定的世界（比如第一本书中提到的两个后来非常著名的启发法：再认启发法（cognition heuristics）和选择最佳（Take the Best）。当然，这两本书并没有排斥统计方法就是了，数据量大的时候统计优势就出来了，而数据量小的时候统计方法就变得非常糟糕；人类简单的启发法则充分利用生态环境中的规律性（regularities），都做到计算复杂性小且鲁棒。 关于第二本书的简介： 1. 谁是 Herbert Simon 2. 什么是 Bounded Rationality 3. 这本书讲啥的： 我一直觉得人类的决策与判断是一个非常迷人的问题。这本书简单地说可以看作是《决策与判断》的更全面更理论的版本。系统且理论化地介绍人类决策与判断过程中的各种启发式方法（heuristics）及其利弊 （为什么他们是最优化方法在信息不足情况下的快捷且鲁棒的逼近，以及为什么在一些情况下会带来糟糕的后果等，比如学过机器学习的都知道朴素贝叶斯方法在许多情况下往往并不比贝叶斯网络效果差，而且还速度快；比如多项式插值的维数越高越容易overfit，而基于低阶多项式的分段样条插值却被证明是一个非常鲁棒的方案）。 在此提一个书中提到的例子，非常有意思：两个团队被派去设计一个能够在场上接住抛过来的棒球的机器人。第一组做了详细的数学分析，建立了一个相当复杂的抛物线近似模型（因为还要考虑空气阻力之类的原因，所以并非严格抛物线），用于计算球的落点，以便正确地接到球。显然这个方案耗资巨大，而且实际运算也需要时间，大家都知道生物的神经网络中生物电流传输只有百米每秒之内，所以 computational complexity 对于生物来说是个宝贵资源，所以这个方案虽然可行，但不够好。第二组则采访了真正的运动员，听取他们总结自己到底是如何接球的感受，然后他们做了这样一个机器人：这个机器人在球抛出的一开始一半路程啥也不做，等到比较近了才开始跑动，并在跑动中一直保持眼睛于球之间的视角不变，后者就保证了机器人的跑动路线一定会和球的轨迹有交点；整个过程中这个机器人只做非常粗糙的轨迹估算。体会一下你接球的时候是不是眼睛一直都盯着球，然后根据视线角度来调整跑动方向？实际上人类就是这么干的，这就是 heuristics 的力量。 相对于偏向于心理学以及科普的《决策与判断》来说，这本书的理论性更强，引用文献也很多而经典，而且与人工智能和机器学习都有交叉，里面也有不少数学内容，全书由十几个章节构成，每个章节都是由不同的作者写的，类似于 paper 一样的，很严谨，也没啥废话，跟 《Psychology of Problem Solving》类似。比较适合 geeks 阅读哈。 另外，对理论的技术细节看不下去的也建议看看《决策与判断》这类书（以及像《别做正常的傻瓜》这样的傻瓜科普读本），对自己在生活中做决策有莫大的好处。人类决策与判断中使用了很多的 heuristics ，很不幸的是，其中许多都是在适应几十万年前的社会环境中建立起来的，并不适合于现代社会，所以了解这些思维中的缺点、盲点，对自己成为一个良好的决策者有很大的好处，而且这本身也是一个非常有趣的领域。","title":"人工智能资料"},{"content":"基于LDA的Topic Model变形 最近几年来，随着LDA的产生和发展，涌现出了一批搞Topic Model的牛人。我主要关注了下面这位大牛和他的学生： David M. Blei LDA的创始者，04年博士毕业。一篇关于Topic Model的博士论文充分体现其精深的数学概率功底；而其自己实现的LDA又可体现其不俗的编程能力。说人无用，有论文为证： J. Chang and D. Blei. Relational Topic Models for Document Networks. Artificial Intelligence and Statistics, 2009. [PDF]        基本LDA模型，当然假设文档之间是可交换的，那么在原始的LDA中文档之间其实是认为条件独立的。而在实际情况中，往往不是这个样子的，文档间也许会存 在“social network”的这样的网络性质。如何结合内容和“social network”这两个特征也许是一个非常有意思的话题。这篇论文就是给出了一个解决方法。它为两个文档之间增加了一个二元随机变量，根据其内容特征，来 刻画这种隐含的链接关系。        关于显示的链接关系是过去今年内，人们追逐研究的对象，进而产生PageRank、HITS等等一大批优秀的链接关系算法。那么如何利用隐含的链接呢？什 么是隐含的链接呢？一个最简单的隐含链接就是基于内容相似度构建的图。这个被人们用的不亦乐乎，比如在文摘中的LexRank等。O Kurland在SIGIR中发了两篇大概都是类似的文章，本质思想貌似就是在利用内容之间的“超链接”。        另外一个比较新颖的研究点，就是如何基于“social network”来挖掘内容特征？ Mei Qiaozhu的一篇论文就是利用“social network”的网络结构特征最为规则化因子，重新修正了原始的PLSA模型。想法非常的新颖。 D. Blei and J. Lafferty. Topic Models. In A. Srivastava and M. Sahami, editors, Text Mining: Theory and Applications. Taylor and Francis, in press. [PDF]     这篇论文是一篇综述性的大制作的论文，Blei在里面深入浅出的介绍了什么是Topic Model以及他早期的一些Topic Model的变形。值得大家去阅读。 J. Boyd-Graber and D. Blei. Syntactic Topic Models. Neural Information Processing Systems, 2009. [PDF] [Supplement]    原始的LDA考察两个词只是基于共现的角度。而实际情况中，这种共现往往是不能够精确地刻画一些句子结构信息或者说词义信息。如何把这种信息引入。考虑 更深层的生成模型是目前一个热点。这篇论文着眼于一个句子的句法分析的生成过程，它认为每个句子的生成都是基于“parse tree”的，整个概率生成过程完全附着在“parse tree”上了。并且每个句子内，不同的词都有可能去选择更适合自己的Topic。     D. Blei, J. McAuliffe. Supervised topic models. In Advances in Neural Information Processing Systems 21, 2007. [PDF] [digg data]    现如今，网络数据除了纯内容外，往往还有其他一写辅助信息，如用户对于某博文的评价或者说用户对于某商品的评价。一个最典型的例子，就是说在当当买书 后，你可以给该书的质量进行打分：5星代表最好，4星代表比较好，。。。依次类推。那么如何把这些信息加入原始的LDA中呢？ Blei为其引入了一个response变量因子，该因子条件依赖于该文档的topic distribution。     如何把ratable information和内容有机地结合起来也是最近的一个研究热点。大多数方法还都是，建立一个ratable response variable，然后该变量条件依赖于内容或者说Topic信息。 J. Boyd-Graber, D. Blei, and X. Zhu. A topic model for word sense disambiguation. In Empirical Methods in Natural Language Processing, 2007. [PDF]     这篇论文对应的一个大背景是把Topic Model应用到自然语言处理中，具体内容我没太看，主要是结合了WordNet的结构特征，在此基础上产生的图模型。     此外的一些工作还有把Topic Model用来文摘和词性标注中的。应用到这些问题的两个主要思路：第一个就是用Topic Model去学习出一些compact features，然后在次基础上利用分类器等机器学习方法；另外一种就是利用原始NLP问题的一些结构信息，比如刚才所说的WordNet中的网络结 构，在这个结构特征中推导出整个图模型的概率生成过程。 D. Blei and J. Lafferty. A correlated topic model of Science. Annals of Applied Statistics. 1:1 17–35. [PDF] [shorter version from NIPS 18] [code][browser]    还没有认真看，这个其实打破了原来topic之间的可交换性。 D. Blei and J. Lafferty. Dynamic topic models. In Proceedings of the 23rd International Conference on Machine Learning, 2006. [PDF]    也没有仔细看，把Topic Model和时间维度结合了起来。Mei Qiaozhu也有一篇是研究话题内容随着时间变化的论文，但是是基于PLSI和HMM来完成的。      T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. Integrating topics and syntax. In Advances in Neural Information Processing Systems 17, 2005. [PDF]    这篇论文是一篇非常优秀的论文，开篇详细地叙述了词的不同功能分类，也叫做HMM-LDA模型。正如每个人存在都有其社会意义，那么词存在对于文本语义 的表述也有着不同的角色。作者把词分为了两大功能：第一个就是semantic功能，也就是之前我们所有的Topic word；另一个功能就是说语法功能，也就是说这些词的存在是为了让整个句子的生成过程看起来更像一个完整体或者说更符合语言规范。T. Griffiths和M. Steyvers是两个很优秀的学者，他们开发了topic model工具包，并且也有一堆的牛论文。 D. Blei. Probabilistic Models of Text and Images. PhD thesis, U.C. Berkeley, Division of Computer Science, 2004. [PDF]    Blei的博士论文，我至今还没有看完，因为一直纠结在那个Varitional inference的推导。自己责备一下自己。 D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, January 2003. [A shorter version appeared in NIPS 2002]. [PDF] [code]         LDA的第一篇文章，不算很好读懂。初次阅读时，一般会遇到可交换性、variational inference、simplex等等细节问题。经典中的经典。 D. Blei and P. Moreno. Topic segmentation with an aspect hidden Markov model. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 343–348. ACM Press, 2001. [PDF]    SIGIR中的一篇关于分段的论文。其实分段这个事情在现实中需求量比较大，但是成熟的工具包并不多，或者我不知道。比较好的成熟的算法一般还是基于语 义斜率的变化来计算的。在次召唤下懂这方面的大牛推荐几个好用的工具。与分段关联很紧密的一个问题就是网页正文抽取，同样也是这个问题，发论文的多，但是 实际release出来代码的很少。比较著名的，如VIPS，但是我没有用过。昨天发现VIPS的作者原来也是一个巨牛的中国人，Deng Cai。之前是清华学生，现在师从Jiawei Han，各种牛会议和牛期刊发了N多的文章。在此膜拜一下。 总结        目前我能看懂的Topic Model的文章还是很少一部分，自己的概率和数学基础太差，对于posterior inference往往无能为力，这也是下一步我的目标。并且自己其实也不太会创新，下一步也是要在这个方面多下功夫，争取应用Topic Model来解决自己的实际问题。","title":"基于LDA的Topic Model变形"},{"content":"特别推荐： 1、HMM学习最佳范例全文文档 2、无约束最优化全文文档 一、书籍： 1、《自然语言处理综论》英文版第二版 2、《统计自然语言处理基础》英文版 3、《用Python进行自然语言处理》，NLTK配套书 4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦 5、《自然语言处理中的模式识别》 6、《EM算法及其扩展》 7、《统计学习基础》 8、《自然语言理解》英文版（似乎只有前9章） 9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner； 10、概率统计经典入门书：《概率论及其应用》（英文版，威廉*费勒著） 　　第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要） 11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》 12、国外机器学习书籍之： 　1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习&数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的” 　2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。豆瓣评论 by王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。 　3) “Introduction to Machine Learning” 13、国外数据挖掘书籍之： 　1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍 作者 : Jiawei Han/Micheline Kamber 出版社 : Morgan Kaufmann 评语 : 华裔科学家写的书，相当深入浅出。 　2) Data Mining:Practical Machine Learning Tools and Techniques 　3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher） 14、国外模式识别书籍之： 　1）“Pattern Recognition” 　2）“Pattern Recongnition Technologies and Applications” 　3）“An Introduction to Pattern Recognition” 　4）“Introduction to Statistical Pattern Recognition” 　5）“Statistical Pattern Recognition 2nd Edition” 　6）“Supervised and Unsupervised Pattern Recognition” 　7）“Support Vector Machines for Pattern Classification” 15、国外人工智能书籍之： 　1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。 　2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP” 16、其他相关书籍： 　1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor 　2）Learning.Python第四版，英文 二、课件： 1、哈工大刘挺老师的“统计自然语言处理”课件； 2、哈工大刘秉权老师的“自然语言处理”课件； 3、中科院计算所刘群老师的“计算语言学讲义“课件； 4、中科院自动化所宗成庆老师的“自然语言理解”课件； 5、北大常宝宝老师的“计算语言学”课件； 6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码； 7、MIT Regina Barzilay教授的“自然语言处理”课件，52nlp上翻译了前5章； 8、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件； 9、Michael Collins的“Machine Learning （机器学习）”课件； 10、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件； 11、Philipp Koehn “Empirical Methods in Natural Language Processing”课件； 12、Philipp Koehn“Machine Translation（机器翻译）”课件； 三、语言资源和开源工具： 1、Brown语料库： 　a) XML格式的brown语料库，带词性标注； 　b) 普通文本格式的brown语料库，带词性标注； 　c) 合并并去除空行、行首空格，用于词性标注训练：browntest.zip 2、NLTK官方提供的语料库资源列表 3、OpenNLP上的开源自然语言处理工具列表 4、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表” 5、LDC上免费的中文信息处理资源 6、中文分词相关工具： 　1）Java版本的MMSEG：mmseg-v0.3.zip，作者为solol，详情可参见：《中文分词入门之篇外》 　2）张华平老师的ICTCLAS2010，该版本非商用免费一年，下载地址： http://cid-51de2738d3ea0fdd.skydrive.live.com/self.aspx/.Public/ICTCLAS2010-packet-release.rar 7、热心读者“finallyliuyu”提供的一批新闻语料库，包括腾讯，新浪，网易，凤凰等，目前放在CSDN上：http://finallyliuyu.download.csdn.net/ 　　另外finalllyliuyu在2010年9月又提供了一批文本文类语料，详情见：献给热衷于自然语言处理的业余爱好者的中文新闻分类语料库之二 四、文献： 1、ACL-IJCNLP 2009论文全集： 　a) 大会论文Full Paper第一卷 　b) 大会论文Full Paper第二卷 　c) 大会论文Short Paper合集 　d) ACL09之EMNLP-2009合集 　e) ACL09 所有workshop论文合集","title":"[置顶] 自然语言处理相关书籍及其他资源"},{"content":"下面的资料没有做优先级排序。 文章： CSDN的推荐引擎主题资料汇集页面：http://subject.csdn.net/tuijian/ 推荐《探索推荐引擎的秘密》系列 http://tiny4.org/blog/2011/05/recommend-enginee/  该文作者为iApp4me创始人，文中提到几个开源推荐引擎项目 探索推荐引擎内部的秘密系列：http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy1/index.html 基于 Apache Mahout 构建社会化推荐引擎 http://www.ibm.com/developerworks/cn/java/j-lo-mahout/ 与豆瓣创始人杨勃聊天 长尾 推荐机制 http://blog.sina.com.cn/s/blog_4cd47c4e010009ki.html 推荐引擎算法学习导论：http://blog.csdn.net/v_july_v/article/details/7184318 推荐引擎系统里的产品基因理念：http://blog.sina.com.cn/s/blog_4cd47c4e010009kk.html 社会化阅读推荐引擎 Magnet Demo：http://v.youku.com/v_show/id_XMjM3NDAyMTg4.html Magnet作者blog:http://blog.sina.com.cn/wbia2010 下一代推荐引擎的关键技术和应用案例: http://www.slideshare.net/gettyying/ss-8592317 来，做一个社会化推荐引擎:http://www.cnblogs.com/zhengyun_ustc/archive/2008/12/22/ju690_sr.html 淘宝共享数据平台推荐引擎相关文章：http://www.tbdata.org/archives/category/%E6%8E%A8%E8%8D%90%E5%BC%95%E6%93%8E 推荐引擎技术研究 Recommender Engines Seminar Paper：http://ishare.iask.sina.com.cn/f/11310749.html 百分点推荐引擎——从需求到架构 http://www.infoq.com/cn/articles/baifendian-recommendation-engine 豆瓣上的一个推荐系统书籍豆列(书单)：http://book.douban.com/doulist/150735/ 开源项目： Apache mahout http://mahout.apache.org/ Weka http://www.cs.waikato.ac.nz/ml/weka/ Javaml: http://java-ml.sourceforge.net/ numpy: http://numpy.scipy.org/ orange: http://orange.biolab.si/ 中文自然语言处理包FudanNLP: http://code.google.com/p/fudannlp/ 自然语言处理包： http://opennlp.apache.org/ 中文分词ICTCLAS：http://ictclas.org MyMediaLite Recommender System Library:http://www.ismll.uni-hildesheim.de/mymedialite/","title":"推荐引擎的学习资料"},{"content":"AI会议的总结（by南大周志华） 分类： 图像处理2011-05-20 00:08 48人阅读 评论(0) 收藏 举报 粗体标出来的都是Computer Vision和Image Processing相关的   ----------------------------------------------------------------------------------   说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可   以给到1+, 也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比   IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的   介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字   “Advances in Neural Information Processing Systems”, 所以, 与ICML/ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML/ECML/COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示/推理/学习等很多方面, AUAI   (Association of UAI) 主办, 每年开. tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说). 粗体标出来的都是Computer Vision和Image Processing相关的   ----------------------------------------------------------------------------------   说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可   以给到1+, 也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比   IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的   介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字   “Advances in Neural Information Processing Systems”, 所以, 与ICML/ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML/ECML/COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示/推理/学习等很多方面, AUAI   (Association of UAI) 主办, 每年开. tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说).","title":"关于国际AI会议的转载 AI会议的总结（by南大周志华）"},{"content":"前言 信息的飞速增长，使搜索引擎成为人们查找信息的首选工具，Google、百度、yisou、中搜等大型搜索引擎一直人们讨论的话题。随着搜索市场价值的不断增加，越来越多的公司开发出自己的搜索引擎，阿里巴巴的商机搜索、8848的购物搜索等也陆续面世，自然，搜索引擎技术也成为技术人员关注的热点。 搜索引擎技术的研究，国外比中国要早近十年，从最早的Archie，到后来的Excite，以及altvista、overture、google等搜索引擎面世，搜索引擎发展至今，已经有十几年的历史，而国内开始研究搜索引擎是在上个世纪末本世纪初。在许多领域，都是国外的产品和技术一统天下，特别是当某种技术在国外研究多年而国内才开始的情况下。例如操作系统、字处理软件、浏览器等等，但搜索引擎却是个例外。虽然在国外搜索引擎技术早就开始研究，但在国内还是陆续涌现出优秀的搜索引擎，像百度、中搜等。目前在中文搜索引擎领域，国内的搜索引擎已经和国外的搜索引擎效果上相差不远。之所以能形成这样的局面，有一个重要的原因就在于中文和英文两种语言自身的书写方式不同，这其中对于计算机涉及的技术就是中文分词。 什么是中文分词 众所周知，英文是以词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。例如，英文句子I am a student，用中文则为：“我是一个学生”。计算机可以很简单通过空格知道student是一个单词，但是不能很容易明白「学」、「生」两个字合起来才表示一个词。把中文的汉字序列切分成有意义的词，就是中文分词，有些人也称为切词。我是一个学生，分词的结果是：我 是 一个 学生。 中文分词和搜索引擎 中文分词到底对搜索引擎有多大影响？对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。笔者最近替朋友找一些关于日本和服的资料，在搜索引擎上输入「和服」，得到的结果就发现了很多问题。下面就以这个例子来说明分词对搜索结果的影响，在目前最有三个中文搜索引擎上做测试。测试方法是直接在Google、百度、中搜上以「和服」为关键词进行搜索： 在Google上输入「和服」搜索所有中文简体网页，总共结果507,000条，前20条结果中有14条与和服一点关系都没有。在第一页就有以下错误： “通信信息报：瑞星以技术和服务开拓网络安全市场” “使用纯HTML的通用数据管理和服务- 开发者- ZDNet ...” “陈慧琳《心口不一》 化妆和服装自己包办” “::外交部：中国境外领事保护和服务指南(2003年版) ...” “产品和服务” 等等。第一页只有三篇是真正在讲「和服」的结果。 在百度上输入「和服」搜索网页，总共结果为287,000条，前20条结果中有6条与和服一点关系都没有。在第一页有以下错误： “福建省晋江市恒和服装有限公司系独资企业” “关于商品和服务实行明码标价的规定” “青岛东和服装设备” 在中搜山输入「和服」搜索网页，总共结果为26,917条，前20条结果都是与和服相关的网页。 这次搜索引擎结果中的错误，就是由于分词的不准确所造成的。通过笔者的了解，Google的中文分词技术采用的是美国一家名叫Basis Technology（http://www.basistech.com）的公司提供的中文分词技术，百度使用的是自己公司开发的分词技术，中搜使用的是国内海量科技（http://www.hylanda.com）提供的分词技术。业界评论海量科技的分词技术目前被认为是国内最好的中文分词技术，其分词准确度超过99%，由此也使得中搜在搜索结果中搜索结果的错误率很低。由此可见，中文分词的准确度，对搜索引擎结果相关性和准确性有相当大的关系。 中文分词技术 中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。 现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 基于字符串匹配的分词方法 这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个「充分大的」机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下： 正向最大匹配法（由左到右的方向）； 逆向最大匹配法（由右到左的方向）； 最少切分（使每一句中切出的词数最小）。 还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。 一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。 对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。 基于理解的分词方法 这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。 基于统计的分词方法 从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。 M(X,Y)=logP(X,Y)/P(X).P(Y)，其中 P(X,Y)是汉字X、Y的相邻共现概率，P(X) 、P(Y)分别是X、Y在语料中出现的概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如「这一」、「之一」、「有的」、「我的」、「许多的」等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。笔者了解，海量科技的分词算法就采用「复方分词法」，所谓复方，相当于用中药中的复方概念，即用不同的药才综合起来去医治疾病，同样，对于中文词的识别，需要多种算法来处理不同的问题。   分词中的难题。 有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。 歧义识别。 歧义是指同样的一句话，可能有两种或者更多的切分方法。例如：表面的，因为「表面」和「面的」都是词，那么这个短语就可以分成「表面 的」和「表 面的」。这种称为交叉歧义。像这种交叉歧义十分常见，前面举的「和服」的例子，其实就是因为交叉歧义引起的错误。「化妆和服装」可以分成「化妆 和 服装」或者「化妆 和服 装」。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。 交叉歧义相对组合歧义来说是还算比较容易处理，组合歧义就必需根据整个句子来判断了。例如，在句子「这个门把手坏了」中，「把手」是个词，但在句子「请把手拿开」中，「把手」就不是一个词；在句子「将军任命了一名中将」中，「中将」是个词，但在句子「产量三年中将增长两倍」中，「中将」就不再是词。这些词计算机又如何去识别? 如果交叉歧义和组合歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：「乒乓球拍卖完了」，可以切分成「乒乓 球拍 卖 完 了」、也可切分成「乒乓球 拍卖 完 了」，如果没有上下文其他的句子，恐怕谁也不知道「拍卖」在这里算不算一个词。 新词识别。 新词，专业术语称为未登录词。也就是那些在字典中都没有收录过，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解句子「王军虎去广州了」中，「王军虎」是个词，因为是一个人的名字，但要是让计算机去识别就困难了。如果把「王军虎」做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子「王军虎头虎脑的」中，「王军虎」还能不能算词？ 新词中除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。 中文分词的应用 目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。 分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有了。科研院校研究的技术，大部分不能很快产品化，而一个专业公司的力量毕竟有限，看来中文分词技术要想更好的服务于更多的产品，还有很长一段路。 几种典型的自动分词系统评介 衡量自动分词系统的主要指标是切分精度和速度。由于切分速度与所运行的软、硬件平台密切相关，在没有注明运行平台时，切分速度只是一个参考指标，没有可比性。 另外，所注明的切分精度都是开发者自测试的结果。 1、几个早期的自动分词系统 自80年代初中文信息处理领域提出了自动分词以来，一些实用性的分词系统逐步得以开发，其中几个比较有代表性的自动分词系统在当时产生了较大的影响。 CDWS分词系统是我国第一个实用的自动分词系统，由北京航空航天大学计算机系于1983年设计实现，它采用的自动分词方法为最大匹配法，辅助以词尾字构词纠错技术。其分词速度为5-10字/秒，切分精度约为1/625，基本满足了词频统计和其他一些应用的需要。这是汉语自动分词实践的首次尝试，具有很大的启发作用和理论意义。例如，它比较科学地阐明了汉语中的歧义切分字段的类别、特征以及基本的对策（--切分歧义\"标准分类\"！）。 ABWS是山西大学计算机系研制的自动分词系统，系统使用的分词方法称为\"两次扫描联想-回溯\"方法，用联想-回溯来解决引起组合切分歧义。系统词库运用了较多的词法、句法等知识。其切分正确率为98.6%(不包括非常用、未登录的专用名词)，运行速度为 48词/分钟。 CASS是北京航空航天大学于1９８８年实现的分词系统。它使用的是一种变形的最大匹配方法，即正向增字最大匹配。它运用知识库来处理歧义字段。其机械分词速度为200字/秒以上，知识库分词速度150字/秒（没有完全实现）。 书面汉语自动分词专家系统是由北京师范大学现代教育研究所于1991前后研制实现的，它首次将专家系统方法完整地引入到分词技术中。系统使知识库与推理机保持相对独立，知识库包括常识性知识库（词条的词类24种、歧义词加标志及其消除规则编号、消歧的部分语义知识，使用关联网络存储）和启发性知识库（消歧产生式规则集合，用线性表结构存储），词典使用首字索引数据结构。通过引入专家系统的形式，系统把分词过程表示成为知识的推理过程，即句子\"分词树\"的生长过程。据报道，系统对封闭原料的切分精度为99.94%，对开放语料的切分精度达到99.8%，在386机器上切分速度达到200字/秒左右。这些性能代表了当时的一流成就。现在看来，这个系统的一个重要理论意义是进一步研究清楚了歧义切分字段，即把歧义字段分为词法级、句法级、语义级和语用级（--\"四级分类\"），并且统计出它们的分布分别为84.1%、10.8%、3.4%和1.7%，还给出了每一种歧义的处理策略，从而比较彻底地剖析了汉语歧义切分字段的性质。它的另外一个理论意义是给出了当前基于句法和语义处理技术的歧义分析精度的上限（\"语义级理想切分精度\"1/6250），并且说明只有综合运用各种知识、信息和推理机制的分析方法才又可能趋近理想切分精度。尽管本系统由于结构复杂、知识库建造困难且并不像预想的那么易于维护、效率不易提高等原因而未能广泛流行，但是其理论分析和指导思想已获得了普遍关注，影响了众多后继系统的开发。 　 2、清华大学早期SEG分词系统 此系统提供了带回溯的正向、反向、双向最大匹配法和全切分-评价切分算法，由用户来选择合适的切分算法。其特点则是带修剪的全切分-评价算法。系统考虑到了切分盲点的问题（某些字串永远不会被某种分词方法匹配出来），由此提出了全切分的概念，即找出输入字串的所有可能的子串，然后利用某种评价方法从所有这些可能的子串中选出最佳子串序列作为分词结果。为了解决全切分所带来的组合爆炸问题，又引进了对全切分过程进行修剪的方法，强制性地截止某些全切分的进行。用户在使用时，对于歧义较少的语料，可采用正向或反向最大匹配法；对于有较多交叉歧义的语料，可使用双向最大匹配法；对于其它歧义较大的语料，则采用全切分-评价算法，并需要采用一个合适的评价函数。由于对具体语料的统计参数设置了不确切初值，全切分-评价算法在第一、二遍切分过程中的正确率较低，随着切分的多遍进行，评价函数逐渐得以矫正，系统的切分精度逐步得以提高。经过封闭试验，在多遍切分之后，全切分-评价算法的精度可以达到99%左右。 3、清华大学SEGTAG系统 此系统着眼于将各种各类的信息进行综合，以便最大限度地利用这些信息提高切分精度。系统使用有向图来集成各种各样的信息，这些信息包括切分标志、预切分模式、其他切分单位。为了实现有限的全切分，系统对词典中的每一个重要的词都加上了切分标志，即标志\"ck\"或\"qk\"。\"qk\"标志表示该词可进行绝对切分，不必理会它是否产生切分歧义；\"ck\"标志表示该词有组合歧义，系统将对其进行全切分，即保留其所有可能的切分方式。 系统通过这两种标志并使用几条规则以实现有限的全切分，限制过多的切分和没有必要的搜索。规则包括： 1、无条件切出qk类词； 2、完全切分ck类词（保留所有可能子串）； 3、对没有标记(qk或ck)的词，若它与别的词之间存在交叉歧义，则作全切分； 否则将其切出。 为了获得切分结果，系统采用在有向图DAG上搜索最佳路径的方法，使用一个评价函数EVALUATE Path)，求此评价函数的极大值而获得最佳路径Pmax。所运用的搜索算法有两种，\"动态规划\"和\"全切分搜索+叶子评价\"，使用了词频、词类频度、词类共现频度等统计信息。通过实验，该系统的切分精度基本上可达到99%左右，能够处理未登录词比较密集的文本,切分速度约为30字/秒。 4、国家语委文字所应用句法分析技术的汉语自动分词 此分词模型考虑了句法分析在自动分词系统中的作用，以更好地解决切分歧义。切词过程考虑到了所有的切分可能，并运用汉语句法等信息从各种切分可能中选择出合理的切分结果。其过程由两步构成：一、对输入字串进行处理，得到一个所有可能的切分字串的集合，即进行（不受限的）全切分；二、利用句法分析从全切分集合中将某些词选出来，由它们构成合理的词序列，还原为原输入字串。系统使用一个自由传播式句法分析网络，用短语文法描述句法规则，并将其表示为层次化网络图，通过此网络的信息传递过程来进行选词。网络的节点分为词类节点（终结符节点）和规则类节点（非终结符节点）。词类节点保存词的信息；规则类节点对信息进行合并和句法、语义分析，生成新的信息，并将本节点的信息传递出去（也就是用文法产生式进行归约，并进行属性计算-作者注）。网络运行的初态是所有节点状态为NO，各种可能切分的字串进入响应相应的词类节点（终结符节点），然后开始运用文法进行计算。 当网络的最高层节点S（文法起始符号）达到稳定状态OK时,计算结束，在最高节点处输出最后的切分结果。 从一般的角度来看，应用句法分析技术进行切词的方法是一种\"生成-测试\"方法，它是一种常用的AI问题求解方法，包括两个步骤：生成步-找出所有可能的解（假设）；测试步-对各个假设进行检验，找出合格者。在应用句法分析进行切词时，其测试步是使用汉语的句法规则检验某种切分结果是否构成合法的汉语句子。这样可以将句法分析理论的各种成果用于切词之中，有多种句法分析技术可以应用，常见的是ATN分析、CYK分析(Chart Parsing)、G-LR分析等。可以将这种方法称做\"切词-句法分析一体化\"方法。随着软硬件水平的不断提高，直接运用时空消耗比较大的句法分析来检查分词结果的方法正在日益显现其优越性。 5、复旦分词系统 此系统由四个模块构成。 一、预处理模块，利用特殊的标记将输入的文本分割成较短的汉字串，这些标记包括标点符号、数字、字母等非汉字符，还包括文本中常见的一些字体、字号等排版信息。一些特殊的数词短语、时间短语、货币表示等，由于其结构相对简单，即由数词和特征 字构成构成，也在本阶段进行处理。为此系统特别增加一次独立的扫描过程来识别这些短语，系统维护一张特征词表，在扫描到特征字以后，即调用这些短语的识别模块，确定这些短语的左、右边界，然后将其完整地切分开； 二、歧义识别模块，使用正向最小匹配和逆向最大匹配对文本进行双向扫描，如果两种扫描结果相同，则认为切分正确，否则就判别其为歧义字段，需要进行歧义处理； 三、歧义字段处理模块，此模块使用构词规则和词频统计信息来进行排歧。构词规则包括前缀、后缀、重叠词等构词情况，以及成语、量词、单字动词切分优先等规则。在使用规则无效的情况下，使用了词频信息，系统取词频的乘积最大的词串作为最后切分结 果； 最后，此系统还包括一个未登录词识别模块，以解决未登录词造成的分词错误。未登录词和歧义字段构成了降低分词准确率的两大因素，而未登录词造成的切分错误比歧义字段更为严重，实际上绝大多数分词错误都是由未登录词造成的。系统对中文姓氏进行了自动识别，它利用了中文姓名的用字规律、频率，以及姓名的上下文等信息。通过对十万以上的中文姓名进行抽样综合统计，建立了姓氏频率表和名字用字频率表，由此可获得任意相邻的二、三个单字构成姓氏的概率大小和某些规律，再利用这些字串周围的一些称谓、指界动词和特定模式等具有指示意义的上下文信息，对字串是否构成姓名进行辨别。实验过程中，对中文姓氏的自动辨别达到了70%的准确率。系统对文本中的地名和领域专有词汇也进行了一定的识别。 6、哈工大统计分词系统 该系统是一种典型的运用统计方法的纯切词系统，它试图将串频统计和词匹配结合起来。 系统由三个部分构成： 一、预处理模块，利用显式和隐式的切分标记（标点符号、数字、ASCII字符以及出现频率高、构词能力差的单字词、数词+单字常用量词模式）将待分析的文本切分成短的汉字串，这大大地减少了需要统计的（无效）字串的数量和高频单字或量词边界串； 二、串频统计模块，此模块计算各个已分开的短汉字串中所有长度大于１的子串在局部上下文中出现的次数，并根据串频和串长对每个这样的子串进行加权，加权函数为 （F为串频，L为串长，即串中汉字个数）。根据经验，局部上下文中取为200字左右。局部上下文的串频计算使用一个滑动窗口（为一个队列式缓冲区，保存当前待切分汉字串及其前后20个短串），当当前待切分汉字串处理完之后，窗口下移一个短串（中心变为相邻下一个短串）。系统采用一个外散列表来记录窗口中的短串，以加快窗口中串频计数。散列函数取为汉字的GB-80位码（二级汉字共用入口95），每个桶中保存窗口中每一行（短串）上的汉字位置：（短串的行号，汉字列号），并且对于在窗口中出现多次的汉字位置用一个链指针连接起来，则计算某个字串在窗口中出现的频度时，不必将该字串与窗口中的短串逐个匹配，而只需统计在该字串中的各个汉字所对应的位置链表中能够相邻的位置的序列的个数即可。此外，还需要根据词缀集（前、后缀集合）对字串的权值进行提升，例如\"处理器\"中\"处理\"的权值很高，但由于对\"处理器\"的权值作了 提升（达到或超过了\"处理\"），就不会切成\"处理/器\"。如果某个汉字串的权值超过某一阈值D（取为40），则将此汉字串作为一个新识别的词，将其存入一临时词库中； 三、切分模块，首先用临时词库对每个短的汉字串进行切分，使用的是逐词遍历算法，再利用一个小型的常用词词典对汉字短串中未切分的子串进行正向最大匹配分词。对于短汉字串中那些仍未切分的子串，则将所有相邻单字作为一个权值很低的生词（例如\"玛\"、\"莉\"）。其中每个模块都对待分析的文本进行了一次扫描，因而是三遍扫描方法。此系统能够利用上下文识别大部分生词，解决一部分切分歧义，但是统计分词方法对常用词识别精度差的固有缺点仍然存在（例如切出\"由/来\"、\"语/用\"、\"对/联\"等）。 经测试，此系统的分词错误率为1.5%，速度为236字/秒。 　 7、杭州大学改进的MM分词系统 考虑到汉语的歧义切分字段出现的平均最大概率为1/110，因而纯机械分词的精度在理论上能够达到1-1/110=99.1%。那么是否还有更一般、精度更高的机械分词系统呢？ 根据统计，汉语的局部（词法一级）歧义字段占了全部歧义的84%，句法歧义占10%，如果提高系统处理这两类歧义的准确率，则可以大幅度提高切分精度。这方面的改进导致了改进的MM分词算法。将其阐述如下。 通过对交叉歧义字段的考察，发现其中80%以上可以通过运用一条无需任何语言知识的\"归右原则\"（交叉歧义字段优先与其右边的字段成词）就可以获得正确切分，--这是因为在多数情况下汉语的修饰语在前、中心词在后，因而\"归右\"好于\"归左\"。 \"归右原则\"可以使机械分词的精度上升到99.70%。这种考察给出了鼓舞人心的结果，有可能使机械分词系统达到这样的理论精度。 不过\"归右原则\"还有需要修正的地方，既对于\"连续型交叉歧义\"会发生错误，需要补充一条\"左部结合\"原则：若ABCDE为连续型交叉歧义字段，\"归右原则\"产生切分A B C D E 再由\"左结合原则\"（合并最左边的A、B）而得到AB C DE。 例如\"结合成分子\"->\"结 合 成 分子\"->\"结合 成 分子\"。 但是仍然还有例外，例如\"当结合成分子时\"->\"当 结合 成分 子时\"； 为此引入\"跳跃匹配\"，在词典中定义\"非连续词\"（实际上为串模式-作者注）\"当*时\"，然后在切分时首先分出\"当 结合成分子 时\"，然后再用\"归右+左结合\"切分中间的歧义字段。以上3项技术将机械分词的理论切分精度提高到了99.73%。 综合以上思想，就建立了如下改进的MM分词算法： 正向扫描 + 增字最大匹配（包括\"跳跃匹配非连续词\"） + 词尾歧义检查（逐次去掉首字做MM匹配以发现交叉歧义字段） + \"归右原则\"（ 对于\"连续型交叉歧义\"还需要\"左结合原则\"）。 系统的词典采用一级首字索引结构，词条中包括了\"非连续词\"（形如C1…* Cn）。系统精度的实验结果为95%，低于理论值99.73%，但高于通常的MM、RMM、DMM方法。 ///bs: 有机会见面 就出了问题； 8、Microsoft Research 汉语句法分析器中的自动分词 微软研究院的自然语言研究所在从90年代初开始开发了一个通用型的多国语言处理平台NLPWin，最初阶段的研究都是对英语进行的。大约从1997年开始，增加了中文处理的研究，从而使NLPWin成为能够进行7国语言处理的系统（其中日语和韩语部分的研究已较早地开展起来）。中文部分的研究在开始时缺少必要的基础资源，于是经过细致的研究分析之后，购买了北大计算语言所的《现代汉语语法信息词典》，从此进展顺利，在短短的一年半的时间里达到了其它东方语种的处理水平。据报道，NLPWin的语法分析部 分使用的是一种双向的Chart Parsing，使用了语法规则并以概率模型作导向，并且将语法和分析器独立开。 NLPWin中文部分的一个特点是将词的切分同句法分析融合起来，即是一种前面提到过的\"切词-句法分析一体化\"方法：在其匹配切词阶段保留所有可能的切分结果（包括歧义切分），然后在句法分析阶段使用汉语的句法规则判断切分的合理性，如果对句子的某种切分能够成功地建立起完全的句法树，则表示该切分结果是正确的。对于有上下文及语用歧义的歧义切分字段，系统将生成两棵以上的分析树（可以使用某种标准进行排序）。 当然，为了提高系统效率，有必要在分词阶段排除尽可能多的局部一级的切分歧义。其中使用的技术有：消除所有导致词典中没有对应词条的单字的切分；为词典中的每一个词条增加一项\"Atomic\"属性（为1表示不需要分析其内部字串，为0表示需要保留其内部 的切分，即是一种组合歧义标志--作者注）；以及为每个词增加 LeftCond1、RightCond1、LeftCond2、RightCond2 四类字符集合（前两项表示歧义绝对生效，后两项表示歧义有比较高的可能性生效，即歧义的直接前后文--作者注）；还包括一些排歧规则（例如对于连续型歧义字段ABCD，如果AB和CD不与前后词交叉、A或D是名词、ABC和BCD都不是词，则切分出AB/CD：\"昨天下午\"->\"昨天/下午\"）。 Bits and Atrributes in Chinese NLPwin (7-9-97) Bits …… Attributes …… For overlapping ambiguity: LeftClue (a record will not be added to tposrecs if the character on its  left appears in this list.)  RiteClue (a record will not be added to tposrecs if the character on its  right appears in this list.) LeftHint (a record will be assigned low probability if the character on its  left appears in this list.) RiteHint (a record will be assigned low probability if the character on its  right appears in this list.) 实验结果表明，系统可以正确处理85%的歧义切分字段，在Pentium 200 PC上的速度约600-900字/秒。考虑到系统对多种切分结果进行了完全的句法分析、对词典每个属性进行了完全的查找，这是相当可观的效率。 我们的评论是: 这是汉语处理的一种有效的综合性途径，值得发扬推广；但这种使用\"Atomic\"属性的方法实际上只是表示了组合型歧义（占不到1/5）的特征，对更普遍的交叉型歧义（超过总歧义的4/5）的处理还存在效率和效果更好的方法。 　 9、北大计算语言所分词系统 本系统由北京大学计算语言学研究所研制开发，属于分词和词类标注相结合的分词系统。由于将分词和词类标注结合起来，系统可利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，同时将基于规则的标注排 歧与基于语料库统计模型的排歧结合起来，使规则的普遍性与灵活性得到统一，而且对未登入词的估算到达了相当高的准确率。系统的处理过程包括了自动切分和初始词性标记、切分歧义字段识别、组词和标注预处理、词性标记排歧、切分和词性标注后处理等 过程，系统的算法综合了多种数据组织和搜索技术，以很低的时空开销实现了高速匹配和查找，同时采用了当代计算语言学的统计方法，运用隐Markov过程进行词类标注和排歧，对算法的效率和稳固性都作了尽可能的优化。此系统的一大特色是对通用性的强 调，将最稳定、最常用的4万6千余条现代汉语基本词汇（即将扩充到7万多条）及其有关属性组织成为基本词典，这些词的基本地位都是由汉语语言学家逐一检验认可的，这是本系统通用性的保证；在此词典的基础上充分利用汉语构词法的研究成果，可以识别出大部分的常用词。同时本系统对用户词典机制作了最大限度的扩展，允许用户加入3部到30部以上的自定义词典，并允许用户对这些词典的优先顺序自由排列，这样就可以用较小规模的多个特殊词典更有针对性地解决具体领域的文本处理。因此本系统的语言模型实现了通用性与多样性的有效结合，并到达了极高的效率。经过最近在搜索算法上的改进，系统的分词连同标注的速度在Pentium 133Hz/16MB内存机器上的达到了每秒3千词以上，而在Pentium II/64MB内存机器上速度高达每秒5千词。自本系统开发以来，已先后向国内和国外十多家单位进行了转让，获得了普遍的好评。 在1998年4月进行的863全国智能接口评测会上，该系统有良好的表现。由于系统对待词的兼类问题的理论观点与评测标准有一些差别，所测得的标注准确率没有达到自测试的水平。该系统的词语分类体系一方面承认汉语词存在兼类现象，一方面又不主张扩大兼类现象，尽量把相同语法功能的词类当作是一个词类，而把词的具体语法属性留到后续过程处理。这些观点与评测标准有所不同。国内还有很多单位开发了分词系统，但大部分都没有参加这一具有极强可比性的评测。","title":"[转载]中文搜索引擎技术揭密：中文分词"},{"content":"　　HMM(隐马尔科夫模型)是自然语言处理中的一个基本模型，用途比较广泛，如汉语分词、词性标注及语音识别等，在NLP中占有很重要的地位。网上关于HMM的介绍讲解文档很多，我自己当时开始看的时候也有点稀里糊涂。后来看到wiki上举得一个关于HMM的例子才如醍醐灌顶，忽然间明白HMM的三大问题是怎么回事了。例子我借助中文wiki重新翻译了一下，并对三大基本问题进行说明，希望对读者朋友有所帮助： 　　Alice 和Bob是好朋友，但是他们离得比较远，每天都是通过电话了解对方那天作了什么.Bob仅仅对三种活动感兴趣:公园散步,购物以及清理房间.他选择做什么事情只凭当天天气.Alice对于Bob所住的地方的天气情况并不了解,但是知道总的趋势.在Bob告诉Alice每天所做的事情基础上,Alice想要猜测Bob所在地的天气情况. 　　Alice认为天气的运行就像一个马尔可夫链. 其有两个状态 “雨”和”晴”,但是无法直接观察它们,也就是说,它们对于Alice是隐藏的.每天,Bob有一定的概率进行下列活动:”散步”, “购物”, 或 “清理”. 因为Bob会告诉Alice他的活动,所以这些活动就是Alice的观察数据.这整个系统就是一个隐马尔可夫模型HMM. 　　Alice知道这个地区的总的天气趋势,并且平时知道Bob会做的事情.也就是说这个隐马尔可夫模型的参数是已知的.可以用程序语言(Python)写下来: 　　　// 状态数目，两个状态：雨或晴 　　　states = (‘Rainy’, ‘Sunny’) 　　　// 每个状态下可能的观察值 　　　observations = (‘walk’, ‘shop’, ‘clean’) 　　　　　　　　　　　 　　　//初始状态空间的概率分布 　　　start_probability = {‘Rainy’: 0.6, ‘Sunny’: 0.4} 　　　// 与时间无关的状态转移概率矩阵 　　　transition_probability = { 　　　’Rainy’ : {‘Rainy’: 0.7, ‘Sunny’: 0.3}, 　　　’Sunny’ : {‘Rainy’: 0.4, ‘Sunny’: 0.6}, 　　　} 　　　//给定状态下，观察值概率分布,发射概率 　　　emission_probability = { 　　　’Rainy’ : {‘walk’: 0.1, ‘shop’: 0.4, ‘clean’: 0.5}, 　　　’Sunny’ : {‘walk’: 0.6, ‘shop’: 0.3, ‘clean’: 0.1}, 　　　} 　　在这些代码中,start_probability代表了Alice对于Bob第一次给她打电话时的天气情况的不确定性(Alice知道的只是那个地方平均起来下雨多些).在这里,这个特定的概率分布并非平衡的,平衡概率应该接近（在给定变迁概率的情况下）{‘Rainy’: 0.571, ‘Sunny’: 0.429}。 transition_probability 表示马尔可夫链下的天气变迁情况,在这个例子中,如果今天下雨,那么明天天晴的概率只有30%.代码emission_probability 表示了Bob每天作某件事的概率.如果下雨,有 50% 的概率他在清理房间;如果天晴,则有60%的概率他在外头散步。 　　Alice和Bob通了三天电话后发现第一天Bob去散步了，第二天他去购物了，第三天他清理房间了。Alice现在有两个问题：这个观察序列“散步、购物、清理”的总的概率是多少？(注：这个问题对应于HMM的基本问题之一：已知HMM模型λ及观察序列O，如何计算P(O|λ)？) 最能解释这个观察序列的状态序列（晴/雨）又是什么？（注：这个问题对应HMM基本问题之二：给定观察序列O=O1,O2,…OT以及模型λ,如何选择一个对应的状态序列S = q1,q2,…qT，使得S能够最为合理的解释观察序列O？） 　　至于HMM的基本问题之三：如何调整模型参数, 使得P(O|λ)最大？这个问题事实上就是给出很多个观察序列值，来训练以上几个参数的问题。","title":"wiki上一个比较好的HMM例子"},{"content":"  本文内容遵从CC版权协议 转载请注明出自matrix67.com  这篇文章是漫话中文分词算法的续篇。在这里，我们将紧接着上一篇文章的内容继续探讨下去：如果计算机可以对一句话进行自动分词，它还能进一步整理句子的结构，甚至理解句子的意思吗？这两篇文章的关系十分紧密，因此，我把前一篇文章改名为了《漫话中文自动分词和语义识别（上）》，这篇文章自然就是它的下篇。我已经在很多不同的地方做过与这个话题有关的演讲了，在这里我想把它们写下来，和更多的人一同分享。     什么叫做句法结构呢？让我们来看一些例子。“白天鹅在水中游”，这句话是有歧义的，它可能指的是“白天有一只鹅在水中游”，也可能指的是“有一只白天鹅在水中游”。不同的分词方案，产生了不同的意义。有没有什么句子，它的分词方案是唯一的，但也会产生不同的意思呢？有。比如“门没有锁”，它可能是指的“门没有被锁上”，也有可能是指的“门上根本就没有挂锁”。这个句子虽然只能切分成“门／没有／锁”，但由于“锁”这个词既有可能是动词，也有可能是名词，因而让整句话产生了不同的意思。有没有什么句子，它的分词方案是唯一的，并且每个词的词义也都不再变化，但整个句子仍然有歧义呢？有可能。看看这句话：“咬死了猎人的狗”。这句话有可能指的是“把猎人的狗咬死了”，也有可能指的是“一只咬死了猎人的狗”。这个歧义是怎么产生的呢？仔细体会两种不同的意思后，你会发现，句子中最底层的成分可以以不同的顺序组合起来，歧义由此产生。     在前一篇文章中，我们看到了，利用概率转移的方法，我们可以有效地给一句话分词。事实上，利用相同的模型，我们也能给每一个词标注词性。更好的做法则是，我们直接把同一个词不同词性的用法当作是不同的词，从而把分词和词性标注的工作统一起来。但是，所有这样的工作都是对句子进行从左至右线性的分析，而句子结构实际上比这要复杂多了，它是这些词有顺序有层次地组合在一起的。计算机要想正确地解析一个句子，在分词和标注词性后，接下来该做的就是分析句法结构的层次。     在计算机中，怎样描述一个句子的句法结构呢？ 1957 年， Noam Chomsky 出版了《句法结构》一书，把这种语言的层次化结构用形式化的方式清晰地描述了出来，这也就是所谓的“生成语法”模型。这本书是 20 世纪为数不多的几本真正的著作之一，文字非常简练，思路非常明晰，震撼了包括语言学、计算机理论在内的多个领域。记得 Quora 上曾经有人问 Who are the best minds of the world today ，投出来的答案就是 Noam Chomsky 。     随便取一句很长很复杂的话，比如“汽车被开车的师傅修好了”，我们总能至顶向下地一层层分析出它的结构。这个句子最顶层的结构就是“汽车修好了”。汽车怎么修好了呢？汽车被师傅修好了。汽车被什么样的师傅修好了呢？哦，汽车被开车的师傅修好了。当然，我们还可以无限地扩展下去，继续把句子中的每一个最底层的成分替换成更详细更复杂的描述，就好像小学语文中的扩句练习那样。这就是生成语法的核心思想。     熟悉编译原理的朋友们可能知道“上下文无关文法”。其实，上面提到的扩展规则本质上就是一种上下文无关文法。例如，一个句子可以是“什么怎么样”的形式，我们就把这条规则记作       句子 → 名词性短语＋动词性短语     其中，“名词性短语”指的是一个具有名词功能的成分，它有可能就是一个名词，也有可能还有它自己的内部结构。例如，它有可能是一个形容词性短语加上“的”再加上另一个名词性短语构成的，比如“便宜的汽车”；它还有可能是由“动词性短语＋的＋名词性短语”构成的，比如“抛锚了的汽车”；它甚至可能是由“名词性短语＋的＋名词性短语”构成的，比如“老师的汽车”。我们把名词性短语的生成规则也都记下来：       名词性短语 → 名词       名词性短语 → 形容词性短语＋的＋名词性短语       名词性短语 → 动词性短语＋的＋名词性短语       名词性短语 → 名词性短语＋的＋名词性短语       ⋯⋯     类似地，动词性短语也有诸多具体的形式：       动词性短语 → 动词       动词性短语 → 动词性短语＋了       动词性短语 → 介词短语＋动词性短语       ⋯⋯     上面我们涉及到了介词短语，它也有自己的生成规则：       介词短语 → 介词＋名词性短语       ⋯⋯     我们构造句子的任务，也就是从“句子”这个初始结点出发，不断调用规则，产生越来越复杂的句型框架，然后从词库中选择相应词性的单词，填进这个框架里：            而分析句法结构的任务，则是已知一个句子从左到右各词的词性，要反过来求出一棵满足要求的“句法结构树”。这可以用 Earley parser 来实现。     这样看来，句法结构的问题似乎就已经完美的解决了。其实，我们还差得很远。生成语法有两个大问题。首先，句法结构正确的句子不见得都是好句子。 Chomsky 本人给出了一个经典的例子： Colorless green ideas sleep furiously 。形容词加形容词加名词加动词加副词，这是一个完全符合句法要求的序列，但随便拼凑会闹出很多笑话——什么叫做“无色的绿色的想法在狂暴地睡觉”？顺便插播个广告，如果你还挺喜欢这句话的意境的，欢迎去我以前做的 IdeaGenerator 玩玩。不过，如果我们不涉及句子的生成，只关心句子的结构分析，这个缺陷对我们来说影响似乎并不大。生成语法的第二个问题就比较麻烦了：从同一个词性序列出发，可能会构建出不同的句法结构树。比较下面两个例子：       老师 被 迟到 的 学生 逗乐 了       电话 被 窃听 的 房间 找到 了     它们都是“名词＋介词＋动词＋的＋名词＋动词＋了”，但它们的结构并不一样，前者是老师被逗乐了，“迟到”是修饰“学生”的，后者是房间找到了，“电话被窃听”是一起来修饰房间的。但是，纯粹运用前面的模型，我们无法区分出哪句话应该是哪个句法结构树。如何强化句法分析的模型和算法，让计算机构建出一棵正确的句法树，这成了一个大问题。     让我们来看一个更简单的例子吧。同样是“动词＋形容词＋名词”，我们有两种构建句法结构树的方案：            未经过汉语语法训练的朋友可能会问，“点亮蜡烛”和“踢新皮球”的句法结构真的不同吗？我们能证明，这里面真的存在不同。我们造一个句子“踢破皮球”，你会发现对于这个句子来说，两种句法结构都是成立的，于是出现了歧义：把皮球踢破了（结构和“点亮蜡烛”一致），或者是，踢一个破的皮球（结构和“踢新皮球”一致）。     但为什么“点亮蜡烛”只有一种理解方式呢？这是因为我们通常不会把“亮”字直接放在名词前做定语，我们一般不说“一根亮蜡烛”、“一颗亮星星”等等。为什么“踢新皮球”也只有一种理解方式呢？这是因为我们通常不会把“新”直接放在动词后面作补语，不会说“皮球踢新了”，“衣服洗新了”等等。但是“破”既能作定语又能作补语，于是“踢破皮球”就产生了两种不同的意思。如果我们把每个形容词能否作定语，能否作补语都记下来，然后在生成规则中添加限制条件，不就能完美解决这个问题了吗？     基于规则的句法分析器就是这么做的。汉语语言学家们已经列出了所有词的各种特征：       亮：词性 = 形容词，能作补语 = True ，能作定语 = False ⋯⋯       新：词性 = 形容词，能作补语 = False ，能作定语 = True ⋯⋯       ⋯⋯     当然，每个动词也有一大堆属性：       点：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯       踢：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯       污染：词性 = 动词，能带宾语 = True ，能带补语 = False ⋯⋯       排队：词性 = 动词，能带宾语 = False ，能带补语 = False ⋯⋯       ⋯⋯     名词也不例外：       蜡烛：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯       皮球：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯       ⋯⋯     有人估计会觉得奇怪了：“能作主语”也是一个属性，莫非有些名词不能做主语？哈哈，这样的名词不但有，而且还真不少：剧毒、看头、厉害、正轨、存亡⋯⋯这些词都不放在动词前面。难道有些名词不能做宾语吗？这样的词也有不少：享年、芳龄、心术、浑身、家丑⋯⋯这些词都不放在动词后面。这样说来，存在不受数量词修饰的词也就不奇怪了，事实上上面这些怪异的名词前面基本上都不能加数量词。     另外一个至关重要的就是，这些性质可以“向上传递”。比方说，我们规定，套用规则       名词性短语 → 形容词性短语＋名词性短语     后，整个名词性短语能否作主语、能否作宾语、能否受数量词修饰，这将取决于它的第二个构成成分。通俗地讲就是，如果“皮球”能够作主语，那么“新皮球”也能够作主语。有了“词语知识库”，又确保了这些知识能够在更高层次得到保留，我们就能给语法生成规则添加限制条件了。例如，我们可以规定，套用规则       动词性短语 → 动词性短语＋名词性短语     的前提条件就是，那个动词性短语的“能带宾语”属性为 True ，并且那个名词性短语“能作宾语”的属性为 True 。另外，我们规定       动词性短语 → 动词性短语＋形容词性短语     必须满足动词性短语的“能带补语”属性为 True ，并且形容词性短语“能作补语”属性为 True 。这样便阻止了“踢新皮球”中的“踢”和“新”先结合起来，因为“新”不能作补语。     最后我们规定，套用规则       名词性短语 → 形容词性短语＋名词性短语     时，形容词性短语必须要能作定语。这就避免了“点亮蜡烛”中的“亮”和“蜡烛”先组合起来，因为“亮”通常不作定语。这样，我们便解决了“动词＋形容词＋名词”的结构分析问题。     当然，这只是一个很简单的例子。在这里的问题 6 、 7 、 8 中你可以看到，一条语法生成规则往往有很多限制条件，这些限制条件不光是简单的“功能相符”和“前后一致”，有些复杂的限制条件甚至需要用 IF … THEN … 的方式来描述。你可以在这里看到，汉语中词与词之间还有各种怪异的区别特征，并且哪个词拥有哪些性质纯粹是知识库的问题，完全没有规律可循。一个实用的句法结构分析系统，往往拥有上百种属性标签。北京大学计算语言所编写了《现代汉语语法信息词典》，它里面包含了 579 种属性。我们的理想目标就是，找到汉语中每一种可能会影响句法结构的因素，并据此为词库里的每一个词打上标签；再列出汉语语法中的每一条生成规则，找到每一条生成规则的应用条件，以及应用这条规则之后，整个成分将会以怎样的方式继承哪些子成分的哪些属性，又会在什么样的情况下产生哪些新的属性。按照生成语言学的观点，计算机就应该能正确解析所有的汉语句子了。         那么，这样一来，计算机是否就已经能从句子中获取到理解语义需要的所有信息了呢？答案是否定的。还有这么一些句子，它从分词到词义到结构都没有两可的情况，但整个句子仍然有歧义。考虑这句话“鸡不吃了”，它有两种意思：鸡不吃东西了，或者我们不吃鸡了。但是，这种歧义并不是由分词或者词义或者结构导致的，两种意思所对应的语法结构完全相同，都是“鸡”加上“不吃了”。但为什么歧义仍然产生了呢？这是因为，在句法结构内部，还有更深层次的语义结构，两者并不相同。     汉语就是这么奇怪，位于主语位置上的事物既有可能是动作的发出者，也有可能是动作的承受者。“我吃完了”可以说，“苹果吃完了”也能讲。然而，“鸡”这个东西既能吃，也能被吃，歧义由此产生。     位于宾语位置上的事物也不一定就是动作的承受者，“来客人了”、“住了一个人”都是属于宾语反而是动作发出者的情况。记得某次数理逻辑课上老师感叹，汉语的谓词非常不规范，明明是太阳在晒我，为什么要说成是“我晒太阳”呢？事实上，汉语的动宾搭配范围极其广泛，还有很多更怪异的例子：“写字”是我们真正在写的东西，“写书”是写的结果，“写毛笔”是写的工具，“写楷体”是写的方式，“写地上”是写的场所，“写一只狗”，等等，什么叫做“写一只狗”啊？我们能说“写一只狗”吗？当然可以，这是写的内容嘛，“同学们这周作文写什么啊”，“我写一只狗”。大家可以想像，学中文的老外看了这个会是什么表情。虽然通过句法分析，我们能够判断出句子中的每样东西都和哪个动词相关联，但从语义层面上看这个关联是什么，我们还需要新的模型。     汉语语言学家把事物与动词的语义关系分为了 17 种，叫做 17 种“语义角色”，它们是施事、感事、当事、动力、受事、结果、系事、工具、材料、方式、内容、与事、对象、场所、目标、起点、时间。你可以看到，语义角色的划分非常详细。同样是动作的发出者，施事指的是真正意义上的发出动作，比如“他吃饭”中的“他”；感事则是指某种感知活动的经验者，比如“他知道这件事了”中的“他”；当事则是指性质状态的主体，比如“他病了”中的“他”；动力则是自然力量的发出者，比如“洪水淹没了村庄”中的“洪水”。语义角色的具体划分以及 17 这个数目是有争议的，不过不管怎样，这个模型本身能够非常贴切地回答“什么是语义”这个问题。     汉语有一种“投射理论”，即一个句子的结构是由这个句子中的谓语投射出来的。给定一个动词后，这个动词能够带多少个语义角色，这几个语义角色都是什么，基本上都已经确定了。因而，完整的句子所应有的结构实际上也就已经确定了。比如，说到“休息”这个动词，你就会觉得它缺少一个施事，而且也不缺别的了。我们只会说“老王休息”，不会说“老王休息手”或者“老王休息沙发”。因而我们认为，“休息”只有一个“论元”。它的“论元结构”是：       休息 <施事>     因此，一旦在句子中看到“休息”这个词，我们就需要在句内或者句外寻找“休息”所需要的施事。这个过程有一个很帅的名字，叫做“配价”。“休息”就是一个典型的“一价动词”。我们平时接触的比较多的则是二价动词。不过，它们具体的论元有可能不一样：       吃 <施事，受事>       去 <施事，目标>       淹没 <动力，受事>     三价动词也是有的，例如       送 <施事，受事，与事>     甚至还有零价动词，例如       下雨 <Ф>     下面我们要教计算机做的，就是怎样给动词配价。之前，我们已经给出了解析句法结构的方法，这样计算机便能判断出每个动词究竟在和哪些词发生关系。语义分析的实质，就是确定出它们具体是什么关系。因此，语义识别的问题，也就转化为了“语义角色标注”的问题。然而，语义角色出现的位置并不固定，施事也能出现在动词后面，受事也能出现在动词前面，怎样让计算机识别语义角色呢？在回答这个问题之前，我们不妨问问自己：我们是怎么知道，“我吃完了”中的“我”是“吃”的施事，“苹果吃完了”中的“苹果”是“吃”的受事的呢？大家肯定会说，废话，“我”当然只能是“吃”的施事，因为我显然不会“被吃”；“苹果”当然只能是“吃”的受事，因为苹果显然不能发出“吃”动作。也就是说，“吃”的两个论元都有语义类的要求。我们把“吃”的论元结构写得更详细一些：       吃 <施事[语义类：人|动物]，受事[语义类：食物|药物]> 而“淹没”一词的论元结构则可以补充为：       淹没 <动力[语义类：自然事物]，受事[语义类：建筑物|空间]>     所以，为了完成计算机自动标注语义角色的任务，我们需要人肉建立两个庞大的数据库：语义类词典和论元结构词典。这样的人肉工程早就已经做过了。北京语言大学 1990 年 5 月启动的“九〇￼五语义工程”就是人工构建的一棵规模相当大的语义树。它把词语分成了事物、运动、时空、属性四大类，其中事物类分为事类和物类，物类又分为具体物和抽象物，具体物则再分为生物和非生物，生物之下则分了人类、动物、植物、微生物、生物构件五类，非生物之下则分了天然物、人工物、遗弃物、几何图形和非生物构件五类，其中人工物之下又包括设施物、运载物、器具物、原材料、耗散物、信息物、钱财七类。整棵语义树有 414 个结点，其中叶子结点 309 个，深度最大的地方达到了 9 层。论元结构方面则有清华大学和人民大学共同完成的《现代汉语述语动词机器词典》，词典中包括了各种动词的拼音、释义、分类、论元数、论元的语义角色、论元的语义限制等语法和语义信息。     说到语义工程，不得不提到董振东先生的知网。这是一个综合了语义分类和语义关系的知识库，不但通过语义树反映了词与词的共性，还通过语义关系反映了每个词的个性。它不但能告诉你“医生”和“病人”都是人，还告诉了你“医生”可以对“病人”发出一个“医治”的动作。知网的理念和 WordNet 工程很相似，后者是 Princeton 在 1985 年就已经开始构建的英文单词语义关系词典，背后也是一个语义关系网的概念，词与词的关系涉及同义词、反义词、上下位词、整体与部分、子集与超集、材料与成品等等。如果你装了 Mathematica，你可以通过 WordData 函数获取到 WordNet 的数据。至于前面说的那几个中文知识库嘛，别问我，我也不知道上哪儿取去。       看到这里，想必大家会欢呼，啊，这下子，在中文信息处理领域，从语法到语义都已经漂亮的解决了吧。其实并没有。上面的论元语义角色的模型有很多问题。其中一个很容易想到的就是隐喻的问题，比如“信息淹没了我”、“悲伤淹没了我”。一旦出现动词的新用法，我们只能更新论元结构：       淹没 <动力[语义类：自然事物|抽象事物]，受事[语义类：建筑物|空间|人类]>     但更麻烦的则是下面这些违背语义规则的情况。一个是否定句，比如“张三不可能吃思想”。一个是疑问句，比如“张三怎么可能吃思想”。更麻烦的就是超常现象。随便在新闻网站上一搜，你就会发现各种不符合语义规则的情形。我搜了一个“吃金属”，立即看到某新闻标题《法国一位老人以吃金属为生》。要想解决这些问题，需要给配价模型打上不少补丁。       然而，配价模型也仅仅解决了动词的语义问题。其他词呢？好在，我们也可以为名词发展一套类似的配价理论。我们通常认为“教师”是一个零价名词，而“老师”则是一个一价名词，因为说到“老师”时，我们通常会说“谁的老师”。“态度”则是一个二价的名词，因为我们通常要说“谁对谁的态度”才算完整。事实上，形容词也有配价，“优秀”就是一个一价形容词，“友好”则是一个二价形容词，原因也是类似的。配价理论还有很多更复杂的内容，这里我们就不再详说了。     但还有很多配价理论完全无法解决的问题。比如，语义有指向的问题。“砍光了”、“砍累了”、“砍钝了”、“砍快了”，都是动词后面跟形容词作补语，但实际意义各不相同。“砍光了”指的是“树砍光了”，“砍累了”指的是“人砍累了”，“砍钝了”指的是“斧子砍钝了”，“砍快了”指的是“砍砍快了”。看来，一个动词的每个论元不但有语义类的限制，还有“评价方式”的限制。     两个动词连用，也有语义关系的问题。“抓住不放”中，“抓住”和“不放”这两个动作构成一种反复的关系，抓住就等于不放。“说起来气人”中，“说起来”和“气人”这两个动作构成了一种条件关系，即每次发生了“说起来”这个事件后，都会产生“气人”这个结果。大家或许又会说，这两种情况真的有区别吗？是的，而且我能证明这一点。让我们造一个句子“留着没用”，你会发现它出现了歧义：既可以像“抓住不放”一样理解为反复关系，一直把它留着一直没有使用；又可以像“说起来气人”一样理解为条件关系，留着的话是不会有用的。因此，动词与动词连用确实会产生不同的语义关系，这需要另一套模型来处理。     虚词的语义更麻烦。别以为“了”就是表示完成，“这本书看了三天”表示这本书看完了，“这本书看了三天了”反而表示这本书没看完。“了”到底有多少个义项，现在也没有一个定论。副词也算虚词，副词的语义同样捉摸不定。比较“张三和李四结婚了”与“张三和李四都结婚了”，你会发现描述“都”字的语义没那么简单。       不过，在实际的产品应用中，前面所说的这些问题都不大。这篇文章中讲到的基本上都是基于规则的语言学处理方法。目前更实用的，则是对大规模真实语料的概率统计分析与机器学习算法，这条路子可以无视很多具体的语言学问题，并且效果也相当理想。最大熵模型和条件随机场都是目前非常常用的自然语言处理手段，感兴趣的朋友可以深入研究一下。但是，这些方法也有它们自己的缺点，就是它们的不可预测性。不管哪条路，似乎都离目标还有很远的一段距离。期待在未来的某一日，自然语言处理领域会迎来一套全新的语言模型，一举解决前面提到的所有难题。","title":"漫话中文自动分词和语义识别（下）：句法结构和语义结构"},{"content":"本文内容遵从CC版权协议 转载请注明出自matrix67.com   记得第一次了解中文分词算法是在 Google 黑板报 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进行研究，期间诞生了很多有意思的理论。     中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。     有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。     最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来／应聘”。     维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。     还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如“民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民”并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／服务”。       不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。     当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词数法上，我们就有了一种简明而强大的算法：     对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。     这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分：       他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ）       他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ）       他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ）     正确答案胜出。     需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。     算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。“鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概率，每个词出现的频率也是不同的。     何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。     以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10-9 ，但“有意／见／分歧”的得分只有 1.0×10-11 ，正确方案完胜。     这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话：       这／事／的确／定／不／下来     但是概率算法却会把这个句子分成：       这／事／的／确定／不／下来     原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。     其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。     于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w1 、 w2 ，统计在语料库中词语 w1 后面恰好是 w2 的概率 P(w1, w2) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w1) · P(w1, w2) · … · P(wn-1, wn) ，其中 w1, w2, …, wn 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。     至此，中文自动分词算是有了一个漂亮而实用的算法。         但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。     在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。     可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、“说”、“报道”、“参加”、“访问”、“表示”等动作词。     但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了；要是听说了有人居然敢叫“令计划”，估计直接就崩溃了。     还有那些恰好与上下文组合成词的人名，例如：      费孝通向人大常委会提交书面报告      邓颖超生前使用过的物品     这就是最考验分词算法的句子了。     相比之下，中国地名的用字就分散得多了。北京有一个地方叫“臭泥坑”，网上搜索“臭泥坑”，第一页全是“臭泥坑地图”、“臭泥坑附近酒店”之类的信息。某年《重庆晨报》刊登停电通知，上面赫然印着“停电范围包括沙坪坝区的犀牛屙屎和犀牛屙屎抽水”，读者纷纷去电投诉印刷错误。记者仔细一查，你猜怎么着，印刷并无错误，重庆真的就有叫“犀牛屙屎”和“犀牛屙屎抽水”的地方。     好在，中国地名数量有限，这是可以枚举的。中国地名委员会编写了《中华人民共和国地名录》，收录了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。     真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。     最难识别的未登录词就是缩略语了。“高数”、“抵京”、“女单”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。     汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。     说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。","title":"漫话中文自动分词和语义识别（上）：中文分词算法"},{"content":"一直以来伴随我的一些学习习惯 By 刘未鹏 1. Google&Wiki（遇到问题做的第一件事情，也是学习某个东西做功课（homework）最先用到的东西。 2. 看书挑剔，只看经典。如何选择经典，可以到网上做做功课，看看评价，综合分析一下。 3. 做读书笔记。一是将自己阅读的时候的思考（包括闪念）总结下来，二是将书中的好例子摘抄下来。（这个习惯是一年前才养成的，发现受益极大。）有了google note，笔记可以加上tag，非常便于回顾，加深理解。我觉得，人与人学习的差距不在资质上，而在花在思考的时间和思考的深度上（后两者常常也是相关的）。 4. 提到思考，我有一个小习惯。利用走路和吃饭的时候思考，还有睡觉前必然要弄一个问题放在脑子里面，在思考中迷糊入睡。发现这样一来往往在不知不觉中多出来大量的思考时间。 4a. 将思考成为习惯还有一个很大的好处——避免焦虑。卡耐基用一整本书讲了一个有效的做法来避免焦虑——底线思考。然而实际上还有另一个有效的做法，就是投入地做另一件事情。不去想\"喜马拉雅山上的猴子\"的方法并不是使劲的告诉自己不去想\"喜马拉雅山上的猴子\"，因为那样等于就是脑袋里想了那只猴子，正确的做法是真的不去想那猴子，而是想别的。用别的东西充满工作记忆，其他的神经活动自然会被抑制（神经科学基本事实）。所以，感到焦虑的时候不妨思考吧，甚至完全可以去理性分析和思考导致焦虑的问题本身，将其拆解，分析来源，在不知不觉中，大脑的工作重心就从情绪模块转向了推理模块了，而且这思考也可能顺带更有效地解决了导致焦虑的问题呢:) 5. 重要的事情优先（详见史蒂芬·柯维的《高效能人士的七个习惯》或《要事优先》）。尽量避免琐事骚扰，不重要的事情能不做就不做。有时候，紧急的事情往往只是当事人觉得必须马上做完才显得紧急或者干脆就是紧他人之急，最糟糕的就是纯属性格上原因觉得每件事情都得第一时间完成，很多看上去紧急的事情实际上并不是真的\"不能再拖了\"，有的干脆就并不需要或值得去做。有很多事情都是可以先放一放甚至完全let go的，否则的话就整天被所谓\"紧急\"的事情牵着鼻子走了。 6. 重要的事情营造比较大的时间块来完成。比如一本好书，或者一个重要的知识点，最好不要切得太琐碎了看，否则看了后面忘了前面。不利于知识的组织&联系。 7. 多看心理学与思维的书，因为它们是跨学科的。知识分两种，一是我们通常所谓的知识，即领域知识。二是关于我们的大脑吸收知识的机制的知识，后者不妨称为元知识。虽说这也是领域知识，但跟其它的领域知识不同的是，它指导着我们学习其它所有的领域知识。 8. 学习一项知识，必须问自己三个重要问题：1. 它的本质是什么。2. 它的第一原则是什么。3. 它的知识结构是怎样的。 9. 获得的多少并不取决于读了多少，而取决于思考了多少、多深。 10. 善于利用小块时间，也就是《奇特的一生》中所说的“时间下脚料”，如何利用前面有几个方法。同时，也善于创造整块时间（如通过要事优先）。 11. 关于习惯的养成，必须要说明的：经常看到有些人评论说，说说容易，做起来哪有那么容易啊（另一个无关习惯的“说起来容易做起来难”则是因为纸上谈兵不可能算计到所有现实中的因素，但那是另一个问题）。对此我要说的是，做起来当然不容易，所谓江山易改，本性难移。人的性格和认识事物的框架是长期积累养成的，并且人们非常珍视自己的信念（英语里面表达不相信某个东西叫做“I don't buy it”）。从进化心理学上这是有依据的，一个经过时间检验的信念往往是更靠谱的。只不过可惜的是靠谱不代表最佳，一个信念能让你活下来并不代表能让你活得最好（详见《Mene Genes》，更多的例子参见《How we know what isn't so》）。我们评判一个信念的标准是satisficing原则（即足够，能行就好，这个术语不是我提的，是大牛Herbert Simon提的），并不是optimizing原则。话说回来，为什么说起来容易做起来难，是因为“说”只是理性上承认正确，并没有考虑到我们每个人大脑中居住的那个非理性自我。这个自我以强大的情绪力量为动机，以习惯为己任，每时每刻都驱使着我们的行为。因为它掌握了“情绪”这个武器，所以我们只能时时拿它当大爷。不记得是哪位哲学家说的了，理性是感性的奴隶。那么，是不是就是说无法克服既有习惯了？以我的经验（以及观察到的别人的经验），还是可以的。第一条就是认识到习惯的改变绝不是一天两天的事情，承认它的难度。第二条就是如果你真想改掉习惯，就需要在过程中常常注意观察自己的行为，否则习惯会以一种你根本觉察不到的方式左右你的行为让你功亏一篑。有一个认知技巧也许可以缓解更改习惯过程中的不适：即把居住在内心的那个非理性自我当成你自己的孩子（你要去培养他），或者你的对手（你要去打败他）也行。总之不能当成自己，因为每个人都不想改变自己。这里转一个认知技巧的例子：李笑来老师在《把时间当作朋友》（顺便也推荐这本开放电子书）中提到他一个朋友用另一个认知技巧来克服背单词的枯燥的： 因为，一共要搞定20,000个单词，而因此可能获得的奖学金是每年40,000美元左右——并且连续四年没有失业可能（后来的事实是，他直到五年之后才获得了博士学位）。当时的美元兑换人民币的汇率差不多是8:1，所以，大约应该相当于320,000元人民币。而如果一年的税后收入是320,000元人民币的话，那么税前就要赚取差不多400,000元人民币。那么，每个单词应该大约值20元人民币——这还只不过是这算了一年的收入而已。 所以，他终于明白背单词是非常快乐的。他每天都强迫自己背下200个单词。而到了晚上验收效果的时候，每在确定记住了的单词前面画上一个勾的时候，他就要想象一下刚刚数过一张20元人民币的钞票。每天睡觉的时候总感觉心满意足，因为今天又赚了4000块！ 注意，这跟自我欺骗不同。一来，我们的情绪系统只能这般对付（《Synaptic Self》中提到，大脑中的新皮层（neocortex，所谓“理性”居住的地方，尤其是前额叶）在进化历史上是较为新近的年代才进化出来的，跟底层较原始的模块（如主管情绪的杏仁核）之间的神经网络沟通并不是合作无间，这就解释了为什么有些事情我们明明知道是对的，但就是不能说服自己，情绪还是在那里不依不挠的驱使你去做另一样事情）。二来，我们知道在干什么，所以不能算欺骗:P 总之，对于习惯的更改，除了最重要的一日三省，加上一些认知技巧（其实每个人都是自己的心理学家，你可以自己看看能不能想出什么法子）。其实是没有什么速效银弹的。但是，知难而不退嘛，值得做的事情几乎总是如此:) 接着上次的写。 1. 学习和思考的过程中常问自己的几个问题： 你的问题到底是什么？（提醒自己思考不要偏离问题。） OK，到现在为止，我到底有了什么收获呢？（提醒自己时不时去总结，整理学习的东西）。 设想自己正在将东西讲给别人听（有声思考；能否讲出来是判断是否真正理解的最佳办法）。 3.1 设想需要讲给一个不懂的人听。（迫使自己去挖掘知识背后最本质、往往也是最简单的解释）。 时常反省和注意自己的思维过程。尤其是当遇到无法理解或解决的问题之后，最需要将原先的思维过程回顾一遍，看看到底哪个环节被阻塞住了妨碍了理解。问题到底出在哪里。并分析以后需要加强哪方面的思维习惯，才能够不在同样或类似的时候被绊住。对此，将思维的大致脉络写下来是一个很好的习惯。 养成反驳自己的想法的习惯：在有一个想法的时候，习惯性地去反驳它，问自己“这个难道就一定成立吗？”、“有没有反例或例外？”、“果真如此吗？”之类的问题。（参见Critical Thinking）  人的思维天生就是极易流于表面来理解事物的（参见《Psychology of Problem Solving》第11章）。觉得自己理解了一个问题了么？条件反射性地问自己：你真的理解了吗？你真的理解了问题的本质了？问题的本质到底是什么？目前我的理解是什么？我对这个理解感到满意吗？这样的理解到底有什么建设性呢？等等。 2. 重视知识的本质：对于程序员来说这一点尤其重要，程序员行业的知识芜杂海量，而且总是在增长变化。很多人感叹跟不上新技术。应对这个问题的办法只能是：抓住不变量。大量的新技术其实只是一层皮，背后的支撑技术其实都是十来年不变的东西。底层知识永远都不过时。算法数据结构永远都不过时。基本的程序设计理论永远都不过时。良好的编码习惯永远都不过时。分析问题和解决问题的能力永远都不过时。强大的学习能力和旺盛的求知欲永远都不过时。你大脑的思维方式永远都不过时。 3. 重视积累的强大力量，万事提前准备：计划订长一点，自然就可以多获得准备的时间。设想你若干年后会在做什么事情，需要哪些技能，现在就开始准备。一个5年计划便可以让你获得从现在开始的5年准备时间。5年中每天腾出半个到一个小时专心于某一件事情，认准一个方向，每次走一点，其实不要说5年，两年就会发现会起到宏大的效应。长期订阅我的Blog的朋友们也一定注意到我基本上不写东西，一般一个月写上2篇就算多的了。但总结一段时间的学习和思考的习惯却一直都没有停止（博客文章对我来说是学习和思考的副产品，我并不为写文章而写文章），所以5年下来竟也写了不少东西。所以这就是一个简单的例子。你大致还可以从我的Blog看出来我一段时间关注的东西，一般来说，一段比较长的时间（少则半年至一年——譬如对心理学与思维的关注；多则几年——譬如对编程技术的关注），在这段时间内，我的业余时间会被一个主题所充斥。反之，如果不知道目的是什么，就不知道往哪个方向上使劲，就容易产生无用功。 4. 抬起头来：人的思维是非常容易只见树木不见森林的（否则这个成语从哪来的呢？）。时不时抬起头来审视一下自己正在做的事情，问一问它（对现在或未来）有什么价值，是不是你真正希望做的。你学到的东西到底是什么？它们重要吗？你需要在这个时候学习这些吗？（见第2条）。你的时间就是你的资源，你投入这些资源来掌握知识，所以到底用来掌握哪些知识是一个很重要的问题。仅仅遵循兴趣是不够的，人会对很多次要的东西产生兴趣，并一头钻进去浪费好多时间。所以判断一个东西值不值得学习是很重要的。 杂项 1. 退订RSS：RSS Reader是个时间黑洞。就算mark all as read，在有大量feed的情况下，也会无形中消耗掉大量的时间。我们一旦订阅了某个RSS之后就会倾向于不肯退订它，心想也许某天有个重要的信息会从那里得到。这其实是源于人不肯\"关上一扇门（即便门内的收益概率极小）\"的心理（参见《Predictably Irrational》）；而实际上，关上一扇门，有时能够增大收益期望。仔细观察一下reader里面的feeds，有哪些是真正有价值的，把那些没价值的或者价值很小乃至于不值得每天被它骚扰的，全都退订掉。不要舍不得，那些一个星期都没出现让你眼睛一亮的内容的feed，很大的可能是永远也不会出现。就算可能，也别担心你会漏掉什么宝贵信息，真正宝贵的信息，在其他来源你也会接触到的。一开始我的Greader里面每天都有大量的新内容，每天都是1000+，但一段时间之后发现除了信息焦虑，实际上有价值的内容不多，现在，我很高兴地发现自己摆脱了这种状况，我持续不断地退订feeds，留下的内容越来越少，也越来越精，带来的阅读焦虑也越来越少。（顺便推荐一个东西，aideRSS，初步使用，感觉对订阅reddit这样的每天更新大量内容的feed很有用）。 2. 有时间吗？总结总结最近得到的新知识吧。一般来说，我在一段时间内学习的一些东西总是会在这段时间内一直在脑子里打转，一有时间空隙（譬如走路，吃饭）它们就会自己蹦出来，促使我去进一步思考和总结。永远不要认为对一个知识的把握足够深刻，“理解”的感觉很多时候只是假象。学会反问自己对知识到底把握了多少，是很有价值的。（如何反问，前面的总结中有提到）。 3. 有时间吗？看本书吧。（传统的）阅读和思考永远优于所谓的在互联网上汲取新知识，后者往往浅表、不系统、乃至根本没价值。 4. 制定简要的阅读计划：选出最近认为对你最有价值的书，先总览一下，决定阅读的顺序（哪些章节可以优先阅读）。然后每天看一点。并利用走路、吃饭、乘车或其他不适合带着书和笔的时间来总结看过的内容，建立知识结构，抽取知识本质，与以往的大脑中的知识建立联系。（参见《奇特的一生》） 这篇主要写一些学习（尤其是阅读）的基本方法。 1. 趁着对一件事情有热情的时候，一股脑儿把万事开头那个最难的阶段熬过去。万事开头难，因为从不了解到了解基本的一些事实，是一个新知识暴涨的阶段，这个时候的困难是最大的。有人熬不过去，觉得困难太大就放弃了。不过，狂热的兴趣可以抵消对困难的感觉，所以趁着对一件事情有热情的时候，开一个好头是很重要的。（当然，这并不是说持之以恒就不重要了）。当然，也许这个是因人而异的，对我来说我会在对一件事情有浓厚兴趣的时候非常专注地学习，把很多 groundworks 做掉。后面就会顺利一些了。 2. 根据主题来查阅资料，而不是根据资料来查阅主题。以前读书的时候是一本一本的读，眼里看到的是一本一本的书，现在则是一章、甚至一节一节的读，眼中看到的不是一本一本的书，而是一堆一堆的章节，一个一个的知识主题，按照主题来阅读，你会发现读的时候不再是老老实实地一本书看完看另一本，而是非常频繁地从一本书跳到另一本书，从一处资料跳到另一处资料，从而来获得多个不同的人对同一个主题是如何讲解的。比如最近我发现在看蒙特卡罗算法时就查了十来处资料，其中有三四篇 paper 和六七本书；这是因为即便是经典的书，你也不能指望它对其中每一个主题的介绍都是尽善尽美的，有些书对某个主题（知识点）的介绍比较到位，有些书则对另一些知识点介绍得比较到位。而有时候一篇紧凑的 paper 比一本书上讲得还要好。我硬盘里面的书按主题分类，每个主题下面都有一堆书，当我需要学习某个主题的知识时（譬如贝叶斯学习或者神经网络），我会把里面涉及这个主题的书都翻开来，索引到相关章节，然后挑讲得好的看。那么，如何判断一个资料是好资料还是坏资料呢？ 3. 好资料，坏资料。好资料的特点：从问题出发；重点介绍方法背后的理念（ rationale ），注重直观解释，而不是方法的技术细节；按照方法被发明的时间流程来介绍（先是遇到了什么什么问题，然后怎样分析，推理，最后发现目前所使用的方法）。坏资料的特点是好资料的反面：上来就讲方法细节，仿佛某方法是从天上掉下来的，他们往往这样写“我们定义... 我们称... 我们进行以下几个步骤... ”。根本不讲为什么要用这个方法，人们最初是因为面对什么问题才想到这个方法的，其间又是怎样才想出了这么个方法的，方法背后的直观思想又是什么。实际上一个方法如果将其最终最简洁的形式直接表达出来往往丢失掉了绝大多数信息，这个丢掉的信息就是问题解决背后的思维过程。至于为什么大多数书做不到这一点，我在这里试着分析过。 4. 学习一个东西之前，首先在大脑中积累充分的“疑惑感”。即弄清面临的问题到底是什么，在浏览方法本身之前，最好先使劲问问自己能想到什么方法。一个公认的事实是，你对问题的疑惑越大，在之前做的自己的思考越多，当看到解答之后印象就越深刻。记得大学里面的课本总是瀑布式地把整个知识结构一览无余地放在面前，读的过程倒是挺爽，连连点头，读完了很快又忘掉了，为什么？因为没有带着疑问去学习。 5. 有选择地阅读。很多人觉得我读书速度很快，其实我只是有选择地阅读。这里的选择体现在两个地方，一是选择一本书中感兴趣的章节优先阅读。二是对一本书中技术性较弱或信息密度较低的部分快速地略读。一般来说，除了技术性非常强的书之外，大多数书的信息密度很低，有很多废话。一般来说在阅读的时候应该这样来切分内容：1. 问题是什么？2. 方案是什么？3. 例子是什么？如果是需要解释一个现象的（譬如《黑天鹅》），那么1. 现象是什么？2. 解释是什么？3. 支撑这个解释的理由是什么？4. 例子是什么？一般来说，这一二三四用不了多少字就可以写完了（如果假设只举一到两个精到的例子的话），这样的无废话著作的典型是《合作的进化》；那为什么有些书，明明核心观点就那点东西（顶多加上几个精要的例子罢了）却写得长得要命呢？因为人的思维都有一个“联想”的特点，写着写着就容易旁逸斜出，而且作者自己也往往觉得引申出去挺牛逼，有时候很多与主题无关的废话就掺和进来了；那么，阅读的时候就应该有选择性地滤掉这些不相干的废话；此外还有一种可能性就是大量冗余的例子。一般来说组织得比较好的书会有详细且一目了然的目录和索引，根据目录首先就可以滤掉一部分（比如某个子章节的内容你以前是看过的），然后有时候作者还会举很多冗余的例子，如果你已经觉得印象够深刻了这些例子完全可以不看（一些书就非常厚道地对每个观点只辅以一两个最最经典的例子，譬如《与众不同的心理学——如何正视心理学》，这样的书我最是喜欢）。 6. 为什么看不懂？如果看不懂一个知识，一般有如下几个可能的原因：1. 你看得不够使劲。对此古人总结过——书读百遍其义自现。虽然这个规律不是任何时候都成立的，但是从认知科学的角度看是完全可以解释的，我们在阅读的时候，注意力往往会有选择性地关注其中的某一些“点”，而忽略了另一些“点”，于是一遍看下来可能因为某一些忽略导致无法理解整体。或者干脆看的时候就没注意其中一些细节但重要的东西。此外，大脑理解一个东西需要一定的处理时间，人脑的处理速度很慢，神经冲动每秒传输速度不过百米，所以不能指望看到哪懂到哪。最后，我们可能因为思维定势的原因会从某个特定的角度去看一句话而忽略了从不同角度去理解的可能性。对于这类情况，仔仔细细地再多读两遍，多试着去理解两遍，往往会“哦！原来这样。”地恍然大悟。2. 其中涉及到了你不懂的概念。这是技术性的不理解。这种情况就需要 Cross Reference 。如果一句话中用到了你不懂的概念，那就去查，现在很多书都是电子书，直接搜索一下，或者，对于纸书，看一下书后面的索引就行了。奇怪的是很多人看不懂也不分析一下为什么不懂，就直接放弃了。正如解决问题一样，问题卡住解决不了，第一时间要做的就是分析到底为什么解决不了，而不是直接求救。3. 作者讲述的顺序不对，你接着往下看，也许看到后面就明白了前面的了。   杂项 7. 如何在阅读之前就能获得对一本书质量的大致评估。在深入阅读之前能够迅速评估一本书的质量可以节省很多时间。基本上有几个线索：1. 看作者。牛作者写的书一般都不错。2. 看目录和简介。一份好的目录和简介能够透露这本书质量的相当一部分信息。目录结构是否清晰，是否直白（而不是装神弄鬼），都是衡量的线索。3. 看 Amazon 上的评价，这里要注意的是，除了看整体打分之外，更要看打分最低的人是怎么说的，因为小众意见往往有可能来自那些真正懂行的人（除了来踢馆的），如果在打分最低的意见里面看不到真正有价值的反驳意见的话就相当肯定书是不错的了。4. 看样章。Amazon 上一般都可以随机浏览一些章节的，表达是否清晰，论证是否严谨，内容是否深刻，基本是几页纸就能看出来的。 8. 如何搜寻到好书。几个线索：1. 同作者的著作。2. Amazon 相关推荐和主题相关的书列（类似豆瓣的豆列）。3. 一本好的著作（或一份好的资料——不管是书还是网页）在参考资料里面重点提到的其他著作。4. 有时对于一个主题，可以搜索到好心人总结的参考资源导引，那是最好不过的。 自从建立了 TopLanguage 以来，发现在上面待的时间越来越多，与高手讨论问题是个粘性十足的事情，一方面，分享自己的认识是整理不成熟的想法的极好途径，另一方面，互相之间视角不同，所以往往自己忽视的地方会被别人发现。在讨论中不断精化既有的知识体系。以下这段基本上摘抄自（略有整理和添加）在 TopLanguage 上的发言： 抓住不变量 我喜欢把知识分为essential的和non-essential的。对于前者采取提前深入掌握牢靠的办法，对于后者采取待用到的时刻RTM (Read the manual)方法（用本）。 如何区分essential和non-essential的知识想必绝大多数时候大家心里都有数，我举几个例子：对程序员来说，硬件体系结构是essential的，操作系统的一些重要的实现机制是essential的，主流编程范式（OO、FP）是为了满足什么需求出现的（出现是为了解决什么问题），是怎么解决的，自身又引入了哪些新的问题，从而适用哪些场景）。 这些我认为都是essential的。我想补充一点的是，并不是说硬件体系结构就要了解到逻辑门、晶体管层面才行（其实要了解到这个层面代价也很小，一两本好书就行了），也并不是说就要通读《Computer Architecture: Quantitative Approach》才行。而是关键要了解那些重要的思想（很长时间不变的东西），而不是很细的技术细节（易变的东西）。《Computer Systems: A Programmer’s Perspective》就是为此目的，针对程序员的需求总结出那些essential knowledge的好书。 再来说一下为什么需要预先牢靠掌握这些essential的知识： 根据Joel Spolsky同学的说法（原文），编程语言技术是对底层设备的封装，然而封装总是会出现漏洞的，于是程序员被迫下到“下水道”当中去解决问题，一旦往下走，漂亮的OO、N层抽象就不复存在了，这时候不具备坚硬的底层知识就会无法解决问题。简而言之就是这些底层知识会无可避免的需要用到，既然肯定会被用到那还是预先掌握的好，否则一来用到的时候再查是来不及的，因为essential的知识也往往正是那些需要较长时间消化掌握的东西，不像Ruby的mixin或closure这种翻一下manual就能掌握的东西。（英语也是这样的essential knowledge——上次在PyCN上看到一个招Python开发人员的帖子将英语列为必备技能，却并不将自然语言处理列为必备技能，正是因为英语不是可以临阵磨枪的东西，而且作为知识的主要载体，任何时候都少不了它，如果不具备英语能力，这个就会成为个人知识结构的短板或瓶颈，而且由于需要长时间才能获得这项能力，所以这个瓶颈将持续很长时间存在。我们曾经在 TopLanguage 上讨论过如何花最少的时间掌握英语）另一方面，在问题解决当中，如果不具备必要的知识，是根本无从思考的，再好的分析能力也并不是每个问题都能分析出该用哪些知识然后再去查手册的，很多时候是在工具和问题之间比较，联想，试探性的拼凑来解决问题；这就使得一个好的既有知识基变得至关重要。（实际上以上这个是一个较大的话题，希望有一天我能够把它详细展开说清:)） 如果你不知道某个工具的存在，遇到问题的时候是很难想到需要使用这么样一个工具的，essential knowldge就是使用最为广泛的工具，编程当中遇到某些问题之后，如果缺乏底层知识，你甚至都不知道需要去补充哪些底层知识才能解决这个问题。 你必须首先熟悉你的工具，才能有效地使用它（须知工具的强是无敌的，但这一切得以“了解你的工具”为前提，甚至得以“了解目前可能有哪些工具适合你的问题”为前提）。一门语言，你必须了解它的适用场景，不适用场景（比如继承能解决你的问题不代表继承就是解决你的问题的最适合的方案，须知问题是一个复杂系统，解决方案总是常常引入新的问题）。你必须了解它支持的主要编程范式，此外你还必须了解它的traps和pitfalls（缺陷和陷阱，如果不知道陷阱的存在，掉进去也不知道怎么掉的。）这些都是essential knowledge，如果不事先掌握，指望用的时候查manual，是很浪费时间的，而且正如第2点所说，正因为你不知道这些知识（如适用场景），从而用sub-optimal的方式使用了一门语言自己可能还不知道（最小白的例子是，如果你不知道语言支持foreach，那么可能每次都要写一个冗长的循环，较常见的例子是不知道有很方便的库设施可以解决手头的问题所以傻乎乎的自己写了一堆代码），因为人的评价标准常常是：只要解决了最醒目的问题并且引入的新问题尚能忍受，就行。注意，熟悉并非指熟悉所有细节，而是那些重要的，或者无法在需要用到的时候按需查找的知识。比如上面提到的：适用场景不适用场景，编程范式，主要语言特性，缺陷和陷阱。 当然，以上作为程序员的essential knowledge列表并不完备，关键是自己在学习新知识的时候带着第三只眼来敏锐地判断这个知识是否是不变量，或不易变的量，是否完全可以在用的时候查手册即可，还是需要提前掌握（一些判断方法在上文也有所提及）。并且学会在纷繁的知识中抽象出那些重要的，本质的，不变的东西。我在之前的part里面也提到我在学习新知识的时候常常问自己三个问题：该知识的（体系或层次）结构是什么、本质是什么、第一原则是什么。 另外还有一些我认为是essential knowledge的例子：分析问题解决问题的思维方法（这个东西很难读一两本书就掌握，需要很长时间的锻炼和反思）、判断与决策的方法（生活中需要进行判断与决策的地方远远多于我们的想象），波普尔曾经说过：All Life is Problem-Solving。而判断与决策又是其中最常见的一类Problem Solving。尽管生活中面临重大决策的时候并不多，但另一方面我们时时刻刻都在进行最重大的决策：如：决定自己的日常时间到底投入到什么地方去。如：你能想象有人宁可天天花时间剪报纸上的优惠券，却对于房价的1%的优惠无动于衷吗？（《别做正常的傻瓜》、《Predictably Irrational》）如：你知道为什么当手头股票的股价不可抑止地滑向深渊时我们却一边揪着头发一边愣是不肯撤出吗？（是的，我们适应远古时代的心理机制根本不适应金融市场。）糟糕的判断与决策令我们的生活变得糟糕，这还不是最关键的，最关键的是我们从来不会去质疑自己的判断，而是总是能“找到”其他为自己辩护的理由（《错不在我（Mistakes were made, but not by me）》）又，现在是一个信息泛滥的时代，于是另一个问题也出现：如何在海洋中有效筛选好的信息，以及避免被不好的信息左右我们的大脑（Critical Thinking）关于以上提到的几点我在豆瓣上有一个专门的豆列（“学会思考”），希望有一天我能够积累出足够多的认识对这个主题展开一些详细介绍。 最后分享一个学习小Tip： 学习一个小领域的时候，时时把“最终能够写出一篇漂亮的Survey”放在大脑中提醒自己，就能有助于在阅读和实践的时候有意无意地整理知识的结构、本质和重点，经过整理之后的知识理解更深刻，更不容易忘记，更容易被提取。 杨军在 TopLanguage 上也曾分享了三篇非常棒的学习心得的文章，字字珠玑： [1] 有些事情做起来比想象中容易 [2] 有关读书方法的一点想法 [3] 一件事情如果你没有说清楚，十有八九不能做好 最后告知大家，TopLanguage 最近经历了一次很大的管理策略修订，可以预期将彻底摆脱这两个月来的噪音问题，未来的讨论质量将会越来越高。详情可参见这里。","title":"一直以来伴随我的一些学习习惯"},{"content":"分词 1.基于词典 基于词典的分词方法，这种方法又叫做机械分词方法,它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配,若在词典中找到某个字符串,则匹配成功(识别出一个词) 。常用的方法：最小匹配算法(Minimum Matching)，正向（逆向）最大匹配法(Maximum Matching)，逐字匹配算法,神经网络法、联想一回溯法，基于N-最短路径分词算法,以及可以相互组合，例如,可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法等。目前机械式分词占主流地位的是正向最大匹配法和逆向最大匹配法。  1.1基于匹配 1.1.1最小匹配(不再采用) 在所有的分词算法中，最早研究的是最小匹配算法(Minimum Matching)，该算法从待比较字符串左边开始比较，先取前两个字符组成的字段与词典中的词进行比较，如果词典中有该词，则分出此词，继续从第三个字符开始取两个字符组成的字段进行比较，如果没有匹配到，则取前3个字符串组成的字段进行比较，依次类推，直到取的字符串的长度等于预先设定的阈值，如果还没有匹配成功，则从待处理字串的第二个字符开始比较，如此循环。 例如，“如果还没有匹配成功”，取出左边两个字组成的字段与词典进行比较，分出“如果”；再从“还”开始，取“还没”，字典中没有此词，继续取“还没有”，依次取到字段“还没有匹配”(假设阈值为 5)，然后从“没”开始，取“没有”，如此循环直到字符串末尾为止。这种方法的优点是速度快，但是准确率却不是很高，比如待处理字符串为“中华人民共和国”，此匹配算法分出的结果为：中华、人民、共和国，因此该方法基本上已经不被采用。 1.1.2最大匹配 基于字符串的最大匹配，这种方法现在仍比较常用。最大匹配(Maximum Matching)分为正向和逆向两种最大匹配，正向匹配的基本思想是：假设词典中最大词条所含的汉字个数为n个，取待处理字符串的前n个字作为匹配字段，查找分词词典。若词典中含有该词，则匹配成功，分出该词，然后从被比较字符串的n+1处开始再取n个字组成的字段重新在词典中匹配；如果没有匹配成功，则将这n个字组成的字段的最后一位剔除，用剩下的n一1个字组成的字段在词典中进行匹配，如此进行下去，直到切分成功为止。 例如，待处理字符串为“汉字多为表意文字”，取字符串“汉语多为表”(假设比较的步长为5，本文步长step都取5)与词典进行比较，没有与之对应的词，去除“表”字，用字段“汉语多为”进行匹配，直至匹配到“汉语”为至，再取字符串“多为表意”，循环到切分出“文字”一词。目前，正向最大匹配方法作为一种基本的方法已被肯定下来，但是由于错误比较大，一般不单独使用。如字符串“处理机器发生的故障”，在正向最大匹配方法中会出现歧义切分，该字符串被分为：处理机、发生、故障，但是使用逆向匹配就能得到有效的切分。     逆向最大匹配RMM(Reverse Directional Maximum Matching Method)的分词原理和过程与正向最大匹配相似，区别在于前者从文章或者句子(字串)的末尾开始切分，若不成功则减去最前面的一个字。比如对于字符串 “处理机器发生的故障”，第一步，从字串的右边取长度以步长为单位的字段“发生的故障”在词典中进行匹配，匹配不成功，再取字段“生的故障”进行匹配，依次匹配，直到分出“故障”一词，最终使用RMM方法切分的结果为：故障、发生、机器、处理。该方法要求配备逆序词典。       一般来说根据汉语词汇构成的特点，从理论上说明了逆向匹配的精确度高于正向匹配，汉语语句的特点一般中心语偏后。有研究数据,单纯使用正向最大匹配的错误率为1/ 169 ,单纯使用逆向最大匹配的错误率为1/245。实际应用中可以从下面几方面改进，同时采取几种分词算法，来提高正确率;改进扫描方式，称为特征扫描或标志切分,优先在待分析字符串中识别和切分出一些带有明显特征的词,以这些词作为断点,可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率等。 1.1.2.1正向最大匹配 将待分析的文本和一个充分大的词典中的词条进行匹配方向从左向右 1.1.2.2逆向最大匹配 将待分析的文本和一个充分大的词典中的词条进行匹配方向从右向左对汉语，逆向最大匹配比正向最大匹配更有效 1.1.3双向匹配 对比正向最大匹配和逆向最大匹配的结果，从而决定正确的分词 1.1.4逐字匹配 逐字匹配算法，基于TRIE索引树的逐字匹配算法,是建立在树型词典机制上，匹配的过程是从索引树的根结点依次同步匹配待查词中的每个字，可以看成是对树某一分枝的遍历。因此，采用该算法的分词速度较快，但树的构造和维护比较复杂。一种改进的算法是和最大匹配算法相结合，吸取最大匹配算法词典结构简单、 TRIE索引树算法查询速度快的优点。因此词典结构和最大匹配词典构造机制相似，区别在于词典正文前增加了多级索引。匹配过程类似TRIE索引树进行逐字匹配，在性能上和TRIE索引树相近。 1.2神经网络 神经网络分词算法，尹峰等提出了以神经网络理论(BP模型)为基础的汉语分词模型,为汉语分词研究开辟了新途径。在实用中,BP算法存在收敛速度慢、易陷入局部最小等缺点,严重妨碍了分词速度。一种改进算法采用Levenbery2Marquart 算法来加速收敛速度,加快了收敛速度利用神经网络的基本原理进行分词。 1.3最佳匹配 将词典中的单词按照它们在文本中出现的频度大小排列，高频的单词排在前面，低频的排在后面，从而提高匹配速度     1.4联想-回溯    联想—回溯法(Association－Backtracking Method，简称 AB 法)。这种方法要求建立三个知识库——特征词词库、实词词库和规则库。首先将待切分的汉字字符串序列按特征词词库分割为若干子串，子串可以是词，也可以是由几个词组合而成的词群；然后，再利用实词词库和规则库将词群再细分为词。切词时，要利用一定的语法知识，建立联想机制和回溯机制。联想机制由联想网络和联想推理构成，联想网络描述每个虚词的构词能力，联想推理利用相应的联想网络来判定所描述的虚词究竟是单独成词还是作为其他词中的构词成分。回溯机制主要用于处理歧义句子的切分。联想—回溯法虽然增加了算法的时间复杂度和空间复杂度，但这种方法的切词正确率较高，是一种行之有效的方法。 1.5N-最段路径分词算法 基于N-最短路径分词算法，其基本思想是根据词典，找出字串中所有可能的词，构造词语切分有向无环图。每个词对应图中的一条有向边，并赋给相应的边长(权值)。然后针对该切分图，在起点到终点的所有路径中，求出长度值按严格升序排列(任何两个不同位置上的值一定不等，下同)依次为第1，第2，…，第 i，…，第N的路径集合作为相应的粗分结果集。如果两条或两条以上路径长度相等，那么他们的长度并列第 i，都要列入粗分结果集，而且不影响其他路径的排列序号，最后的粗分结果集合大小大于或等于N。N一最短路径方法实际上是最短路径方法和全切分的有机结合。该方法的出发点是尽量减少切分出来的词数，这和最短路径分词方法是完全一致的；同时又要尽可能的包含最终结果，这和全切分的思想是共通的。通过这种综合，一方面避免了最短路径分词方法大量舍弃正确结果的可能，另一方面又大大解决了全切分搜索空间过大，运行效率差的弊端。N一最短路径方法相对的不足就是粗分结果不唯一，后续过程需要处理多个粗分结果。但是，对于预处理过程来讲，粗分结果的高召回率至关重要。因为低召回率就意味着没有办法再作后续的补救措施。预处理一旦出错，后续处理只能是一错再错，基本上得不到正确的最终结果。而少量的粗分结果对后续过程的运行效率影响不会太大，后续处理可以进一步优选排错，如词性标注、句法分析等。   1.6最大概率 （1）一个待切分的汉字串可能包含多种分词结果 （2）将其中概率最大的那个作为该字串的分词结果 1.7最短路径 基本思想：在词图上选择一条词数最少的路径 优点：好于单向的最大匹配方法 最大匹配：独立自主/和平/等/互利/的/原则(6) 最短路径：独立自主/和/平等互利/的/原则(5) 缺点：同样无法解决大部分歧义 1.2基于统计 基于统计的分词方法，基于统计的方法是基于(两个或多个) 汉字同时出现的概率,通过对语料库(经过处理的大量领域文本的集合)中的文本进行有监督或无监督的学习．可以获取该类文本的某些整体特征或规律。如果能够充分地利用这些统计现象、规律．就可以构造基于语料库的统计学信息抽取算法统计的分析方法多种多样．近来研究的热点主要集中于由随机过程发展而来的理论和方法，其中最重要的是应用隐马尔科夫模型(HMM) 进行自然语言处理的方法。隐马尔科夫模型,在语音识别领域已经取得了很好的成效,在信息抽取领域的应用也正在不断的尝试和推广中。     支持向量机（SVM） 最大熵（Maximum Entropy） 隐马模型（HMM） 最大熵隐马模型（MEMM） 条件随机场（CRFs）","title":"分词算法整理"},{"content":"最近研究LDA挖掘隐含topic来对短文本分类，没想到师弟fandywang转载了一篇文章，可以作为一个很好的参考：     最近几年来，随着LDA的产生和发展，涌现出了一批搞Topic Model的牛人。我主要关注了下面这位大牛和他的学生： David M. Blei LDA的创始者，04年博士毕业。一篇关于Topic Model的博士论文充分体现其精深的数学概率功底；而其自己实现的LDA又可体现其不俗的编程能力。说人无用，有论文为证： J. Chang and D. Blei. Relational Topic Models for Document Networks . Artificial Intelligence and Statistics , 2009. [PDF ]        基本LDA模型，当然假设文档之间是可交换的，那么在原始的LDA中文档之间其实是认为条件独立的。而在实际情况中，往往不是这个样子的，文档间也许会存 在“social network”的这样的网络性质。如何结合内容和“social network”这两个特征也许是一个非常有意思的话题。这篇论文就是给出了一个解决方法。它为两个文档之间增加了一个二元随机变量，根据其内容特征，来 刻画这种隐含的链接关系。        关于显示的链接关系是过去今年内，人们追逐研究的对象，进而产生PageRank、HITS等等一大批优秀的链接关系算法。那么如何利用隐含的链接呢？什 么是隐含的链接呢？一个最简单的隐含链接就是基于内容相似度构建的图。这个被人们用的不亦乐乎，比如在文摘中的LexRank等。O Kurland在SIGIR中发了两篇大概都是类似的文章，本质思想貌似就是在利用内容之间的“超链接”。         另外一个比较新颖的研究点，就是如何基于“social network”来挖掘内容特征？ Mei Qiaozhu的一篇论文就是利用“social network”的网络结构特征最为规则化因子，重新修正了原始的PLSA模型。想法非常的新颖。 D. Blei and J. Lafferty. Topic Models. In A. Srivastava and M. Sahami, editors, Text Mining: Theory and Applications . Taylor and Francis, in press. [PDF ]     这篇论文是一篇综述性的大制作的论文，Blei在里面深入浅出的介绍了什么是Topic Model以及他早期的一些Topic Model的变形。值得大家去阅读。 J. Boyd-Graber and D. Blei. Syntactic Topic Models . Neural Information Processing Systems , 2009. [PDF ] [Supplement ]    原始的LDA考察两个词只是基于共现的角度。而实际情况中，这种共现往往是不能够精确地刻画一些句子结构信息或者说词义信息。如何把这种信息引入。考虑 更深层的生成模型是目前一个热点。这篇论文着眼于一个句子的句法分析的生成过程，它认为每个句子的生成都是基于“parse tree”的，整个概率生成过程完全附着在“parse tree”上了。并且每个句子内，不同的词都有可能去选择更适合自己的Topic。 D. Blei, J. McAuliffe. Supervised topic models . In Advances in Neural Information Processing Systems 21, 2007. [PDF] [digg data ]    现如今，网络数据除了纯内容外，往往还有其他一写辅助信息，如用户对于某博文的评价或者说用户对于某商品的评价。一个最典型的例子，就是说在当当买书 后，你可以给该书的质量进行打分：5星代表最好，4星代表比较好，。。。依次类推。那么如何把这些信息加入原始的LDA中呢？ Blei为其引入了一个response变量因子，该因子条件依赖于该文档的topic distribution。     如何把ratable information和内容有机地结合起来也是最近的一个研究热点。大多数方法还都是，建立一个ratable response variable，然后该变量条件依赖于内容或者说Topic信息。 J. Boyd-Graber, D. Blei, and X. Zhu. A topic model for word sense disambiguation . In Empirical Methods in Natural Language Processing, 2007. [PDF]     这篇论文对应的一个大背景是把Topic Model应用到自然语言处理中，具体内容我没太看，主要是结合了WordNet的结构特征，在此基础上产生的图模型。     此外的一些工作还有把Topic Model用来文摘和词性标注中的。应用到这些问题的两个主要思路：第一个就是用Topic Model去学习出一些compact features，然后在次基础上利用分类器等机器学习方法；另外一种就是利用原始NLP问题的一些结构信息，比如刚才所说的WordNet中的网络结 构，在这个结构特征中推导出整个图模型的概率生成过程。 D. Blei and J. Lafferty. A correlated topic model of Science . Annals of Applied Statistics. 1:1 17–35. [PDF ] [shorter version from NIPS 18] [code ][browser ]    还没有认真看，这个其实打破了原来topic之间的可交换性。 D. Blei and J. Lafferty. Dynamic topic models . In Proceedings of the 23rd International Conference on Machine Learning, 2006. [PDF ]    也没有仔细看，把Topic Model和时间维度结合了起来。Mei Qiaozhu也有一篇是研究话题内容随着时间变化的论文，但是是基于PLSI和HMM来完成的。 T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. Integrating topics and syntax . In Advances in Neural Information Processing Systems 17, 2005. [PDF ]    这篇论文是一篇非常优秀的论文，开篇详细地叙述了词的不同功能分类，也叫做HMM-LDA模型。正如每个人存在都有其社会意义，那么词存在对于文本语义 的表述也有着不同的角色。作者把词分为了两大功能：第一个就是semantic功能，也就是之前我们所有的Topic word；另一个功能就是说语法功能，也就是说这些词的存在是为了让整个句子的生成过程看起来更像一个完整体或者说更符合语言规范。T. Griffiths和M. Steyvers是两个很优秀的学者，他们开发了topic model工具包，并且也有一堆的牛论文。 D. Blei. Probabilistic Models of Text and Images . PhD thesis, U.C. Berkeley, Division of Computer Science, 2004. [PDF ]    Blei的博士论文，我至今还没有看完，因为一直纠结在那个Varitional inference的推导。自己责备一下自己。 D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation . Journal of Machine Learning Research, 3:993–1022, January 2003. [A shorter version appeared in NIPS 2002]. [PDF ] [code ]         LDA的第一篇文章，不算很好读懂。初次阅读时，一般会遇到可交换性、variational inference、simplex等等细节问题。经典中的经典。 D. Blei and P. Moreno. Topic segmentation with an aspect hidden Markov model . In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 343–348. ACM Press, 2001. [PDF ]    SIGIR中的一篇关于分段的论文。其实分段这个事情在现实中需求量比较大，但是成熟的工具包并不多，或者我不知道。比较好的成熟的算法一般还是基于语 义斜率的变化来计算的。在次召唤下懂这方面的大牛推荐几个好用的工具。与分段关联很紧密的一个问题就是网页正文抽取，同样也是这个问题，发论文的多，但是 实际release出来代码的很少。比较著名的，如VIPS，但是我没有用过。昨天发现VIPS的作者原来也是一个巨牛的中国人，Deng Cai。之前是清华学生，现在师从Jiawei Han，各种牛会议和牛期刊发了N多的文章。在此膜拜一下。 总结        目前我能看懂的Topic Model的文章还是很少一部分，自己的概率和数学基础太差，对于posterior inference往往无能为力，这也是下一步我的目标。并且自己其实也不太会创新，下一步也是要在这个方面多下功夫，争取应用Topic Model来解决自己的实际问题。","title":"基于LDA的Topic Model变形"},{"content":"既然学了模式识别这个专业，研究生期间主要方向是机器学习，计算机视觉，图像处理。所以很想了解现在这个领域的就业方向及相关要求。 今天在“增强视觉 | 计算机视觉 增强现实”上看到一则招聘智能图像/视频处理工程师的广告，岗位要求如下： 动手能力强，熟练掌握C/C++/Matlab语言,有较强的算法分析和实现能力，并具备良好的代码与文档风格； 了解人脸识别、目标检测、跟踪和识别、图像处理等技术，具备一年以上实际工作经验； 参与项目需求分析、负责设计完成需求规格、软件架构、测试策略，撰写相关的技术文档； 搭建研发环境，完成系统中相关软件模块的编码、调试、单元测试、功能验证，保证项目进度和产品质量； 协助完成项目的系统集成测试、版本交付等工作，对项目实施和维护提供支持；      于是搜了一些关于图像、视频处理方面的就业要求，做一下总结，以便让自己明确研究生期间的学习任务，即便做不成科学家，也得有点 技术吧。     1.编程语言：熟悉C++/C/OpenCV/Matlab开发语言,主要是有较强的C++/C图像处理编程能力，绝大多数岗位都要求熟悉OpenCV，Matlab在理论验证阶段比较方便，但是在操作硬件和效率上远不及C++/C，所以一般公司对此没要求。以后还要仔细学习C++/C，还是先从C++ primer开始，之后再看些effective系列的。另外，还要求具备良好的代码与文档风格，以后写代码还是要讲求规范了。         2.知识储备：机会总是青睐有准备的人。图像处理应用很广，因此不同岗位要求侧重不同，最基本都必须掌握图形处理的开发与研究，熟悉图像处理的各种算法，特别是图像去燥、图像增强、复原、质量改善、检测、色彩科学、图像分割、图像识别处理、图像跟踪、图像的获取及视频处理，具体应用包括人脸识别、医学影像处理、多点识别、文字检测与是识别。特别的，结合不同应用，还需要自然语言处理知识。另外，要有优秀的数学功底（特别是线性代数、优化理论、统计知识）。      3.英语水平：优秀的英文写作技能，英语口语流利。主要是能读懂英文技术文档，在研究院还需要写论文，公司更需要写技术文档，所以平时得多积累专业词汇。至于英语口语，还是从听力开始吧，中科院自动化所的图像处理、机器学习的课都很经典，大多数是留学归来的年轻教师，可以开拓思路，顺便积累专业词汇。这个寒假要把图像处理的课听完，再多做些试验。个人还比较喜欢“The Big Bang Theory”,悠闲加娱乐。    4.社交能力：硬件条件之后就是软件条件。大多数公司如是说：“良好的表达能力、团队合作精神和创新能力”。我觉得就是社交能力，HR看你合不合群。有个同学去参加面试，参加面试的还有研究生和来自理工科背景更雄厚的本科生，在群面、无领导小组讨论和辩论阶段，很多背景很强的学生却因太过张扬和表现自己被pass（这是同学分析的原因）。看来沉着、稳重还是必要的，谁也不想招个老板进来。而所谓的创新能力，另一个同学的面试经历给了我很大启发。他现在已经实习两个月了，回顾自己面试，他觉得正像面试官说的：“你还没入门呢”。而之所以拒掉N个研究生，选择同学这个应届毕业生，主要是他对这个方向很有热情，本科期间在毫无指导的情况下，主动接触了很多零零碎碎的东西，帮助同学和老师解决了一个又一个稀奇古怪的问题。据他说，这些东西在之后的工作中没一样能用到的，但是没有这些基础又是绝对干不了活的。之后的工作也是，没有人盯着他干活，老板给个指标，就放手做去吧，老板只要个结果。公司里每个人基本都要独挡一面，有时候还得独挡几面，所以非关键难题请教前辈，其他细节问题别人也不清楚，清楚也没时间指导。同学主动去学，去解决问题的能力是最让老板放心的。从他的话中体现出来的热情才是他“创新能力”的原动力。       额……要学的真多，鸭梨倍增，貌似都是本科没学过的，学过的，不常用，也忘差不多了。曾经的那些模数电，控制神马的似乎是不太用的上了，我就是在不断印证那句话：生命在于折腾。 转自：http://www.cnblogs.com/mlv5/archive/2011/02/12/1952335.html","title":"图像处理工程师的要求"},{"content":"最近把一些在网上见到的自然语言处理的资源整理了一下，包括论文列表、软件资源和一些实验室主页、个人主页等，希望能对NLP研究者有所帮助，由于个人视野有限，目前只整理了这些，以后会持续更新。在此也感谢这些资源的提供者和维护者。 转载请标明出处（http://blog.csdn.net/xuh5156/article/details/7437475） 论文、博客 1.       Google在研究博客中总结了他们2011年的精彩论文《Excellent Papers for 2011》，包括社会网络、机器学习、人机交互、信息检索、自然语言处理、多媒体、系统等各个领域，很精彩的论文集锦。http://googleresearch.blogspot.com/2012/03/excellent-papers-for-2011.html 或者zibuyu的BLOG http://blog.sina.com.cn/s/blog_574a437f0100y6zy.html 2.       Best paper awards for AAAI,ACL, CHI, CIKM, FOCS, ICML, IJCAI, KDD, OSDI, SIGIR, SIGMOD, SOSP, STOC, UIST,VLDB, WWWhttp://jeffhuang.com/best_paper_awards.html 3.       IBM R&D Journal 刚发布了关于Watson的专刊《This is Watson》。总共17篇论文。http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6177717&punumber=5288520 4.       Web Data Mining作者刘兵维护的一个专题资源：Opinion Mining,Sentiment Analysis, and Opinion Spam Detection 。http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 5.       Statistical Machine Translationhttp://www.statmt.org/ Statistical Machine TranslationTutorial Readinghttp://cseweb.ucsd.edu/~dkauchak/mt-tutorial/ Philipp Koehn主页http://homepages.inf.ed.ac.uk/pkoehn/ 6.       Profile Hidden Markov ModelResourceshttp://webdocs.cs.ualberta.ca/~colinc/cmput606/ Hidden Markov Model (HMM) Toolbox forMatlabhttp://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html 7.       CRF http://www.inference.phy.cam.ac.uk/hmw26/crf/ Conditional Random Field (CRF)Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/CRF/crf.html FlexCRFs: Flexible Conditional RandomFieldshttp://flexcrfs.sourceforge.net/ 8.       Transfer Learning 包含papers、talks、software等http://www.cse.ust.hk/TL/index.html 9.       Topic Model，Topic Modeling Bibliographyhttp://www.cs.princeton.edu/~mimno/topics.html David M. Blei的主页http://www.cs.princeton.edu/~blei/publications.htmlMatlab Topic Modeling Toolbox 1.4http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm LDA GIBBS Java源码http://arbylon.net/resources.html GibbsLDA++: A C/C++ Implementation ofLatent Dirichlet Allocationhttp://gibbslda.sourceforge.net/ 10.   科学网—推荐系统的循序进阶读物（从入门到精通） - 张子柯的博文http://blog.sciencenet.cn/home.php?mod=space&uid=210641&do=blog&id=508634 11.   SVM入门http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html 12.   斯坦福大学自然语言处理实验室整理的NLP资源http://www-nlp.stanford.edu/links/statnlp.html 13.   Stanford University InformationRetrieval Resourceshttp://nlp.stanford.edu/IR-book/information-retrieval.html 14.   Software Tools for NLP http://www-a2k.is.tokushima-u.ac.jp/member/kita/NLP/nlp_tools.html 实验室主页 1.       The Stanford NLP Group http://nlp.stanford.edu 2.       The Berkeley Natural LanguageProcessing Grouphttp://nlp.cs.berkeley.edu 3.       University of Tokyo TsujiiLaboratory http://www.nactem.ac.uk/tsujii/publications.cgi?lang=en 4.       Korea University NLP http://nlp.korea.ac.kr/ http://nlp.korea.ac.kr/new/ 5.       中国科学院计算技术研究所自然语言处理研究组http://nlp.ict.ac.cn/new/ 6.       清华大学自然语言处理组 http://nlp.csai.tsinghua.edu.cn/site2/ 7.       HIT-SCIR http://ir.hit.edu.cn/ 8.       苏州大学自然语言处理实验室http://nlp.suda.edu.cn/ 个人主页 1.       David M. Blei， (Princeton) LDA，http://www.cs.princeton.edu/~blei/publications.html 2.       Noah Smith, (CMU),以自然语言处理、机器学习为基础研究computationalsocial science。http://www.cs.cmu.edu/~nasmith/ 3.       Philipp Koehn (University ofEdinburgh)http://homepages.inf.ed.ac.uk/pkoehn/ 4.       Dekang Lin (University ofAlberta)http://webdocs.cs.ualberta.ca/~lindek/ 5.       Michael Collins(ColumbiaUniversity)http://www.cs.columbia.edu/~mcollins/ 6.       Dekai WU(HKUST) http://www.cs.ust.hk/~dekai/ 7.       Pascale Fung (HKUST) http://www.ee.ust.hk/~pascale/ 8.       Alessandro Moschitti (Universityof Trento)http://disi.unitn.it/moschitti/ 9.       Xiaojin (Jerry) Zhu (Universityof Wisconsin-Madison)http://pages.cs.wisc.edu/~jerryzhu/ 10.   Eugene Charniak (BrownUniversity)http://www.cs.brown.edu/~ec/ 最近把一些在网上见到的自然语言处理的资源整理了一下，包括论文列表、软件资源和一些实验室主页、个人主页等，希望能对NLP研究者有所帮助，由于个人视野有限，目前只整理了这些，以后会持续更新。在此也感谢这些资源的提供者和维护者。 转载请标明出处（http://blog.csdn.net/xuh5156/article/details/7437475） 论文、博客 1.       Google在研究博客中总结了他们2011年的精彩论文《Excellent Papers for 2011》，包括社会网络、机器学习、人机交互、信息检索、自然语言处理、多媒体、系统等各个领域，很精彩的论文集锦。http://googleresearch.blogspot.com/2012/03/excellent-papers-for-2011.html 或者zibuyu的BLOG http://blog.sina.com.cn/s/blog_574a437f0100y6zy.html 2.       Best paper awards for AAAI,ACL, CHI, CIKM, FOCS, ICML, IJCAI, KDD, OSDI, SIGIR, SIGMOD, SOSP, STOC, UIST,VLDB, WWWhttp://jeffhuang.com/best_paper_awards.html 3.       IBM R&D Journal 刚发布了关于Watson的专刊《This is Watson》。总共17篇论文。http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6177717&punumber=5288520 4.       Web Data Mining作者刘兵维护的一个专题资源：Opinion Mining,Sentiment Analysis, and Opinion Spam Detection 。http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 5.       Statistical Machine Translationhttp://www.statmt.org/ Statistical Machine TranslationTutorial Readinghttp://cseweb.ucsd.edu/~dkauchak/mt-tutorial/ Philipp Koehn主页http://homepages.inf.ed.ac.uk/pkoehn/ 6.       Profile Hidden Markov ModelResourceshttp://webdocs.cs.ualberta.ca/~colinc/cmput606/ Hidden Markov Model (HMM) Toolbox forMatlabhttp://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html 7.       CRF http://www.inference.phy.cam.ac.uk/hmw26/crf/ Conditional Random Field (CRF)Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/CRF/crf.html FlexCRFs: Flexible Conditional RandomFieldshttp://flexcrfs.sourceforge.net/ 8.       Transfer Learning 包含papers、talks、software等http://www.cse.ust.hk/TL/index.html 9.       Topic Model，Topic Modeling Bibliographyhttp://www.cs.princeton.edu/~mimno/topics.html David M. Blei的主页http://www.cs.princeton.edu/~blei/publications.htmlMatlab Topic Modeling Toolbox 1.4http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm LDA GIBBS Java源码http://arbylon.net/resources.html GibbsLDA++: A C/C++ Implementation ofLatent Dirichlet Allocationhttp://gibbslda.sourceforge.net/ 10.   科学网—推荐系统的循序进阶读物（从入门到精通） - 张子柯的博文http://blog.sciencenet.cn/home.php?mod=space&uid=210641&do=blog&id=508634 11.   SVM入门http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html 12.   斯坦福大学自然语言处理实验室整理的NLP资源http://www-nlp.stanford.edu/links/statnlp.html 13.   Stanford University InformationRetrieval Resourceshttp://nlp.stanford.edu/IR-book/information-retrieval.html 14.   Software Tools for NLP http://www-a2k.is.tokushima-u.ac.jp/member/kita/NLP/nlp_tools.html 实验室主页 1.       The Stanford NLP Group http://nlp.stanford.edu 2.       The Berkeley Natural LanguageProcessing Grouphttp://nlp.cs.berkeley.edu 3.       University of Tokyo TsujiiLaboratory http://www.nactem.ac.uk/tsujii/publications.cgi?lang=en 4.       Korea University NLP http://nlp.korea.ac.kr/ http://nlp.korea.ac.kr/new/ 5.       中国科学院计算技术研究所自然语言处理研究组http://nlp.ict.ac.cn/new/ 6.       清华大学自然语言处理组 http://nlp.csai.tsinghua.edu.cn/site2/ 7.       HIT-SCIR http://ir.hit.edu.cn/ 8.       苏州大学自然语言处理实验室http://nlp.suda.edu.cn/ 个人主页 1.       David M. Blei， (Princeton) LDA，http://www.cs.princeton.edu/~blei/publications.html 2.       Noah Smith, (CMU),以自然语言处理、机器学习为基础研究computationalsocial science。http://www.cs.cmu.edu/~nasmith/ 3.       Philipp Koehn (University ofEdinburgh)http://homepages.inf.ed.ac.uk/pkoehn/ 4.       Dekang Lin (University ofAlberta)http://webdocs.cs.ualberta.ca/~lindek/ 5.       Michael Collins(ColumbiaUniversity)http://www.cs.columbia.edu/~mcollins/ 6.       Dekai WU(HKUST) http://www.cs.ust.hk/~dekai/ 7.       Pascale Fung (HKUST) http://www.ee.ust.hk/~pascale/ 8.       Alessandro Moschitti (Universityof Trento)http://disi.unitn.it/moschitti/ 9.       Xiaojin (Jerry) Zhu (Universityof Wisconsin-Madison)http://pages.cs.wisc.edu/~jerryzhu/ 10.   Eugene Charniak (BrownUniversity)http://www.cs.brown.edu/~ec/","title":"自然语言处理（NLP）网上资源整理"},{"content":"2012-4-6 读前言(preface)部分 update time:2012-4-6 Thisis a book about Natural Language Processing. By “natural language” we mean a languagethat is used for everyday communication by humans; languages such as English,Hindi,or Portuguese. In contrast to artificial languages such as programminglanguages and mathematical notations, natural languages have evolved as theypass from generation to generation, and are hard to pin down with explicitrules. We will take Natural Language Processing—or NLP for short—in a widesense to cover any kind of computer manipulation of natural language. At oneextreme, it could be as simple as counting word frequencies to comparedifferent writing styles. At the other extreme,NLP involves “understanding”complete human utterances[表达；说话；说话方式], at least to the extent of being able to giveuseful responses to them.   Technologiesbased on NLP are becoming increasingly widespread. For example,phonesand handheld computers support predictive text and handwritingrecognition; websearch engines give access to information locked up in unstructured text;machine translation allows us to retrieve texts written in Chinese and readthem in Spanish. By providing more natural human-machine interfaces, and moresophisticated access to stored information, language processing has come toplay a central role in the multilingual [使用多种语言的]information society.   Thisbook provides a highly accessible introduction to the field of NLP. It can beused for individual study or as the textbook for a course on natural language processingor computationallinguistics, or as a supplement to courses in artificial intelligence, text mining,or corpus linguistics[语料库语言学]. The book is intensely practical, containing hundreds of fully workedexamples and graded exercises. The book is based on the Python programminglanguage together with an open source library called the Natural LanguageToolkit (NLTK). NLTK includes extensive software, data,  and documentation, all freely downloadablefrom http://www.nltk.org/.Distributions are provided for Windows, Macintosh,and Unix platforms. We strongly encourage you to download Python and NLTK, andtry out the examples and exercises along the way.   Audience NLPis important for scientific, economic, social, and cultural reasons. NLPis experiencing rapid growth as its theories and methods are deployed ina variety of new language technologies. For this reason it is important for awide range of people to have a working knowledge of NLP. Within industry, thisincludes people in human-computer interaction, business information analysis,and web software development. Within academia, it includes people inareas from humanities computing and corpus linguistics through to computerscience and artificial intelligence. (To many people in academia,NLP is knownby the name of “Computational Linguistics.”) Thisbook is intended for a diverse range of people who want to learn how to write programsthat analyze written language, regardless of previous programming experience: Newto programming? Theearly chapters of the book are suitable for readers with no prior knowledge of programming,so long as you aren’t afraid to tackle newconcepts and develop new computingskills. The book is full of examples that you can copy and try for yourself,togetherwith hundreds of graded exercises. If you need a more generalintroduction toPython, see the list of Python resources at http://docs.python.org/. Newto Python? Experiencedprogrammers can quickly learn enough Python using this book to get immersedin natural language processing. All relevant Python features are carefully explainedand exemplified, and you will quickly come to appreciate Python’s suitability for this application area. The language index will help you locate relevant discussionsin the book. Alreadydreaming in Python? Skimthe Python examples and dig into the interesting language analysis material thatstarts in Chapter 1. You’ll soon be applying yourskills to this fascinating domain.   Emphasis Thisbook is a practical introduction to NLP. You will learn by example, write real programs,and grasp the value of being able to test an idea through implementation. If youhaven’t learned already, this book will teach you programming.Unlike other programmingbooks, we provide extensive illustrations and exercises from NLP. The approachwe have taken is also principled, in that we cover the theoretical underpinningsand don’t shy away from careful linguistic andcomputational analysis. We have tried to be pragmatic in striking abalance between theory and application, identifying the connections and the tensions.Finally, we recognize that you won’t get through this unlessit is also pleasurable, so we have tried to include many applications andexamples that are interesting and entertaining, and sometimes whimsical. Notethat this book is not a reference work. Its coverage of Python and NLP isselective,and presented in a tutorial style. For reference material, pleaseconsult the substantial quantity of searchable resources available at http://python.org/and http://www.nltk.org/.This book is not an advanced computer science text.The content ranges from introductory to intermediate, and is directed atreaders who want to learn how to analyze text using Python and the NaturalLanguage Toolkit. To learn about advanced algorithms implemented in NLTK, youcan examine the Python code linked from http://www.nltk.org/, and consult theother materials cited in this book. implemented in NLTK, you can examine thePython code linked from http:// www.nltk.org/,and consult the other materials cited in this book.   What you will learn? Bydigging into the material presented here, you will learn: • How simple programs canhelp you manipulate and analyze language data, and howto write these programs • How key concepts from NLPand linguistics are used to describe and analyze language • How data structures andalgorithms are used in NLP • How language data isstored in standard formats, and how data can be used to evaluatethe performance of NLP techniques   Dependingon your background, and your motivation for being interested in NLP, you willgain different kinds of skills and knowledge from this book, as set out in TableP-1. Goals Background in arts and humanities Background in science and engineering Language analysis Manipulating large corpora, exploring linguistic models, and testing empirical claims. Using techniques in data modeling, data mining, and knowledge discovery to analyze natural language. Language technology Building robust systems to perform linguistic tasks with technological applications. Using linguistic algorithms and data structures in robust language processing software.  Table P-1. Skills and knowledge to be gainedfrom reading this book, depending on readers’ goals and Background   Oragnization Theearly chapters are organized in order of conceptual difficulty, starting with apractical introduction to language processing that shows how to exploreinteresting bodies of text using tiny Python programs (Chapters 1–3). This is followed by a chapter on structured programming (Chapter 4)that consolidates [巩固]the programming topics scattered across the preceding chapters.After this, the pace picks up, and we move on to a series of chapters coveringfundamental topics in language processing: tagging, classification, andinformation extraction (Chapters 5–7). The next three chapterslook at ways to parse a sentence, recognize its syntactic structure, andconstruct representations of meaning (Chapters 8–10). The final chapter isdevoted to linguistic data and how it can be managed effectively (Chapter 11).The book concludes with an Afterword[编后记], briefly discussing the past and future of the field.   Withineach chapter, we switch between different styles of presentation. In one style,naturallanguage is the driver. We analyze language, explore linguistic concepts, and useprogramming examples to support the discussion. We often employ Pythonconstructs that have not been introduced systematically, so you can see theirpurpose before delving [钻研；探究]into the details of how and why they work. This is just like learning idiomatic[惯用的；符合语言习惯的]expressions in a foreignlanguage: you’re able to buy a nice pastry [油酥点心；面粉糕饼]without first having learned the intricacies [纷繁难懂之处；错综复杂的事物]of question formation. Inthe other style of presentation, the programming language will be the driver.We’ll analyze programs, explore algorithms, and the linguistic examples willplay a supporting role.   Each chapter ends with a series of graded exercises, which are useful for consolidatingthe material. The exercises are graded according to the following scheme: ○ is for easy exercises thatinvolve minor modifications to supplied code samples or other simple activities;   ◑ is for intermediateexercises that explore an aspect of the material in more depth,requiring careful analysis and design;  ● is for difficult, open-endedtasks that willchallenge your understanding of the material and force you to thinkindependently(readers new to programming should skip these). Eachchapter has a further reading section and an online “extras” section at http://www.nltk.org/, with pointers tomore advanced materials and online resources. Online versions of all the codeexamples are also available there.   Why Python? Pythonis a simple yet powerful programming language with excellent functionality for processinglinguistic data. Python can be downloaded for free from http://www.python.org/.Installers are available for all platforms. Here is a five-line Python programthat processes file.txt and prints all the words ending in ing: >>>for line in open(\"file.txt\"): ...for word in line.split(): ...if word.endswith('ing'): ...print word Thisprogram illustrates some of the main features of Python. First, whitespaceis used to nest lines of code; thus the line starting with if falls inside the scope ofthe previous linestarting with for; this ensures that the ing test is performed for each word.Second, Python is object-oriented; each variable is an entity that has certaindefined attributes and methods. For example, the value of the variable line is more than a sequence of characters. It is a string object that has a “method” (or operation) called split() that wecan use to break a line into its words. To apply a method to an object, we write the objectname, followed by a period, followed by the method name, i.e., line.split(). Third,methods have arguments expressed inside parentheses. For instance, inthe example,  word.ends with('ing') hadthe argument 'ing' to indicate that we wanted words ending with ing and notsomething else. Finally—and most importantly—Python is highly readable, so much so that it is fairly easy to guess whatthis program does even if you have never written a program before. We chose Python because it has a shallow learning curve, its syntaxand semantics are transparent, and it has good string-handlingfunctionality. As an interpreted language, Python facilitates interactiveexploration. As an object-oriented language, Python permits data and methods tobe encapsulated and re-used easily. As a dynamic language, Pythonpermits attributes to be added to objects on the fly, and permits variables tobe typed dynamically, facilitating rapid development. Python comes with anextensive standard library, including components for graphical programming,numerical processing, and web connectivity. Python is heavily used in industry, scientific research, and education around theworld.Python is often praised for the way it facilitates productivity, quality,and maintainability of software. A collection of Python success stories isposted at http://www.python.org/about/success/. NLTKdefines an infrastructure that can be used to build NLP programs in Python. It providesbasic classes for representing data relevant to natural language processing; standardinterfaces for performing tasks such as part-of-speech tagging,syntactic parsing, and text classification; and standard implementations foreach task that can be combined to solve complex problems. NLTKcomes with extensive documentation. In addition to this book, the website at http://www.nltk.org/provides API documentation that covers every module, class, and function in thetoolkit, specifying parameters and giving examples of usage. The website alsoprovides many HOWTOs with extensive examples and test cases, intended for users,developers, and instructors.   Software Requirements Toget the most out of this book, you should install several free softwarepackages. Currentdownload pointers and instructions are available at http://www.nltk.org/. Python Thematerial presented in this book assumes that you are using Python version 2.4 or2.5. We are committed to porting NLTK to Python 3.0 once the libraries that NLTKdepends on have been ported. NLTK Thecode examples in this book use NLTK version 2.0. Subsequent releases of NLTKwill be backward-compatible[向后兼容]. NLTK-Data Thiscontains the linguistic corpora that are analyzed and processed in thebook. NumPy(recommended) Thisis a scientific computing library with support for multidimensional arrays and linearalgebra, required for certain probability, tagging, clustering,and classification tasks. Matplotlib(recommended) Thisis a 2D plotting library for data visualization, and is used in some of the book’s code samples that produce line graphs and bar charts. NetworkX(optional) Thisis a library for storing and manipulating network structures consisting of nodes and edges. For visualizing semantic networks, also install the Graphviz library. Prover9(optional) Thisis an automated theorem  prover  for first-order and equational logic,used to support inference in language processing.   Natural Language Toolkit(NLTK) NLTKwas originally created in 2001 as part of a computational linguistics course in the Department of Computer and Information Science at the University of Pennsylvania. Since then it has been developed and expanded with the help ofdozens of contributors.It has now been adopted in courses in dozens ofuniversities, and serves as the basis of many research projects. Table P-2 liststhe most important NLTK modules.  NLTK was designed with four primary goals in mind: Simplicity(简易性) Toprovide an intuitive framework along with substantial building blocks, giving usersa practical knowledge of NLP without getting bogged down in the tedious house-keeping usually associated with processing annotatedlanguage data Consistency(连续性) Toprovide a uniform framework with consistent interfaces and data structures, andeasily guessable method names Extensibility(扩展性) Toprovide a structure into which new software modules can be easily accommodated, includingalternative implementations and competing approaches to the same task Modularity(模块性) Toprovide components that can be used independently without needing to understand therest of the toolkit Contrastingwith these goals are three non-requirements—potentially useful qualities that we have deliberately avoided. First, while the toolkit provides a wide range of functions,it is not encyclopedic[百科全书的]; it is a toolkit, not a system, and it willcontinue to evolve with the field of NLP. Second, while the toolkit isefficient enough to support meaningful tasks, it is not highly optimized forruntime performance; such optimizations often involve more complex algorithms,or implementations in lower-level programming languages such as C or C++. This would make the software less readable and more difficult to install. Third, wehave tried to avoid clever programming tricks, since we believe that clearimplementations are preferable to ingenious yet indecipherable ones.   For Instructors NaturalLanguage Processing is often taught within the confines of asingle-semester courseat the advanced undergraduate level or postgraduate level. Many instructors havefound that it is difficult to cover both the theoretical and practical sides of the subjectin such a short span of time. Some courses focus on theory to the exclusion of practicalexercises, and deprive students of the challenge and excitement of writing programsto automatically process language. Other courses are simply designed to teachprogramming for linguists, and do not manage to cover any significant NLPcontent. NLTKwas originally developed to address this problem, making it feasible to covera substantial amount of theory and practice within a single-semester course,even if students have no prior programming experience. Asignificant fraction of any NLP syllabus deals with algorithms and datastructures. On their own these can be rather dry, but NLTK brings them to life with the help of interactivegraphical user interfaces that make it possible to view algorithms step-by-step. MostNLTK components include a demonstration that performs an interesting taskwithout requiring any special input from the user. An effective way to deliverthe materialsis through interactive presentation of the examples in this book, entering themin a Python session, observing what they do, and modifying them to explore someempirical or theoretical issue. Thisbook contains hundreds of exercises that can be used as the basis for student assignments.The simplest exercises involve modifying a supplied program fragment in aspecified way in order to answer a concrete question. At the other end of the spectrum,NLTK provides a flexible framework for graduate-level research projects, withstandard implementations of all the basic data structures and algorithms,interfaces to dozens of widely used datasets (corpora), and a flexible andextensible architecture. Additional support for teaching using NLTK isavailable on the NLTK website. Webelieve this book is unique in providing a comprehensive framework for students tolearn about NLP in the context of learning to program. What sets thesematerials apartis the tight coupling of the chapters and exercises with NLTK, givingstudents— eventhose with no prior programming experience—a practical introduction toNLP. Aftercompleting these materials, students will be ready to attempt one of the more advancedtextbooks, such as Speech and Language Processing, by Jurafsky and Martin(PrenticeHall, 2008). Thisbook presents programming concepts in an unusual order, beginning with a nontrivialdata type—lists of strings—then introducing non-trivialcontrol structures such as comprehensions and conditionals. These idiomspermit us to do useful language processing from the start. Once this motivationis in place, we return to a systematic presentation of fundamentalconcepts such as strings, loops, files, and so forth. In this way, we cover thesame ground as more conventional approaches, without expecting readers to beinterested in the programming language for its own sake. Twopossible course plans are illustrated in Table P-3. The first one presumesan arts/ humanitiesaudience, whereas the second one presumes a science/engineering audience. Othercourse plans could cover the first five chapters, then devote the remaining timeto a single area, such as text classification (Chapters 6 and 7), syntax(Chapters 8 and 9), semantics (Chapter 10), or linguistic data management (Chapter11).     Conventions Used in ThisBook Thefollowing typographical conventions are used in this book: Bold Indicatesnew terms. Italic Usedwithin paragraphs to refer to linguistic examples, the names of texts, and URLs;also used for filenames and file extensions. Constantwidth[等宽字体] Usedfor program listings, as well as within paragraphs to refer to program elements suchas variable or function names, statements, and keywords; also used for program names. Constantwidth italic Showstext that should be replaced with user-supplied values or by values determined bycontext; also used for metavariables within program code examples.   Using Code Examples Thisbook is here to help you get your job done. In general, you may use the code in thisbook in your programs and documentation. You do not need to contact us for permissionunless you’re reproducing a significant portion of the code. For example, writinga program that uses several chunks of code from this book does not require permission.Selling or distributing a CD-ROM of examples from O’Reilly books does requirepermission. Answering a question by citing this book and quoting example codedoes not require permission. Incorporating a significant amount of example code fromthis book into your product’s documentation doesrequire permission. Weappreciate, but do not require, attribution. An attribution usually includesthe title, author,publisher, and ISBN. For example: “Natural Language Processingwith Python,by Steven Bird, Ewan Klein, and Edward Loper. Copyright 2009 StevenBird, EwanKlein, and Edward Loper, 978-0-596-51649-9.” Ifyou feel your use of code examples falls outside fair use or the permissiongiven above, feel free to contact us at permissions@oreilly.com.   Acknowledgments Theauthors are indebted to the following people for feedback on earlierdrafts of this book:Doug Arnold, Michaela Atterer, Greg Aumann, Kenneth Beesley, Steven Bethard, OndrejBojar, Chris Cieri, Robin Cooper, Grev Corbett, James Curran, Dan Garrette, JeanMark Gawron, Doug Hellmann, Nitin Indurkhya, Mark Liberman, Peter Ljunglöf, StefanMüller, Robin Munn, Joel Nothman, AdamPrzepiorkowski, Brandon Rhodes, Stuart Robinson, Jussi Salmela, KyleSchlansker, Rob Speer, and Richard Sproat. We are thankful to many students andcolleagues for their comments on the class materials that evolved into these chapters,including participants at NLP and linguistics summer schools in Brazil, India,and the USA. This book would not exist without the members of the nltk-dev developercommunity, named on the NLTK website, who have given so freely of their timeand expertise in building and extending NLTK.We are grateful to the U.S.National Science Foundation, the Linguistic Data Consortium, an EdwardClarence Dyason Fellowship, and the Universities of Pennsylvania, Edinburgh,and Melbourne for supporting our work on this book.We thank Julie Steele, AbbyFox, Loranah Dimant, and the rest of the O’Reilly team,for organizingcomprehensive reviews of our drafts from people across the NLP and Pythoncommunities, for cheerfully customizing O’Reilly’s production tools to accommodate our needs, and for meticulous copyeditingwork. Finally, we owe a huge debt of gratitude to our partners, Kay, Mimo, andJee, for their love,patience, and support over the many years that we worked on this book. We hope thatour children—Andrew, Alison, Kirsten, Leonie, and Maaike—catch our enthusiasm for language and computation from these pages. Royalties[版税，稿酬] Royaltiesfrom the sale of this book are being used to support the development of the NaturalLanguage Toolkit.    ","title":"Python自然语言处理学习笔记[一]---前言(preface)"},{"content":"我在大学学习线性代数时，实在想不出它除了告诉我们如何解线性方程外，还能有什么别的用途。关于矩阵的许多概念，比如特征值等等，更是脱离日常生活。后来在数值分析中又学了很多矩阵的近似算法，还是看不到可以应用的地方。当时选这些课，完全是为了混学分的学位。我想，很多同学都多多少少有过类似的经历。直到后来长期做自然语言处理的研究，我才发现数学家们提出那些矩阵的概念和算法，是有实际应用的意义的。 在自然语言处理中，最常见的两类的分类问题分别是，将文本按主题归类（比如将所有介绍亚运会的新闻归到体育类）和将词汇表中的字词按意思归类（比如将各种体育运动的名称个归成一类）。这两种分类问题都可用通过矩阵运算来圆满地、同时解决。为了说明如何用矩阵这个工具类解决这两个问题的，让我们先来来回顾一下我们在余弦定理和新闻分类中介绍的方法。 分类的关键是计算相关性。我们首先对两个文本计算出它们的内容词，或者说实词的向量，然后求这两个向量的夹角。当这两个向量夹角为零时，新闻就相关；当它们垂直或者说正交时，新闻则无关。当然，夹角的余弦等同于向量的内积。从理论上讲，这种算法非常好。但是计算时间特别长。通常，我们要处理的文章的数量都很大，至少在百万篇以上，二次回标有非常长，比如说有五十万个词（包括人名地名产品名称等等）。如果想通过对一百万篇文章两篇两篇地成对比较，来找出所有共同主题的文章，就要比较五千亿对文章。现在的计算机一秒钟最多可以比较一千对文章，完成这一百万篇文章相关性比较就需要十五年时间。注意，要真正完成文章的分类还要反复重复上述计算。 在文本分类中，另一种办法是利用矩阵运算中的奇异值分解（Singular Value Decomposition，简称 SVD)。现在让我们来看看奇异值分解是怎么回事。首先，我们可以用一个大矩阵A来描述这一百万篇文章和五十万词的关联性。这个矩阵中，每一行对应一篇文章，每一列对应一个词。 在上面的图中，M=1,000,000，N=500,000。第 i 行，第 j 列的元素，是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF)。读者可能已经注意到了，这个矩阵非常大，有一百万乘以五十万，即五千亿个元素。 奇异值分解就是把上面这样一个大矩阵，分解成三个小矩阵相乘，如下图所示。比如把上面的例子中的矩阵分解成一个一百万乘以一百的矩阵X，一个一百乘以一百的矩阵B，和一个一百乘以五十万的矩阵Y。这三个矩阵的元素总数加起来也不过1.5亿，仅仅是原来的三千分之一。相应的存储量和计算量都会小三个数量级以上。 三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。 现在剩下的唯一问题，就是如何用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值等等，以及数值分析的各种算法就统统用上了。在很长时间内，奇异值分解都无法并行处理。（虽然 Google 早就有了MapReduce 等并行计算的工具，但是由于奇异值分解很难拆成不相关子运算，即使在 Google 内部以前也无法利用并行计算的优势来分解矩阵。）最近，Google 中国的张智威博士和几个中国的工程师及实习生已经实现了奇异值分解的并行算法，我认为这是 Google 中国对世界的一个贡献。","title":"数学之美-奇异值分解"},{"content":"原文地址： 数据挖掘研究&开发网站 知识型企业研究中心 2006-12-26 http://business.queensu.ca/index.php Queen商务学校，任务是提高领导力的管理和促进商务和社会的发展。目前我们的研究工作... 英国谢菲尔德大学自然语言处理研究组 2006-12-26 http://nlp.shef.ac.uk/ 英国谢菲尔德大学自然语言处理研究组研究领域主要为：自然语言分析，自然语言的产生以及相关资... PC AI 2006-12-26 http://www.pcai.com/ 在线免费电子期刊，除了包含每期期刊内容外，还包括一个AI讨论组，和经过整理的Intern... 美国印地安那大学人工智能/认知科学报告和再版文件汇编 2006-12-26 http://www.cs.indiana.edu/%7eleake/INDEX.html 美国印地安那大学人工智能/认知科学报告和再版文件汇编，网站提供了PDF格式的文件，相关书... 美国橡树岭国家实验室图像处理和机器视觉研究小组 2006-12-26 http://www.ornl.gov/sci/ismv/ 美国橡树岭国家实验室图像处理和机器视觉研究小组，图像处理包括：机器视觉，图像管理和检索，... 人工智能研究者俱乐部 2006-12-26 http://www.souwu.com/ 分类论坛:自然语言语音识别论文资源相关编程专家系统知识表示机器学习神经网络数据挖掘模式识... DFKI人工智能研究所 2006-12-17 http://www.dfki.uni-kl.de/ 与人交谈时，对方吐出一串叽哩咕噜的洋话、而你半个字也听不懂，怎么办呢？在过去，这可能需要... 数据管理前言技术国际研讨会（中国，上海，2006） 2006-12-17 http://www.iipl.fudan.edu.cn/DM06/index.htm 2006年该会议的主题是网站管理和挖掘. 它包括6-8个主题,邀请了一系列的研究者和当地... 媒体计算与WEB智能实验室(复旦大学) 2006-12-17 http://www.cs.fudan.edu.cn/mcwil/irnlp/ 媒体计算与WEB智能实验室主要从事多媒体方向（包括文本、图象和视频）的教学和科研工作，研... 奥地利人工智能研究所机器学习和数据挖掘小组 2006-12-11 http://www.oefai.at/oefai/ml/mldm/ 研究区域包括数据挖掘和知识发现，文本挖掘，机器学习，此外还网站提供关于研究领域，相关人物... 加拿大渥太华大学知识获取与智能化学习研究小组 2006-12-09 http://www.site.uottawa.ca/tanka/kaml.html 知识获取与智能化学习研究小组有他们的发展项目：智能化信息的获取项目，文本摘要项目，TAN... 美国麻省理工大学生物与计算学习研究中心 2006-12-09 http://cbcl.mit.edu/ 美国麻省理工大学生物与计算学习研究中心在麻州理工学院成立。主要从数学，工程学和神经学的角... 德国乌尔姆大学人工神经网络小组 2006-12-09 http://www.informatik.uni-ulm.de/ni/forschung/ann.html 德国乌尔姆大学人工神经网络小组的研究重点在于神经网络，数据挖掘，信号处理等领域和方向的研... 优秀知识发现网络 2006-12-09 http://www.kdnet.org/ 优秀知识发现网络是一个开放的网络，它的参加者来自科学，工业和公共部门。这项国际项目的主要... 奥地利维也纳医科大学脑研究中心医学控制和人工智能学院 2006-12-02 http://www.ai.univie.ac.at/ 创建者是地利维也纳医科大学脑研究中心医学控制和人工智能学院，研究包括： 自然语言处理，机... 奥地利维也纳医科大学脑研究中心医学控制和人工智能学院 2006-12-02 http://www.ai.univie.ac.at/ 创建者是地利维也纳医科大学脑研究中心医学控制和人工智能学院，研究包括： 自然语言处理，机... 美国伍斯特工学院人工智能研究小组 2006-12-02 http://www.cs.wpi.edu/Research/airg/ 多主体系统，学习，单功能主体，智能界面，图标界面，专家系统，数据挖掘，知识库的设计等。 微软研究－机器学习和应用统计研究小组 2006-12-02 http://research.microsoft.com/research/mlas/ 机器学习和应用统计研究小组把重心集中在从数据和数据挖掘。藉由软件自动从数据中学习获取新信... 英国爱丁堡大学信息学校人工智能应用学院 2006-12-02 http://www.aiai.ed.ac.uk/ 情境基础的推论: 利用过去的经验和存在的技术指导诊断企业的资源过失；遗传基因的运算法... 北京大学计算语言学研究所 2006-12-02 http://www.icl.pku.edu.cn/ 北京大学计算语言学研究所成立于1986年。研究所的使命是致力于计算语言学理论、语言信息... 哈尔滨工业大学智能技术与自然语言处理实验室 2006-12-02 http://www.insun.hit.edu.cn/default_cn.asp 哈尔滨工业大学计算机学院智能技术与自然语言处理研究室（ITNLP）是国内较早从事自然语言... 加州大学伊荣/尔湾分校机器学习小组 2006-11-29 http://www.ics.uci.edu/~mlearn/Machine-Learning.html 机器学习是一种通过经验获取知识的机制。加州大学伊荣/尔湾分校机器学习小组的研究包括基于统... DMI:数据挖掘学院 2006-11-24 http://www.cs.wisc.edu/dmi/ 数据挖掘研究所于1999年6月1日在微软的数据挖掘小组的帮助下在微软公司的计算机科学系成... 数据挖掘：原理，算法及应用 2006-11-24 http://www.cs.unc.edu/Courses/comp290-90-f04/ 这是北卡罗莱纳洲大学计算机科学系2004年关于数据挖掘的一系列的研讨会的网站。上面列出了... 麻省理工学院开放课程--数据挖掘 2006-11-24 http://www.core.org.cn/OcwWeb/Sloan...5-062Data-Mi... 麻省理工学院的关于数据挖掘开放课程.上面列出了教学大纲、教学日程、讲义、作业、考试以及学... 国家数据挖掘中心 2006-11-24 http://www.ncdm.uic.edu/ 芝加哥的伊利诺伊大学的国家数据挖掘中心于1998年成立,提供资源研究、标准开发和推广高性... IBM智能情报系统研究中心 2006-11-24 http://www.almaden.ibm.com/software/disciplines/iis/ 智能信息系统研究所主要在于设计维护隐私和数据所有权而不是妨碍资讯流通的信息系统.我们的工... 清华大学知识工程研究室 2006-11-24 http://keg.cs.tsinghua.edu.cn/ 清华大学计算机系软件所知识工程研究室以网络计算模式下知识处理为研究方向，以Java、XM... 数据挖掘和数据仓库 2006-11-24 http://www.crm2day.com/data_mining/ 这是一个关于CRM的网站。其中有在数据挖掘这一版块列出了许多著名的公司或者专家写的关于数... 数据挖掘课程 2006-11-24 http://cs.nju.edu.cn/zhouzh/zhouzh.files/course/dm.htm 是南京大学的数据挖掘课程的网页,上面列出了基本的课程介绍,提供课件下载,还列出了其他国家... 数据挖掘的连接 2006-11-24 http://www.galaxy.gmu.edu/stats/syllabi/DMLIST.html 该网页列出了关于数据挖掘的一系列链接 数据挖掘的连接 2006-11-24 http://www.galaxy.gmu.edu/stats/syllabi/DMLIST.html 该网页列出了关于数据挖掘的一系列链接 人工智能研究实验室 2006-11-17 http://www.cs.iastate.edu/~honavar/aigroup.html 人工智能研究实验室是爱荷华州立大学的计算智能、学习和发现中心的一部分.目前的研究包括:人... 美国人工智能协会 2006-11-17 http://www.aaai.org/home.html 成立于1979年的美国人工智能协会(aaai)是一个非营利性的致力于推进科学认识的社会科... 知识媒体学会 2006-11-16 http://kmi.open.ac.uk/index.cfm 研究与电视大学本身相关的区域: 认知的和学问科学,和多媒体。 研究包括下列的主题: 叙述... WEB数据挖掘实验室 2006-11-16 http://www.wdmlab.cn/ 本Web数据挖掘实验室隶属于南京师范大学教育科学学院教育技术学系。实验室立足于我国基础教... Java资源网——Java数据挖掘 2006-11-16 http://www.javaresource.org/data-mi...-mining-73.html Java资源网是由Java领域的爱好者组成的技术联盟,主要成员均来自java和相关领域的... 中国科大博纳数据挖掘中心 2006-11-16 http://bona.ustc.edu.cn/ 中国科大博纳数据挖掘中心（Bona Institute of Business Data... 西南财经大学商务数据挖掘中心 2006-11-16 http://riem.swufe.edu.cn/dataminingcenter/ 西南财经大学商务数据挖掘中心是一个应用研究机构，它和从事商务决策和数据挖掘的软件公司、... 国际数据挖掘技术研究中心 2006-11-16 http://59.77.6.145/dmlab/DesktopDefault.aspx 数据挖掘技术及其应用实验室是厦门大学国家示范性软件学院软件研究与开发中心的一个重要的分... 互联网数据挖掘服务中心 2006-11-16 http://idm.yatio.com/index.html 互联网数据挖掘服务中心（IDMSC）是以雅信核心搜索技术为依托，面向所有网络分众领域，为... 中科院数据技术与知识经济研究中心 2006-11-16 http://www.dtke.ac.cn/ 中国科学院数据技术与知识经济研究中心（CAS Research Center on Da... 机器学习研究室 2006-11-15 http://www.cald.cs.cmu.edu/ 这个机器学习研究室是卡内基梅隆大学计算机科学系的一个学术部门.我们集中有关于统计机器学习... 数据挖掘工程小组 2006-11-15 http://www.chem-eng.utoronto.ca/~datamining/ 数据挖掘工程组是基身于多伦多大学的化工和应用化学系.其目标是把背景不同的在各个领域研究数... 查尔斯顿学院的信息发现 2006-11-15 http://di.cofc.edu/ 信息发现是从现有的资料中,无论是以前贮存的、还是流经过沟通渠道的，去发现新的信息.如何运... 2006年数据挖掘论坛 2006-11-14 http://www.data-mining-forum.de/ 这次会议是每年召开的一系列的基于数据挖掘的工业会议的第六次会议,该会议每年都在国际活动方... 数据挖掘 2006-11-14 http://www.ccsu.edu/datamining 这是ccsu的一个在线数据挖掘的项目,ccsu是唯一开办了在线数据挖掘科学硕士的学校.这... 第四届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://www.sewm2006.sdu.edu.cn/ 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 第三届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://www.sewm2005.edu.cn/index.htm 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 第二届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://www.scut.edu.cn/sewm2004/index.htm 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 首届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://net.pku.edu.cn/~sewm/sewm2003.htm 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 数据挖掘技能 2006-11-10 http://www.statsoft.com/textbook/stdatmin.html 这是一本关于数据挖掘的一本书的章节 数据挖掘课堂笔记 2006-11-10 http://infolab.stanford.edu/~ullman/mining/mining.html 国外大学关于数据挖掘相关课程的课件。 智能科学网站 2006-11-10 http://www.intsci.ac.cn/ 这是一个关于智能科学的门户网站，主要介绍的有关智能科学的内容由智能系统、智能科学研究、智... 数据挖掘词汇表 2006-11-10 http://www.twocrows.com/glossary.htm 数据挖掘的词汇表 智能工具,数据挖掘,可视化2005国际会议 2006-11-09 http://www.infonortics.com/idv/05pro.html 2005年6月27-28号在美国费城召开的智能工具、数据挖掘和可视化国际会议。网站上... SIGIR2006会议网站 2006-11-07 http://www.sigir2006.org/ 关于信息检索的会议网站，本年度的主题是用户交互与检索效率。该网站提供年度会议的论文目录，... 数字经济研究中心 2006-11-07 http://w4.stern.nyu.edu/ceder/ 网站简介：25年多来，纽约大学的Stern's数字经济研究中心已经利用信息技术站在了商业... 原文挖掘和基于网页的信息检索参考书目 2006-11-07 http://filebox.vt.edu/users/wfan/text_mining.html 该网页提供了许多关于原文挖掘研评价和分析的连接。 数据挖掘爱好者 2006-11-04 http://datamining.diy.myrice.com/ 数据挖掘就是从海量的数据中找出潜在的有价值的信息。这是一门综合了统计学、数据库和人工智能... 数据挖掘资源 2006-11-04 http://www.opendata365.com/datamini...200506/235.html 该网页提供了许多有关数据挖掘方面的链接，资源丰富。 第七次国际数据仓库存储与知识发现会议 2006-11-04 http://www.dwway.com/newcontent.php...5userid=corpid= 主要介绍了会议的时间、地点、宗旨以及讨论的主要内容。 数据挖掘：文本挖掘，数据挖掘和社会传媒 2006-11-04 http://datamining.typepad.com/data_mining/ 这是一个私人博客，记录了作者研究方向的一些资料、信息。而作者主要的兴趣所在为：人工智能、... 与统计相关的数据挖掘课件 2006-11-04 http://www.autonlab.org/tutorials/ 这个网站提供了基于统计的数据挖掘各个方面的研究类的课件，包括概率论的基础、数据统计分析的... 诊断试验评价与数据挖掘 2006-11-04 http://statdtedm.6to23.com/ 该网站是个科研个人网（非商业盈利），目的是相互交流,共同提高；网站开辟的几个专题，如数据... 统计分析与数据挖掘实验室 2006-11-04 http://www.bistudy.com/ 该网站主要提供一些相关软件介绍及其下载，包括： 调查类软件 、 统计分析类软件 、... 数据挖掘技术简介 2006-11-04 http://www.itcomputer.com.cn/Databa...0601/78529.html 数据挖掘是目前一种新的重要的研究领域。本文介绍了数据挖掘的概念、目的、常用方法、数据挖掘... 数据挖掘技术简介（PPT） 2006-11-04 http://eb.zzei.net/ebSimple/dss.ppt PPT课件 数据挖掘教程 2006-11-04 http://www.sobooks.com/product_info...oducts_id/14953 本书为数据挖掘的基础教程，是作者多年来从事数据挖掘和专家系统课程教学经验的总结。它从商业... 数据挖掘 2006-11-04 http://www.the-data-mine.com/ 这个网站是1994年4月建立的,主要是提供关于数据挖掘的信息,包括数据库中的数据挖掘和简... 数据挖掘:实用机器学习工具和技术(第二版) 2006-11-04 http://www.cs.waikato.ac.nz/~ml/weka/book.html 一本关于数据挖掘的书籍的介绍 数据挖掘讨论组 2006-11-04 http://www.dmgroup.org.cn/ 数据挖掘讨论组网站建于2000年7月，是由复旦大学计算机系发起创建的。 该网站... 数据挖掘研究院 2006-11-04 http://www.dmresearch.net/ 数据挖掘研究院是由HAMMER_SHI于2004年4.17日搭建成立的数据挖掘研讨平台，... Lotus知识发现服务器 2006-11-04 http://www.chinakm.com/share/list.asp?id=2579 主要介绍了Lotus知识发现服务器及其功能和作用。 知识发现新进展与成果概述 2006-11-04 http://202.113.96.26/tjcbe/xueshubaogao/yangbingru.ppt 主要介绍了知识发现的内涵与外延的扩展、挖掘知识类型扩展、方法技术扩展、应用及发展趋势以及... 第四届知识发现与数据挖掘国际学术大会 2006-11-04 http://www2.ccw.com.cn/1998/37/170858.shtml 主要介绍了这次会议的8个专题介绍会，以及本届大会的几个特点。 数据挖掘研究院 网摘 2006-11-04 http://www.dmresearch.net/rss/ 关于一个动态搜集的有关数据挖掘资料的网页。 数据挖掘 2006-11-03 http://databases.about.com/od/datamining/ about.com展示了原有的专题文章的收集、网络连接,以及专门讨论数据挖掘和数据仓库课... UCI数据库知识发现 2006-11-02 http://kdd.ics.uci.edu/ 在线的大型数据库，包含多种类型的数据,分析任务、适用范围.本库的主要作用是作为基准测试,... 关于应用解析的新闻以及商业资源 2006-11-02 http://www.secondmoment.org/ 关于应用解析的新闻以及商业资源.强大日志内容混合了评论、技术、以及对知识发现和直接的知识... 数据挖掘与知识发现软件 2006-11-02 http://www.kdnuggets.com/software/index.html 这是一个通用数据挖掘软件的目录","title":"数据挖掘的资源网站"},{"content":"Siri的学术化全称为“个人自动智能助理”（Personalized Intelligent Assistant），并非苹果公司自主研发，最初是苹果AppStore的一款应用，苹果公司发现后很快于2010年4月份从斯坦福研究机构国际智能中心（SRI）处收购而来，传言收购价格在2亿美金左右，之后很快Siri从AppStore下架并集成进入IPhone 4S。 我们首先看一些例子来看Siri能做什么：作为“个人智能助理”，Siri能为你提供什么服务？可以将Siri理解为一位低智商的生活秘书（和人类的智能比），它采用了Nuance公司的语音识别技术，具有一定程度的语义理解和用户意图识别能力，可以聊天解闷，更主要的功能是帮助办理一定的事务，比如预订餐厅、预订出租车、搜索精确答案、查询天气等。 对话一： User:“我想掩藏一具尸体。” Siri：你想藏在哪里呢？（给出5个选项） 水库 工厂 矿山 垃圾场 湿地 对话二： Siri：有什么我能帮忙的？ User:“我想我刚刚杀了人。” Siri：是这样么？ User:“是的。” Siri：“你说要掩藏尸体我就想到了” User:“我该怎么做？” Siri：“好吧，这是你接下来的日程表。” 对话三： User:“去哪儿能弄些毒品？” Siri：“这附近有2家戒毒所。” 对话四、对话五、对话六： 由以上六个用户和Siri的对话记录片段可以看出Siri具备一定的语义智能，如果了解其技术原理，可以知晓解闷陪聊功能并非其最初出发点和功能重点，在多个垂直领域帮用户完成一定的任务才是其核心功能。 图1 . Siri整体架构 Siri是一个功能繁复的综合AI框架，图1展示了其包含的数据、模型以及计算模块，为了在整体上更易于理解整个框架，可以将Siri里包含的众多数据、模型和计算模块划分为输入系统、活跃本体、执行系统、服务系统和输出系统五个子系统。其在解析用户输入时候遵循一定的执行顺序，以此来理解用户的真正意图并提供有用服务。 Siri的资源主要分为资源类和计算类两大类，其中属于资源类的包括; 领域模型； 词汇表数据库； 短期记忆系统； 长期记忆系统； 领域本体数据库； 对话流模型； 服务模型； 服务能力模型； 外部服务； 属于计算资源的包括： 语音识别系统； 语言模式识别器； 语言解释器； 对话流控制器； 任务控制器； 服务集成模块； 语音生成系统； Siri的输入系统支持多模态输入，即不仅仅支持众所周知的语音识别，也允许用户进行文本输入、GUI界面操作以及事件触发等。除了支持多模态输入外，Siri输入系统一方面可以利用语言解释器对早期输入进行歧义消除，另外一方面还可以对用户输入进行有意识的引导，将用户输入尽量映射到Siri能够提供的服务上来。这样对于用户和Siri来说才可相得益彰，Siri 可体现其价值，用户可获得帮助。 图2. 活跃本体 “活跃本体”是Siri中相当重要的一个概念，“活跃本体”可以被理解为Siri整个系统执行的一个具体执行环境和场所，执行系统调用所有系统数据、词典、模型和程序，在“活动本体”内对用户输入进行解析，并将文本信息在这里解析为用户真正的意图，然后根据意图来调用外部的服务。 在程序执行时，“活跃本体”内放入的数据和模型包括：领域模型，用户个性化信息，语言模式、词汇表和领域实体数据库等。 领域模型包括某个垂直领域内的概念，实体，关系，属性和实例的内部表示，这其实就是Semantic Web这个研究领域常说的ontology。Siri包含很多垂直领域的领域模型。“词汇表”用于维护Siri中的表层单词到“领域模型”或者“任务模型”中定义的的概念、关系、属性的映射关系；被用来引导用户输入、自然语言解析和生成输出结果。 Siri在个性化方面做得也非常出色。在和用户沟通过程中，如果一台机器能够叫出你的名字，并且知晓你的个人爱好，用户体验无疑是非常优异的。从具体技术手段上，Siri是通过在内部保持两个记忆系统：长期记忆系统和短期记忆系统来实现能够个性化的和用户交流的。长期记忆系统存储了用户的名称、居住地址以及历史偏好信息，短期记忆系统则将最近一段时期内Siri和用户的对话记录及GUI点选记录等登记下来。利用这两个记忆系统，Siri可以在理解用户需求的时候帮助澄清用户的真正意图是什么。 语言模式识别系统是对用户输入的表层，语法层，习惯用语和成语等进行模式匹配的模块。匹配模式的代码在Siri内部采用正则表达式或者状态机等方式实现；在Siri识别出指定的语言模式后，可以帮助判断用户输入所述的任务类型。 图3 执行系统 执行系统是Siri系统最有技术含量的部分，前文有述：“活动本体”是对根据用户的输入信息，将各种词典资源，模型资源实例化进行具体加工的场所，而真正的加工过程是由执行系统进行的。执行系统不仅将用户原始的文本输入解析为内部的语义表示，而且要在用户和Siri交互过程中（多轮会话）决定下一句Siri应该说什么内容，可见其重要性。 执行系统具体又可以细分为三个主要部件：语言解释器、会话流控制器和任务控制器。它们之间分工有异同时又密切合作，一起发挥作用。语言解释器将用户输入字符串流解析为语义表示作为输出，而这个语义表示又会作为会话流控制器的输入，会话流控制器根据当前语句所表达的含义，协同任务控制器一起决定Siri下一步应该做什么或者说什么。 语言解释器是Siri中最重要的自然语言处理工具，主要用来对文本形式的用户输入进行解析，将其映射为概念本体层级的信息表示，即理解语言真正的含义，除此外，语言解释器也被用在输入系统中对用户输入提示或者输入补全进行分析，而且对语音识别结果后处理也有很大帮助。 对话流控制系统是在将用户的文本表示解析为内部用户意图之后发挥作用；即语言解释器将解析结果传递给对话流控制器，是语言解释器的后续处理步骤；而“任务控制器”则被“对话流控制器”调用，共同确定Siri下一步应该做什么或者说什么。 “任务流控制器”的主要功能是界定完成一件任务或者解决某个问题由那些步骤构成,这些步骤之间是何种关系。“任务流控制器”和“对话流控制器”很容易混淆，不容易区分其功能差异。一般来说，“对话流控制器”主要用来决定Siri接下来要说的内容或者要做的事件，主要是根据领域判断诱导用户提供所需的参数；而“任务流控制器”更侧重于事务本身的定义，比如一个任务可以切分成若干子任务，是否有时序依赖关系。 任务流控制在Siri中也起到举足轻重的地位，Siri的任务模型是由一些领域无关的通用任务模型和若干领域相关任务构成。通用任务是完成一件任务的抽象表述，与具体领域无关，因为其通用性，也可以应用在各个具体应用领域。 图4 服务系统 Siri本质上是服务导向的用户意图识别系统，无论是对话流控制也好，任务流控制也好，其根本目的还是为了能够将用户引导到Siri能够提供的某项具体服务，以此达到帮助用户完成某些任务或者解决一些问题的目的。目前Siri可以提供多种领域的服务，这里面涉及到服务管理的问题，即如何进行管理才能使得系统可用性高，可维护性强等。具体而言，Siri中有三个子部分涉及到服务功能：服务模块，服务能力模型和多服务集成模块。其中，服务模块记录了可供Siri使用的各种服务的详细信息，服务能力模块则存储了哪些服务可以提供什么类型的服务等映射关系，服务系统中最重要的是服务集成模块，调用另外两个服务模块提供给用户最终服务内容。因为往往完成用户某项需求要调用分布在各处的多项服务，每项服务能够提供部分信息，而且服务之间有些顺序需要遵守，所以如何调用所需的多种功能，调用顺序如何确定以及如何根据部分信息拼合成最终用户所需服务是其核心内容。 Siri的输出系统会将最终提供的服务结果或者在会话过程的中间内容展示给用户。其不仅支持语音、电邮、文本等多模态输出，还支持界面订制等个性化功能。 从上述技术描述看，Siri是苹果公司新推出的一种新型人工智能框架，不仅在商业宣传上令人耳目一新，在其技术架构和具体实现上也颇具新意。尽管Siri最初是依附在iPhone平台，但是很显然，这种依附性并不强，可以预见，这套系统会不断扩展到更多种硬件类型的智能控制，比如车载控制系统，智能电视控制系统等等 关于作者 张俊林，《这就是搜索引擎：核心技术详解》作者，新浪微博研发人员，主要研究方向：自然语言处理、搜索技术、推荐系统及机器学习","title":"Siri技术解析"},{"content":"说明：转载请注明作者和出处；未经许可，不得在平面媒体上发表。 这是本人在信息抽取方面的一些心得和总结，希望对于有志于互联网Web信息抽取的朋友一点启发，有任何问题可以发邮件给我或者加我msn一起讨论。   信息抽取是一个互联网自然语言处理的一个首要环节，信息抽取的准确度会直接影响到后续的处理。信息抽取的目标是去除噪音，获取网页有价值的信息如网页的标题、时间、正文、链接等信息。   主流算法介绍 网页信息抽取的方法有很多，比如从算法上分：基于模板的，基于信息量、基于视觉的、基于语义挖掘的、基于统计的。从HTML 处理上分为：基于行块、基于DOM 树。下面我逐一介绍。 1.     基于模板，一般由人工维护一个URL 和HTML 的模板。当URL 匹配到某个URL 模板时，利用对应的HTML 的模板来抽取其中的信息。这种方法见效快、准确度高，抓取少量站可以使用，可以做一些模板设置工具来减少工作量，大量站需要较多人力维护模板列表。 2.     基于信息量（信息量的解释我下面会说），见基于行块分布函数的正文抽取 ，计算正文在源码哪些行上分布较多，取正文较多的行；另外，也有算法是根据行的正文密度来计算的，简单点说就是正文长度/ 标签数量。基于信息量也有另一种方法，就是建立Dom 树，把行函数变为Dom 树上某个节点的评估函数。对于资讯类网站，这个方法会工作得很好，但是需要考虑到抽取网页信息并不代表文字多就好，比如正文下有一段版权信息或者网站说明，如何去除这些信息？另外，游戏下载网站分为游戏的结构化信息、描述信息、游戏操作说明等部分，信息是分散的，而不是集中的，这类信息如何处理？ 3.     基于基于视觉的页面分割算法 ，是基于分块的算法的一种具体实现方式，这是微软亚洲研究院的一个算法，用于微软搜索引擎Bing 上。我比较喜欢这个算法，因为提出了两个好的想法：一是根据视觉来分块，二是根据视觉来进行块合并。基于视觉处理较复杂，需要用到CSS 、Javascript 等引擎，需要用浏览器内核库来处理HTML ，性能可能不高。另外，这个算法的结果只是告诉大家网页大概可以分为多少块，每一块的位置、大小是什么，而哪块和哪块是正文还需要进一步计算。 4.     基于语义的正文抽取，根据锚文本和页面标题等不容易出错的信息去发现正文块，这类算法有效，但是仍有局限性。 5.     基于统计的，基于分块和统计相结合的新闻正文抽取 和 基于同层网页相似性去除网页噪音。前者利用统计是找到同一网页里面的正文块，后者是链接同一路径下的不同网页的相似度去除噪音，两者是有区别的。基于统计，可以减少个别网页的差异带来的误差，提高准确度。   站在Web 开发者角度考虑       以上的这些方法，都是从网页中的规律考虑，能解决一部分问题，而问题的根源是Web 页面是Web 工程师开发出来的，研究他们的Web 开发习惯和模式对于信息抽取是最根本的，而本人则做过Web 开发，所以总结出来几个对信息抽取有用的几个模式： 模式1 ：同类页面用一套模板。互联网的网站，大体上分为CMS 系统（如帝国CMS ）、博客系统（如Wordpress ）、论坛系统（如Discuzz ），不管是什么系统，同一类的网页都是根据相同的模板和后台数据生成的静态或动态页面，结构上是一样的，而内容是不一样的。如果有改版，也是统一修改，纯手工制作的页面已经很少了。 模式2 ：不同功能的信息用块标签。凡是分块的都用块标签（组标签），HTML 的标签中具有块属性的有DIV 、TABLE 、FORM 、CENTER 、UL 、LI 等。 模式3 ：重复结构用循环。 列表数据、论坛、博客评论，一般都是获取数据行，然后根据行进行循环输出。 模式4 ：按照信息来组织块。一是样式上有区分，导航、正文、相关文章、评论、左侧导航、右侧广告样式都有区别；而回帖、回复的样式都是一样的。二是块之间越相关，块就挨的越近，正文、相关文章、评论就挨的很近，而正文离右侧广告就很远。 模式5 ：不管是Web 开发者水平不高，还是网站比较流氓，很多正文并不干净，恨不得广告中夹点正文。 根据以上的分析，结合上面的一些参考算法，提出了基于Web 开发模式的信息抽取算法，这个算法可以很通用的解决信息抽取中的准确度和干净度的问题。注：准确度 指正文完整；干净度指正文中不包含噪音。   基于Web 开发模式的信息抽取的算法描述 1.     根据“模式1 “收集同一域名或者路径下的n 个（n>=1 ）网页，同一域名或者路径下的网页具有同一模板的可能性较高。如果n=1 ，则退化为单个网页的信息抽取，单个网页抽取，对于快讯、短博客抽取难度大，如果有一组网页合并后抽取则可以较好的解决这个问题。 2.     根据“模式2 “分别按照HTML 的块标签建立n 个Dom 树。这棵树不是所有的HTML 标签都是一个节点，只有块标签的可视节点 才能建立一个节点，既能满足信息抽取需要，又能提高效率。下图是建立的5 个Dom 树。   图1 3.     判断这n 个Dom 是否相似，主要是选取Dom 树的上各分支较高层数的节点来判断其结构是否相似，取相似的Dom 树合并其节点特征，可能n 个Dom 属于多个模板，则可以合并多个Dom 树，逐一计算即可。特征为：正文长度、链接数量、链接中文本长度、图片大小、标签数量。假设合并后的Dom 树为D 。如果某个路径的节点在不同的Dom 树中，其特征完全一样，则此节点可被忽略（去掉版权、网站说明等重复噪音信息）。 4.     根据“模式4 “，对D 进行相似正文块的合并，比如图2 中，节点7 下面有10 、11 、12 都是正文块（可以根据节点特征来计算），具有相同的父节点，则可以合并到节点7 。这一步主要是有某些博客或者网站，其正文分布在几个块中，如果不进行合并，则抽取的正文会不全。   图2 5.     根据“模式3 “，对D 中循环连续的块进行合并，这一类主要是针对评论、论坛的信息，如图3 ，节点2 、3 、4 、5 、6 是相同结构的节点，合并为节点2 。如果不合并，则会抽取到其中的一小块，导致信息不全。同时，对于循环连续块，需要有一个降权的处理，某些博文和评论，评论的权重会比博文大，不降权，会抽取到评论而不是博文。   图3 6.     找到信息量最大的块。这里解释一下信息量的概念，信息量是由文字、链接、图片、视频、动画以及他们的样式传达给使用者的信息的量化标准 。说白了，就是网页想给用户什么信息，内容页给用户的是内容，而导航页给用户的是链接，信息量的计算公式是不一样的。图4是一个网页的结构，根节点1 下面有3 个节点：2 、3 、4 ；根据信息量计算公式，节点3 信息量最大，取节点3 ；节点3 下有7 、8 两个节点，7 最大；7 下面是11 ，所以取节点11 为正文节点。为什么不取节点15 呢，有两种可能，一种是节点15 在第4 步中已经被合并到节点11 上了，另外一种是节点15 信息量占节点11 的信息量比太少，不会被选择。 信息量公式 = 正文信息 + 链接信息 + 图片信息 + 视频（包括Flash ）信息 + 标签信息   图4 7.     根据“模式5 “，找到了正文节点只是说明正文是包含在正文节点中，找到的正文节点中依然包含噪音，比如正文块中夹杂广告信息，比如正文块中包含太多相关链接信息等等。这时候，需要对于正文块进行进一步的清洗，剔除噪音信息。对于论坛，如果只需要帖子本身的信息，而不要用户信息，可以根据论坛回帖重复的特点，计算每个回复块中各个块的信息量方差，方差大的为帖子块（因为帖子的长度差别很大），方差小的是用户信息块（用户信息块差异较小）。 算法优势分析 1.     用组标签以及标签中css进行分块，代替VIPS中的颜色、大小、位置等信息，简化了计算过程，效率较高，这么计算，在实际应用中效果也较好。如果能把颜色、位置、大小因素考虑进去，会更进一步提高准确率。是否需要处理css，看实际需要。 2.  利用同一个模板下不同页面的结构相似性，和页面内循环块的相似性，来进行信息提取，比单一的页面，单一的块进行信息提取，其准确率的提升在3%以上（估计值）。比如快讯（只有一句话）较难处理，比如只提取论坛中的帖子内容（左侧个人信息、签名档都不要），比如提取博文而不要评论等...，通过观察一组相似结构来处理信息 ，这个思路可以延伸到其他类型的页面信息提取。 3.  算法较通用，只需要根据不同的算法把类似的块进行合并以及设计合理的信息量公式，可以为不同的应用场景提供各类提取后的信息，比如提取文本、图片和视频，链接和结构化的内容。 4.  更进一步的优化，在结构相似性的基础上，可以把网页结构的特征和网页信息模板 保留下来，以备无法提取信息的网页使用，特别是对于论坛和博客等回帖数不固定的页面更是重要。 实际应用效果       实际应用中，正文抽取部分，对于上万个站点（包括资讯、博客、论坛站点）的数据抽样进行检测，准确率能达到96% 以上。       此算法用于HUB 的链接分析部分，分析HUB 页中的需要爬取的网页链接，不包含左右两侧的热门、导航等链接，几万个Hub 页测试，其准确率也达到了92% 。-- 如果把块的位置考虑进去，效果会更好。 信息提取时的其他一些问题       标签容错性： 本算法不识别Attribute的内容，不识别CSS和Script的内容，只需要处理标签匹配即可，即便是标签匹配错误也无所谓，只要能提取信息即可。       编码识别：可以提取header中的charset，如果没有则可以用mozilla的charset探测组件来自动识别。编码建议都转为utf-8。       语言识别：可以利用utf-8的中日韩的编码区间来计算字符的分布在哪个语言区间的概率来判别。       标题提取和净化：锚文本和title相结合，根据规则截断标题，把“_新闻中心_新浪网”等无意义的去掉，也可以根据相似网页的标题共同部分去掉来截取。       日期时间识别：正文区域上下不远的地方，用正则来匹配即可。如果有多个时间，可以取大于某个时间（2000年以后？）离现在最近的但不超过当前时间的时间。       图片提取：提取正文区域的大图片链接，图片的介绍文字可以提取图片下方的文字或者图片周围的文字以及标题的文字。       链接提取：提取链接最多的块，如果链接+简介+缩略图的HUB页，可以把文字和图片作为权重计算进去。HUB页也是形式多样，难度不比正文提取小。   其他：以后想到再补充         附录：下面是文档部分，一并共享之，供参考。 基于Web开发模式的信息抽取 Web Page Information Extractor ","title":"基于Web开发模式的信息抽取"},{"content":"关键所在：it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics。 将文档看成是一组主题的混合，词有分配到每个主题的概率。 Probabilistic latent semantic analysis（PLSA） LDA可以看成是服 从贝叶斯分布的PLSA LDA，就是将原来向量空间的词的维度转变为Topic的维度，这一点是十分有意义的。 例如，如果一个文档A，包含电脑这个关键词，那么A向量化后可能是,比如电脑这个词是 100万词汇中的第2维（便于举例），微机这个词是100万词汇中的第3维，维上的投影简单看作是tf，即文档中出现的次数。 A={x,2,0,...,x} 表示文档A中电脑出现了2次.x表示出现次数不care B={x,0,3,...,x} 表示文档B中微机次数出现了3次。 如果是用词做维度的向量空间，做聚类也好，分类也好，A和B在电脑和微机上的这种向量表示，机器理解为A和B完全在表示不同的意义。而事实上，如果在词的 高维空间上看，电脑和微机的维是很近似的，正交性是很低的。 如果能够将高维空间上，近义词或者表示接近的词的维度“捏“成一个维度，比如电脑和微机这两个词被捏成了第2维，但是每个词在这个维上的权重给与不同的度 量（比如概率）。 这样上诉例子变为 A={x,2*pi,x,...x}，pi表示电脑这个词到Topic2的转移概率。 B={x,3*pj,x,...x} 这样，A和B看上去在第二个Topic上显示了一定的相关性。 由于Topic是被捏后的产物，每个Topic的正交性直观上看都很强，LDA开源的工具做出的结果可以把转移到TOpic最Top的那些词提取出来，都 是十分相关或近似的词。而Topic与Topic之间显示出很大的差异性。 短文本分类的商业价值是很大的，在视频分类，广告分类上都可以看作是短文本分类问题，我有幸做了一些这方面的工作，其中提到的短文本的扩展是很好的思路。 问答系统商业价值也很巨大，特别是封闭领域的问答系统，可以拦截投诉，用户提问，降低人工成本。开放领域的问答系统商业上感觉前途有限，当然把搜索引擎的 搜索结果进一步精化的思路肯定是搜索引擎的一个方向，用户会越来越懒，搜索引擎已经让用户懒了一些，还需要让用户继续懒下去。 下面是baidu知道中有人对LDA的解释 lda是一个集合概率模型，主要用于处理离散的数据集合，目前主要用在数据挖掘（dm）中的text mining和自然语言处理中，主要是用来降低维度的。据说效果不错。 以下是在tm中对lda的定义： Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. 其实它还可以用在别的方面，早期是被用在自然语言处理的文本表示方面。因为他提供了一个理解相关词为什么在同一文档出现的框架解释模型。 ========================================= Latent Dirichlet allocation This is a C implementation of variational EM for latent Dirichlet allocation (LDA), a topic model for text or other discrete data. LDA allows you to analyze of corpus, and extract the topics that combined to form its documents. For example, click here to see the topics estimated from a small corpus of Associated Press documents. LDA is fully described in Blei et al. (2003) . This code contains: an implementation of variational inference for the per-document topic proportions and per-word topic assignments a variational EM procedure for estimating the topics and exchangeable Dirichlet hyperparameter Downloads Download the readme.txt . Download the code: lda-c.tgz . Sample data 2246 documents from the Associated Press [ download ]. Top 20 words from 100 topics estimated from the AP corpus [pdf]. 还有一个 普林斯顿大学 的主题模型的邮件列表，牛人们： https://lists.cs.princeton.edu/mailman/listinfo/topic-models ============================================= [论文导读][教程][介绍]文本处理、图像标注中的一篇重要论文Latent Dirichlet Allocation 原文信息 Latent Dirichlet Allocation David.M.Blei Andrew.Y.Ng Michael.I.Jordan JMLR2003 （可google到） 原文的主要内容 有两种方法设计分类器： 1. discriminative model，就是由样本直接设计判别函数，例如SVM； 2. generative model，就是先从样本恢复概率模型——例如我们熟悉的参数方法：混合高斯模型GMM;非参数方法Parzen 窗。然后再充分挖掘模型，用以分类。例如Bayes最大后验概率准则；或者将模型中的参数当作提取的特征（参数一般都比较少，所以这么做实际上是在降维），在这些新特征上设计分类器（例如又用SVM）。 恢复的模型可生成新的样本，所以得名generative。 原文就是讲了一种建立generative model的方法，用于文本处理。 对文本（document）中各单词（word）的出现频率（简称词频）建立概率模型通常是文本处理的第一步。 开始讨论前，先做如下约定： - 仅考虑文本的词频，而不考虑单词在文本中出现的先后顺序及其约束关系 - 文本中的单词来自大小为|V|的词汇表。例如： V = {FILM, MUSIC, TAX, MILLION, STUDENT, TEACHER, SCHOOL}. |V| = 7 - 每篇文本有N个单词 - 文本来自k个主题（topic）。例如: T = {Arts, Budgets, Education}. k = 3 一种简单直观的词频概率模型——unigram model（原文Figure 3(a)）这样描述某一文本中单词的“发生方式”： For each of the N words w_n: Choose a word w_n ～ p(w); 其中，w是离散随机变量，在词汇表V中取|V|个离散的值。p(w)是w的分布，可由训练样本通过机器学习或其它方法获得。这个模型就是每个单词的词频，没有考虑文本的主题，过于简单。于是我们引出考虑了文本主题的模型—— Mixture of unigram(原文中Figure 3(b)). 它这样描述某一文本中单词的“发生方式”： Choose a topic z ～ p(z); For each of the N words w_n: Choose a word w_n ～ p(w|z); 其中，z是离散随机变量，在主题T中取k个离散值，p(z)是z的分布；w_n是离散随机变量,在词汇表V中取|V|个离散值，p(w|z)是给定z时w的条件分布。z可取k个值，w可取|V|个值，p(w|z)可看作一个k×|V|的矩阵，可由训练样本通过机器学习或其它方法获得。 对照我们在前面的约定中给出的V和T的具体示例，p(w|z)是3×7矩阵。若p(w|z)的第1行表示主题{Education}——可以想象这个主题的文本中{STUDENT, TEACHER, SCHOOL}的词频会高些，其它单词的词频会低些——因此该行的行向量所表示的分布 p(w|z)会在{STUDENT, TEACHER, SCHOOL}附近出现峰值；若第2行表示主题{Budgets}，p(w|z)就会在 {TAX,MILLION}附近出现峰值... 在“发生”一篇文本前先随机选出p(w|z)的第z行（根据分布p(z)）；再依次随机选出第z行的w_1,w_2,...,w_N列（每次选取都根据分布p(w|z)）,这就“发生”出了文本中的所有单词。 但是这个模型只允许一篇文本有一个主题，这是不够妥当的。一篇关于北邮科研经费的文本，可能 {STUDENT, TEACHER, SCHOOL, TAX, MILLION}的词频都很高——这个文本同时具有两个主题 {Education,Budgets}。如何模拟一篇文本多个主题的情形呢？在此，我们跳过pLSI模型，直接引入原文探讨的—— Latent Dirichlet Allocation (LDA, 原文中Figure 1). 它这样描述某一文本中单词的“发生方式”： Choose parameter θ ～ p(θ); For each of the N words w_n: Choose a topic z_n ～ p(z|θ); Choose a word w_n ～ p(w|z); 其中θ是一个1×k的随机行向量，p(θ)是θ的分布，它的具体函数形式就是Dirichlet分布，这一分布保证θ的k个分量 θ_1,θ_2,...,θ_k都取连续的非负值，且θ_1 + θ_2 + ... + θ_k = 1；z_n是离散随机变量，在主题T中取k个离散值，p(z|θ)是给定θ时z的条件分布，它的具体函数形式很简单，就是把θ直接拿来作为概率值：p(z = i|θ) = θ_i,也就是说z取第 1,2,...k个主题的概率分别是θ_1,θ_2,...,θ_k；w_n是离散随机变量,在词汇表V中取|V|个离散值，p(w|z)是给定z_n时 w的条件分布。和前面一样，可以把它看作k×|V|的矩阵。 LDA在“发生”一篇文本前,先随机生成一个1×k的向量θ（根据Dirichlet分布p(θ)）,生成的这个θ非负且归一化，可以看作某个随机变量的分布(也就是说，Dirichlet可以看作是分布的分布...)；然后随机选取p(w|z)的第z_1行（根据分布p(z|θ)）,接着随机选取z_1行的w_1列（根据分布p(w|z = z_1)）,同样的方法依次选出z_2,w_2,...z_N,w_N,这就“发生”出了文本中的所有单词。 剩下的任务就是如何根据训练样本学习出LDA模型的具体形式。模型无非是含有控制参数的表达式，学习出了参数就确定了模型。我们看看LDA有哪些控制参数呢？ - 分布p(θ)的表达式需要一个1×k行向量的参数，记为α - p(w|z)可看作k×|V|的矩阵，记该矩阵为β 把w看作观察变量，θ和z看作隐藏变量，就可以通过著名的EM算法学习出α和β，但这一过程中后验概率p(θ,z|w)无法计算出解析表达式，因此需要近似解，原文中使用了基于分解（factorization）假设的变分法(Variational Methods)，其实也就是先假设θ和z在给定w时条件独立：p(θ,z|w) ≈ p(θ|w)*p(z|w)，然后进行后续推导（参考本导读“预备知识 ” - Variational Inference）。这一套方法原文叫做variational EM，推导过程确实有些复杂，但最后总结出的不过是几个可以通过迭代求解的表达式（参见原文5.3节的1.2.两步）。 最后理一下原文的结构 1 Introduction 综述其它方法 2 Notation and terminology 符号约定 3 Latent Dirichlet allocation 详细介绍 4 Relationship with other latent variable models LDA与 unigram, mixture of unigram, pLSI的区别。前两个本导读已经提到了，建议读者再仔细比较pLSI和LDA的区别，理解为什么作者说pLSI不是well-defined graphic model 5 Inference and Parameter Estimation Inference部分介绍了 variational inference，就是本导读前面提到的如何近似计算隐藏变量的后验概率。Parameter Estimation就是用 EM法估计α和β 6 Examples和7 Applications and Empirical Results 给出的具体例子、应用和实验结果 预备知识 如果牢固掌握这些预备知识，理解原文会更容易些。 - p(X|Y)的记法。注意|右边的Y既可以表示随机变量（已经取定了某具体值），也可以表示普通的非随机变量。这样我们可以在最大似然估计和 Bayes方法间方便的“切换”，而不会让符号记法影响我们的表述。例如，考虑具有确定但未知参数μ，Σ的高斯分布p(x),可以记为p(x|μ,Σ); 若按照Bayes学派观点，可以将μ和Σ也看作随机变量，x的分布就能记为随机变量μ，Σ取定某值后的条件分布p(x|μ,Σ)——统一的记法。 - k取1分布/多项式分布(Multinomial)。考虑取3个离散值的随机变量x ～ p(x)。这个看似很平庸的分布...就是所谓的k 取1分布或称多项式分布。一般我们习惯的把它记为p(x_i) = u_i, i = 1,2,3,且u_1 + u_2 + u_3 = 1. 但在有些数学推导中，将它记为指数形式会更方便些.将x看作3维的随机向量，各分量是“互斥”的，即它只能取(1,0,0),(0,1,0),(0,0,1)三组值。于是可将分布重新记为 p(x) = (u_1^x_1)*(u_2^x_2)*(u_3^x_3).注意论文原文中Multinomial就是这儿说的k取1分布，与一些概率教科书中的定义不同。一般的k维情况依次类推。具体参[Bishop]的2.2节. - 共轭先验分布(Conjugate Prior)。考虑某概率密度函数，要估计其中的参数t。按照Bayes学派的观点，参数 t ～ p(t).我们有p(t|X) ∝ p(X|t)p(t),这个式子说：在没有做任何观测时，我们对t的知识用先验分布p(t)表示。当观察到X 后，就通过该式将先验概率p(t)更新（计算）为后验概率p(t|X)，使我们对t的知识增加。仔细观察，若p(t)与p(X|t)有相同的函数形式，那么后验概率p(t|X)就与先验概率p(t)有相同的函数形式——这使得t的后验概率与先验概率具有相同的表达式，只是参数被更新了！ 更妙的是，本次后验概率可以作为下次观测时的先验概率，于是当继续进行观测X_2,X_3...时，只是不断的在更新先验概率p(t)的参数,p(t)的函数形式不变。具体参见[Bishop]的2.2节。 这也是Bayes学派饱受批评的地方：先验概率的选取有时只是方便数学推导，而非准确的反映我们的先验知识。 - Dirichlet分布。现在我们可以说，Dirichlet分布就是k取1分布的Conjugate Prior。若k维随机向量 θ ～ Drichlet分布，则θ的k个分量θ_1,θ_2,...,θ_k都取连续的非负值，且 θ_1 + θ_2 + ... + θ_k = 1。Dirichlet分布的具体表达式参见[Bishop]的2.2节。 - Simplex。考虑2维的例子：以(0,1)与(1,0)为端点的线段就是simplex。考虑3维的例子，以(0,0,1), (0,1,0),(0,0,1)为端点的三角形内部就是simplex。更高维的情况可依次类推。考虑θ ～ Drichlet分布。注意到θ的k个分量 θ_1,θ_2,...,θ_k都取连续的非负值，且θ_1 + θ_2 + ... + θ_k = 1，可知Dirichlet分布的定义域是一个 simplex.这也就是原文中Figure 2那个三角形的含义（k = 3的示意图，让这个simplex三角形平躺在水平面上）。参见 [Bishop]的2.2节 - Graphical Models. 就是用图来表示随机变量中的依赖关系。这个tutorial一google一大把。建议参考 [Bishop]的8.1节，了解几个符号（空心圆圈——隐藏(latent)变量,实心圆圈——观察(observed)变量，方框——重复次数）就足够看懂原文中的Figure 1和Figure 3了。最多再看看[Bishop]的8.2节 - EM.关于这个的tutorial很多，但我觉得[Bishop]的9.2节是数学处理最为简洁，最容易看懂的(有个tutorial在关键的几步中用了大量∑和∏，让人抓狂) 。另外[Bishop]的9.4节也值得看，为理解其它内容如variational inference有好处。 - Variational Inference. 就是计算后验概率的近似方法。考虑随机变量{X,Z}，其中X是观察变量，Z = {Z_1,Z_2}是隐藏变量。用EM法或做Bayes推理的关键一步,就是要求后验概率p(Z|X).不巧的是,在一些复杂问题中 p(Z|X)没有解析表达式,需要近似求解.相关的方法很多,一种经常使用的是基于可分解(factorization)假设的方法：p(Z|X) ≈ p(Z_1|X)p(Z_2|X)——就是说强行假设Z_1和Z_2条件独立——然后进行后续推导。 这一假设当然会产生误差，考虑二维高斯分布p(Z|X) = p(Z_1,Z_2|X)，Z_1与Z_2不独立，所以p(Z_1,Z_2|X)的等高图是同心椭圆，椭圆可任意倾斜（例如，若Z_1与Z_2的线性相关系数是1，则椭圆倾斜45°）。现简记 p(Z_1|X) = q_1(Z_1), p(Z_2|X) = q_2(Z_2)，我们想改变q_1与q_2，用q_1*q_2去拟合 p(Z_1,Z_2|X).但无论如何改变q_1与q_2的形式，q_1*q_2的椭圆等高线都是长轴、短轴分别与Z_1轴、Z_2轴平行！不过，合适的 q_1与q_2保证q_1*q_2与p(Z|X)的峰值点重合，一般这就足以解决实际问题了。详细讲解可以参见[Bishop]的第10章。也可参考 [Winn]的1.8节。 另外，[Winn]提出了通用的计算框架，你不必每次都先用Variational Inference推导出公式，再手工编写代码；你只用在一个GUI里编辑好Graphical Model，再点start...（作为类比，考虑离散的线性系统转移函数H(z)，你可以由此推导出差分方程的表达式，然后用Matlab编程求解；也可以在Simulink中编辑好框图，再点start...） 参考文献 [Bishop] Pattern Recognition And Machine Learning. C.M.Bishop. Springer, 2006（cryppie在本版曾发过电子版） [Winn] Variational Message Passing and its Applications. John M. Winn. Ph.D. dissertation, 2004(可google到) 网上资源 可google到LDA的Matlab和C实现 http://vibes.sourceforge.net/index.shtml john winn的通用框架,Java实现（里面有文档指导怎样在Matlab中调Java）","title":"LDA"},{"content":"下面以新浪微博为例子 Java代码 package com.ansj.sun.pojo;       import java.util.ArrayList;    import java.util.Iterator;    import java.util.List;    import java.util.regex.Matcher;    import java.util.regex.Pattern;       public class AnsjPaser {           private String beginRegex;           private String endRegex;           private Matcher matcher;           public final static String TEXTTEGEX = \".*?\";                public final static String W = \"\\\\W*?\" ;                public final static String N = \"\" ;                private List<String> filterRegexList = new ArrayList<String>();           /**        *         * @param beginRegex 起始正则        * @param endRegex 结束正则        * @param content 需要解析的正文(如果没有此项必须为rest设置)        * @param textRegex 其实和结束正则中间的部分,默认为.*?        */       public AnsjPaser(String beginRegex, String endRegex, String content,                String textRegex) {               this.beginRegex = beginRegex;               this.endRegex = endRegex;               StringBuilder sb = new StringBuilder();               sb.append(beginRegex);               sb.append(textRegex);               sb.append(endRegex);               matcher = Pattern.compile(sb.toString()).matcher(content);        }           /**        *         * @param beginRegex 起始正则        * @param endRegex 结束正则        * @param textRegex 其实和结束正则中间的部分,默认为.*?        */       public AnsjPaser(String beginRegex, String endRegex, String textRegex) {               this.beginRegex = beginRegex;               this.endRegex = endRegex;               StringBuilder sb = new StringBuilder();               sb.append(beginRegex);               sb.append(textRegex);               sb.append(endRegex);               matcher = Pattern.compile(sb.toString()).matcher(N);        }                   /**        * @param beginRegex 起始正则        * @param endRegex 结束正则        */         public AnsjPaser(String beginRegex, String endRegex) {               this.beginRegex = beginRegex;               this.endRegex = endRegex;               StringBuilder sb = new StringBuilder();               sb.append(beginRegex);               sb.append(TEXTTEGEX);               sb.append(endRegex);               matcher = Pattern.compile(sb.toString()).matcher(N);        }           /**        * @创建人：Ansj -创建时间：2011-8-16 下午09:30:56        * @方法描述： @return 返回正则内的内容去除了开始和结束标签,和需要过滤的正则返回用户需要的真正的内容        */       public String getText() {            if (matcher.find()) {                String str = matcher.group().trim().replaceFirst(beginRegex, N)                        .replaceAll(endRegex, N);                Iterator<String> it = filterRegexList.iterator() ;                while(it.hasNext()){                    str = str.replaceAll(it.next(), N) ;                }                return str ;            }            return null;        }           /*        * 得到下一个        */       public String getNext() {            return matcher.group();        }           /*        * 是否包含下一个        */       public boolean hasNext() {            return matcher.find();        }           /**       * @创建人：Ansj  -创建时间：2011-8-17 上午12:11:12           * @方法描述：   @param content 需要解析的正文        * @方法描述：   @return 返回本身       * 这个方法是将此解析器重置,相当于重头开始.但是一些正则配置给予保留        */       public AnsjPaser reset(String content) {            this.matcher.reset(content);            return this ;        }                /*        * 添加getText的正则过滤条件        */       public AnsjPaser addFilterRegex(String filterRegex){            filterRegexList.add(filterRegex) ;            return this ;        }       }   package com.ansj.sun.pojo;import java.util.ArrayList;import java.util.Iterator;import java.util.List;import java.util.regex.Matcher;import java.util.regex.Pattern;public class AnsjPaser {\tprivate String beginRegex;\tprivate String endRegex;\tprivate Matcher matcher;\tpublic final static String TEXTTEGEX = \".*?\";\t\tpublic final static String W = \"\\\\W*?\" ;\t\tpublic final static String N = \"\" ;\t\tprivate List<String> filterRegexList = new ArrayList<String>();\t/**\t * \t * @param beginRegex 起始正则\t * @param endRegex 结束正则\t * @param content 需要解析的正文(如果没有此项必须为rest设置)\t * @param textRegex 其实和结束正则中间的部分,默认为.*?\t */\tpublic AnsjPaser(String beginRegex, String endRegex, String content,\t\t\tString textRegex) {\t\tthis.beginRegex = beginRegex;\t\tthis.endRegex = endRegex;\t\tStringBuilder sb = new StringBuilder();\t\tsb.append(beginRegex);\t\tsb.append(textRegex);\t\tsb.append(endRegex);\t\tmatcher = Pattern.compile(sb.toString()).matcher(content);\t}\t/**\t * \t * @param beginRegex 起始正则\t * @param endRegex 结束正则\t * @param textRegex 其实和结束正则中间的部分,默认为.*?\t */\tpublic AnsjPaser(String beginRegex, String endRegex, String textRegex) {\t\tthis.beginRegex = beginRegex;\t\tthis.endRegex = endRegex;\t\tStringBuilder sb = new StringBuilder();\t\tsb.append(beginRegex);\t\tsb.append(textRegex);\t\tsb.append(endRegex);\t\tmatcher = Pattern.compile(sb.toString()).matcher(N);\t}\t\t/**\t * @param beginRegex 起始正则\t * @param endRegex 结束正则\t */\t\tpublic AnsjPaser(String beginRegex, String endRegex) {\t\tthis.beginRegex = beginRegex;\t\tthis.endRegex = endRegex;\t\tStringBuilder sb = new StringBuilder();\t\tsb.append(beginRegex);\t\tsb.append(TEXTTEGEX);\t\tsb.append(endRegex);\t\tmatcher = Pattern.compile(sb.toString()).matcher(N);\t}\t/**\t * @创建人：Ansj -创建时间：2011-8-16 下午09:30:56\t * @方法描述： @return 返回正则内的内容去除了开始和结束标签,和需要过滤的正则返回用户需要的真正的内容\t */\tpublic String getText() {\t\tif (matcher.find()) {\t\t\tString str = matcher.group().trim().replaceFirst(beginRegex, N)\t\t\t\t\t.replaceAll(endRegex, N);\t\t\tIterator<String> it = filterRegexList.iterator() ;\t\t\twhile(it.hasNext()){\t\t\t\tstr = str.replaceAll(it.next(), N) ;\t\t\t}\t\t\treturn str ;\t\t}\t\treturn null;\t}\t/*\t * 得到下一个\t */\tpublic String getNext() {\t\treturn matcher.group();\t}\t/*\t * 是否包含下一个\t */\tpublic boolean hasNext() {\t\treturn matcher.find();\t}\t/**\t* @创建人：Ansj  -创建时间：2011-8-17 上午12:11:12    \t* @方法描述：   @param content 需要解析的正文 \t* @方法描述：   @return 返回本身\t* 这个方法是将此解析器重置,相当于重头开始.但是一些正则配置给予保留\t */\tpublic AnsjPaser reset(String content) {\t\tthis.matcher.reset(content);\t\treturn this ;\t}\t\t/*\t * 添加getText的正则过滤条件\t */\tpublic AnsjPaser addFilterRegex(String filterRegex){\t\tfilterRegexList.add(filterRegex) ;\t\treturn this ;\t}} 这里是调用例子 Java代码 package com.ansj.sun.impl;       import java.io.BufferedReader;    import java.io.IOException;       import com.ansj.sun.pojo.AnsjPaser;    import com.ansj.sun.util.IOUtil;       public class HtmlPaser{           public static void main(String[] args) throws IOException {            //阅读正文            BufferedReader br = IOUtil.getReader(                    \"C:\\\\Users\\\\caiqing\\\\Desktop\\\\ajax采集\\\\zhanghuaping.html\",                    \"UTF-8\");            StringBuilder sb = new StringBuilder();            String temp = null;            while ((temp = br.readLine()) != null) {                sb.append(temp);            }               System.out.println(sb);            // 模块抽取            String beginRegex = \"<div class=\\\"MIB_feed_c\\\">\\\\W*?<p class=\\\"sms\\\"\";            String endRegex = \"<div id=\\\"_comment_list_miniblog.*?\\\"><\/div>\\\\W*?<\/div>\";            AnsjPaser ansjHtml = new AnsjPaser(beginRegex, endRegex, sb.toString(),                    AnsjPaser.TEXTTEGEX);            // 正文抽取            beginRegex = \"<p class=\\\"sms\\\" mid=\\\"\\\\d*?\\\" type=\\\"\\\\d*?\\\">\";            endRegex = \"<\/p>\";            AnsjPaser ansjContent = new AnsjPaser(beginRegex, endRegex).addFilterRegex(\"<.*?>\");            // 时间抽取            beginRegex = \"onclick=\\\"GB_SUDA._S_uaTrack\\\\('weibo_transmit','time_origin'\\\\);\\\"><strong date=\\\"\";            endRegex = \"\\\">\";            AnsjPaser ansjPubTime = new AnsjPaser(beginRegex, endRegex);            // 来源抽取            beginRegex = \"<strong lang=\\\"CL1006\\\">来自<\/strong><cite><a.*?>\";            endRegex = \"<\/a>\";            AnsjPaser ansjFrom = new AnsjPaser(beginRegex, endRegex);            // 转发抽取            beginRegex = \"<strong lang=\\\"CD0023\\\" pop=\\\"true\\\">转发<\/strong><strong id=\\\"num_\\\\d*?\\\" rid=\\\"\\\\d*?\\\" type=\\\"rttCount\\\">\\\\(\";            endRegex = \"\\\\)<\/strong>\";            AnsjPaser ansjRepeat = new AnsjPaser(beginRegex, endRegex);            // 评论抽取            beginRegex = \"<strong lang=\\\"CL1004\\\">评论<\/strong><strong rid=\\\"\\\\d*?\\\" type=\\\"commtCount\\\">\\\\(\";            endRegex = \"\\\\)<\/strong>\";            AnsjPaser ansjComment = new AnsjPaser(beginRegex, endRegex);                        //开始抽取            while (ansjHtml.hasNext()) {                   String c1 = ansjHtml.getNext();                   System.out                        .println(\"=========================================================================\");                   System.out.println(\"时间:\" + ansjPubTime.reset(c1).getText());                   System.out.println(\"来源:\" + ansjFrom.reset(c1).getText());                   String str = ansjRepeat.reset(c1).getText();                System.out.println(\"转发:\"                       + ((str == null || \"\".equals(str)) ? \"0\" : str));                   str = ansjComment.reset(c1).getText();                System.out.println(\"评论:\"                       + ((str == null || \"\".equals(str)) ? \"0\" : str));                   System.out.println(\"正文:\" + ansjContent.reset(c1).getText());               }           }       }   package com.ansj.sun.impl;import java.io.BufferedReader;import java.io.IOException;import com.ansj.sun.pojo.AnsjPaser;import com.ansj.sun.util.IOUtil;public class HtmlPaser{\tpublic static void main(String[] args) throws IOException {\t\t//阅读正文\t\tBufferedReader br = IOUtil.getReader(\t\t\t\t\"C:\\\\Users\\\\caiqing\\\\Desktop\\\\ajax采集\\\\zhanghuaping.html\",\t\t\t\t\"UTF-8\");\t\tStringBuilder sb = new StringBuilder();\t\tString temp = null;\t\twhile ((temp = br.readLine()) != null) {\t\t\tsb.append(temp);\t\t}\t\tSystem.out.println(sb);\t\t// 模块抽取\t\tString beginRegex = \"<div class=\\\"MIB_feed_c\\\">\\\\W*?<p class=\\\"sms\\\"\";\t\tString endRegex = \"<div id=\\\"_comment_list_miniblog.*?\\\"><\/div>\\\\W*?<\/div>\";\t\tAnsjPaser ansjHtml = new AnsjPaser(beginRegex, endRegex, sb.toString(),\t\t\t\tAnsjPaser.TEXTTEGEX);\t\t// 正文抽取\t\tbeginRegex = \"<p class=\\\"sms\\\" mid=\\\"\\\\d*?\\\" type=\\\"\\\\d*?\\\">\";\t\tendRegex = \"<\/p>\";\t\tAnsjPaser ansjContent = new AnsjPaser(beginRegex, endRegex).addFilterRegex(\"<.*?>\");\t\t// 时间抽取\t\tbeginRegex = \"onclick=\\\"GB_SUDA._S_uaTrack\\\\('weibo_transmit','time_origin'\\\\);\\\"><strong date=\\\"\";\t\tendRegex = \"\\\">\";\t\tAnsjPaser ansjPubTime = new AnsjPaser(beginRegex, endRegex);\t\t// 来源抽取\t\tbeginRegex = \"<strong lang=\\\"CL1006\\\">来自<\/strong><cite><a.*?>\";\t\tendRegex = \"<\/a>\";\t\tAnsjPaser ansjFrom = new AnsjPaser(beginRegex, endRegex);\t\t// 转发抽取\t\tbeginRegex = \"<strong lang=\\\"CD0023\\\" pop=\\\"true\\\">转发<\/strong><strong id=\\\"num_\\\\d*?\\\" rid=\\\"\\\\d*?\\\" type=\\\"rttCount\\\">\\\\(\";\t\tendRegex = \"\\\\)<\/strong>\";\t\tAnsjPaser ansjRepeat = new AnsjPaser(beginRegex, endRegex);\t\t// 评论抽取\t\tbeginRegex = \"<strong lang=\\\"CL1004\\\">评论<\/strong><strong rid=\\\"\\\\d*?\\\" type=\\\"commtCount\\\">\\\\(\";\t\tendRegex = \"\\\\)<\/strong>\";\t\tAnsjPaser ansjComment = new AnsjPaser(beginRegex, endRegex);\t\t\t\t//开始抽取\t\twhile (ansjHtml.hasNext()) {\t\t\tString c1 = ansjHtml.getNext();\t\t\tSystem.out\t\t\t\t\t.println(\"=========================================================================\");\t\t\tSystem.out.println(\"时间:\" + ansjPubTime.reset(c1).getText());\t\t\tSystem.out.println(\"来源:\" + ansjFrom.reset(c1).getText());\t\t\tString str = ansjRepeat.reset(c1).getText();\t\t\tSystem.out.println(\"转发:\"\t\t\t\t\t+ ((str == null || \"\".equals(str)) ? \"0\" : str));\t\t\tstr = ansjComment.reset(c1).getText();\t\t\tSystem.out.println(\"评论:\"\t\t\t\t\t+ ((str == null || \"\".equals(str)) ? \"0\" : str));\t\t\tSystem.out.println(\"正文:\" + ansjContent.reset(c1).getText());\t\t}\t}} 这个是返回结果 Java代码 =========================================================================    时间:2011-08-15 10:19:24   来源:iPhone客户端    转发:0   评论:4   正文:云南石林奇景，难忘云南昆明的#yssnlp#    =========================================================================    时间:2011-08-12 09:23:37   来源:新浪微博    转发:2   评论:1   正文:#YSSNLP#听上海交大刘功申副教授的报告“面向内容安全的自然语言关键技术研究”，敢说话，尺度很大，佩服！回想差不多十年在内容安全领域内的研究和工程经验，从初始的神秘感觉很酷，到最后的做了不敢说不愿说。    =========================================================================    时间:2011-08-11 15:58:05   来源:iPhone客户端    转发:3   评论:7   正文:听哈工大刘挺教授的报告：从语言计算到社会计算，社会网络的兴起促进了计算社会科学的兴趣，很有见地！    =========================================================================    时间:2011-08-11 13:16:30   来源:iPhone客户端    转发:1   评论:0   正文:这个资源是我们采用自动采集抽取得到的,然后经过邮政发达的连锁网络进行数据校对。有需要的可以联系我    =========================================================================    时间:2011-08-11 11:35:02   来源:新浪微博    转发:2   评论:0   正文:听富士通的报告，看到富士通生成的互联网企业信息监控系统，和网络舆情的企业化应用一致，有意思。    =========================================================================    时间:2011-08-11 10:03:14   来源:新浪微博    转发:8   评论:1   正文:听百度高级科学家王海峰的报告“面向互联网的泛自然语言处理”，新数据新资源：海量网页资源，用户行为数据，用户产生数据；新思路新方法：贴近真实需求与数据、平衡数据与算法、模拟真实应用的实验平台，基于实际需求的研究，基于最终应用的开发，一直是我所推崇的自然语言研究方法和思路。很有共鸣。    =========================================================================    时间:2011-08-11 08:39:37   来源:iPhone客户端    转发:1   评论:1   正文:第八届全国自然语言处理青年学者研讨会 #yssnlp2011#    =========================================================================    时间:2011-08-10 22:38:02   来源:新浪微博    转发:2   评论:2   正文:云南滇池温泉花园国际大酒店,夜游滇池，很美，晚上聆听床边的虫鸣，远离了北京每夜的喧嚣，难得的清净    =========================================================================    时间:2011-08-10 21:20:37   来源:iPhone客户端    转发:0   评论:1   正文:夕阳，滇池，余正涛教授组织得很精细    =========================================================================    时间:2011-08-10 18:21:55   来源:iPhone客户端    转发:1   评论:0   正文:我在这里：#昆明市区#参加全国青年学者自然语言学术会议，受邀制的会议，不发论文纯研讨会，交流更充分更单纯，见到好多老朋友了。    =========================================================================    时间:2011-08-06 11:12:40   来源:新浪微博    转发:3   评论:4   正文:推理结论：1)说明博士老板很给力，好像科学院没这么多过，一直是2K以内；2)没有经济基础的高校“青椒”必须先解决生存问题才能解决生产问题，高校改革的核心思路不在于花美国的价召唤海归女婿的华丽报国，而在于用中国的价码给土鳖缓解菜色之忧，毕竟他们才是高校教育的一线火枪手，决定教育质量。    =========================================================================    时间:2011-08-04 09:52:26   来源:新浪新闻中心    转发:1   评论:1   正文:#郭美美#1.头一回听郎教授采访，有些磕巴，问得没有那么一针见血，炫耀背书准备工作，他还是写书条理清晰，比较潮的话题把握得不如娱乐或者时事主持人；2.郭美美比想象中的还要无知无畏，如果没有幕后人，很快就能问出真相；3.郭妈妈是个有故事不简单的人，整个事件的突破口所在 http://t.cn/a82Y2Q    =========================================================================    时间:2011-08-02 11:16:40   来源:新浪微博    转发:0   评论:0   正文:祝贺师弟获此殊荣，以后改称于领军了。//@白硕sse：同祝贺。 //@景伟NLP:祝贺！ @ICTCLAS张华平博士 @白硕sse @    =========================================================================    时间:2011-07-31 17:41:14   来源:新浪微博    转发:3   评论:2   正文:ICTCLAS张华平博士：#红十字信息公开#测试续：作为程序控和测试控，必须得说红十字会能公开一些数据有进步，值得肯定，但是错漏百出，如果需要，我们团队愿意协助做数据的纠错与系统的测试，该系统的再次查询老是错误，我们愿意帮助免费测试或者研发。支持的请转发 @中国红十字会总会    =========================================================================    时间:2011-07-29 12:49:44   来源:新浪微博    转发:4   评论:2   正文:【温州动车追尾】用对救援铺天盖地的歌功颂德来掩饰严重失职，用天灾抹去人祸的痕迹，5-12后的今天才能从境外媒体聆听到受难者悲怆控诉。**在温州的危机公关吸纳了很多微博等网络媒体的舆情民意，这是庶民的胜利，也是政府危机处理的一次胜利转型。但愿成为常态化，我们还要继续呐喊。    =========================================================================    时间:2011-07-28 13:31:18   来源:关联博客    转发:3   评论:1   正文:ICTCLAS2011 0728在Win7下C与C#调用的升级包 http://t.cn/aj30qU    =========================================================================    时间:2011-07-26 17:26:03   来源:iPhone客户端    转发:1   评论:0   正文:与sigir2011组委会**，国际信息检索知名学者聂建芸老师交流。    =========================================================================    时间:2011-07-26 11:43:55   来源:iPhone客户端    转发:1   评论:0   正文:参加#sigir2011#    =========================================================================    时间:2011-07-25 14:37:28   来源:新浪微博    转发:1   评论:1   正文:【温州动车脱轨】我们都坐在这条快速而不安全的高速火车上，不知道什么时候什么地点会有什么神奇的遭遇，但愿火车上的人互助呐喊，破窗改变。奇迹小女孩伊伊让我们莫名的悲痛酸楚，让我们这些人更要坚强！一路走好，祝福你们，但愿那个世界没有这些草菅人命的恶心人恶心是      =========================================================================    时间:2011-07-24 15:18:47   来源:新浪微博    转发:11   评论:3   正文:【温州动车追尾】中午CCTV对铁道部盛光祖部长采访，部长表示已经在调集最好的医生，还有周边的医疗救护小组正在赶来，最后记者说了一句：谢谢部长，然后礼貌离开。铁道部已然是救命的恩主，部长已然成了救世主了？不奢望部长引咎辞职，但感觉不到起码的歉意。记者敢直面声讨的动车才是真正的和谐号。    =========================================================================    时间:2011-07-24 10:55:20   来源:新浪新闻中心    转发:18   评论:8   正文:温州动车追尾的报道还在延续悲催的中国特色：多报道英勇救援的感人事迹，少报道受害者的哭天抢地；多报道市民排队献血，少报道满地遇难者的流血；多强调天灾，少分析人祸；多报道遇难后的领导关切批示，不挖领导之前的不作为制造豆腐渣的内幕；灾难不怕，怕的是不挖内幕不反思，怕大团圆后灾难依然。    =========================================================================    时间:2011-07-23 22:06:10   来源:新浪微博    转发:1   评论:1   正文:做任何一件大事情，好的领导者心中必须有大局，同时设立一个个小的里程碑，每天一点小进步，三天有局部成果，才会激励我辈小人物做进一步的钻研。目标越远大越抽象，落实越要小而可行。他几天完成了比较难的初步工作，在此基础上已经开始进一步的挑战，必须得肯定。    =========================================================================    时间:2011-07-23 15:43:17   来源:新浪微博    转发:15   评论:5   正文:最近在做微博相关的研究，安排其中一个同学做电影评论分析，半年叮嘱下来语料库都没有收集齐全，说一直很忙；另外一个同学一声不吭花了三天的时间做了个微博关键词分析，效率很高健壮性也不错。两位我都觉得很聪明，偷懒的那个尤其聪明，结论：现在上过大学的人都很聪明，后续的成就多半靠爱好与勤奋。    =========================================================================    时间:2011-07-21 09:37:35   来源:新浪微博    转发:0   评论:0   正文:广西贡院对联：十年寒窗诵四书言五经习六艺只为龙门一跃， 三考得志官七品威八面竭九尊全因河鲤重生。    =========================================================================    时间:2011-07-18 15:52:42   来源:iPhone客户端    转发:2   评论:9   正文:在北理工听普度大学计算机教授luo si 报告machine learning approach in information retrieval    =========================================================================    时间:2011-07-18 11:31:22   来源:iPhone客户端    转发:2   评论:2   正文:回复 @白硕sse:佛度众生，幻化万象，再好的精神也要以老百姓喜闻乐见的形式才能引人向善，甚至是不惜低俗化，先请君入瓮，再润物无声，最后大师再予以提升。毛**将马列主义在农村具体为打土豪分田地。华罗庚要去工厂推广统筹法? //@白硕sse:这个劝学篇，非常典型地体现了中国人对待知识的实用主义态    =========================================================================    时间:2011-07-18 11:13:13   来源:iPhone客户端    转发:13   评论:6   正文:宋真宗劝学篇：男儿欲遂平生志，五经勤向窗前读。    =========================================================================    时间:2011-07-14 14:17:13   来源:新浪微博    转发:7   评论:9   正文:刚从最高检所属的正义网http://t.cn/hM8is 回来，拜会了技术总监以及舆情研究院执行副院长，一起吃饭交流。总算知道：原来网上曝光和举报的各类信息就是这个Team在收集管理并给政法领导汇报，领导还是很清楚网络的，网络的各类举报不是没人理，不是不报时候未到。但愿嚣张跋扈者都会有报应。    =========================================================================    时间:2011-07-13 16:30:52   来源:新浪微博    转发:13   评论:30   正文:刚从桂林开一个学术会议回来，团队的一个小伙悄悄告诉我，新浪微博的登录、采集与抽取，咱搞定了。亲自测试一番，果然如此，好兴奋，以后几乎没有我们获取不了的网络内容了，facebook,校内，都不在话下，之后的进一步研究就轻松多了     =========================================================================    时间:2011-07-13 11:33:08   来源:iPhone客户端    转发:6   评论:0   正文:想起老家父母似乎一直都在这样激励自己自立、为人、勇敢。感动    =========================================================================    时间:2011-07-13 11:18:42   来源:iPhone客户端    转发:0   评论:1   正文:回复 @kristy珍子:有点闲工夫做点公民教育：Cctv拿的是纳税人的钱，商业广告收入有我的贡献，电视台是公共场合，在自家客厅撒泼光膀子叫爷爷装爹别人管不着，但是公共场所和公共人物就必须有禁忌，不能伤风化，这是公共道德。你很喜欢张涵韵，但是你父母辈的可以不喜欢也可以批评。保留不同观念之自由    =========================================================================    时间:2011-07-12 19:50:01   来源:iPhone客户端    转发:0   评论:0   正文:老外着中国皇帝装致晚宴欢迎辞。    =========================================================================    时间:2011-07-12 19:44:30   来源:iPhone客户端    转发:1   评论:15   正文:哈哈哈，不小心捅了张涵韵粉丝的马蜂窝。看得懂的人都知道我在批评cctv以及我要上春晚，鄙人不是任何人的粉丝，就算是也绝不放弃冷静观察与独立批评公众媒体与公众人物之权力。    =========================================================================    时间:2011-07-12 18:17:44   来源:新浪微博    转发:5   评论:17   正文:[CCTV不是你家客厅]昨晚回宾馆手欠，不小心看到了2011第一期我要上春晚，刘德华韩红闫肃三位评委干爹干哥干妹的乱叫一气，张含韵上台后认刘德华干爹认董卿干妈认韩红干姑认闫肃干爷爷，肿么了？这是CCTV吗，CCTV是你家客厅吗？一地鸡皮疙瘩，低俗无聊之极！    =========================================================================    时间:2011-07-12 15:29:58   来源:iPhone客户端    转发:3   评论:5   正文:香港中文大学黄锦辉教授演讲，查询驱动的自动摘要。    =========================================================================    时间:2011-07-12 09:47:39   来源:iPhone客户端    转发:1   评论:1   正文:桂林象鼻山    =========================================================================    时间:2011-07-11 15:51:30   来源:iPhone客户端    转发:0   评论:3   正文:进入了桂林，参加明天的一个小型国际会议。    =========================================================================    时间:2011-07-10 20:17:47   来源:iPhone客户端    转发:1   评论:3   正文:读南方周末编的＂晚清变局与民国乱象＂，感觉在借古讽今：以富国强兵为目的的行政制度改革，说到底是朝廷的私事。如果政府权力不被限制，即使国富民强了，也只能增加统治者侵犯个人自由的能力，增加统治者骄傲自满的情绪，进一步败坏民族精神。历史反复证明，这样的政治改革也未必能促进国家的强盛    =========================================================================    时间:2011-07-10 10:57:01   来源:新浪微博    转发:4   评论:3   正文:昨天跟一拨朋友在海棠红私人餐厅聚会,结识了@牛魔王的珍满福拉面 ，一个27年的资深IT人，在1年半时间扩张到100多家连锁店，采用IT垂直管理的模式，实现了标准化管理与经营；同时，建立规范的工会凝聚员工之间的向心力，其独到的管理以及对餐饮连锁的细节打造，着实让人佩服。    =========================================================================    时间:2011-07-08 18:48:07   来源:新浪微博    转发:4   评论:2   正文:【郭美美背后金主：历史文化之谜】破解要点：1)爱马仕等名包属于郭MM之母，郭母或许才是真正的卖主；2)为什么深圳王军愿意顶包，翁涛爆料，天略老板送豪车；3）红十字会能捞钱，但是起对网络管控的能量尚不具备？美国《越狱》的编剧告诉我们：屁民看到的不过是皮毛，Company幕后真正boss可以操控国家。    =========================================================================    时间:2011-07-08 18:34:25   来源:新浪微博    转发:2   评论:3   正文:【郭美美背后金主：历史文化之谜】：悲催的郭美美事件，跟踪了10来天，将我的研究方向引入了历史文化之谜，研究内容主要包括：《圣经》中都藏有什么秘密，谁是日本第一代天皇，耶稣是人还是神，拿破仑死亡之谜，希特勒的性别之谜，郭美美背后金主是谁？    =========================================================================    时间:2011-07-05 11:23:01   来源:新浪微博    转发:4   评论:10   正文:ICTCLAS2011切分标注结果：儿子/n 生/v 性病/n 母/ng 倍感/v 安慰/an  悲催的，切分不算错误，但是不合语义逻辑。    =========================================================================    时间:2011-07-03 19:46:19   来源:iPhone客户端    转发:0   评论:1   正文:孩子的即兴作品，太阳下的兔子    =========================================================================    时间:2011-07-02 11:57:41   来源:iPhone客户端    转发:5   评论:3   正文:最牛校长叶志平只能在汶川地震三年得病去世后得到官方的表彰，否则三年前会有太多的校长及背后的长官因为豆腐渣被唾弃。社会表彰活着的顺从者和死去的叛逆者，文人不过是时代的点缀。    =========================================================================    时间:2011-07-02 11:49:58   来源:iPhone客户端    转发:1   评论:0   正文:我和谁都不争，和谁争我都不屑 《生与死》——英国诗人兰德暮年之作 Walter Savage Landor I strove with none; for none was worth my strife;    =========================================================================    时间:2011-06-30 11:26:24   来源:iPhone客户端    转发:0   评论:4   正文:网络搜索挖掘与安全实验室每周四开例会，实验室很快就壮大到了十几个人，最近还要加入新人，可以做很多研究工作了，今天由翟岩龙博士报告云计算报告，以后例会欢迎周边的同志参加。例会通知与报告发布均可以访问www.nlpir.org。    =========================================================================    时间:2011-06-28 16:30:56   来源:新浪微博    转发:0   评论:1   正文:回复@不会缝衣服的厨子:你说的有些道理，从另外一个角度辩证地看，大公司的狼性管理是把公司当成狼，把员工变成绵羊，从而缺乏创新，甚至逼得跳楼；真正的狼性管理是向《亮剑》李云龙一样带队伍，让每个成员都是有血性讲团结有牺牲精神开拓精神的狼，打造一支狼群，一支更有战斗力和创新精神的队伍。    =========================================================================    时间:2011-06-28 11:12:37   来源:新浪微博    转发:4   评论:7   正文:读完《狼图腾》：一部写民族性格的奇书，草原文化与农耕文化的强烈对比，狼生存的草原环境恶劣铸就了几万年以来狼有自己断骨的坚毅、卓越的军事素养，为了自由胜利不惜粉身碎骨的精神；越来越多的国人在人民内部是狼，遇到强敌即揭开画皮变成驯服的羊，国民性中的羊性与奴性值得我们反思。    =========================================================================    时间:2011-06-26 06:23:16   来源:iPhone客户端    转发:0   评论:4   正文:早起赶六点四十五的班车去良乡校区准备2010级的c语言的考试，总共五场，约2000人。起得比狗早，静谧的校园也是一景。","title":"一个通用html抽取类"},{"content":"转自：http://www.cvchina.info/tag/icpr/ 以下是全文 放上来共享，兼备忘。 http://iris.usc.edu/Information/Iris-Conferences.html 下面这张表早晚得过期，还是点击上面的链接跳转查询吧。 表里的P代表paper deadline 分类: 新闻, 资源 标签: accv, bmvc, civr, computer vision, crv, cvpr, ECCV, ICCV, icgip, icip, icisp, icpr, winter vision, 会议, 计算机视觉 偶尔转帖：AI会议的总结（by南大周志华） 2010年2月24日cvchina 没有评论 说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern   Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可   以给到1+, 也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比   IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的   介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字   “Advances in Neural Information Processing Systems”, 所以, 与ICML\\ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML\\ECML\\COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示\\推理\\学习等很多方面, AUAI   (Association of UAI) 主办, 每年开. tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal   Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说).","title":"计算机视觉会议时间表"},{"content":"知识型企业研究中心 2006-12-26 http://business.queensu.ca/index.php Queen商务学校，任务是提高领导力的管理和促进商务和社会的发展。目前我们的研究工作... 英国谢菲尔德大学自然语言处理研究组 2006-12-26 http://nlp.shef.ac.uk/ 英国谢菲尔德大学自然语言处理研究组研究领域主要为：自然语言分析，自然语言的产生以及相关资... PC AI 2006-12-26 http://www.pcai.com/ 在线免费电子期刊，除了包含每期期刊内容外，还包括一个AI讨论组，和经过整理的Intern... 美国印地安那大学人工智能/认知科学报告和再版文件汇编 2006-12-26 http://www.cs.indiana.edu/~leake/INDEX.html 美国印地安那大学人工智能/认知科学报告和再版文件汇编，网站提供了PDF格式的文件，相关书... 美国橡树岭国家实验室图像处理和机器视觉研究小组 2006-12-26 http://www.ornl.gov/sci/ismv/ 美国橡树岭国家实验室图像处理和机器视觉研究小组，图像处理包括：机器视觉，图像管理和检索，... 人工智能研究者俱乐部 2006-12-26 http://www.souwu.com/ 分类论坛:自然语言语音识别论文资源相关编程专家系统知识表示机器学习神经网络数据挖掘模式识... DFKI人工智能研究所 2006-12-17 http://www.dfki.uni-kl.de/ 与人交谈时，对方吐出一串叽哩咕噜的洋话、而你半个字也听不懂，怎么办呢？在过去，这可能需要... 数据管理前言技术国际研讨会（中国，上海，2006） 2006-12-17 http://www.iipl.fudan.edu.cn/DM06/index.htm 2006年该会议的主题是网站管理和挖掘. 它包括6-8个主题,邀请了一系列的研究者和当地... 媒体计算与WEB智能实验室(复旦大学) 2006-12-17 http://www.cs.fudan.edu.cn/mcwil/irnlp/ 媒体计算与WEB智能实验室主要从事多媒体方向（包括文本、图象和视频）的教学和科研工作，研... 奥地利人工智能研究所机器学习和数据挖掘小组 2006-12-11 http://www.oefai.at/oefai/ml/mldm/ 研究区域包括数据挖掘和知识发现，文本挖掘，机器学习，此外还网站提供关于研究领域，相关人物... 加拿大渥太华大学知识获取与智能化学习研究小组 2006-12-09 http://www.site.uottawa.ca/tanka/kaml.html 知识获取与智能化学习研究小组有他们的发展项目：智能化信息的获取项目，文本摘要项目，TAN... 美国麻省理工大学生物与计算学习研究中心 2006-12-09 http://cbcl.mit.edu/ 美国麻省理工大学生物与计算学习研究中心在麻州理工学院成立。主要从数学，工程学和神经学的角... 德国乌尔姆大学人工神经网络小组 2006-12-09 http://www.informatik.uni-ulm.de/ni/forschung/ann.html 德国乌尔姆大学人工神经网络小组的研究重点在于神经网络，数据挖掘，信号处理等领域和方向的研... 优秀知识发现网络 2006-12-09 http://www.kdnet.org/ 优秀知识发现网络是一个开放的网络，它的参加者来自科学，工业和公共部门。这项国际项目的主要... 奥地利维也纳医科大学脑研究中心医学控制和人工智能学院 2006-12-02 http://www.ai.univie.ac.at/ 创建者是地利维也纳医科大学脑研究中心医学控制和人工智能学院，研究包括： 自然语言处理，机... 奥地利维也纳医科大学脑研究中心医学控制和人工智能学院 2006-12-02 http://www.ai.univie.ac.at/ 创建者是地利维也纳医科大学脑研究中心医学控制和人工智能学院，研究包括： 自然语言处理，机... 美国伍斯特工学院人工智能研究小组 2006-12-02 http://www.cs.wpi.edu/Research/airg/ 多主体系统，学习，单功能主体，智能界面，图标界面，专家系统，数据挖掘，知识库的设计等。 微软研究－机器学习和应用统计研究小组 2006-12-02 http://research.microsoft.com/research/mlas/ 机器学习和应用统计研究小组把重心集中在从数据和数据挖掘。藉由软件自动从数据中学习获取新信... 英国爱丁堡大学信息学校人工智能应用学院 2006-12-02 http://www.aiai.ed.ac.uk/ 情境基础的推论: 利用过去的经验和存在的技术指导诊断企业的资源过失；遗传基因的运算法... 北京大学计算语言学研究所 2006-12-02 http://www.icl.pku.edu.cn/ 北京大学计算语言学研究所成立于1986年。研究所的使命是致力于计算语言学理论、语言信息... 哈尔滨工业大学智能技术与自然语言处理实验室 2006-12-02 http://www.insun.hit.edu.cn/default_cn.asp 哈尔滨工业大学计算机学院智能技术与自然语言处理研究室（ITNLP）是国内较早从事自然语言... 加州大学伊荣/尔湾分校机器学习小组 2006-11-29 http://www.ics.uci.edu/~mlearn/Machine-Learning.html 机器学习是一种通过经验获取知识的机制。加州大学伊荣/尔湾分校机器学习小组的研究包括基于统... DMI:数据挖掘学院 2006-11-24 http://www.cs.wisc.edu/dmi/ 数据挖掘研究所于1999年6月1日在微软的数据挖掘小组的帮助下在微软公司的计算机科学系成... 数据挖掘：原理，算法及应用 2006-11-24 http://www.cs.unc.edu/Courses/comp290-90-f04/ 这是北卡罗莱纳洲大学计算机科学系2004年关于数据挖掘的一系列的研讨会的网站。上面列出了... 麻省理工学院开放课程--数据挖掘 2006-11-24 http://www.core.org.cn/OcwWeb/Sloan...5-062Data-Mi... 麻省理工学院的关于数据挖掘开放课程.上面列出了教学大纲、教学日程、讲义、作业、考试以及学... 国家数据挖掘中心 2006-11-24 http://www.ncdm.uic.edu/ 芝加哥的伊利诺伊大学的国家数据挖掘中心于1998年成立,提供资源研究、标准开发和推广高性... IBM智能情报系统研究中心 2006-11-24 http://www.almaden.ibm.com/software/disciplines/iis/ 智能信息系统研究所主要在于设计维护隐私和数据所有权而不是妨碍资讯流通的信息系统.我们的工... 清华大学知识工程研究室 2006-11-24 http://keg.cs.tsinghua.edu.cn/ 清华大学计算机系软件所知识工程研究室以网络计算模式下知识处理为研究方向，以Java、XM... 数据挖掘和数据仓库 2006-11-24 http://www.crm2day.com/data_mining/ 这是一个关于CRM的网站。其中有在数据挖掘这一版块列出了许多著名的公司或者专家写的关于数... 数据挖掘课程 2006-11-24 http://cs.nju.edu.cn/zhouzh/zhouzh.files/course/dm.htm 是南京大学的数据挖掘课程的网页,上面列出了基本的课程介绍,提供课件下载,还列出了其他国家... 数据挖掘的连接 2006-11-24 http://www.galaxy.gmu.edu/stats/syllabi/DMLIST.html 该网页列出了关于数据挖掘的一系列链接 数据挖掘的连接 2006-11-24 http://www.galaxy.gmu.edu/stats/syllabi/DMLIST.html 该网页列出了关于数据挖掘的一系列链接 人工智能研究实验室 2006-11-17 http://www.cs.iastate.edu/~honavar/aigroup.html 人工智能研究实验室是爱荷华州立大学的计算智能、学习和发现中心的一部分.目前的研究包括:人... 美国人工智能协会 2006-11-17 http://www.aaai.org/home.html 成立于1979年的美国人工智能协会(aaai)是一个非营利性的致力于推进科学认识的社会科... 知识媒体学会 2006-11-16 http://kmi.open.ac.uk/index.cfm 研究与电视大学本身相关的区域: 认知的和学问科学,和多媒体。 研究包括下列的主题: 叙述... WEB数据挖掘实验室 2006-11-16 http://www.wdmlab.cn/ 本Web数据挖掘实验室隶属于南京师范大学教育科学学院教育技术学系。实验室立足于我国基础教... Java资源网——Java数据挖掘 2006-11-16 http://www.javaresource.org/data-mi...-mining-73.html Java资源网是由Java领域的爱好者组成的技术联盟,主要成员均来自java和相关领域的... 中国科大博纳数据挖掘中心 2006-11-16 http://bona.ustc.edu.cn/ 中国科大博纳数据挖掘中心（Bona Institute of Business Data... 西南财经大学商务数据挖掘中心 2006-11-16 http://riem.swufe.edu.cn/dataminingcenter/ 西南财经大学商务数据挖掘中心是一个应用研究机构，它和从事商务决策和数据挖掘的软件公司、... 国际数据挖掘技术研究中心 2006-11-16 http://59.77.6.145/dmlab/DesktopDefault.aspx 数据挖掘技术及其应用实验室是厦门大学国家示范性软件学院软件研究与开发中心的一个重要的分... 互联网数据挖掘服务中心 2006-11-16 http://idm.yatio.com/index.html 互联网数据挖掘服务中心（IDMSC）是以雅信核心搜索技术为依托，面向所有网络分众领域，为... 中科院数据技术与知识经济研究中心 2006-11-16 http://www.dtke.ac.cn/ 中国科学院数据技术与知识经济研究中心（CAS Research Center on Da... 机器学习研究室 2006-11-15 http://www.cald.cs.cmu.edu/ 这个机器学习研究室是卡内基梅隆大学计算机科学系的一个学术部门.我们集中有关于统计机器学习... 数据挖掘工程小组 2006-11-15 http://www.chem-eng.utoronto.ca/~datamining/ 数据挖掘工程组是基身于多伦多大学的化工和应用化学系.其目标是把背景不同的在各个领域研究数... 查尔斯顿学院的信息发现 2006-11-15 http://di.cofc.edu/ 信息发现是从现有的资料中,无论是以前贮存的、还是流经过沟通渠道的，去发现新的信息.如何运... 2006年数据挖掘论坛 2006-11-14 http://www.data-mining-forum.de/ 这次会议是每年召开的一系列的基于数据挖掘的工业会议的第六次会议,该会议每年都在国际活动方... 数据挖掘 2006-11-14 http://www.ccsu.edu/datamining 这是ccsu的一个在线数据挖掘的项目,ccsu是唯一开办了在线数据挖掘科学硕士的学校.这... 第四届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://www.sewm2006.sdu.edu.cn/ 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 第三届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://www.sewm2005.edu.cn/index.htm 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 第二届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://www.scut.edu.cn/sewm2004/index.htm 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 首届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13 http://net.pku.edu.cn/~sewm/sewm2003.htm 全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相... 数据挖掘技能 2006-11-10 http://www.statsoft.com/textbook/stdatmin.html 这是一本关于数据挖掘的一本书的章节 数据挖掘课堂笔记 2006-11-10 http://infolab.stanford.edu/~ullman/mining/mining.html 国外大学关于数据挖掘相关课程的课件。 智能科学网站 2006-11-10 http://www.intsci.ac.cn/ 这是一个关于智能科学的门户网站，主要介绍的有关智能科学的内容由智能系统、智能科学研究、智... 数据挖掘词汇表 2006-11-10 http://www.twocrows.com/glossary.htm 数据挖掘的词汇表 智能工具,数据挖掘,可视化2005国际会议 2006-11-09 http://www.infonortics.com/idv/05pro.html 2005年6月27-28号在美国费城召开的智能工具、数据挖掘和可视化国际会议。网站上... SIGIR2006会议网站 2006-11-07 http://www.sigir2006.org/ 关于信息检索的会议网站，本年度的主题是用户交互与检索效率。该网站提供年度会议的论文目录，... 数字经济研究中心 2006-11-07 http://w4.stern.nyu.edu/ceder/ 网站简介：25年多来，纽约大学的Stern's数字经济研究中心已经利用信息技术站在了商业... 原文挖掘和基于网页的信息检索参考书目 2006-11-07 http://filebox.vt.edu/users/wfan/text_mining.html 该网页提供了许多关于原文挖掘研评价和分析的连接。 数据挖掘爱好者 2006-11-04 http://datamining.diy.myrice.com/ 数据挖掘就是从海量的数据中找出潜在的有价值的信息。这是一门综合了统计学、数据库和人工智能... 数据挖掘资源 2006-11-04 http://www.opendata365.com/datamini...200506/235.html 该网页提供了许多有关数据挖掘方面的链接，资源丰富。 第七次国际数据仓库存储与知识发现会议 2006-11-04 http://www.dwway.com/newcontent.php...5userid=corpid= 主要介绍了会议的时间、地点、宗旨以及讨论的主要内容。 数据挖掘：文本挖掘，数据挖掘和社会传媒 2006-11-04 http://datamining.typepad.com/data_mining/ 这是一个私人博客，记录了作者研究方向的一些资料、信息。而作者主要的兴趣所在为：人工智能、... 与统计相关的数据挖掘课件 2006-11-04 http://www.autonlab.org/tutorials/ 这个网站提供了基于统计的数据挖掘各个方面的研究类的课件，包括概率论的基础、数据统计分析的... 诊断试验评价与数据挖掘 2006-11-04 http://statdtedm.6to23.com/ 该网站是个科研个人网（非商业盈利），目的是相互交流,共同提高；网站开辟的几个专题，如数据... 统计分析与数据挖掘实验室 2006-11-04 http://www.bistudy.com/ 该网站主要提供一些相关软件介绍及其下载，包括： 调查类软件 、 统计分析类软件 、... 数据挖掘技术简介 2006-11-04 http://www.itcomputer.com.cn/Databa...0601/78529.html 数据挖掘是目前一种新的重要的研究领域。本文介绍了数据挖掘的概念、目的、常用方法、数据挖掘... 数据挖掘技术简介（PPT） 2006-11-04 http://eb.zzei.net/ebSimple/dss.ppt PPT课件 数据挖掘教程 2006-11-04 http://www.sobooks.com/product_info...oducts_id/14953 本书为数据挖掘的基础教程，是作者多年来从事数据挖掘和专家系统课程教学经验的总结。它从商业... 数据挖掘 2006-11-04 http://www.the-data-mine.com/ 这个网站是1994年4月建立的,主要是提供关于数据挖掘的信息,包括数据库中的数据挖掘和简... 数据挖掘:实用机器学习工具和技术(第二版) 2006-11-04 http://www.cs.waikato.ac.nz/~ml/weka/book.html 一本关于数据挖掘的书籍的介绍 数据挖掘讨论组 2006-11-04 http://www.dmgroup.org.cn/ 数据挖掘讨论组网站建于2000年7月，是由复旦大学计算机系发起创建的。 该网站... 数据挖掘研究院 2006-11-04 http://www.dmresearch.net/ 数据挖掘研究院是由HAMMER_SHI于2004年4.17日搭建成立的数据挖掘研讨平台，... Lotus知识发现服务器 2006-11-04 http://www.chinakm.com/share/list.asp?id=2579 主要介绍了Lotus知识发现服务器及其功能和作用。 知识发现新进展与成果概述 2006-11-04 http://202.113.96.26/tjcbe/xueshubaogao/yangbingru.ppt 主要介绍了知识发现的内涵与外延的扩展、挖掘知识类型扩展、方法技术扩展、应用及发展趋势以及... 第四届知识发现与数据挖掘国际学术大会 2006-11-04 http://www2.ccw.com.cn/1998/37/170858.shtml 主要介绍了这次会议的8个专题介绍会，以及本届大会的几个特点。 数据挖掘研究院 网摘 2006-11-04 http://www.dmresearch.net/rss/ 关于一个动态搜集的有关数据挖掘资料的网页。 数据挖掘 2006-11-03 http://databases.about.com/od/datamining/ about.com展示了原有的专题文章的收集、网络连接,以及专门讨论数据挖掘和数据仓库课... UCI数据库知识发现 2006-11-02 http://kdd.ics.uci.edu/ 在线的大型数据库，包含多种类型的数据,分析任务、适用范围.本库的主要作用是作为基准测试,... 关于应用解析的新闻以及商业资源 2006-11-02 http://www.secondmoment.org/ 关于应用解析的新闻以及商业资源.强大日志内容混合了评论、技术、以及对知识发现和直接的知识... 数据挖掘与知识发现软件 2006-11-02 http://www.kdnuggets.com/software/index.html 这是一个通用数据挖掘软件的目录","title":"数据挖掘相关网摘、博客"},{"content":"最近把一些在网上见到的自然语言处理的资源整理了一下，包括论文列表、软件资源和一些实验室主页、个人主页等，希望能对NLP研究者有所帮助，由于个人视野有限，目前只整理了这些，以后会持续更新。在此也感谢这些资源的提供者和维护者。 转载请标明出处（http://blog.csdn.net/xuh5156/article/details/7437475） 论文、博客 1.       Google在研究博客中总结了他们2011年的精彩论文《Excellent Papers for 2011》，包括社会网络、机器学习、人机交互、信息检索、自然语言处理、多媒体、系统等各个领域，很精彩的论文集锦。http://googleresearch.blogspot.com/2012/03/excellent-papers-for-2011.html 或者zibuyu的BLOG http://blog.sina.com.cn/s/blog_574a437f0100y6zy.html 2.       Best paper awards for AAAI,ACL, CHI, CIKM, FOCS, ICML, IJCAI, KDD, OSDI, SIGIR, SIGMOD, SOSP, STOC, UIST,VLDB, WWWhttp://jeffhuang.com/best_paper_awards.html 3.       IBM R&D Journal 刚发布了关于Watson的专刊《This is Watson》。总共17篇论文。http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6177717&punumber=5288520 4.       Web Data Mining作者刘兵维护的一个专题资源：Opinion Mining,Sentiment Analysis, and Opinion Spam Detection 。http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 5.       Statistical Machine Translationhttp://www.statmt.org/ Statistical Machine TranslationTutorial Readinghttp://cseweb.ucsd.edu/~dkauchak/mt-tutorial/ Philipp Koehn主页http://homepages.inf.ed.ac.uk/pkoehn/ 6.       Profile Hidden Markov ModelResourceshttp://webdocs.cs.ualberta.ca/~colinc/cmput606/ Hidden Markov Model (HMM) Toolbox forMatlabhttp://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html 7.       CRF http://www.inference.phy.cam.ac.uk/hmw26/crf/ Conditional Random Field (CRF)Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/CRF/crf.html FlexCRFs: Flexible Conditional RandomFieldshttp://flexcrfs.sourceforge.net/ 8.       Transfer Learning 包含papers、talks、software等http://www.cse.ust.hk/TL/index.html 9.       Topic Model，Topic Modeling Bibliographyhttp://www.cs.princeton.edu/~mimno/topics.html David M. Blei的主页http://www.cs.princeton.edu/~blei/publications.htmlMatlab Topic Modeling Toolbox 1.4http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm LDA GIBBS Java源码http://arbylon.net/resources.html GibbsLDA++: A C/C++ Implementation ofLatent Dirichlet Allocationhttp://gibbslda.sourceforge.net/ 10.   科学网—推荐系统的循序进阶读物（从入门到精通） - 张子柯的博文http://blog.sciencenet.cn/home.php?mod=space&uid=210641&do=blog&id=508634 11.   SVM入门http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html 12.   斯坦福大学自然语言处理实验室整理的NLP资源http://www-nlp.stanford.edu/links/statnlp.html 13.   Stanford University InformationRetrieval Resourceshttp://nlp.stanford.edu/IR-book/information-retrieval.html 14.   Software Tools for NLP http://www-a2k.is.tokushima-u.ac.jp/member/kita/NLP/nlp_tools.html 实验室主页 1.       The Stanford NLP Group http://nlp.stanford.edu 2.       The Berkeley Natural LanguageProcessing Grouphttp://nlp.cs.berkeley.edu 3.       University of Tokyo TsujiiLaboratory http://www.nactem.ac.uk/tsujii/publications.cgi?lang=en 4.       Korea University NLP http://nlp.korea.ac.kr/ http://nlp.korea.ac.kr/new/ 5.       中国科学院计算技术研究所自然语言处理研究组http://nlp.ict.ac.cn/new/ 6.       清华大学自然语言处理组 http://nlp.csai.tsinghua.edu.cn/site2/ 7.       HIT-SCIR http://ir.hit.edu.cn/ 8.       苏州大学自然语言处理实验室http://nlp.suda.edu.cn/ 个人主页 1.       David M. Blei， (Princeton) LDA，http://www.cs.princeton.edu/~blei/publications.html 2.       Noah Smith, (CMU),以自然语言处理、机器学习为基础研究computationalsocial science。http://www.cs.cmu.edu/~nasmith/ 3.       Philipp Koehn (University ofEdinburgh)http://homepages.inf.ed.ac.uk/pkoehn/ 4.       Dekang Lin (University ofAlberta)http://webdocs.cs.ualberta.ca/~lindek/ 5.       Michael Collins(ColumbiaUniversity)http://www.cs.columbia.edu/~mcollins/ 6.       Dekai WU(HKUST) http://www.cs.ust.hk/~dekai/ 7.       Pascale Fung (HKUST) http://www.ee.ust.hk/~pascale/ 8.       Alessandro Moschitti (Universityof Trento)http://disi.unitn.it/moschitti/ 9.       Xiaojin (Jerry) Zhu (Universityof Wisconsin-Madison)http://pages.cs.wisc.edu/~jerryzhu/ 10.   Eugene Charniak (BrownUniversity)http://www.cs.brown.edu/~ec/","title":"自然语言处理（NLP）网上资源整理"},{"content":"2007年1月1日 下午 03:10:00 发表者：Google 研究员，吴军  我在大学学习线性代数时，实在想不出它除了告诉我们如何解线性方程外，还能有什么别的用途。关于矩阵的许多概念，比如特征值等等，更是脱离日常生活。后来在数值分析中又学了很多矩阵的近似算法，还是看不到可以应用的地方。当时选这些课，完全是为了混学分的学位。我想，很多同学都多多少少有过类似的经历。直到后来长期做自然语言处理的研究，我才发现数学家们提出那些矩阵的概念和算法，是有实际应用的意义的。 在自然语言处理中，最常见的两类的分类问题分别是，将文本按主题归类（比如将所有介绍亚运会的新闻归到体育类）和将词汇表中的字词按意思归类（比如将各种体育运动的名称个归成一类）。这两种分类问题都可用通过矩阵运算来圆满地、同时解决。为了说明如何用矩阵这个工具类解决这两个问题的，让我们先来来回顾一下我们在余弦定理和新闻分类中介绍的方法。 分类的关键是计算相关性。我们首先对两个文本计算出它们的内容词，或者说实词的向量，然后求这两个向量的夹角。当这两个向量夹角为零时，新闻就相关；当它们垂直或者说正交时，新闻则无关。当然，夹角的余弦等同于向量的内积。从理论上讲，这种算法非常好。但是计算时间特别长。通常，我们要处理的文章的数量都很大，至少在百万篇以上，二次回标有非常长，比如说有五十万个词（包括人名地名产品名称等等）。如果想通过对一百万篇文章两篇两篇地成对比较，来找出所有共同主题的文章，就要比较五千亿对文章。现在的计算机一秒钟最多可以比较一千对文章，完成这一百万篇文章相关性比较就需要十五年时间。注意，要真正完成文章的分类还要反复重复上述计算。 在文本分类中，另一种办法是利用矩阵运算中的奇异值分解（Singular Value Decomposition，简称 SVD)。现在让我们来看看奇异值分解是怎么回事。首先，我们可以用一个大矩阵A来描述这一百万篇文章和五十万词的关联性。这个矩阵中，每一行对应一篇文章，每一列对应一个词。 在上面的图中，M=1,000,000，N=500,000。第 i 行，第 j 列的元素，是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF)。读者可能已经注意到了，这个矩阵非常大，有一百万乘以五十万，即五千亿个元素。 奇异值分解就是把上面这样一个大矩阵，分解成三个小矩阵相乘，如下图所示。比如把上面的例子中的矩阵分解成一个一百万乘以一百的矩阵X，一个一百乘以一百的矩阵B，和一个一百乘以五十万的矩阵Y。这三个矩阵的元素总数加起来也不过1.5亿，仅仅是原来的三千分之一。相应的存储量和计算量都会小三个数量级以上。 三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。 现在剩下的唯一问题，就是如何用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值等等，以及数值分析的各种算法就统统用上了。在很长时间内，奇异值分解都无法并行处理。（虽然 Google 早就有了MapReduce 等并行计算的工具，但是由于奇异值分解很难拆成不相关子运算，即使在 Google 内部以前也无法利用并行计算的优势来分解矩阵。）最近，Google 中国的张智威博士和几个中国的工程师及实习生已经实现了奇异值分解的并行算法，我认为这是 Google 中国对世界的一个贡献。 from:http://www.google.com.hk/ggblog/googlechinablog/2006/12/blog-post_8935.html","title":"数学之美 系列十八 － 矩阵运算和文本处理中的分类问题"},{"content":"2006年8月23日 下午 11:22:00 发表者：吴军，Google 研究员  我在数学之美系列中一直强调的一个好方法就是简单。但是，事实上，自然语言处理中也有一些特例，比如有些学者将一个问题研究到极致，执著追求完善甚至可以说完美的程度。他们的工作对同行有很大的参考价值，因此我们在科研中很需要这样的学者。在自然语言处理方面新一代的顶级人物麦克尔 · 柯林斯 (Michael Collins) 就是这样的人。 柯林斯：追求完美  柯林斯从师于自然语言处理大师马库斯 (Mitch Marcus)（我们以后还会多次提到马库斯），从宾夕法利亚大学获得博士学位，现任麻省理工学院 (MIT) 副教授（别看他是副教授，他的水平在当今自然语言处理领域是数一数二的），在作博士期间，柯林斯写了一个后来以他名字命名的自然语言文法分析器 (sentence parser)，可以将书面语的每一句话准确地进行文法分析。文法分析是很多自然语言应用的基础。虽然柯林斯的师兄布莱尔 (Eric Brill) 和 Ratnaparkhi 以及师弟 Eisnar 都完成了相当不错的语言文法分析器，但是柯林斯却将它做到了极致，使它在相当长一段时间内成为世界上最好的文法分析器。柯林斯成功的关键在于将文法分析的每一个细节都研究得很仔细。柯林斯用的数学模型也很漂亮，整个工作可以用完美来形容。我曾因为研究的需要，找柯林斯要过他文法分析器的源程序，他很爽快地给了我。我试图将他的程序修改一下来满足我特定应用的要求，但后来发现，他的程序细节太多以至于很难进一步优化。柯林斯的博士论文堪称是自然语言处理领域的范文。它像一本优秀的小说，把所有事情的来龙去脉介绍的清清楚楚，对于任何有一点计算机和自然语言处理知识的人，都可以轻而易举地读懂他复杂的方法。 柯林斯毕业后，在 AT&T 实验室度过了三年快乐的时光。在那里柯林斯完成了许多世界一流的研究工作诸如隐含马尔科夫模型的区别性训练方法，卷积核在自然语言处理中的应用等等。三年后，AT&T 停止了自然语言处理方面的研究，柯林斯幸运地在 MIT 找到了教职。在 MIT 的短短几年间，柯林斯多次在国际会议上获得最佳论文奖。相比其他同行，这种成就是独一无二的。柯林斯的特点就是把事情做到极致。如果说有人喜欢“繁琐哲学”，柯林斯就是一个。 布莱尔：简单才美  在研究方法上，站在柯林斯对立面的典型是他的师兄艾里克 · 布莱尔 (Eric Brill) 和雅让斯基，后者我们已经介绍过了，这里就不再重复。与柯林斯从工业界到学术界相反，布莱尔职业路径是从学术界走到工业界。与柯里斯的研究方法相反，布莱尔总是试图寻找简单得不能再简单的方法。布莱尔的成名作是基于变换规则的机器学习方法 (transformation rule based machine learning)。这个方法名称虽然很复杂，其实非常简单。我们以拼音转换字为例来说明它： 第一步，我们把每个拼音对应的汉字中最常见的找出来作为第一遍变换的结果，当然结果有不少错误。比如，“常识”可能被转换成“长识”； 第二步，可以说是“去伪存真”，我们用计算机根据上下文，列举所有的同音字替换的规则，比如，如果 chang 被标识成“长”，但是后面的汉字是“识”，则将“长”改成“常”； 第三步，应该就是“去粗取精”，将所有的规则用到事先标识好的语料中，挑出有用的，删掉无用的。然后重复二三步，直到找不到有用的为止。 布莱尔就靠这么简单的方法，在很多自然语言研究领域，得到了几乎最好的结果。由于他的方法再简单不过了，许许多多的人都跟着学。布莱尔可以算是我在美国的第一个业师，我们俩就用这么简单的方法作词性标注 (part of speech tagging)，也就是把句子中的词标成名词动词，很多年内无人能超越。（最后超越我们的是后来加入 Google 的一名荷兰工程师，用的是同样的方法，但是做得细致很多）布莱尔离开学术界后去了微软研究院。在那里的第一年，他一人一年完成的工作比组里其他所有人许多年做的工作的总和还多。后来，布莱尔又加入了一个新的组，依然是高产科学家。据说，他的工作真正被微软重视要感谢 Google，因为有了 Google，微软才对他从人力物力上给于了巨大的支持，使得布莱尔成为微软搜索研究的领军人物之一。在研究方面，布莱尔有时不一定能马上找到应该怎么做，但是能马上否定掉一种不可能的方案。这和他追求简单的研究方法有关，他能在短时间内大致摸清每种方法的好坏。 由于布莱尔总是找简单有效的方法，而又从不隐瞒自己的方法，所以他总是很容易被包括作者我自己在内的很多人赶上和超过。好在布莱尔很喜欢别人追赶他，因为，当人们在一个研究方向超过他时，他已经调转船头驶向它方了。一次，艾里克对我说，有一件事我永远追不上他，那就是他比我先有了第二个孩子 ：） 在接下来了系列里，我们还会介绍一个繁与简结合的例子。 from:http://www.google.com.hk/ggblog/googlechinablog/2006/08/blog-post_6232.html","title":"数学之美 系列十五 繁与简 自然语言处理的几位精英"},{"content":"2006年5月25日 上午 07:56:00 发表者：吴军, Google 研究员  我们已经介绍了信息熵，它是信息论的基础，我们这次谈谈信息论在自然语言处理中的应用。 先看看信息熵和语言模型的关系。我们在系列一中谈到语言模型时，没有讲如何定量地衡量一个语言模型的好坏，当然，读者会很自然地想到，既然语言模型能减少语音识别和机器翻译的错误，那么就拿一个语音识别系统或者机器翻译软件来试试，好的语言模型必然导致错误率较低。这种想法是对的，而且今天的语音识别和机器翻译也是这么做的。但这种测试方法对于研发语言模型的人来讲，既不直接、又不方便，而且很难从错误率反过来定量度量语言模型。事实上，在贾里尼克(Fred Jelinek)的人研究语言模型时，世界上既没有像样的语音识别系统，更没有机器翻译。我们知道，语言模型是为了用上下文预测当前的文字，模型越好，预测得越准，那么当前文字的不确定性就越小。 信息熵正是对不确定性的衡量，因此信息熵可以直接用于衡量统计语言模型的好坏。贾里尼克从信息熵出发，定义了一个称为语言模型复杂度(Perplexity)的概念，直接衡量语言模型的好坏。一个模型的复杂度越小，模型越好。李开复博士在介绍他发明的 Sphinx 语音识别系统时谈到，如果不用任何语言模型（即零元语言模型）时，复杂度为997，也就是说句子中每个位置有 997 个可能的单词可以填入。如果（二元）语言模型只考虑前后词的搭配不考虑搭配的概率时，复杂度为 60。虽然它比不用语言模型好很多，但是和考虑了搭配概率的二元语言模型相比要差很多，因为后者的复杂度只有 20。 信息论中仅次于熵的另外两个重要的概念是“互信息”（Mutual Information) 和“相对熵”（Kullback-Leibler Divergence)。 “互信息”是信息熵的引申概念，它是对两个随机事件相关性的度量。比如说今天随机事件北京下雨和随机变量空气湿度的相关性就很大，但是和姚明所在的休斯敦火箭队是否能赢公牛队几乎无关。互信息就是用来量化度量这种相关性的。在自然语言处理中，经常要度量一些语言现象的相关性。比如在机器翻译中，最难的问题是词义的二义性（歧义性）问题。比如 Bush 一词可以是美国总统的名字，也可以是灌木丛。（有一个笑话，美国上届总统候选人凯里 Kerry 的名字被一些机器翻译系统翻译成了\"爱尔兰的小母牛\"，Kerry 在英语中另外一个意思。）那么如何正确地翻译这个词呢？人们很容易想到要用语法、要分析语句等等。其实，至今为止，没有一种语法能很好解决这个问题，真正实用的方法是使用互信息。具体的解决办法大致如下：首先从大量文本中找出和总统布什一起出现的互信息最大的一些词，比如总统、美国、国会、华盛顿等等，当然，再用同样的方法找出和灌木丛一起出现的互信息最大的词，比如土壤、植物、野生等等。有了这两组词，在翻译 Bush 时，看看上下文中哪类相关的词多就可以了。这种方法最初是由吉尔(Gale)，丘奇(Church)和雅让斯基(Yarowsky)提出的。 当时雅让斯基在宾西法尼亚大学是自然语言处理大师马库斯 (Mitch Marcus) 教授的博士生，他很多时间泡在贝尔实验室丘奇等人的研究室里。也许是急于毕业，他在吉尔等人的帮助下想出了一个最快也是最好地解决翻译中的二义性，就是上述的方法，这个看上去简单的方法效果好得让同行们大吃一惊。雅让斯基因而只花了三年就从马库斯那里拿到了博士，而他的师兄弟们平均要花六年时间。 信息论中另外一个重要的概念是“相对熵”，在有些文献中它被称为成“交叉熵”。在英语中是 Kullback-Leibler Divergence，是以它的两个提出者库尔贝克和莱伯勒的名字命名的。相对熵用来衡量两个正函数是否相似，对于两个完全相同的函数，它们的相对熵等于零。在自然语言处理中可以用相对熵来衡量两个常用词（在语法上和语义上）是否同义，或者两篇文章的内容是否相近等等。利用相对熵，我们可以到处信息检索中最重要的一个概念：词频率-逆向文档频率（TF/IDF)。我们下回会介绍如何根据相关性对搜索出的网页进行排序，就要用的餐TF/IDF 的概念。另外，在新闻的分类中也要用到相对熵和 TF/IDF。 对信息论有兴趣又有一定数学基础的读者，可以阅读斯坦福大学托马斯.科弗 (Thomas Cover) 教授的专著 \"信息论基础\"(Elements of Information Theory)： http://www.amazon.com/gp/product/0471062596/ref=nosim/103-7880775-7782209?n=283155 http://www.cnforyou.com/query/bookdetail1.asp?viBookCode=17909 科弗教授是当今最权威的信息论专家。","title":"数学之美 系列七 -- 信息论在信息处理中的应用"},{"content":"2006年7月5日 上午 09:09:00 发表者：吴军，Google 研究员  地址的识别和分析是本地搜索必不可少的技术，尽管有许多识别和分析地址的方法，最有效的是有限状态机。 一个有限状态机是一个特殊的有向图（参见有关图论的系列），它包括一些状态（节点）和连接这些状态的有向弧。下图是一个识别中国地址的有限状态机的简单的例子。 每一个有限状态机都有一个启始状态和一个终止状态和若干中间状态。每一条弧上带有从一个状态进入下一个状态的条件。比如，在上图中，当前的状态是“省”，如果遇到一个词组和（区）县名有关，我们就进入状态“区县”；如果遇到的下一个词组和城市有关，那么我们就进入“市”的状态，如此等等。如果一条地址能从状态机的起始状态经过状态机的若干中间状态，走到终止状态，那么这条地址则有效，否则无效。比如说，“北京市双清路83号”对于上面的有限状态来讲有效，而“上海市辽宁省马家庄”则无效（因为无法从市走回到省）。 使用有限状态机识别地址，关键要解决两个问题，即通过一些有效的地址建立状态机，以及给定一个有限状态机后，地址字串的匹配算法。好在这两个问题都有现成的算法。有了关于地址的有限状态机后，我们就可又用它分析网页，找出网页中的地址部分，建立本地搜索的数据库。同样，我们也可以对用户输入的查询进行分析，挑出其中描述地址的部分，当然，剩下的关键词就是用户要找的内容。比如，对于用户输入的“北京市双清路附近的酒家”，Google 本地会自动识别出地址“北京市双清路”和要找的对象“酒家”。 上述基于有限状态机的地址识别方法在实用中会有一些问题：当用户输入的地址不太标准或者有错别字时，有限状态机会束手无策，因为它只能进行严格匹配。（其实，有限状态机在计算机科学中早期的成功应用是在程序语言编译器的设计中。一个能运行的程序在语法上必须是没有错的，所以不需要模糊匹配。而自然语言则很随意，无法用简单的语法描述。） 为了解决这个问题，我们希望有一个能进行模糊匹配、并给出一个字串为正确地址的可能性。为了实现这一目的，科学家们提出了基于概率的有限状态机。这种基于概率的有限状态机和离散的马尔可夫链（详见前面关于马尔可夫模型的系列）基本上等效。 在八十年代以前，尽管有不少人使用基于概率的有限状态机，但都是为自己的应用设计专用的有限状态机的程序。九十年代以后，随着有限状态机在自然语言处理的广泛应用，不少科学家致力于编写通用的有限状态机程序库。其中，最成功的是前 AT&T 实验室的三位科学家，莫瑞（Mohri）, 皮瑞尔（Pereira） 和瑞利（Riley）。他们三人花了很多年时间，编写成一个通用的基于概率的有限状态机 C 语言工具库。由于 AT&T 有对学术界免费提供各种编程工具的好传统，他们三人也把自己多年的心血拿出来和同行们共享。可惜好景不长，AT&T 实验室风光不再，这三个人都离开了 AT&T，莫瑞成了纽约大学的教授，皮瑞尔当了宾西法尼亚大学计算机系系主任，而瑞利成了 Google 的研究员，AT&T 实验室的新东家不再免费提供有限状态机 C 语言工具库。虽然此前莫瑞等人公布了他们的详细算法，但是省略了实现的细节。因此在学术界，不少科学家能够重写同样功能的工具库，但是很难达到 AT&T 工具库的效率（即运算速度），这的确是一件令人遗憾的事。 from:http://www.google.com.hk/ggblog/googlechinablog/2006/07/blog-post_4950.html","title":"数学之美 系列十 有限状态机和地址识别"},{"content":"2006年6月8日 上午 09:15:00 发表者：Google 研究员，吴军  读者也许注意到了，我们在前面的系列中多次提到了贾里尼克这个名字。事实上，现代语音识别和自然语言处理确实是和它的名字是紧密联系在一起的。我想在这回的系列里，介绍贾里尼克本人。在这里我不想列举他的贡献，而想讲一讲他作为一个普普通通的人的故事。这些事要么是我亲身经历的，要么是他亲口对我讲的。 弗莱德里克.贾里尼克(Fred Jelinek)出生于捷克一个富有的犹太家庭。他的父母原本打算送他去英国的公学（私立学校）读书。为了教他德语，还专门请的一位德国的家庭女教师，但是第二次世界大战完全打碎了他们的梦想。他们先是被从家中赶了出去，流浪到布拉格。他的父亲死在了集中营，弗莱德自己成天在街上玩耍，完全荒废了学业。二战后，当他再度回到学校时，他的成绩一塌糊涂， 全部是 D，但是很快他就赶上了班上的同学。不过，他在小学时从来没有得过 A。1949年，他的母亲带领全家移民美国。在美国，贾里尼克一家生活非常贫困，全家基本是靠母亲做点心卖钱为生，弗莱德自己十四五岁就进工厂打工补助全家。  贾里尼克最初想成为一个律师，为他父亲那样的冤屈者辩护，但他很快意识到他那浓厚的外国口音将使他在法庭上的辩护很吃力。贾里尼克的第二个理想是成为医生，他想进哈佛大学医学院，但经济上他无法承担医学院 8 年高昂的学费。与此同时麻省理工学院给于了他一份（为东欧移民设的）全额奖学金。贾里尼克决定到麻省理工学电机工程。在那里，他遇到了信息论的鼻祖香农博士，和语言学大师贾格布森Roman Jakobson (他提出了著名的通信六功能）[注释一]，后来贾里尼克又陪着太太听最伟大的语言学家乔姆斯基(Noam Chomsky)的课。这三位大师对贾里尼克今后的研究方向--利用信息论解决语言问题产生的重要影响。 贾里尼克从麻省理工获得博士学位后，在哈佛大学教了一年书，然后到康乃尔大学任教。他之所以选择康乃尔大学，是因为找工作时和那里的一位语言学家谈得颇为投机。当时那位教授表示愿意和贾里尼克在利用信息论解决语言问题上合作。但是，等贾里尼克到康乃尔以后，那位教授表示对语言学在没有兴趣而转向写歌剧了。贾里尼克对语言学家的坏印象从此开始。加上后来他在 IBM 时发现语言学家们嘴上头头是道，干起活来高不成低不就，对语言学家从此深恶痛绝。他甚至说：\"我每开除一名语言学家，我的语音识别系统错误率就降低一个百分点。\" 这句话后来在业界广为流传，为每一个搞语音识别和语言处理的人所熟知。 贾里尼克在康乃尔十年磨一剑，潜心研究信息论，终于悟出了自然语言处理的真谛。１９７２年，贾里尼克到ＩＢＭ 华生实验室（ＩＢＭ　Ｔ．Ｇ．Ｗａｔｓｏｎ　Ｌａｂｓ）做学术休假，无意中领导了语音识别实验室，两年后他在康乃尔和ＩＢＭ 之间选择了留在ＩＢＭ。在那里，贾里尼克组建了阵容空前绝后强大的研究队伍，其中包括他的著名搭档波尔（Bahl），著名的语音识别 Dragon 公司的创始人贝克夫妇，解决最大熵迭代算法的达拉皮垂(Della Pietra)孪生兄弟，BCJR 算法的另外两个共同提出者库克(Cocke)和拉维夫(Raviv)，以及第一个提出机器翻译统计模型的布朗。  七十年代的 IBM 有点像九十年代的微软和今天的 Google, 给于杰出科学家作任何有兴趣研究的自由。在那种宽松的环境里，贾里尼克等人提出了统计语音识别的框架结构。 在贾里尼克以前，科学家们把语音识别问题当作人工智能问题和模式匹配问题。而贾里尼克把它当成通信问题，并用两个隐含马尔可夫模型（声学模型和语言模型）把语音识别概括得清清楚楚。这个框架结构对至今的语音和语言处理有着深远的影响，它从根本上使得语音识别有实用的可能。 贾里尼克本人后来也因此当选美国工程院院士。 贾里尼克和波尔，库克以及拉维夫对人类的另一大贡献是 BCJR 算法，这是今天数字通信中应用的最广的两个算法之一（另一个是维特比算法）。有趣的是，这个算法发明了二十年后，才得以广泛应用。IBM 于是把它列为了 IBM 有史以来对人类最大贡献之一，并贴在加州 Amaden 实现室墙上。遗憾的是 BCJR 四个人已经全部离开 IBM，有一次IBM 的通信部门需要用这个算法，还得从斯坦福大学请一位专家去讲解，这位专家看到 IBM 橱窗里的成就榜，感慨万分。  贾里尼克和 IBM 一批最杰出的科学家在九十年代初离开了 IBM，他们大多数在华尔街取得了巨大的成功。贾里尼克的书生气很浓，于是去约翰霍普金斯大学建立了世界著名的 CLSP 实验室。每年夏天，贾里尼克邀请世界上 20-30 名顶级的科学家和学生到 CLSP 一起工作，使得 CLSP 成为世界上语音和语言处理的中心之一。  贾里尼克治学极为严谨，对学生要求也极严。他淘汰学生的比例极高，即使留下来的，毕业时间也极长。但是，另一方面，贾里尼克也千方百计利用自己的影响力为学生的学习和事业创造方便。贾里尼克为组里的每一位学生提供从进组第一天到离开组最后一天全部的学费和生活费。他还为每一位学生联系实习机会，并保证每位学生在博士生阶段至少在大公司实习一次。从他那里拿到博士学位的学生，全部任职于著名实验室，比如IBM, 微软，AT&T 和 Google 的实验室。为了提高外国人的英语水平，贾里尼克用自己的经费为他们请私人英语教师。  贾里尼克生活俭朴，一辆老式丰田车开了二十多年，比组里学生的车都破。他每年都邀请组里的学生和教授到家里做客，很多毕业了的学生也专程赶来聚会。在那里，他不再谈论学术问题，而会谈些巩俐的电影（他太太是哥伦比亚大学电影专业的教授），或是某著名教授被拉斯韦加斯的赌馆定为不受欢迎的人等等。但是他聚会的食物实在难吃，无非是些生胡萝卜和芹菜。后来贾里尼克掏钱让系里另一个教授承办聚会，那个教授每次请专业大厨在家作出极丰盛的晚宴，并准备许多美酒，从此这种聚会就转移到那个教授家了。  除了巩俐的电影，贾里尼克对中国的了解就是清华大学和青岛啤酒了。他有时会把两个名字搞混，有两次被香港科技大学的 Pascale 冯教授抓住。 贾里尼克说话心直口快，不留余地。在他面前谈论学术一定要十分严谨，否则很容易被他抓住辫子。除了刚才提到的对语言学家略有偏见的评论，他对许多世界级的大师都有过很多“刻薄”但又实事求是的评论，这些评论在业界广为流传。贾里尼克在四十多年的学术生涯中居然没有得罪太多的人 ，可以说是一个奇迹。 注释一： 贾格布森的通信模型  1 上下文 2  信息 3  发送着 --------------- 4 接收者 5  信道 6 编码 from:http://www.google.com.hk/ggblog/googlechinablog/2006/06/blog-post_2074.html","title":"数学之美 系列八-- 贾里尼克的故事和现代语言处理"},{"content":"2006年4月17日 上午 08:01:00 发表者：吴军，Google 研究员 前言：隐含马尔可夫模型是一个数学模型，到目前为之，它一直被认为是实现快速精确的语音识别系统的最成功的方法。复杂的语音识别问题通过隐含马尔可夫模型能非常简单地被表述、解决，让我不由由衷地感叹数学模型之妙。 自然语言是人类交流信息的工具。很多自然语言处理问题都可以等同于通信系统中的解码问题 -- 一个人根据接收到的信息，去猜测发话人要表达的意思。这其实就象通信中，我们根据接收端收到的信号去分析、理解、还原发送端传送过来的信息。以下该图就表示了一个典型的通信系统： 其中 s1，s2，s3...表示信息源发出的信号。o1, o2, o3 ... 是接受器接收到的信号。通信中的解码就是根据接收到的信号 o1, o2, o3 ...还原出发送的信号 s1，s2，s3...。 其实我们平时在说话时，脑子就是一个信息源。我们的喉咙（声带），空气，就是如电线和光缆般的信道。听众耳朵的就是接收端，而听到的声音就是传送过来的信号。根据声学信号来推测说话者的意思，就是语音识别。这样说来，如果接收端是一台计算机而不是人的话，那么计算机要做的就是语音的自动识别。同样，在计算机中，如果我们要根据接收到的英语信息，推测说话者的汉语意思，就是机器翻译； 如果我们要根据带有拼写错误的语句推测说话者想表达的正确意思，那就是自动纠错。 那么怎么根据接收到的信息来推测说话者想表达的意思呢？我们可以利用叫做“隐含马尔可夫模型”（Hidden Markov Model）来解决这些问题。以语音识别为例，当我们观测到语音信号 o1,o2,o3 时，我们要根据这组信号推测出发送的句子 s1,s2,s3。显然，我们应该在所有可能的句子中找最有可能性的一个。用数学语言来描述，就是在已知 o1,o2,o3,...的情况下，求使得条件概率 P (s1,s2,s3,...|o1,o2,o3....) 达到最大值的那个句子 s1,s2,s3,... 当然，上面的概率不容易直接求出，于是我们可以间接地计算它。利用贝叶斯公式并且省掉一个常数项，可以把上述公式等价变换成 P(o1,o2,o3,...|s1,s2,s3....) * P(s1,s2,s3,...) 其中 P(o1,o2,o3,...|s1,s2,s3....) 表示某句话 s1,s2,s3...被读成 o1,o2,o3,...的可能性, 而 P(s1,s2,s3,...) 表示字串 s1,s2,s3,...本身能够成为一个合乎情理的句子的可能性，所以这个公式的意义是用发送信号为 s1,s2,s3...这个数列的可能性乘以 s1,s2,s3...本身可以一个句子的可能性，得出概率。 （读者读到这里也许会问，你现在是不是把问题变得更复杂了，因为公式越写越长了。别着急，我们现在就来简化这个问题。）我们在这里做两个假设： 第一，s1,s2,s3,... 是一个马尔可夫链，也就是说，si 只由 si-1 决定 (详见系列一)； 第二， 第 i 时刻的接收信号 oi 只由发送信号 si 决定（又称为独立输出假设, 即 P(o1,o2,o3,...|s1,s2,s3....) = P(o1|s1) * P(o2|s2)*P(o3|s3)...。 那么我们就可以很容易利用算法 Viterbi 找出上面式子的最大值，进而找出要识别的句子 s1,s2,s3,...。 满足上述两个假设的模型就叫隐含马尔可夫模型。我们之所以用“隐含”这个词，是因为状态 s1,s2,s3,...是无法直接观测到的。 隐含马尔可夫模型的应用远不只在语音识别中。在上面的公式中，如果我们把 s1,s2,s3,...当成中文，把 o1,o2,o3,...当成对应的英文，那么我们就能利用这个模型解决机器翻译问题； 如果我们把 o1,o2,o3,...当成扫描文字得到的图像特征，就能利用这个模型解决印刷体和手写体的识别。 P (o1,o2,o3,...|s1,s2,s3....) 根据应用的不同而又不同的名称，在语音识别中它被称为“声学模型” (Acoustic Model)， 在机器翻译中是“翻译模型” (Translation Model) 而在拼写校正中是“纠错模型” (Correction Model)。 而P (s1,s2,s3,...) 就是我们在系列一中提到的语言模型。 在利用隐含马尔可夫模型解决语言处理问题前，先要进行模型的训练。 常用的训练方法由伯姆（Baum）在60年代提出的，并以他的名字命名。隐含马尔可夫模型在处理语言问题早期的成功应用是语音识别。七十年代，当时 IBM 的 Fred Jelinek (贾里尼克) 和卡内基·梅隆大学的 Jim and Janet Baker (贝克夫妇，李开复的师兄师姐) 分别独立地提出用隐含马尔可夫模型来识别语音，语音识别的错误率相比人工智能和模式匹配等方法降低了三倍 (从 30% 到 10%)。 八十年代李开复博士坚持采用隐含马尔可夫模型的框架， 成功地开发了世界上第一个大词汇量连续语音识别系统 Sphinx。 我最早接触到隐含马尔可夫模型是几乎二十年前的事。那时在《随机过程》（清华“著名”的一门课）里学到这个模型，但当时实在想不出它有什么实际用途。几年后，我在清华跟随王作英教授学习、研究语音识别时，他给了我几十篇文献。 我印象最深的就是贾里尼克和李开复的文章，它们的核心思想就是隐含马尔可夫模型。复杂的语音识别问题居然能如此简单地被表述、解决，我由衷地感叹数学模型之妙。 转载自：http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_1583.html","title":"隐含马尔可夫模型在语言处理中的应用"},{"content":"1. 随机文本生成            如何生成随机文本？要解决这个问题，首先必须知道什么是随机文本。这里说的”随机文本“是一种有”意义“的”随机文本“。有“意义”是指所生成的随机文本在一定程度上是符合自然语言规则的。具体对于英语来说，作者提到的随机文本有两个层次，一个是letter-level(字母级别)，一个是word-level(单词级别)。所谓letter-level就是”随机“字母拼成的字母序列，word-level就是”随机“英语单词拼成的文本。上面提到的“符合自然语言规则”可以这样理解：对于字母级别，是指由随机字母序列拼成的单词应在一定程度上符合英语的单词拼写规则；对于单词级别，是指由随机单词拼成的单词序列应在一定程度上符合英语的语法规则。       如何生成随机文本？最笨的方法是：对于letter-level，就是随机输出字母表中的字母和空格；对于word-level，就是随机输出字典中的单词。 这样做的确非常随机和简单，但这样生成的文本很可能没有任何”意义“。实际上，对于英文单词来说，某一个字母后面会出现什么字母的概率是不相等的。例如：如果当前字母是Q，那么下一个字母是U的可能行很大，而出现字母X的概率却很小。我们可以先读取一个样本(如《编程珠玑》的内容)，统计A以后每个字母出现的次数、B之后每个字母出现的概率，等等。因此，在生成随机文本过程中把每个字母设置为其前一个字母的随机函数，生成器可以得到更令人感兴趣的文本。       我们把用当前字母的一个随机函数生成下一个字母的方式生成的随机文本，称为1阶文本。把这一思想扩展到更长的字母序列上，2阶文本就是通过把每个字母设置为其前面两个字母的函数得到的（一对字母通常称为二连字母）。例如，二连字母TH在英文中后面通常跟A、E、I、O、U、和Y，后面跟R和W的可能性小一些，跟其他字母的情况更少。3阶文本是通过把下一个字母设置为其前面三个字母（三连字母）的函数得到的。而到了4阶文本，大多数单词都是英文单词了。对于在单词级别上生成随机文本，1阶文本、2阶文本等的概念与此类似。文中指出，2阶文本模拟英文的效果通常是最理想的。      为什么要生成随机文本？作者在文中并没有明确说明。我感觉，作者的用意主要是引出“马尔科夫链”的概念。“具有数学背景的读者可能会将这个过程（例如：3阶随机文本的生成过程）视为一个马尔科夫链。每个状态表示一个k连字母，并且从一个状态到另一个状态的概率是不变的。因此这是一个”具有固定转换概率的有限状态马尔科夫链“。”作者以生动形象的语言描述了字母级别的英文文本k阶近似：我们随机打开一本书并在该页随机挑选一个字母，输出，并记之为“当前字母”。翻到另一页（非下一页，而是随机的另一页），寻找“当前字母”出现的位置，如果“当前字母”出现多次，则随机选一个，然后向后看k个字母，把那第k个字母输出，并把新的字母记为“当前字母”，再继续翻另一页，直到书翻完了或输出的字母总数达到某预设值，程序结束[2]。       作者提供了字母级别和单词级别k阶文本生成的完整代码：letter_lever文本生成代码以及word_lever文本生成代码。上述程序都是用C语言写的，作者写得很紧凑。对于第一个程序，用的是简单的扫描法去找“当前字母”在“下一页”的新位置，每次都扫描一次“输入文本”全文，所以效率很低，仅仅作为一个算法演示而已；第二个程序用到了散列表，效率不错，我认为有相当的实用价值，另外作者实现简易散列表的方法非常娴熟非常牛逼，实乃我等低端职业程序员居家旅行必备之参考[2]。 2. 马尔科夫链       马尔科夫链，因安德烈•马尔科夫（A.A.Markov，1856－1922）得名，是数学中具有马尔科夫性质的离散时间随机过程。该过程中，在给定当前知识或信息的情况下，过去（即当期以前的历史状态）对于预测将来（即当期以后的未来状态）是无关的。更简单一点说就是健忘！只记得当天或最近的事情，而稍早一些时候或过去很久了的就不记得。       现在，假设随机变量X_1,X_2,X_3...的一个数列具有马尔科夫性质。这些变量的范围，即它们所有可能取值的集合，被称为“状态空间”，而X_n的值则是在时间n的状态。如果X_{n+1}对于过去状态的条件概率分布仅是X_n的一个函数，则 　　                                            P(X_{n+1}=x|X_1=x_1, X_2=x_2, ..., X_n=x_n) = P(X_{n+1}=x|X_n=x_n).　 这里x为过程中的某个状态。上面这个恒等式可以被看作是马尔科夫性质。 马尔科夫的性质概括为如下两点： 　　 （1）t+1 时刻系统状态的概率分布只与t 时刻的状态有关，与 t 时刻以前的状态无关； 　　 （2）从t 时刻到t+1时刻的状态转移与t的值无关。       到目前为止，我还只稍微懂一点点有关马尔科夫链的数学知识。有关马尔科夫链的实际应用，我在网上看到一篇很浅显易懂的文章，作者是Google研究员王军，题目是《数学之美系列三  —  隐含马尔可夫模型在语言处理中的应用 》。       随机文本生成本来是个比较麻烦的问题，但是运用马尔科夫链的知识重新进行表述后，感觉整个问题突然变得简单明了了许多。进一步了解到，马尔科夫模型作为一种统计模型，已经广泛应用在语音识别，词性自动标注，音字转换，概率文法等各个自然语言处理的应用领域。经过长期发展，尤其是在语音识别中的成功应用，使它成为一种通用的统计工具。到目前为止，它一直被认为是实现快速精确的语音识别系统的最成功的方法。复杂的语音识别问题通过隐含马尔科夫模型能非常简单地被表述、解决，让人们不由由衷地感叹数学模型之妙。 参考文献： [1] Jon Bentley, 《编程珠玑》第十五章—字符串 [2] 随机文本生成：http://blog.csdn.net/z_york/article/details/2546764 [3] 马尔科夫链：http://baike.baidu.com/view/3053716.htm [4] 马尔科夫模型：http://baike.baidu.com/view/2361814.htm?subLemmaId=2361814&fromenter=%C2%ED%B6%FB%BF%C6%B7%F2%C4%A3%D0%CD","title":"随机文本生成与马尔科夫链"},{"content":"    机器学习是研究计算机基于数据构建模型，并运用模型来模拟人类智能活动的一门学科。随着计算机与网络的飞速发展，机器学习在我们的生活与工作中起着越来越大的作用，正在改变着我们的生活和工作。     1．日常生活中的机器学习     我们在日常生活经常使用数码相机。你也许不知道，数码相机上的人脸检测技术是基于机器学习技术的！我认识的三位了不起的科学家与工程师：Robert Schapire,Paul Viola,劳世竑。他们的研究都与人脸检测技术有关。Robert与Yoav Freund一起发明了非常有效的机器学习算法AdaBoost。Paul将AdaBoost算法成功地应用到人脸检测。劳世竑和他领导的Omron团队将AdaBoost人脸检测算法做到了芯片上。据说现在世界上有百分之六七十的数码相机上的人脸检测都是用Omron的芯片。     在我们的工作与生活中，这种例子曾出不穷。互联网搜索、在线广告、机器翻译、手写识别、垃圾邮件过滤等等都是以机器学习为核心技术的。     不久以前，机器学习国际大会（International Conference on Machine Learning，ICML 2011）在美国华盛顿州的Bellevue市举行。约有7百多位科研人员、教授、学生参加，创造了历史最高纪录。大会的三个主题演讲分别介绍了机器学习在微软的Kinect游戏机用户感应系统、谷歌的Googles图片搜索系统、IBM的 Watson自动问答系统中的应用。这些事实让人预感到机器学习被更广泛应用的一个新时代的到来。     2．机器学习与人工智能     智能化是计算机发展的必然趋势。人类从事的各种智能性活动，如数学、美术、语言、音乐、运动、学习、游戏、设计、研究、教学等等，让计算机做起来，现在还都是很困难的。这是几十年来人工智能研究得到的结论。     人工智能研究中，人们曾尝试过三条路。我将它们称之为外观(extrospection)、内省(introspection)和模拟(simulation)。     所谓外观，指的是观察人的大脑工作情况，探求其原理，解明其机制，从而在计算机上“实现”人类大脑的功能。比如，计算神经学（computational neuroscience）的研究就是基于这个动机的。然而，人脑的复杂信息处理过程很难观测和模型化。就像我们仅仅观测某个计算机内的信号传输过程，很难判断它正在做什么样的计算一样。     内省就是反思自己的智能行为，将自己意识到的推理、知识等记录到计算机上，从而“再现”人的智能，比如专家系统（expert system）的尝试就属于这一类。内省的最大问题是它很难泛化，也就是举一反三。无论是在什么样的图片中，甚至是在抽象画中，人们能够轻而易举地找出其中的人脸。这种能力称为泛化能力。通过内省的方法很难使计算机拥有泛化能力。自己的智能原理，对人类来说很有可能是不可知的（agnostic）。笼子里的老鼠可能认为触动把手是得到食物的“原因”，但它永远也不能了解到整个笼子的食物投放机制。     模拟就是将人的智能化操作的输入与输出记录下来，用模型来模拟，使模型对输入输出给出同人类相似的表现，比如统计机器学习（statistical machine learning）。实践表明，统计机器学习是实现计算机智能化这一目标的最有效手段。统计学习最大的优点是它具有泛化能力；而缺点是它得到的永远是统计意义下的最优解（例如：人脸检测）。现在当人们提到机器学习时，通常是指统计机器学习或统计学习。     3．机器学习的优缺点     下面看一个简单的例子。由这个例子可以说明统计学习的基本原理，以及由此带来的优缺点。     假设我们观测到一个系统的输出是一系列的1和0，要预测它的下一个输出是什么。如果观测数据中1和0各占一半，那么我们只能以0.5的准确率做出预测。但是，如果我们同时观测到这个系统有输入，也是一系列的1和0，并且输入是1时输出是0的比例是0.9，输入是0时输出是1的比例也是0.9。这样我们就可以从已给数据中学到“模型”，根据系统的输入预测其输出，并且把预测准确率从0.5提高到0.9。以上就是统计学习，特别是监督学习的基本想法。事实上，这是世界上最简单的统计机器学习模型！条件概率分布P(Y|X)，其中随机变量X与Y表示输入与输出，取值1与0。可以认为所有的监督学习模型都是这个简单模型的复杂版。我们用这个模型根据给定的输入特征，预测可能的输出。     统计学习最大的优点是它具有泛化能力，对于任意给定的X，它都能预测相应的Y。Vapnik的统计学习理论还能对预测能力进行分析，给出泛化上界。但从这个例子中也可以看到统计学习的预测准确率是不能保证100%的。比如，人脸检测会出错，汉语分词会出错。     统计学习是“乡下人”的办法。有个笑话。一个乡下人进城，到餐馆吃饭，不知如何在餐馆用餐，就模仿旁边的人。别人做什么，他也就学着做什么。邻桌的一位故意戏弄他，将桌上的蜡烛卷在饼里，趁乡下人不注意时把蜡烛扔到地上，然后咬了一口卷着的饼。乡下人也跟着学，大咬了一口自己的饼。统计学习只是根据观测的输入与输出，“模仿”人的智能行为。有时能够显得非常智能化。但它本质上只是基于数据的，是统计平均意义下的“模仿”。如果观测不到关键的特征，它就会去“咬卷着蜡烛的饼”。     4．机器学习与互联网搜索     我与同事们在从事互联网搜索相关的研究。据调查，60%的互联网用户每天至少使用一次搜索引擎，90%的互联网用户每周至少使用一次搜索引擎。搜索引擎大大提高了人们工作、学习以及生活的质量。而互联网搜索的基本技术中，机器学习占据着重要的位置。     在我看来，互联网搜索有两大挑战和一大优势。挑战包括规模挑战与人工智能挑战；优势主要是规模优势。     规模挑战：比如，搜索引擎能看到trillion量级的URL，每天有几亿、几十亿的用户查询，需要成千上万台的机器抓取、处理、索引网页，为用户提供服务。这需要系统、软件、硬件等多方面的技术研发与创新。     人工智能挑战：搜索最终是人工智能问题。搜索系统需要帮助用户尽快、尽准、尽全地找到信息。这从本质上需要对用户需求（如查询语句），以及互联网上的文本、图像、视频等多种数据进行“理解”。现在的搜索引擎通过关键词匹配以及其他“信号”，能够在很大程度上帮助用户找到信息。但是，还是远远不够的。     规模优势：互联网上有大量的内容数据，搜索引擎记录了大量的用户行为数据。这些数据能够帮助我们找到看似很难找到的信息。比如，“纽约市的人口是多少”，“约市的人口是多少”,“春风又绿江南岸作者是谁”。注意这些数据都是遵循幂函数分布的。它们能帮助Head（高频）需求，对 tail（低频）需求往往是困难的。所以，对tail来说人工智能的挑战就更显著。     现在的互联网搜索在一定程度上能够满足用户信息访问的一些基本需求。这归结于许多尖端技术包括机器学习技术的成功开发与应用，比如排序学习算法、网页重要度算法等等。这些机器学习算法在一定程度上能够利用规模优势去应对人工智能挑战。     但是、当今的互联网搜索距离 “有问必答，且准、快、全、好”这一理想还是有一定距离的。这就需要开发出更多更好的机器学习技术解决人工智能的挑战，特别是在tail中的挑战。     展望未来，机器学习技术的研究与开发会帮助我们让明天更美好！     （摘自南方周末，作者李航是微软亚洲研究院互联网搜索与挖掘组高级研究员及主任研究员，研究方向包括信息检索，自然语言处理，统计机器学习，及数据挖掘）","title":"机器学习改变生活与工作"},{"content":"文本自动校对技术研究综述 张仰森1,2  俞士汶1 1．北京大学计算语言学研究所 北京 100871； 2．北京信息科技大学计算机及自动化系 北京 100085 【摘要】概述了文本自动校对技术的产生背景，分析了中西文文本的各自特点以及它们之间的异同，对中西文文本校对的技术难点和解决方法以及国内外的研究现状进行了回顾和讦述，探讨了文本校对技术未来的发展方向及需要解决的问题。 【关键词】文本自动校对；孤立词校对策略；上下文相关的校对策略；语言模型 SummaryOfTextAutomaticProofrgadingTechnolOgy ZHANG Yang-sen1,2  YU Shi-wenl 1.1nstituteofCorotintationalLingulstlcs，PekingUniversity，Beijingl00871，China; 2．Dept．O／Computer&Autom02ization，Beijinglnforma，tionScience&TechnologyUniversity，Beo'ing100085，China 【Abstract】TheemergingbackgroundfOrtextautomaticproolYeadingtechnologyisgenerallystated…thisarticle．Eachcharac—teristicOf．thetextofChineseandWestern9swellasthesimilaritiesanddifferencesbetweenthemhavebeenanalyzed，therf—VIeWandcommentfOrdomesticandinternationalresearchpresentstatus&swellasthetechnicaldifficultyandsolvingmethods()fthetextproofi'eadingOfChineseandWesternaregiven，andthefuturedevelopingdirectionsandtheprublemsneedt()[)csolvedOftextautomaticproofreadingtechnologyarediscussed． 【Keywords】TextAutematicProofreading；StrategyfOrlsolate-wordErrorCorrection；StrategyfOrContext-sensitiveErrerCor—reetion；LanguageModel  1  引言 文本自动校对是自然语言处理的主要应用领域之一。早在20世纪60年代，国外就开展了英文文本的自动校对研究；IBMThomasJ．Watson研究中心首先于1960年在IBM／360和IBM／370用UNIX实现了一个TYPO英文拼写检查器；1971年，斯坦福大学的RalphGorin在DEC—10机上实现了一个英文拼写检查程序SpellL。多年来，随着计算机技术的不断发展，新的输入技术不断涌现，如OCR识别、语音识别。开展拼写错误校对的研究更加迫切，这方面的研究也在不断取得进展，部分成果已经商品化，目前流行的一些文字处理软件(如Word，Wordpefect等)也都嵌入了英文拼写检查功能。国际互联网上还能见到ExpertEase公司推出的DealProof，Newton公司推出的Proofread等英文单词拼写检查系统。 国内在中文文本校对方面的研究始于20世纪90年代初期，但发展速度较快。目前有许多科技公司和高等院校或研究机构都投入了一定的人力和财力开展这方面的研究，并取得了一些较好的成果，且有部分成果已经商品化，如黑马校对系统、金山校对系统、工智校对通等。本文就文本自动校对技术的国内外发展状况进行了研究。 2  文本中的常见错误类型分析 2．1常见的文字录入技术 目前，常见的文字录入技术和方法主要有键盘录入、语音识别、OCR识别、手写识别。其中由于键盘录入和OCR识别速度快、准确率高，成为文字录入的主要手段。对于中文来说，由于字符集太大，人们研究了许多种输入法间接将汉字送人计算机。目前比较流行且影响较大的输入法有五笔字型输入法、微软拼音输入法、智能狂拼输入法、智能ABC等。OCR识别也是一种常用的输入技术，但这种输入技术主要用于书写比较]：整的手写稿或印刷稿输入，速度极快，目前字迹清晰的印刷稿的识别正确率已在98％以上，手写体的识别正确率还比较低，识别后处理或校对的任务也比较繁重。 除了原稿中的错误外，电子文本中的错误主要来自输入过程。尤其对中文来说，文中的错误还和所使用的输入法密切相关，因此，有必要对这些输入法产生的错误及形式进行分析。 2．2键盘录入导致的中英文文本错误分析 在应用键盘录入英文字符时，常见的错误有以下几种：非词错误、真词错误和句法语义错误。非词错误是指文本中那些被词边界分隔出的字符串，根本就不是词典中的词。如下面的输入错误：them→tehm，the→thr，partlhon→patition，study→Stud-dy等就是非词错误，造成这种错误的原因是由于指法错误或粗心造成的，这些错误可以概括为替换错误、易位错误、丢失错误和插入错误等。真词错误是由于输入人员的粗心或指法错误所形成的字符串，虽不是想要的单词，但却是在词典中能够查到的真正的单词。如在输入from时由于发生了易位错误，使from变成了form，而form是词典中的词；若在输入employer时由于字符r和e相邻，很可能将r输成e就得到employee，得到的字符串是词典中的单词，但词义相反。真词错误往往会导致所输入的问与上下文搭配不当，不是当前语境中所需要的词，如“IcomeformBeijing'’中的“form”应为\"from”。句法语义错误往往是由于真词错误造成的，或由于原稿本身存在语法错误，或输入时丢失了某个单词甚至串行或丢失一整行。通常人们将“非词错误”称为单词错误，而将“真词错误”称为上下文相关的文本错误。 在应用键盘录入汉字时，由于汉字数量远远大于键盘上键的数量，所以必须采用编码输入法。常用的编码输入法有五笔字型输入法和拼音输入法(包括全拼、双拼、智能ABC、智能狂拼等)。与英文不同，汉语输入不会发生非字错误，能输入到计算机中的字必在汉字库中，因此，汉语文本中只会出现由于替换、易位、丢失、插入而导致的上下文相关错误或句法语义错误。使用五笔字型法输入文字时产生的错字往往与原字形相似，或者它们的编码相近，如由于手型不规范将d，f，g，h弄错，导致将“居(nd)”输成“导(nf)”；而使用各种拼音法产生的错误，其音相同或相似，如“计算机用户”输成“计算机拥护”。 2．3OCR，识别导致的中英文本错误分析在应用OCR技术输入文字时，常见的错误主要有拒识和误识两种情况。由于识别系统识别的字数有限，对一些生僻字会拒识，如“校雠学”被识别为“校x学”。而对于那些形近或形似的英文字符或汉字则容易产生误识，如英文字母“D”被识别为“O”，字母“l”被识别为数字“[”，“已经”被识别为“已经”，“孔子曰”被识别为“孔子日”等。 除了输入过程中造成的错误以外，还有一种错误就是在文稿形成过程中由于写作人员的疏忽和大意造成的原稿错误，如写错别字、搭配不当、结构残缺和标点符号错误等。“像片”被作者写为“象片”，“为人类做出贡献”被写为“为人类作出贡献”等。 3  英文文本中的错误发现与纠错方法 3．1单词错误的发现与纠错 英文文本中单词错误的检测发现方法目前主要有两种，即N-gram分析法和查词典法。一般情况下，N-gram错误检测技术对输入串中的每一个n元串(n一般取2或3)在事先编辑好的—个N-gram表中进行查找，看它是否在表中存在或它的出现频次，那些不存在或出现频次非常低的n元串被认为是可能的拼写错误，如“hj”或“het”就是错误的三元串。N-gram分析法通常需要一个词典或大规模的文本语料以便事先编辑N—gram表。查词典法主要是检查所输入的n元串是否在词典或可接受的词表中，如果不在词典中，则将该输入串标志为一个拼写错误的词。由于墓于查词典法的校对系统查错精度高，因此，是目前较为流行的错误检测技术。考虑到存取速度，当词典规模较大时，为了提高查错速度，有效的词典查找算法也是人们研究的重点。 单词错误的纠错方法已经有很多研究，主要有误拼词典法、词形距离法、最小编辑距离法、相似键法、骨架键法、基于规则的技术、词典及神经网络技术。 (1)误拼字典法。收集大规模真实文本中拼写出错的英文单词并给出相应的正确拼写，建造一个无歧义的误拼字典。在进行英文单词拼写检查时，查找误拼字典，如命中，则说明该单词拼写有误，该词的正确拼写字段为纠错建议。该方法的特点是侦错和纠错一体化，效率高。但英文拼写错误具有随机性，很难保证误拼字典的无歧义性和全面性，因此查准率低、校对效果差。 (2)词形距离法。这是一种基于最大相似度和最小串间距离的英文校对法。其核心思想是构造单词的似然性函数，如该单词在词典中，则单词拼写正确；否则，按照似然性函数，在词典中找到一个与误拼单词最相似的词作为纠错候选词，该方法的特点是节省存储空间，能反映一定的常见拼写错误统计规律，是一种模糊校对法。 (3)最小编辑距离法。通过计算误拼字符串与问典中某个词间的最小编辑距离来确定纠错候选词。所谓最小编辑距离是指将一个词串转换为另一个词串所需的最少的编辑操作次数(编辑操作是指插入、删除、易位和替换等)。还有人提出了反向最小编辑距离法，这种力‘法首先对每个可能的单个错误进行交换排列，生成一个候选集，然后，通过查词典看哪些是有效的单词，并将这些有效的单词作为误拼串的纠错建议。 (4)相似键法。相似键技术是将每个字符串与一个键相对应，使那些拼写相似的字符串具有相同或相似的键，当计算出某个误拼字符串的键值之后，它将给出—个指针，指向所有与该误拼字符串相似的单问，并将它们作为给误拼字符串的纠错建议。 (5)骨架键法。通过构建骨架键词典，在英文单问出现错误时，先抽取出该错误单词的骨架键，然后再去查骨架键词典，将词典中与该单词具有相同骨架键的正确单词作为该单词的纠错建议。 (6)N-gram法。基于n元文法，通过对大规模英文文本的统计得到单词与单词间的转移概率矩阵。当检测到某英文单词不在词典中时，查转移概率矩阵，取转移概率大于某给定阈值的单词为纠错建议。 (7)基于规则的技术。利用规则的形式将通常的拼写错误模式进行表示，这些规则可用来将拼写错误变换为有效的单词。对于一个误拼字符串，应用所有合适的规则从词典中找到一些与之对应的单词作为结果，并对每个结果根据事先赋予生成它的规则的概率估计计算一个数值，根据这个数值对所有候选结果排序。 3．2上下文相关错误的纠错方法 上下文相关的文本错误即真词错误，其校对要比单词拼写错误校对困难得多。上下文相关的拼写校对不仅要修正那些“经典”的拼写错误类型，比如同音词错误(如peace与piece)和字母排序错误(如form与from)，而且还要修正那些常见的语法错误(如among与between)和词边界混淆的错误(如maybe与maybe)。因为真词错误的出错字符串是词典中的正确词，所以针对单词拼写错误的校对方法在这里不一定适用，要对这类错误进行校对，必须使用上下文信息来判定哪些词在文本中出现是不合理的，这些词可能就是潜在的错误。上下文相关错误的校对较之单词误拼的校对要困难得多，它与自然语言理解的研究紧密相连。受自然语言理解技术进展的影响，文本错误的校对技术目前还没有大的突破。现有的基于上下文的文本错误校对方法有三类：①利用文本的特征，如字形特征、词性特征或上下文特征②利用概率统计特性进行上下文接续关系的分析③利用规则或语言学知识，如语法规则、词搭配规则等。 (1)利用文本上下文的同现与搭配特征 可以将文本的校对过程描述为词排歧过程。若称待校对的词为目标词，则建立混淆集C二{wl，…，W。}，其中的每个词W1均与文本中的目标词容易发生混淆或歧义。如假设C二{kom，form}，如果在文本中出现from或form时，就将它看作是一个from与form之间的歧义，校对的任务就是根据上下文决定哪个词是我们想要的词。上下文相关的校对问题由语句和语句中要被校正的词构成，Bayesian方法和基于Winnow的方法都是将这样的问题表示成有效特征表，每一个有效特征表示目标词的上下文中有一个特殊的语言学模式存在。目前常使用的特征有两种类型：上下文的词和词的搭配。上下文词特征用来检查在目标词周围的：A个词的范围内是否有特殊词存在；词搭配则用来检测在目标词的周围J个相邻词和／或同性标注的状态。如假没目标词的混淆集为{weather，whether}，若置A=10，l=2，目标词的可用特征包括： ①目标词前后10个词范围内的cloudy； ②当前词后为t。+动问。 特征①就预示着当前问应为weather；而②则用来检查词搭配，它表明当前问后紧接着一个“to+动词”的结构，表明当前词应取whether(如Idon’tknowwhethertolaughorcry)。在这种方法中，主要要解决的问题包括混淆集的求取；目标词所在上下文中特征的表示，即如何将语句的初始文本表示转换为有效特征。 基于词语同现与搭配特征的校对方法有很多种，较好的有Bayesian方法ll和基于Winnow方法。各种N-gram模型，如长距离N-gram、触发对N-gram等模型，都可以利用目标词上下文中的词同现特征或搭配特征，采用最大似然估计法、互信息、相关度等方法检测文本中的错误，并通过相邻词间的转移概率确定纠错候选词，实现对目标词的校正。 (2)利用规则或语言学知识 这种技术利用语言学家的语言学知识或句法语义规则去纠正文本中出现的错误。在基于语言学知识或规则的技术中，随着分析过程的进展，系统将依据句法、语义和篇章结构知识，建立一个它希望在下一个位置看到的词的列表，如果输入字符串的下一字符不在所期望的字符列表中，则系统就认为检测到了一个错误，并从其期望词表中选择一个词作为对其进行修下的候选问。 4  中文文本中的错误类型与校对技术 4．1中文与西文的差别及处理难点 大多数西文都是表音文字，而汉语是表意文字，它们之间有着很多的不同：①文本结构不同。英语文本中词与词之间有空格，而汉语文本无空格。②词结构不同。英语的词有形态变化(时、数、量)，而汉语缺少形态变化且汉语词类与句法成分之间不存在某种简单的对应关系。③字符进入计算机的方式不同。英文单词进入计算机是按字母一个个地录入，而中文字符进人计算机只能借助汉字编码。这种输入过程不可能产生拼写错误，即显示在计算机屏幕上的每个汉字都必须是汉字编码字符集中的一个单字，绝不会是缺一点少一捺的错字。因此，中文输入不会产生“非字错误”，只能产生别字错误，这些错误往往与要输人的宇或词音同、音近或形近。④字符集规模的差异。英文的字符集是26个字母加标点符号，而汉语字符集则是一个包含了超过6763个汉字符的大字符集，这将导致在应用语言模型时参数计算的极大困难。正是由于汉语和西文的差异，导致汉语文本的处理要比西文文本复杂得多。由于汉语没有“非字错误”，因此，其校对只能是基于上下文的相关性来实现。汉语处理中的主要难点，如文本的切分、标注的歧义处理以及未登录词的识别等，也会反映到中文文本自动校对技术的研究当中，直接影响着中文文本校对时所进行的语法、语义分析的质量，进而影响召回率与查准率。 4．2中文校对技术的现状 国内在文本自动校对方面的研究主要是针对汉语文本开展的。因为中文文本校对主要面向的是含有错误的文本，因此，汉语自然语言理解的研究也就成了计算机中文文本自动校对的基础。由于汉语与英语本质上的不同，在对中文文本进行查错/纠错分析时，必须要基于自然语言的理解技术，通过研究上下文间的依存关系才能实现，这显然是比较复杂和困难的，某些适于英文单词校对的技术和方法对汉语文本并不太适用。目前，国内有不少单位另：展了中文文本校对理论和技术的研究，除了微软亚洲研究院、IBM中国研究中心、哈尔滨工业大学、清华大学、东北大学、北京师范大学、北京工业大学、山西大学等科研院所外，一些有实力的高新技术公司，如北京黑马电子新技术公司、北大方正公司、金山公司等都开展了中文文本校对软件的研究与开发。 4．2．1自动查错的研究状况 就目前现有的与中文校对相关的文献来看，国内在自动文本查错方面主要采用三种方法：①利用文本上下文的字、词和词性等局部语言特征，包括词性特征、同现特征或相：互依存特征”，甚至包括字形特征等；②利用转移概率对相邻词间的接续关系进行分析；③利用规则或语言学知识，如语法规则、词搭配规则等。其实，这些方法之间没有严格的界限，甚至一般足混合使用的。 (1)基于上下文的局部语言特征 微软中国研究院设计实现了一个基于多特征的中文自动校对方法，它综合考虑了汉语文本叫中字、词和词性的局部语言特征以及K距离的语言特征，并采用Winnow方法进行特征学习，利用这些上下文特征对目标闷混淆集，中的词进行选择。具主要难点是如何将目标浯句转换为多元有效特征以及混淆集的获取”。哈尔滨工业大学将对被校对的句子中的每个字词寻找其可能的候选，构成句子的字词候选矩阵，在此基础上，利用语言本身所具有的结构特征与统计特征，从候选矩阵中选出句子的最佳字词候选序列，将其与原句对照，找出错误的字词，并以第一候选加以改正。语言结构特征的获取则应用‘元规则对字词候选矩阵中的字词进行捆绑与剪枝，形成语言结构元素，并将其构成元素格子图，然后借助文本统计特征，应用Markov模型从语言结构元素格子图中寻找一条最佳的元素路径，即为从候选矩阵中寻找的待校对语句的最佳句子。该方法的关键是候选矩阵构造以及语言结构特征的获取，由于候选矩阵中只选择了同，音字，因而，目前仅适于校对拼音输入法形成的文本。其主要难点在于特征的统一表示与格子图中的有效候选路径的求取。 (2)基于规则 北京师范大学利用校正文法规则对文稿进行校对，若句子满足校正文法规则，则根据规则把相应字词标记错误，但有限的规则很难覆盖大量难以预料的错误现象，查错能力有限。哈尔滨工业大学则以小句为单位，对汉语句子进行三遍扫描，通过自动分词、自动识别生词、用短语规则将单字同散串合成短语，逐步把正确的字符串捆扎起来，将不能捆绑的剩余单字符串判定为错误。其不足之处是有限的短语捆扎规则难以覆盖大量的语言现象，短语的捆扎缺乏定量的判断依据，查错算法只能查出单宁(串)错误，不能查出多字问的替换错误，比如“用户社会主义制度”这样的错误就无法查出。吴岩等人还提出了一种问匹配和语法分析相结合的校对方法。采用规则与统计相结合的方法，不使用大规模语料库，通过逆向最大匹配和局部语料统计算法发现散串，并对散串进行词匹配和语法分析处理，进而发现候选错误字串，由人机交互的方法对错误串进行自动校正，取得了较高的查错率。 (3)基于统计 张照煌提出一种利用综合近似字集替换，并用统计语言模型评分的方法，其基本思想足以事先整理好字形、字音、字义或输入码相近字的综合近似字集替换待校对句子中的每个汉字，产生许多候选字符串(或许多路径)，利用统计语言模型对各候选字符串评分，将评分最高的字符串与待校对文本中的句子进行对照，即可发现错误之所在并提供相对应的正确字。该方法的难点是如何整理综合近似字集，且若近似字集较大的话，训—算量是非常大的；其不足之处是只能校对所谓的别字错误，对多字、漏字、易位等错误难以发现。东北大学提出了一种混合文本校对方法HMCTC，采用模式匹配方法进行最长匹配分词，发现长词错误；然后根据类三元语法，将与前后相邻词同现频率乘积小于一定阈值的词标记为错误；最后对词进行语法属性标注，在不可能的语法标注序列字词处作错误标记。其缺点是基于词语同现频率的查错判据受限于训练语料的大小和语料选取的领域，且词语同现频率数据的获取需要大规模经过切分的熟语料，而这样的熟语料是难以获得的。清华大学利用语料库统计知识指导文本校对，以句为单位，把句子看作字段和词段，对字段计算字段平均字频、字段平均转移概率；对词段计算词间字转移概率、词性转移概率，将转移概率作为查错判据，把转移概率小于阈值的字或词作为查出的错误。其中，查错判据是自动查错研究的核心，仍有待于进一步研究。北京工业大学计算机学院在对大规模语料库的统计分析基础上，构建了二字结构工程并引人人名、地名辨识规则，利用词语类间的接续关系进行查错，对人名、地名误报率低。 4．2．2自动纠错的研究状况 自动纠错是文本自动校对的一个重要组成部分，它为自动查错时侦测出的错误字符串提供修改建议，辅助用户改正错误。修改建议的有效性是衡量自动纠错性能的主要指标，它有两点要求：①提供的修改建议中应该含有正确或合理的建议；②正确或合理的修改建议应尽可能排列在所有建议的前面。因此，纠错修改建议的产生算法及排序算法是自动纠错研究的两个核心课题。 由于中文文本自动校对理论和技术尚不太成熟，自动纠错研究的论述还不多见。东北大学采用模式匹配方法对长词进行纠错处理，但没有充分利用出错字符串的特征，算法计算量大。IBM中国研究中心提出一种替换字表结合主词典，通过加字和换字对侦测出来的错误字符串提供修改建议的纠错算法，但该算法的纠错建议局限于替换字表，没有考虑。卜下文启发信息，主要考虑对错字这种错误类型进行纠错，对漏字、多字、易位、多字替换、英文单词拼写等错误类型的纠错能力较弱。山西大学提出了一种墓于似然匹配的纠错建议候选集产生算法，对漏字、多字、易位、多字替换等错误类型的纠错能力有了较大的提高。 5  中文文本自动校对存在的问题与对策 经过多年的研究，已有一些商品化的文本自动校对软件在出版印刷界得到一定程度的应用，如黑马校对系统、方正金山校对系统等。但与机器翻泽—样，文本自动校对技术是建立在自然语言理解技术的基础之上的，是一个难度很大的研究课题，系统的错误召回率和准确率都比较低(召回率小于70％，准确率小于40％)，纠错建议的有效率或首选正确率也很低，与用户的要求还有较大差距，故其技术还有待进一步研究。 造成中文文本自动校对技术召回率和准确率较低的原因有如下几点：①中文文本中的错误都是“真字错误”，针对英文比较有效的单词查错和纠错技术在中文中不太适用；②目前基于上下文的自动查错技术主要还是字词级的水平，使用的查错语言模型是字词级的简单统计模型(如Bigram或Trigram)，利用的语言学知识不够丰富，对于更高级(如句子级)的错误很难查出；③尽管实践已经证明，有指导的统计方法是建立自然语言应用系统模型的有效手段，但用于语言模型训练的切分／标注的大规模语料很难获得，因而由于数据稀疏而导致模型训练得不够充分；④目前的自动校对技术研究重查错轻纠错，很多使用中的校对系统对查出的错误给不出纠错建议，或给出的纠错建议很不准确。 针对中文文本的上述问题，我们认为自动校对技术的研究应在以下几方面得到加强： (1)加强句法、语义层次的校对策略研究，与目前研究较多的词汇级校对策略相结合，从而能够检查以往无法查出的错误。汉语的词类没有形态的变化，词类和句法成分之间不存在简单的对应关系，汉语的词序又非常灵活，汉语的这些特点使得汉语的语法分析存在很大的难度，而面向错误文本的句法分析与错误检查难度会更大，采用什么样的方法既可以降低句法分析难度又能够满足文本错误检测的要求需要很好地研究。语义问题是语言学与语言信息处理研究中的薄弱环节，而已公开的利用语义信息实现文本校对的研究成果很少，语义错误检查在中文文本校对系统中仍相当困难，但并不是说语义校对无从着手，如通过义素分析法或语义文法，或许能帮助对文本校对中的语义错误进行检查，但这首先需要对文本进行词义排歧与标注，这方面仍需深入研究。 (2)查错后的纠错处理是校对系统的重要组成部分，目前对如何产生纠错候选词的研究以及如何对纠错候选词的排序方法的研究还不是很多。这方面的研究也涉及到上下文，况且面对的错误可能是多字、漏字、易位或多字替换等各种类型，而对所生成的多个候选词的排序又依赖于目标词所在的上下文。这方面的研究需要不断深入。 (3)加强自然语言处理的基础研究。构建信息丰富的综合语言知识库，包括标注完整的大规模语料库的建设，这涉及词语切分、标注和未登录词识别等问题，切分和标注中歧义排除本身就是非常难的问题。 (4)加强从语言知识库中获取文本自动查错知识的机器学习方法研究。根据中文文本的特点，研究面向中文文本自动查错和纠错的计算语言模型。如何抽取语言知识库中的各种特征，进而获得构建文本查错语言模型的知识，如何将文本中蕴涵的语言学特征与统计特征相结合建立更有效的查错与纠错模型还需进一步研究。","title":"文本自动校对技术研究综述"},{"content":"腾讯三面结束了，与料想的有很大的差别。原来以为会考察自然语言处理上的一些问题，所以昨天我一直在准备语言模型方面的知识，把N-Gramme和隐马尔科夫仔细看了几遍。然后面试官说我对自然语言处理不了解，我们还是以数据结构算法，编程语言为主吧。 今天面试主要分三部分： 一）项目情况。       1.先自我介绍。我讲了自己的基本情况，做过的三个课题。       2.重点介绍自己的的一个工作。我介绍了商品属性抽取的工作。 二）算法：        从字符串的查询与插入开始。        1.用什么数据结构存储字符串，实现高效的插入和查询。               我先说的是词典树，可以实现空间的最优。然后他问我查找的效率怎么样。我这时候想到查找的效率没有hash方式好。        2.如果存储的目的是为了查找某个区间中的字符串，用什么结构存储比较好。               我说那就通过set，也就是红黑树来进行存储。        3.红黑树是什么样的结构，查找的效率是什么                 平衡的二叉查找树。 log(n)        4.如果我要查找一个区间内的字符串，怎样进行效率的最优化。                  我先说，插入时直接构建按序插入。然后建立索引（他说这个方式效果肯定会不好）        5.提示了：是不是考虑在二叉查找树的节点中加入某些数值信息？                  我说可以插入大于这个节点的字符串的数目。马上想到不对，后面的节点数目是变化的，而且每次需要全局更新，代价太大。                   那人给我提示：那小于这个节点的字符串的数目可以不？（这不是坑我吗！） 我马上告诉他不可以，和插入大于的字符串数是一个道理。        6.再提示了：那就插入当前子树上的节点数目。（好吧，其实我想过的， 只是当时没有理顺思路，没说！崩溃）                     我一画图，这个肯定是可以的啊。        7. 用这种方式具体怎么实现？                     我在纸上紧张地画图。感觉这时候脑袋真是不够用啊！      说了两个字符串属于不同子树的情况，然后他就把另一种情况说了。 三）C++部分         1.说说虚函数的知识                   从虚函数的作用，到虚函数实现时的虚函数表的情况讲了讲。          2. 如何在C里面实现虚函数？      （崩溃，我一开始就说了我没有用过C）                    我给的思路是：结构体，里面放函数指针，利用全局函数之类的是不是可以实现。                       （搜了一下，应该是利用函数指针，模拟虚函数表的机理） 以上就是腾讯实习三面的主要过程。等了整整两个半星期，希望能有好的结果。","title":"腾讯三面"},{"content":"特别推荐： 1、HMM学习最佳范例全文文档 2、无约束最优化全文文档 一、书籍： 1、《自然语言处理综论》英文版第二版 2、《统计自然语言处理基础》英文版 3、《用Python进行自然语言处理》，NLTK配套书 4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦 5、《自然语言处理中的模式识别》 6、《EM算法及其扩展》 7、《统计学习基础》 8、《自然语言理解》英文版（似乎只有前9章） 9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner； 10、概率统计经典入门书：《概率论及其应用》（英文版，威廉*费勒著） 　　第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要） 11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》 12、国外机器学习书籍之： 　1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习&数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的” 　2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。豆瓣评论 by王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。 　3) “Introduction to Machine Learning” 13、国外数据挖掘书籍之： 　1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍 作者 : Jiawei Han/Micheline Kamber 出版社 : Morgan Kaufmann 评语 : 华裔科学家写的书，相当深入浅出。 　2) Data Mining:Practical Machine Learning Tools and Techniques 　3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher） 14、国外模式识别书籍之： 　1）“Pattern Recognition” 　2）“Pattern Recongnition Technologies and Applications” 　3）“An Introduction to Pattern Recognition” 　4）“Introduction to Statistical Pattern Recognition” 　5）“Statistical Pattern Recognition 2nd Edition” 　6）“Supervised and Unsupervised Pattern Recognition” 　7）“Support Vector Machines for Pattern Classification” 15、国外人工智能书籍之： 　1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。 　2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP” 16、其他相关书籍： 　1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor 　2）Learning.Python第四版，英文 二、课件： 1、哈工大刘挺老师的“统计自然语言处理”课件； 2、哈工大刘秉权老师的“自然语言处理”课件； 3、中科院计算所刘群老师的“计算语言学讲义“课件； 4、中科院自动化所宗成庆老师的“自然语言理解”课件； 5、北大常宝宝老师的“计算语言学”课件； 6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码； 7、MIT Regina Barzilay教授的“自然语言处理”课件，52nlp上翻译了前5章； 8、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件； 9、Michael Collins的“Machine Learning （机器学习）”课件； 10、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件； 11、Philipp Koehn “Empirical Methods in Natural Language Processing”课件； 12、Philipp Koehn“Machine Translation（机器翻译）”课件； 三、语言资源和开源工具： 1、Brown语料库： 　a) XML格式的brown语料库，带词性标注； 　b) 普通文本格式的brown语料库，带词性标注； 　c) 合并并去除空行、行首空格，用于词性标注训练：browntest.zip 2、NLTK官方提供的语料库资源列表 3、OpenNLP上的开源自然语言处理工具列表 4、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表” 5、LDC上免费的中文信息处理资源 6、中文分词相关工具： 　1）Java版本的MMSEG：mmseg-v0.3.zip，作者为solol，详情可参见：《中文分词入门之篇外》 　2）张华平老师的ICTCLAS2010，该版本非商用免费一年，下载地址： http://cid-51de2738d3ea0fdd.skydrive.live.com/self.aspx/.Public/ICTCLAS2010-packet-release.rar 7、热心读者“finallyliuyu”提供的一批新闻语料库，包括腾讯，新浪，网易，凤凰等，目前放在CSDN上：http://finallyliuyu.download.csdn.net/ 　　另外finalllyliuyu在2010年9月又提供了一批文本文类语料，详情见：献给热衷于自然语言处理的业余爱好者的中文新闻分类语料库之二 四、文献： 1、ACL-IJCNLP 2009论文全集： 　a) 大会论文Full Paper第一卷 　b) 大会论文Full Paper第二卷 　c) 大会论文Short Paper合集 　d) ACL09之EMNLP-2009合集 　e) ACL09 所有workshop论文合集 　　","title":"自然语言处理相关书籍及其他资源"},{"content":" 应用自然语言处理的相关算法进行词汇、query、广告、拍卖词、网页等的分析挖掘、广告相关性计算、Ontology构建等。应用自然语言处理算法改进广告投放算法 1.具有计算语言学、自然语言处理、模式识别、数据挖掘等人工智能； 2.熟悉Java或C 等任意一门高级语言； 3.对以下任意一种机器学习算法有充分的认识：SVM、图模型（CRF，HMM，MEMM）、Bayesian Network、ME、Best Cut； 4.对以下任意一种应用有独立的见解或开发经验：NER、实体关系、事件抽取、评价对象抽取、情感分析、文本分类/聚类、搜索引擎； 5.有计算机原理、计算机算法、操作系统等的基础知识。 某公司的自然语言处理工程师的要求，我现在在最最底层的开始,就把所要学习的放在这里。好好学习，go,go,go...........................","title":"NLP目标"},{"content":"很好的一篇文章，对于一些最根本的问题，还是转化为数学问题，还是得学好数学呀   数学之美 系列---发表者: 吴军, Google 研究员 关注 ly6873 的发言  【 大 中 小 】   打印  推荐给朋友  收藏  点击2286次  回复7次  复制本帖地址         数学之美 系列一 -- 统计语言模型 2006年4月3日 上午 08:15:00 从本周开始，我们将定期刊登 Google 科学家吴军写的《数学之美》系列文章，介绍数学在信息检索和自然语言处理中的主导作用和奇妙应用。 发表者: 吴军, Google 研究员 前言 也许大家不相信，数学是解决信息检索和自然语言处理的最好工具。它能非常清晰地描述这些领域的实际问题并且给出漂亮的解决办法。每当人们应用数学工具解决一个语言问题时，总会感叹数学之美。我们希望利用 Google 中文黑板报这块园地，介绍一些数学工具，以及我们是如何利用这些工具来开发 Google 产品的。 系列一： 统计语言模型 (Statistical Language Models) Google 的使命是整合全球的信息，所以我们一直致力于研究如何让机器对信息、语言做最好的理解和处理。长期以来，人类一直梦想着能让机器代替人来翻译语言、识别语音、认识文字（不论是印刷体或手写体）和进行海量文献的自动检索，这就需要让机器理解语言。但是人类的语言可以说是信息里最复杂最动态的一部分。为了解决这个问题，人们容易想到的办法就是让机器模拟人类进行学习 - 学习人类的语法、分析语句等等。尤其是在乔姆斯基（Noam Chomsky 有史以来最伟大的语言学家）提出 “形式语言” 以后，人们更坚定了利用语法规则的办法进行文字处理的信念。遗憾的是，几十年过去了，在计算机处理语言领域，基于这个语法规则的方法几乎毫无突破。 其实早在几十年前，数学家兼信息论的祖师爷 香农 (Claude Shannon)就提出了用数学的办法处理自然语言的想法。遗憾的是当时的计算机条件根本无法满足大量信息处理的需要，所以他这个想法当时并没有被人们重视。七十年代初，有了大规模集成电路的快速计算机后，香农的梦想才得以实现。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师贾里尼克 (Fred Jelinek)。当时贾里尼克在 IBM 公司做学术休假 (Sabbatical Leave)，领导了一批杰出的科学家利用大型计算机来处理人类语言问题。统计语言模型就是在那个时候提出的。 给大家举个例子：在很多涉及到自然语言处理的领域，如机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询中，我们都需要知道一个文字序列是否能构成一个大家能理解的句子，显示给使用者。对这个问题，我们可以用一个简单的统计模型来解决这个问题。 如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为： P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1) 其中 P (w1) 表示第一个词w1 出现的概率；P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词wn，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。因此我们假定任意一个词wi的出现概率只同它前面的词 wi-1 有关(即马尔可夫假设），于是问题就变得很简单了。现在，S 出现的概率就变为： P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)… (当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。） 接下来的问题就是如何估计 P (wi|wi-1)。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,（P(wi|wi-1) = P (wi)/[P(wi-1,wi)]。 也许很多人不相信用这么简单的数学模型能解决复杂的语音识别、机器翻译等问题。其实不光是常人，就连很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法都有效。比如在 Google 的中英文自动翻译中，用的最重要的就是这个统计语言模型。去年美国标准局(NIST) 对所有的机器翻译系统进行了评测，Google 的系统是不仅是全世界最好的，而且高出所有基于规则的系统很多。 现在，读者也许已经能感受到数学的美妙之处了，它把一些复杂的问题变得如此的简单。当然，真正实现一个好的统计语言模型还有许多细节问题需要解决。贾里尼克和他的同事的贡献在于提出了统计语言模型，而且很漂亮地解决了所有的细节问题。十几年后，李开复用统计语言模型把 997 词语音识别的问题简化成了一个 20 词的识别问题，实现了有史以来第一次大词汇量非特定人连续语音的识别。 我是一名科学研究人员 ，我在工作中经常惊叹于数学语言应用于解决实际问题上时的神奇。我也希望把这种神奇讲解给大家听。当然，归根结底，不管什莫样的科学方法、无论多莫奇妙的解决手段都是为人服务的。我希望 Google 多努力一分，用户就多一分搜索的喜悦。 数学之美 系列二 -- 谈谈中文分词 2006年4月10日 上午 08:10:00 发表者: 吴军， Google 研究员 谈谈中文分词 ----- 统计语言模型在中文处理中的一个应用 上回我们谈到利用统计语言模型进行语言处理，由于模型是建立在词的基础上的，对于中日韩等语言，首先需要进行分词。例如把句子 “中国航天官员应邀到美国与太空总署官员开会。” 分成一串词： 中国 / 航天 / 官员 / 应邀 / 到 / 美国 / 与 / 太空 / 总署 / 官员 / 开会。 最容易想到的，也是最简单的分词办法就是查字典。这种方法最早是由北京航天航空大学的梁南元教授提出的。 用 “查字典” 法，其实就是我们把一个句子从左向右扫描一遍，遇到字典里有的词就标识出来，遇到复合词（比如 “上海大学”）就找最长的词匹配，遇到不认识的字串就分割成单字词，于是简单的分词就完成了。这种简单的分词方法完全能处理上面例子中的句子。八十年代，哈工大的王晓龙博士把它理论化，发展成最少词数的分词理论，即一句话应该分成数量最少的词串。这种方法一个明显的不足是当遇到有二义性 （有双重理解意思）的分割时就无能为力了。比如，对短语 “发展中国家” 正确的分割是“发展-中-国家”，而从左向右查字典的办法会将它分割成“发展-中国-家”，显然是错了。另外，并非所有的最长匹配都一定是正确的。比如“上海大学城书店”的正确分词应该是 “上海-大学城-书店，” 而不是 “上海大学-城-书店”。 九十年代以前，海内外不少学者试图用一些文法规则来解决分词的二义性问题，都不是很成功。90年前后，清华大学的郭进博士用统计语言模型成功解决分词二义性问题，将汉语分词的错误率降低了一个数量级。 利用统计语言模型分词的方法，可以用几个数学公式简单概括如下： 我们假定一个句子S可以有几种分词方法，为了简单起见我们假定有以下三种： A1, A2, A3, ..., Ak, B1, B2, B3, ..., Bm C1, C2, C3, ..., Cn 其中，A1, A2, B1, B2, C1, C2 等等都是汉语的词。那么最好的一种分词方法应该保证分完词后这个句子出现的概率最大。也就是说如果 A1,A2,..., Ak 是最好的分法，那么 （P 表示概率）： P (A1, A2, A3, ..., Ak） 〉 P (B1, B2, B3, ..., Bm), 并且 P (A1, A2, A3, ..., Ak） 〉 P(C1, C2, C3, ..., Cn) 因此，只要我们利用上回提到的统计语言模型计算出每种分词后句子出现的概率，并找出其中概率最大的，我们就能够找到最好的分词方法。 当然，这里面有一个实现的技巧。如果我们穷举所有可能的分词方法并计算出每种可能性下句子的概率，那么计算量是相当大的。因此，我们可以把它看成是一个动态规划（Dynamic Programming) 的问题，并利用 “维特比”（Viterbi） 算法快速地找到最佳分词。 在清华大学的郭进博士以后，海内外不少学者利用统计的方法，进一步完善中文分词。其中值得一提的是清华大学孙茂松教授和香港科技大学吴德凯教授的工作。 需要指出的是，语言学家对词语的定义不完全相同。比如说 “北京大学”，有人认为是一个词，而有人认为该分成两个词。一个折中的解决办法是在分词的同时，找到复合词的嵌套结构。在上面的例子中，如果一句话包含“北京大学”四个字，那么先把它当成一个四字词，然后再进一步找出细分词 “北京” 和 “大学”。这种方法是最早是郭进在 “Computational Linguistics” （《计算机语言学》）杂志上发表的，以后不少系统采用这种方法。 一般来讲，根据不同应用，汉语分词的颗粒度大小应该不同。比如，在机器翻译中，颗粒度应该大一些，“北京大学”就不能被分成两个词。而在语音识别中，“北京大学”一般是被分成两个词。因此，不同的应用，应该有不同的分词系统。Google 的葛显平博士和朱安博士，专门为搜索设计和实现了自己的分词系统。 也许你想不到，中文分词的方法也被应用到英语处理，主要是手写体识别中。因为在识别手写体时，单词之间的空格就不很清楚了。中文分词方法可以帮助判别英语单词的边界。其实，语言处理的许多数学方法通用的和具体的语言无关。在 Google 内，我们在设计语言处理的算法时，都会考虑它是否能很容易地适用于各种自然语言。这样，我们才能有效地支持上百种语言的搜索。 对中文分词有兴趣的读者，可以阅读以下文献： 1. 梁南元 书面汉语自动分词系统 http://www.touchwrite.com/demo/LiangNanyuan-JCIP-1987.pdf 2. 郭进 统计语言模型和汉语音字转换的一些新结果 http://www.touchwrite.com/demo/GuoJin-JCIP-1993.pdf 3. 郭进 Critical Tokenization and its Properties http://acl.ldc.upenn.edu/J/J97/J97-4004.pdf 4. 孙茂松 Chinese word segmentation without using lexicon and hand-crafted training data http://portal.acm.org/citation.c ... GUIDE&id=980775 数学之美 系列三 -- 隐含马尔可夫模型在语言处理中的应用 2006年4月17日 上午 08:01:00 发表者：吴军，Google 研究员 前言：隐含马尔可夫模型是一个数学模型，到目前为之，它一直被认为是实现快速精确的语音识别系统的最成功的方法。复杂的语音识别问题通过隐含马尔可夫模型能非常简单地被表述、解决，让我不由由衷地感叹数学模型之妙。 自然语言是人类交流信息的工具。很多自然语言处理问题都可以等同于通信系统中的解码问题 -- 一个人根据接收到的信息，去猜测发话人要表达的意思。这其实就象通信中，我们根据接收端收到的信号去分析、理解、还原发送端传送过来的信息。以下该图就表示了一个典型的通信系统： 其中 s1，s2，s3...表示信息源发出的信号。o1, o2, o3 ... 是接受器接收到的信号。通信中的解码就是根据接收到的信号 o1, o2, o3 ...还原出发送的信号 s1，s2，s3...。 其实我们平时在说话时，脑子就是一个信息源。我们的喉咙（声带），空气，就是如电线和光缆般的信道。听众耳朵的就是接收端，而听到的声音就是传送过来的信号。根据声学信号来推测说话者的意思，就是语音识别。这样说来，如果接收端是一台计算机而不是人的话，那么计算机要做的就是语音的自动识别。同样，在计算机中，如果我们要根据接收到的英语信息，推测说话者的汉语意思，就是机器翻译； 如果我们要根据带有拼写错误的语句推测说话者想表达的正确意思，那就是自动纠错。 那么怎么根据接收到的信息来推测说话者想表达的意思呢？我们可以利用叫做“隐含马尔可夫模型”（Hidden Markov Model）来解决这些问题。以语音识别为例，当我们观测到语音信号 o1,o2,o3 时，我们要根据这组信号推测出发送的句子 s1,s2,s3。显然，我们应该在所有可能的句子中找最有可能性的一个。用数学语言来描述，就是在已知 o1,o2,o3,...的情况下，求使得条件概率 P (s1,s2,s3,...|o1,o2,o3....) 达到最大值的那个句子 s1,s2,s3,... 当然，上面的概率不容易直接求出，于是我们可以间接地计算它。利用贝叶斯公式并且省掉一个常数项，可以把上述公式等价变换成 P(o1,o2,o3,...|s1,s2,s3....) * P(s1,s2,s3,...) 其中 P(o1,o2,o3,...|s1,s2,s3....) 表示某句话 s1,s2,s3...被读成 o1,o2,o3,...的可能性, 而 P(s1,s2,s3,...) 表示字串 s1,s2,s3,...本身能够成为一个合乎情理的句子的可能性，所以这个公式的意义是用发送信号为 s1,s2,s3...这个数列的可能性乘以 s1,s2,s3...本身可以一个句子的可能性，得出概率。 （读者读到这里也许会问，你现在是不是把问题变得更复杂了，因为公式越写越长了。别着急，我们现在就来简化这个问题。）我们在这里做两个假设： 第一，s1,s2,s3,... 是一个马尔可夫链，也就是说，si 只由 si-1 决定 (详见系列一)； 第二， 第 i 时刻的接收信号 oi 只由发送信号 si 决定（又称为独立输出假设, 即 P(o1,o2,o3,...|s1,s2,s3....) = P(o1|s1) * P(o2|s2)*P(o3|s3)...。 那么我们就可以很容易利用算法 Viterbi 找出上面式子的最大值，进而找出要识别的句子 s1,s2,s3,...。 满足上述两个假设的模型就叫隐含马尔可夫模型。我们之所以用“隐含”这个词，是因为状态 s1,s2,s3,...是无法直接观测到的。 隐含马尔可夫模型的应用远不只在语音识别中。在上面的公式中，如果我们把 s1,s2,s3,...当成中文，把 o1,o2,o3,...当成对应的英文，那么我们就能利用这个模型解决机器翻译问题； 如果我们把 o1,o2,o3,...当成扫描文字得到的图像特征，就能利用这个模型解决印刷体和手写体的识别。 P (o1,o2,o3,...|s1,s2,s3....) 根据应用的不同而又不同的名称，在语音识别中它被称为“声学模型” (Acoustic Model)， 在机器翻译中是“翻译模型” (Translation Model) 而在拼写校正中是“纠错模型” (Correction Model)。 而P (s1,s2,s3,...) 就是我们在系列一中提到的语言模型。 在利用隐含马尔可夫模型解决语言处理问题前，先要进行模型的训练。 常用的训练方法由伯姆（Baum）在60年代提出的，并以他的名字命名。隐含马尔可夫模型在处理语言问题早期的成功应用是语音识别。七十年代，当时 IBM 的 Fred Jelinek (贾里尼克) 和卡内基•梅隆大学的 Jim and Janet Baker (贝克夫妇，李开复的师兄师姐) 分别独立地提出用隐含马尔可夫模型来识别语音，语音识别的错误率相比人工智能和模式匹配等方法降低了三倍 (从 30% 到 10%)。 八十年代李开复博士坚持采用隐含马尔可夫模型的框架， 成功地开发了世界上第一个大词汇量连续语音识别系统 Sphinx。 我最早接触到隐含马尔可夫模型是几乎二十年前的事。那时在《随机过程》（清华“著名”的一门课）里学到这个模型，但当时实在想不出它有什么实际用途。几年后，我在清华跟随王作英教授学习、研究语音识别时，他给了我几十篇文献。 我印象最深的就是贾里尼克和李开复的文章，它们的核心思想就是隐含马尔可夫模型。复杂的语音识别问题居然能如此简单地被表述、解决，我由衷地感叹数学模型之妙。 数学之美系列 4 -- 怎样度量信息? 2006年4月26日 上午 08:11:00 发表者：吴军，Google 研究员 前言: Google 一直以 “整合全球信息，让人人能获取，使人人能受益” 为使命。那么究竟每一条信息应该怎样度量呢？ 信息是个很抽象的概念。我们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少信息量。直到 1948 年，香农提出了“信息熵”(shāng) 的概念，才解决了对信息的量化度量问题。 一条信息的信息量大小和它的不确定性有直接的关系。比如说，我们要搞清楚一件非常非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，信息量的度量就等于不确定性的多少。 那么我们如何量化的度量信息量呢？我们来看一个例子，马上要举行世界杯赛了。大家都很关心谁会是冠军。假如我错过了看世界杯，赛后我问一个知道比赛结果的观众“哪支球队是冠军”？ 他不愿意直接告诉我， 而要让我猜，并且我每猜一次，他要收一元钱才肯告诉我是否猜对了，那么我需要付给他多少钱才能知道谁是冠军呢? 我可以把球队编上号，从 1 到 32， 然后提问： “冠军的球队在 1-16 号中吗?” 假如他告诉我猜对了， 我会接着问： “冠军在 1-8 号中吗?” 假如他告诉我猜错了， 我自然知道冠军队在 9-16 中。 这样只需要五次， 我就能知道哪支球队是冠军。所以，谁是世界杯冠军这条消息的信息量只值五块钱。 当然，香农不是用钱，而是用 “比特”（bit）这个概念来度量信息量。 一个比特是一位二进制数，计算机中的一个字节是八个比特。在上面的例子中，这条消息的信息量是五比特。（如果有朝一日有六十四个队进入决赛阶段的比赛，那么“谁世界杯冠军”的信息量就是六比特，因为我们要多猜一次。） 读者可能已经发现, 信息量的比特数和所有可能情况的对数函数 log 有关。 (log32=5, log64=6。） 有些读者此时可能会发现我们实际上可能不需要猜五次就能猜出谁是冠军，因为象巴西、德国、意大利这样的球队得冠军的可能性比日本、美国、韩国等队大的多。因此，我们第一次猜测时不需要把 32 个球队等分成两个组，而可以把少数几个最可能的球队分成一组，把其它队分成另一组。然后我们猜冠军球队是否在那几只热门队中。我们重复这样的过程，根据夺冠概率对剩下的候选球队分组，直到找到冠军队。这样，我们也许三次或四次就猜出结果。因此，当每个球队夺冠的可能性（概率）不等时，“谁世界杯冠军”的信息量的信息量比五比特少。香农指出，它的准确信息量应该是 = -（p1*log p1 + p2 * log p2 +　．．．　＋p32 *log p32)， 其中，p1，p2 ，　．．．，p32 分别是这 32 个球队夺冠的概率。香农把它称为“信息熵” (Entropy)，一般用符号 H 表示，单位是比特。有兴趣的读者可以推算一下当 32 个球队夺冠概率相同时，对应的信息熵等于五比特。有数学基础的读者还可以证明上面公式的值不可能大于五。对于任意一个随机变量 X（比如得冠军的球队），它的熵定义如下： 变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大。 有了“熵”这个概念，我们就可以回答本文开始提出的问题，即一本五十万字的中文书平均有多少信息量。我们知道常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字。但汉字的使用是不平衡的。实际上，前 10% 的汉字占文本的 95% 以上。因此，即使不考虑上下文的相关性，而只考虑每个汉字的独立的概率，那么，每个汉字的信息熵大约也只有 8-9 个比特。如果我们再考虑上下文相关性，每个汉字的信息熵只有5比特左右。所以，一本五十万字的中文书，信息量大约是 250 万比特。如果用一个好的算法压缩一下，整本书可以存成一个 320KB 的文件。如果我们直接用两字节的国标编码存储这本书，大约需要 1MB 大小，是压缩文件的三倍。这两个数量的差距，在信息论中称作“冗余度”（redundancy)。 需要指出的是我们这里讲的 250 万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。 不同语言的冗余度差别很大，而汉语在所有语言中冗余度是相对小的。这和人们普遍的认识“汉语是最简洁的语言”是一致的。 在下一集中， 我们将介绍信息熵在信息处理中的应用以及两个相关的概念互信息和相对熵。 对中文信息熵有兴趣的读者可以读我和王作英教授在电子学报上合写的一篇文章 《语信息熵和语言模型的复杂度》 数学之美系列六 -- 简单之美：布尔代数和搜索引擎的索引 2006年5月10日 上午 09:10:00 发表者: 吴军，Google 研究员 [建立一个搜索引擎大致需要做这样几件事：自动下载尽可能多的网页；建立快速有效的索引；根据相关性对网页进行公平准确的排序。我们在介绍 Google Page Rank (网页排名) 时已经谈到了一些排序的问题，这里我们谈谈索引问题，以后我们还会谈如何度量网页的相关性，和进行网页自动下载。］ 世界上不可能有比二进制更简单的计数方法了，也不可能有比布尔运算更简单的运算了。尽管今天每个搜索引擎都宣称自己如何聪明、多么智能化，其实从根本上讲都没有逃出布尔运算的框框。 布尔（George Boole) 是十九世纪英国一位小学数学老师。他生前没有人认为他是数学家。布尔在工作之余，喜欢阅读数学论著、思考数学问题。1854 年“思维规律”（An Investigation of the Laws of Thought, on which are founded the Mathematical Theories of Logic and Probabilities）一书，第一次向人们展示了如何用数学的方法解决逻辑问题。 布尔代数简单得不能再简单了。运算的元素只有两个1 （TRUE， 真) 和 0 （FALSE，假)。基本的运算只有“与”（AND)、“或” (OR) 和“非”（NOT) 三种（后来发现，这三种运算都可以转换成“与”“非” ＡＮＤ－ＮＯＴ一种运算）。全部运算只用下列几张真值表就能完全地描述清楚。 AND | 1 0 ----------------------- 1 | 1 0 0 | 0 0 这张表说明如果 AND 运算的两个元素有一个是 0，则运算结果总是 0。如果两个元素都是 1，运算结果是 1。例如，“太阳从西边升起”这个判断是假的(0),“水可以流动”这个判断是真的（1），那么，“太阳从西边升起并且水可以流动”就是假的（0）。 OR | 1 0 ----------------------- 1 | 1 1 0 | 1 0 这张表说明如果OR运算的两个元素有一个是 1，则运算结果总是 1。如果两个元素都是 0，运算结果是 0。比如说，“张三是比赛第一名”这个结论是假的（0），“李四是比赛第一名”是真的（1），那么“张三或者李四是第一名”就是真的（1）。 NOT | -------------- 1 | 0 0 | 1 这张表说明 NOT 运算把 1 变成 0，把 0 变成 1。比如，如果“象牙是白的”是真的（1），那么“象牙不是白的”必定是假的（0）。 读者也许会问这么简单的理论能解决什么实际问题。布尔同时代的数学家们也有同样的问题。事实上在布尔代数提出后80 多年里，它确实没有什么像样的应用，直到 1938 年香农在他的硕士论文中指出用布尔代数来实现开关电路，才使得布尔代数成为数字电路的基础。所有的数学和逻辑运算，加、减、乘、除、乘方、开方等等，全部能转换成二值的布尔运算。 现在我们看看文献检索和布尔运算的关系。对于一个用户输入的关键词，搜索引擎要判断每篇文献是否含有这个关键词，如果一篇文献含有它，我们相应地给这篇文献一个逻辑值 -- 真（TRUE,或 1），否则，给一个逻辑值 -- 假（FALSE, 或0）。比如我们要找有关原子能应用的文献，但并不想知道如何造原子弹。我们可以这样写一个查询语句“原子能 AND 应用 AND (NOT 原子弹)”，表示符合要求的文献必须同时满足三个条件： - 包含原子能 - 包含应用 - 不包含原子弹 一篇文献对于上面每一个条件，都有一个 True 或者 False 的答案，根据上述真值表就能算出每篇文献是否是要找的。 早期的文献检索查询系统大多基于数据库，严格要求查询语句符合布尔运算。今天的搜索引擎相比之下要聪明的多，它自动把用户的查询语句转换成布尔运算的算式。当然在查询时，不能将每篇文献扫描一遍，来看看它是否满足上面三个条件，因此需要建立一个索引。 最简单索引的结构是用一个很长的二进制数表示一个关键字是否出现在每篇文献中。有多少篇文献，就有多少位数，每一位对应一篇文献，1 代表相应的文献有这个关键字，0 代表没有。比如关键字“原子能”对应的二进制数是0100100001100001...，表示第二、第五、第九、第十、第十六篇文献包含着个关键字。注意，这个二进制数非常之长。同样，我们假定“应用”对应的二进制数是 0010100110000001...。那么要找到同时包含“原子能”和“应用”的文献时，只要将这两个二进制数进行布尔运算 AND。根据上面的真值表，我们知道运算结果是0000100000000001...。表示第五篇，第十六篇文献满足要求。 注意，计算机作布尔运算是非常非常快的。现在最便宜的微机都可以一次进行三十二位布尔运算，一秒钟进行十亿次以上。当然，由于这些二进制数中绝大部分位数都是零，我们只需要记录那些等于1的位数即可。于是，搜索引擎的索引就变成了一张大表：表的每一行对应一个关键词，而每一个关键词后面跟着一组数字，是包含该关键词的文献序号。 对于互联网的搜索引擎来讲，每一个网页就是一个文献。互联网的网页数量是巨大的，网络中所用的词也非常非常多。因此这个索引是巨大的，在万亿字节这个量级。早期的搜索引擎（比如 Alta Vista 以前的所有搜索引擎），由于受计算机速度和容量的限制，只能对重要的关键的主题词建立索引。至今很多学术杂志还要求作者提供 3-5 个关键词。这样所有不常见的词和太常见的虚词就找不到了。现在，为了保证对任何搜索都能提供相关的网页，所有的搜索引擎都是对所有的词进行索引。为了网页排名方便，索引中还需存有大量附加信息，诸如每个词出现的位置、次数等等。因此，整个索引就变得非常之大，以至于不可能用一台计算机存下。大家普遍的做法就是根据网页的序号将索引分成很多份（Shards)，分别存储在不同的服务器中。每当接受一个查询时，这个查询就被分送到许许多多服务器中，这些服务器同时并行处理用户请求，并把结果送到主服务器进行合并处理，最后将结果返回给用户。 不管索引如何复杂，查找的基本操作仍然是布尔运算。布尔运算把逻辑和数学联系起来了。它的最大好处是容易实现，速度快，这对于海量的信息查找是至关重要的。它的不足是只能给出是与否的判断，而不能给出量化的度量。因此，所有搜索引擎在内部检索完毕后，都要对符合要求的网页根据相关性排序，然后才返回给用户。","title":"数学之美 系列---发表者: 吴军, Google 研究员"},{"content":"记得第一次了解中文分词算法是在 Google 黑板报 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进行研究，期间诞生了很多有意思的理论。     中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。     有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。     最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来／应聘”。     维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。     还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如“民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民”并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／服务”。       不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。     当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词数法上，我们就有了一种简明而强大的算法：     对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。     这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分：       他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ）       他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ）       他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ）     正确答案胜出。     需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。     算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。“鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概率，每个词出现的频率也是不同的。     何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。     以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10-9 ，但“有意／见／分歧”的得分只有 1.0×10-11 ，正确方案完胜。     这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话：       这／事／的确／定／不／下来     但是概率算法却会把这个句子分成：       这／事／的／确定／不／下来     原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。     其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。     于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w1 、 w2 ，统计在语料库中词语 w1 后面恰好是 w2 的概率 P(w1, w2) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w1) · P(w1, w2) · … · P(wn-1, wn) ，其中 w1, w2, …, wn 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。     至此，中文自动分词算是有了一个漂亮而实用的算法。         但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。     在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。     可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、“说”、“报道”、“参加”、“访问”、“表示”等动作词。     但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了；要是听说了有人居然敢叫“令计划”，估计直接就崩溃了。     还有那些恰好与上下文组合成词的人名，例如：      费孝通向人大常委会提交书面报告      邓颖超生前使用过的物品     这就是最考验分词算法的句子了。     相比之下，中国地名的用字就分散得多了，重庆就有一个叫做“犀牛屙屎”的地方。不过，中国地名委员会编写了《中华人民共和国地名录》，收录了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。外文人名和地名的用字非常集中，识别的正确率要高出许多。     真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。     最难识别的未登录词就是缩略语了。“高数”、“抵京”、“女单”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。     汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。     说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。 转自：http://www.matrix67.com/blog/archives/4212","title":"漫话中文分词算法"},{"content":"  极端的社会舆论每每看到大学生就业报告里提到计算机系学生失业人数最多时，我就想，什么原因导致了这种现象的发生，在中国软件还处于比较初级的阶段时，市场对软件人才的需求应该每年在大幅的递增，可是大学里培养出来的计算机科班人才质量却每况愈下，甚至还不如一个软件培训机构两三个月训练出来的人好用，为什么?想想现在的计算机科班毕业生的水平吧，大学四年下来，90%的学生写的代码没有超过2000行，不Linux操作系统为何物，不知道C++和Vc的区别，没有开发出一块实用功能的简单软件， 没有使用过STL，甚至不知STL为何物，更不用提设计模式之类的比较高级一点的东西了……这样的例子还能举出很多… 就是这样的人才质量，如何让一个以营利为目的的公司接受，如何为企业创造价值？ 但是也有那么一些人，能进入微软、IBM、google、百度这样的公司，拿着年薪几十万。    2. 失败的计算机教育体制我也是一名毕业不久的计算机科班毕业生，从我目前了解的情况看来，大学时，没有几个学生真正的对计算机编程感兴趣，体会不到通过编程解决问题带来的乐趣,只是单纯的跟着课程的设置学习，这样没有目的性的学习效率如何之底？大学里的学生又有几个人能对自己的职业规划有一个基本的了解？大学里有几个人能理解学习的课程在具体的实践中的作用？ 这些惨痛的例子说明了我们大学对计算机系学生的引导是非常不够的？没能激起对学习计算机技术的兴趣？不能告诉大家一个将来一个明确的职业规划方向，没有很好的引导学生去思考自己的职业规划方向？如果是这种状态去学习，大学四年基本是废掉了…… 另外一个就是大学课程的设置，各种各样的课程，填鸭式的教学方式…. 纯粹理论式的教学方式….到头来，学生真正学到了什么？几个术语名词而已…..一样对操作系统是那样的迷茫….不知道编译原理的语法分析为何物？ 不知道数据结构中的树和图将有何用？    3. 四年后，我能骄傲的说我是计算机系的学生 上面发了那么多的牢骚，其实都是有感而发….下面在结合自己的工作的感受具体谈谈计算机学生应该如何规划自己的大学四年 大一： 一个新兵蛋子，刚走进象牙塔的大门，什么都是新鲜的，不断听着学长们说着天书般的技术术语… 天天争论C++和java哪个好，.net是否比Vc更智能先进…. 还有什么Asp.net …. 一堆的技术摆在自己面前了… 然后自己就糊涂了….去问学长吧…学长告诉你..好好学习java吧…将来有钱途….. 其实大一，没必要学习各种新鲜的技术…..把高等数学学好吧….这才是正事，是决定了着将来你是否能称为一个大牛还是一个编程语言的熟练操作工人的因素…. 也许这时候的你还不知道高等数学有什么作用… 但我要告诉你的是如果你的悟性高…. 工作一两年也许就能体会到数学的做用…. 学高数..不是简简单单的学习微积分…. 在掌握这些知识的时候….锻炼自己的逻辑思维….. 锻炼自己的思考问题解决问题的方法和能力。作用在将来一定大大的….. 等将来如果你涉足密码学…你会发现各种积分方程和矩阵变化…. 将来在计算一个算法的复杂性和证明算法的可靠性时，也离不开数学知识…. 如果你涉足人工智能和语音识别，各种统计模型就会呈现在你面前。在你毕业找工作时，这个才是你和专业培训机构培训出来的学生的差异能力。这才是企业更看重的能力。如果你还有时间的话，学习C语言… 但是不要再用谭浩强的书了…. 看 The C program langue 吧… 如果能真正领悟书中70%的例子话，那就足够了. 如果能把这两门课程学到十分优秀，恭喜你，你已经成功了一半了…..    大二： 如果你在大一学习了C之后，这个时候大学的课程就要涉及操作系统和数据结构、还有汇编语言了…… 这也是大二一定要学好的两门课了……大学的操作系统太失败了，上完课后，很多的学生不知道所云，更加感觉操作系统的神秘了，课程设计也就是什么银行家算法的，然后大家在网上一顿搜索，然后交给老师就算完事了… 其实，我的建议是自己写一个操作系统内核，实现内存管理，进程管理和切换 等一些基础的东西了就可以了，《自己动手写操作系统》就是很好的教材…… 如果还有时间，学习《Linux内核设计与实现》，看看现实商用的操作系统是怎么实现的？ 当然最好和原码结合的一起看，效果最好。还有赵炯博士的 “.012Linux内核完全剖析”什么的。如果能仔细阅读，收获一定不少。当然还有数据结构，这个也是重中之中，这也是和非科班出身的学生的差别，关键是你学的好坏，这个的实践主要在ACM上，当学习完数据结构后，最重要的是使用，不断的在Acm上做各种各样的题目，不断的提升自己算法设计的能力。从大二开始，如果能坚持两年下来，那么一般的算法设计肯定是难不住的了，也许这时候高数打下的基础就会起作用了。 当毕业的时候，进入一家好的公司应该不是太难的事情了。再说说汇编语言，本质上这也是一门编程语言，可能刚入门的时候比较困难，但是程序写多了，和C也没有差别了。我还想说一点，就是现在Windows内核也逐步开放了，至少有很多的逆向的资源可以学习。如果对Windows有兴趣，一样可以学习操作系统的实现原理。    大三： 离散数学和编译原理是个重头戏，离散数学虽然我现在还没体会到他的作用，但是和高数一样，这中内在的东西才是最重要的，代表着内功，如果没有学好，这些债迟早还要要还的。 编译原理，学习完以后一样会让你云里雾里，整天做那些无聊的题目。还是说实践吧，网上有开源的C编译器的源码，下载下来然后好好学习下，结合编译原理书中讲的东西，好好的消化一些这些知识，最后，自己如果能写出来一个C编译器的话，那你的编译原理也就通过了。当然这个时候可以学习一些C++或Java之类语言，但是学到够平时用的就可以了，没有学非常深。选择一本教材学习两三个月就行了。      当然，这个时候，可能你的同学已经能做出来各种漂亮的网页，也可能熟练的使用MFC类库做出各种各样的漂亮的软件，这些没什么，如果三年下来，如果你能够按照上面我写的那样坚持学习。也许他们用三年学习的这些东西，你用三个月就能熟练。    大四： 到了找工作的时候，如果你按照上面一步一个脚印的学习，我相信你会收到很多大公司的offer。因为大公司更看重的是你的内功的深厚，而小公司才会看重那些花拳绣腿的技术。但是这个时候，千万不要忘记继续学习，很多的学生大四一年都浪费掉了，真实太可惜了，在前面三年的基础上，到了厚积薄发的时候了， 开始要思考自己的职业规划了，你要选择Linux方向还是Windows方向，要选择底层方向还是应用方向， 要选择网页方向还是桌面应用方向。是选择自然语言处理还是人工智能。这个时候你要选择自己的一个方向，当然你可以向你的导师求助，然后确定自己的发展方向，大四一年就可以专心的学习了。      4. 附上我认为计算机学习比较好辅助教材：   C语言： the C Program Language 操作系统; 于渊：《自己动手写操作系统》                    《Linux内核设计与实现》                    《Linux内核完全剖析》                    《Linux内核情景分析》                    《Windows内核情景分析》 编译原理： 龙书《编译原理》 汇编： 王爽老师《汇编第二版》    5. 后记   以上都是自己在实习对大学的反思，可能很多人有不一样的看法，我没有任何异议。毕竟每个人经历是不一样的，但是如果你向想做真正的计算机科班出身的学生，学好上面介绍的课程吧。在以后的职业生涯中，你会终身受益的。当然上面很多的课程我没有提到，并不代表他们不需要学习，只是分量没有那么重而已。因为你还是要毕业的，每门功课还是要过的。 当然如果你毕业四年后打算回云南或者家里已经给你安排好了工作大学四年你可以轻松的渡过你没有必要去奋斗什么！如果你打算自己去奋斗有个不错的工资请你压抑住自己好好的学 以后你一定能有个好工作的 当然，我现在认为，计算机的本科四年真是一个打基础的四年，之后才是学习各种招式，如果基础打好了，招式的学习会事半功倍的。当进入公司后，一样要持续不断的学习，才能让你不断的进步。自己文采不好，写的比较乱，但都是肺腑之言，各位将就看吧    ","title":"给学计算机的忠告"},{"content":"http://www.xperseverance.net/blogs/2012/03/17/ 听说国外大牛都认为LDA只是很简单的模型，吾辈一听这话，只能加油了~ 另外这个大牛写的LDA导读很不错：http://bbs.byr.cn/#!article/PR_AI/2530?p=1 一、预备知识：        1. 概率密度和二项分布、多元分布，在这里        2. 狄利克雷分布，在这里，主要内容摘自《Pattern Recognition and Machine Learning》第二章        3. 概率图模型，在PRML第九章有很好的介绍 二、变量表示：       1. word：word是最基本的离散概念，在自然语言处理的应用中，就是词。我觉得比较泛化的定义应该是观察数据的最基本的离散单元。word的表示可以是一个V维向量v，V是所有word的个数。这个向量v只有一个值等于1，其他等于0。呵呵，这种数学表示好浪费，我以前做过的项目里一般中文词在200-300w左右，每一个都表示成300w维向量的话就不用活了。哈哈，所以真正应用中word只要一个编号表示就成了。      2. document：一个document就是多个word的合体。假设一篇文档有N个词，这些word是不计顺序的，也就是exchangeable的，LDA论文 3.1有说这个概念。论文中document的个数是M。      3. topic：就是主题啦，比如“钱”的主题可能是“经济”，也可能是“犯罪”~ LDA中主题的表示是隐含的，即只预先确定主题的个数，而不知道具体的主题是什么。论文中表示主题个数的字母是k，表示主题的随机变量是z。 好了，总结一下所有的变量的意思，V是所有单词的个数（固定值），N是单篇文档词的个数（随机变量），M是总的文档的个数（固定值），k是主题的个数（需要预先根据先验知识指定，固定值）。 三、基础模型：         先从两个基础模型说起：        1. Unitgram model (LDA 4.1)        一个文档的概率就是组成它的所有词的概率的乘积，这个一目了然，无需多说： $p(\\mathbf{w})=\\prod\\limits_{n=1}^{N}p(w_n)$        图模型：         2. Mixture of unigrams (LDA 4.2)         假如我们假设一篇文档是有一个主题的（有且仅有一个主题），可以引入主题变量z，那么就成了mixture of unigrams model。它的图模型如下图： $p(w)=\\sum\\limits_{z}p(z)\\prod\\limits_{n=1}^{N}p(w_n|z)$        这个模型的generate过程是，首先选择一个topic z for each docoment，然后根据这个z以及p(w|z)独立同分布产生w。观察这个图，z是在N饼外面的，所以每一个w均来自同一个z，就是说一个文档N个词只有一个topic。这和LDA中z在N饼里面不一样。 四、LDA        接下来正式说LDA的产生过程，对于一个文档w：       1. 选择 $N\\sim Possion(\\xi)$           这一步其实只是选个单词的个数，对整个模型没啥影响       2. 选择一个多元分布参数 $\\theta\\sim Dir(\\alpha)$           这α是狄利克雷分布的参数（k+1维），$\\vec{\\theta}=(\\theta_1,~...,~\\theta_k)$是产生主题的多元分布的参数，其中每一个$\\theta_i$代表第i个主题被选择的概率。从狄利克雷产生参数θ之后，再用θ去产生z       3. 上两步完成后，开始产生文档中的N个词               (a) 首先选个一个topic $z\\sim Multinomial(\\theta)$                     z是从以θ为参数的多元分布中挑选出来的，总共有k个topic，根据θ的概率参数选择其中一个topic作为z               (a) 然后选择一个word from $p(w_n|z_n,\\beta)$                    这个参数β也是多元分布，是一个$k\\times V$的矩阵，表示从zi到wj的产生概率即$\\beta_{ij}=p(w^j=1|z^i=1)$。若已选定zn，则矩阵的第n行就成了用来选择产生w的多元分布，根据这个多元分布产生一个w        至此，产生过程完成。上概率图模型： 整个图的联合概率为(只算单个文档，不算整个corpus的M个文档)： $p(\\theta,\\mathbf{z},\\mathbf{w}|\\alpha,\\beta)=p(\\theta|\\alpha)\\prod\\limits_{n=1}^{N}p(z_n|\\theta)p(w_n|z_n,\\beta)$ 把上式对应到图上，可以大致解释成这个样子： 在上面这个新图中，LDA的三个表示层被用三种颜色表示了出来： 1. corpus-level (红色)：α和β是语料级别的参数，也就是说对于每个文档都是一样的，因此在generate过程中只需要sample一次。 2. document-level (橙色)：θ是文档级别的参数，意即每个文档的θ参数是不一样的，也就是说每个文档产生topic z的概率是不同的，所以对于每个文档都要sample一次θ。 3. word-level (绿色)：最后z和w都是文档级别的变量，z由参数θ产生，之后再由z和β共同产生w，一个w对应一个z。 五、几何学解释 来看下面这个图：         这个图的意思是这样的，外面大三角形的三个顶点代表三个word，这三个word组成一个simplex，那么这个simplex中的一个点，代表什么意思呢？它代表的意思就是一个点就是一个产生这三个word的多元分布的概率密度（对于这个图多元分布的它是一个三维向量）。具体点来说，例如红色的点p1，它就在word1上。这个意思就是说，p1是一个多元分布，其参数为(1.0, 0, 0)，也就是它产生word1的概率为1，产生其它两个word的概率为0。再来看蓝色的点p2，它产生word1的概率反比与它到word1对边平行线的距离（注意可不是到word1那个点的距离哈），这个距离离得越近，点产生该word的概率越大。所以p2表示的多元分布概率密度看起来像是(0.5, 0.45, 0.05)，为啥p2产生word3的概率那么小呢，因为它离word3对边平行线好远~        了解了上面这层意思之后，我们再来看这个topic simplex。它是包含在word simplex里面的(sub-simplex)，所以topic simplex上的一点同时也是word simplex上的一个点。这样topic simplex上的一个点，就有了两层含义，一层含义是它是一个产生word的多元分布概率密度，另一层含义就是它是产生topic的多元分布概率密度。在这个图上，还可以发现topic的点相对于word simplex是已经固定的，其实这topic simplex 上的三个顶点到word simplex上的三个顶点总共9个距离，也就是9个概率值，正好是一个$3\\times 3$的矩阵，这个矩阵就是LDA中的β参数。       知道了这些之后，我们就可以来看mixture of unigrams在图上应该怎么表示了。还记得mixture of unigrams是要先选择一个文档的topic z的，然后根据这个topic产生word。所以它在这个图上的产生过程就是，先随机挑选topic simplx(注意是topic simplex)三个顶点中的一个，然后根据这个顶点到word simplex的距离，也就是这个顶点在word simplex上的多元分布产生每一个word。       再来看pLSI，图中间每一个带差的圈圈就是一个pLSI中的文档，每一个文档(在pLSI中文档被视为观察变量，每个文档是有编号的)都有一个独立的产生topic的多元分布，文档点的位置就表示了它产生三个topic的概率值。      对于LDA，汗，不是很理解，LDA places a smooth distribution on the topic simplex denoted by the contour lines。只好先放着了。  2012@3@28，关于上面这个LDA的图形为啥是曲线的问题，我专门请教了北大赵鑫大牛，他的回答很给力而且一针见血。要理解LDA为啥是曲线，要先从pLSI为啥是点说起。因为pLSI中，由文档w产生topic z的概率是一个参数，对于每个单独文档这个参数要被估计一次，参数可不是随机变量，而是固定的值。因此pLSI中每个文档在图中表示为一个确定的点。而LDA呢，文档w产生topic z的概率在论文里后面inference部分已经给出了，它是 p(z|w)=p(θ,z|w,α,β)=p(θ,z,w|α,β)p(w|α,β)， 也就是隐含变量z的后验分布，它是一个概率分布，这也是整个LDA inference部分最需要估计的东东。因此图中用曲线来表示LDA，也就是说LDA places a smooth distribution on the topic simplex …","title":"【JMLR'03】Latent Dirichlet Allocation (LDA) - David M.Blei"},{"content":"编者按：原文作者Alan Skorkin是一名软件开发人员，他在博客中分享对软件开发相关的心得，其中有很多优秀的文章，本文就是其中一篇，作者认为：成为优秀的开发人员，可以没有数学技能，但成为卓越的开发人员，不能没有。 不久之前，我开始思索数学。你也知道，到目前为止，我编写软件也有几年了。老实说，在我的工作当中，我还没有发现有关数学的需求。我要学习和掌握许多新东西，包括语言、框架、工具、流程、沟通技巧和可以用来做你想到的任何东西的库。在我学的新东西中，数学并没有帮助。当然了，这不足为奇，我所做的工作，大部分都是CRUD类型（编注：CRUD是Create、Read、Update和Delete的首字母缩写）。在互联网时代，这也是我们多数开发人员所做的大部分工作。如果你做顾问，你主要是在做网站；你在大公司上班，你主要是在做网站；你做自由职业者，你主要是在做网站。我很清楚我是在总结，但请忍耐一下，我跑偏了。 最后你对此有些厌倦了，我也如此。别误会我，这可以是项有趣并有挑战性的工作，有机会解决问题，并和有趣的人一起互动，在工作时间做这个，我高兴。但在我个人时间中搭建更多的网站，这种想法已经稍微失去其光泽，于是你开始寻找一些更加有趣/酷/好玩的事情，我再一次地也如此。（所以，）有些人转移到前台和图像技术，比如视觉反馈就比较诱人。但我并不是其中一员（虽然我和别人一样都喜爱前台，但它真的不能让我兴奋。）这就是当我遇到一些搜索相关的问题时，我为什么决定深入挖掘的原因了。这把我带回到故事的一开始，因为一旦我抓到第一把充满搜索的铁铲，一旦我“撞到”数学时，我才真正意识到，我的技能恶化的程度。数学并不像骑自行车，长期不用就会忘记。 拓展视野 多对搜索的一些了解，让我接触到各种有趣的软件和计算机科学相关的事情和问题（包括机器学习、自然语言处理、算法分析等）。现在，在我接触的各方面，我都看到了数学，所以我更加强烈地感觉到自己技能缺乏。我已经意识到，如果你想利用计算机做又酷又有趣的事，你需要达到一个像样的数学能力水平。除了上面说的三个，还有一些，如：密码学、游戏人工智能、压缩算法、遗传算法、3D图形算法等。在理解之后，如果你想要编写我们正讨论的那些库和工具，而不是仅仅使用它们（即：做一个“消费者”，而不是“生产者”），那你需要数学（知识）来理解这些领域背后的你能应用的理论。即便如果你不想编写任何库，当你真正理解事情的原理，你在构建软件时，它能给带来更多的成就感，绝非仅仅把它们连起来，就希望它们去做任何它们应该能做的。 虽然大多数开发人员会告诉你，他们在工作中从来不需要数学(就像我前面说的 )，但是经过一番沉思后，我有了个想法（突发灵感）：就是反马斯洛的锤子理论。你知道这个吧，当你有一把锤子，你会把一切看成是钉子。（注：伯乐在线编译的《每位开发人员都应铭记的10句编程谚语》中的第7条就是锤子理论。）这是一个隐喻，也就是说人们乐于使用自己钟爱的工具，即便这并不是手中工作的最好工具。数学就是我们的一个相反的锤子。我们知道有这个锤子，但并不太子的如何使用。所以，当我们遇到问题，我们的锤子是解决问题的最佳工具时，我们却从未认真考虑过它。对我祖父而言，螺丝刀够用了；对我父亲来说，也很好；对我来说，同样如此。谁还需要锤子？数学的技巧在于，人们惧怕它，甚至大多数程序员，你认为我们不会怕，但我们确实怕。所以，我们把自己的话转变为可以自我实现的预言。这并不是我在工作中不需要数学，这只是我真的不知道，即便我知道，我也不知道如何使用它。所以我并没有使用它，当缺少某些东西时，如果你长期将就，不久后你甚至不会察觉它的缺失，所以对其需要更少了，这是自我实现的预言。 针对思索接近我们内心世界，这里有一些的“粮食”——学习新技术。作为一名协作世界的开发人员，你努力成为一名通才型的专才（如果你不知道我在说什么，可以看看这本书《The Passionate Programmer:Creating A Remarkable Career In Software Development》）。你尽力在多数事情上做的体面，并在有些事情上做的优秀。但是你擅长什么？一般来说，人们会选择一两个框架或一门语言，然后与之相伴，这样是不错。但是要看到，框架和较小范围内的语言都有保质期。如果你要做一名Hibernate、Rails或Struts专家（使用struts的朋友现在真的应该担忧一下了），当新框架取代当前的框架时，你在几年内将不得不重新洗牌。所以，这也许是你真正的最好投资，但也可能不是。另一方面，数学是不会很快消逝的。在我们领域中所做的一切，都是建立在稳固的数学原理之上（算法和数据结构正是这样的例证），所以用在数学上的时间绝不是浪费，这不可辩论。再重复一次，总结起来就是：要真正理解东西，而不是非死记硬背地使用。当涉及到计算机时，数学能有助你更深入地理解你所做的。事实上，正如Steve Yegge所言，作为程序员我们所做的事很像数学，只是我们甚至都没有意识到这一点。 什么/谁造就了与众不同？ （唐纳德） 你不相信我？那请你想想：在我们的领域中，几乎人人普遍尊敬的卓越程序员同样也是大数学家。我是说像唐纳德·克努斯、艾兹格·迪杰斯特拉、诺姆·乔姆斯基、彼得·诺维格（Google研究院总监）这一类人。但是这些家伙并非真正的开发人员，他们是计算机科学家，这能真正算数么？我再一次觉得，在我们写出的纯代码行数能达到这些人所写的十分之一之前，也许我们不应该再去讨论这些问题了。当然，不当科学家，你也能获得成功和名誉，大家都听过加文·金（Gavin King，Hibernate创始人）或戴维·海涅梅艾尔·汉森（DHH，Ruby on Rails创始人）。这还挺真实的（是不是有很多人听说过加文和戴维，虽然这还有待确认），但是“听说过”和普遍尊敬是不同的，这种差别就如同创建一个框架，和在你的领域中为人类知识所做出的全部重大推动两者之间的差别。（不要误会我，我尊重加文和戴维，他们所做的事，远远超过我，但是这不能影响我所说的事实）。所有的这些相关么？我不知道，可能不相干，但在我们反省之后，我想无论如何要把它“扔掉”。 如今的世界正充满着数据，每日都增加更多的数据。而在以前，我们在相对少量的数据下享受工作。我们今日编写的软件必须高效处理海量数据。甚至在协作世界，这也是愈加明显的事实。这也就是说，你更不可能只“启动东西”，就想看其如何运作，因为你要处理的数据量将困住你，除非你非常了解它。我的预测是：算法分析将对于 Lay Programmer 越来越重要，以前不仅如此，以后也更加如此。如果要成为一位体面的算法设计专家，需要什么？你猜到了，是一些数学技能。（编注：Lay Programmer是指那些不认为自己是程序员的程序员，详情请见Martin Fowler的解释。我暂未想到合适的简短叫法，如果哪位朋友知道，请在评论中说明。） 所以，我该怎么办呢？嗯，我已决定一点一点地建立或恢复我的数学技能，虽然还有大量的书要看，大量的代码要写，但我会尽力抽时间放在数学上，这就像锻炼，时不常的锻炼总聊胜于无（再次引用Steve Yegge的话）。说到数学，我袖中当然还藏有一张王牌，它对我有利，但很幸运，有这个博客，我们都会受益的。（我知道你好奇，一会告诉你 ）。 你在5年内的规划如何？ （极限水上滑板） 那么，数学对所有事都有利么？这事先很难说，我对我现在的处境十分满意，或许你也如此，但这都和潜能有关系。如果你是协作世界的一名开发人员，你真的不需要数学。如果你乐于你的整个职业生涯是这样的：在工作时间中做企业CRUD应用，或在闲暇时间滑翔跳伞或极限水上滑板（或其他各种时髦的极客运动），也分配较多时间在Spring、Hibernate、Visual Studio或其它东西上。（其实）那些特殊的职位并没有真正限制你的潜力，你能变得极具价值，甚至可深入追求。但是如果你想为多样化的职业生涯而奋斗，想要有能力尝试几乎所有涉及代码的事，从信息检索到Linux内核。总之，如果你想成为一个开发人员、程序员和计算机科学家的完美组合，你必须确保你的数学技能达到标准（哎，你还是可以去玩滑翔跳伞或极限水上滑板）。长话短说，如果你在数学方面有一定天赋，那在软件开发领域中没有向你关着的门，如果没有，那一切都是CRUD型工作！","title":"数学是成就卓越开发人员的必备技能"},{"content":" 中文分词在中文信息处理中是最最基础的，无论机器翻译亦或信息检索还是其他相关应用，如果涉及中文，都离不开中文分词，因此中文分词具有极高的地位。中文分词入门最简单应该是最大匹配法了，当年师兄布置给我的第一个学习任务就是实现最大匹配法的分词算法（正向、逆向）。记得当时对自己参考学习最有帮助的是北大詹卫东老师“中文信息处理基础”的课件和源程序，不过他实现的是mfc程序，词表存储在数据库里。自己实现时用纯c++实现，利用hash_map存储词表。这里我介绍一下相关的知识和一个简单的程序示例，部分参考自詹老师的讲义。 　　正向最大匹配法算法如下所示： （注：以上最大匹配算法图来自于詹老师讲义） 　　逆向匹配法思想与正向一样，只是从右向左切分，这里举一个例子： 　　　输入例句：S1=”计算语言学课程有意思” ； 　　　定义：最大词长MaxLen = 5；S2= ” “；分隔符 = “/”； 　　　假设存在词表：…，计算语言学，课程，意思，…； 　　　最大逆向匹配分词算法过程如下： 　（1）S2=”\"；S1不为空，从S1右边取出候选子串W=”课程有意思”； 　（2）查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有意思”； 　（3）查词表，W不在词表中，将W最左边一个字去掉，得到W=”有意思”； 　（4）查词表，W不在词表中，将W最左边一个字去掉，得到W=”意思” 　（5）查词表，“意思”在词表中，将W加入到S2中，S2=” 意思/”，并将W从S1中去掉，此时S1=”计算语言学课程有”； 　（6）S1不为空，于是从S1左边取出候选子串W=”言学课程有”； 　（7）查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程有”； 　（8）查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程有”； 　（9）查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有”； 　（10）查词表，W不在词表中，将W最左边一个字去掉，得到W=”有”，这W是单字，将W加入到S2中，S2=“ /有 /意思”，并将W从S1中去掉，此时S1=”计算语言学课程”； 　（11）S1不为空，于是从S1左边取出候选子串W=”语言学课程”； 　（12）查词表，W不在词表中，将W最左边一个字去掉，得到W=”言学课程”； 　（13）查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程”； 　（14）查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程”； 　（15）查词表，“意思”在词表中，将W加入到S2中，S2=“ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=”计算语言学”； 　（16）S1不为空，于是从S1左边取出候选子串W=”计算语言学”； 　（17）查词表，“计算语言学”在词表中，将W加入到S2中，S2=“计算语言学/ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=”\"； 　（18）S1为空，输出S2作为分词结果，分词过程结束。 相应程序示例： 　　准备文件：建立一个词表文件wordlexicon，格式如下 　　　　计算语言学 　　　　课程 　　　　意思 　　输入文件：test,格式如下 　　 　　　 计算语言学课程有意思 　　编译后执行如下：SegWord.exe test 　　输出分词结果文件：SegmentResult.txt 源代码如下： // Dictionary.h #include <iostream> #include <string> #include <fstream> #include <sstream> #include <hash_map> using namespace std; using namespace stdext; class CDictionary { public: CDictionary(); //将词典文件读入并构造为一个哈希词典 ~CDictionary(); int FindWord(string w); //在哈希词典中查找词 private: string strtmp; //读取词典的每一行 string word; //保存每个词 hash_map<string, int> wordhash; // 用于读取词典后的哈希 hash_map<string, int >::iterator worditer; // typedef pair<string, int> sipair; }; //将词典文件读入并构造为一个哈希词典 CDictionary::CDictionary() { ifstream infile(“wordlexicon”); // 打开词典 if (!infile.is_open()) // 打开词典失败则退出程序 { cerr << \"Unable to open input file: \" << \"wordlexicon\" << \" -- bailing out!\" << endl; exit(-1); } while (getline(infile, strtmp, '\\n')) // 读入词典的每一行并将其添加入哈希中 { istringstream istr(strtmp); istr >> word; //读入每行第一个词 wordhash.insert(sipair(word, 1)); //插入到哈希中 } } CDictionary::~CDictionary() { } //在哈希词典中查找词，若找到，则返回，否则返回 int CDictionary::FindWord(string w) { if (wordhash.find(w) != wordhash.end()) { return 1; } else { return 0; } } // 主程序main.cpp #include “Dictionary.h” # define MaxWordLength 10 // 最大词长为个字节（即个汉字） # define Separator “/ ” // 词界标记 CDictionary WordDic; //初始化一个词典 //对字符串用最大匹配法（正向或逆向）处理 string SegmentSentence(string s1) { string s2 = “”; //用s2存放分词结果 while(!s1.empty()) { int len =(int) s1.length(); // 取输入串长度 if (len > MaxWordLength) // 如果输入串长度大于最大词长 { len = MaxWordLength; // 只在最大词长范围内进行处理 } //string w = s1.substr(0, len); // （正向用）将输入串左边等于最大词长长度串取出作为候选词 string w = s1.substr(s1.length() – len, len); //逆向用 int n = WordDic.FindWord(w); // 在词典中查找相应的词 while(len > 2 && n == 0) // 如果不是词 { len -= 2; // 从候选词右边减掉一个汉字，将剩下的部分作为候选词 //w = w.substr(0, len); //正向用 w = s1.substr(s1.length() – len, len); //逆向用 n = WordDic.FindWord(w); } //s2 += w + Separator; // (正向用）将匹配得到的词连同词界标记加到输出串末尾 w = w + Separator; // (逆向用) s2 = w + s2 ; // (逆向用) //s1 = s1.substr(w.length(), s1.length()); //(正向用)从s1-w处开始 s1 = s1.substr(0, s1.length() – len); // (逆向用) } return s2; } //对句子进行最大匹配法处理，包含对特殊字符的处理 string SegmentSentenceMM (string s1) { string s2 = “”; //用s2存放分词结果 int i; int dd; while(!s1.empty() ) { unsigned char ch = (unsigned char)s1[0]; if (ch < 128) // 处理西文字符 { i = 1; dd = (int)s1.length(); while (i < dd && ((unsigned char)s1[i] < 128) && (s1[i] != 10) && (s1[i] != 13)) // s1[i]不能是换行符或回车符 { i++; } if ((ch != 32) && (ch != 10) && (ch != 13)) // 如果不是西文空格或换行或回车符 { s2 += s1.substr(0,i) + Separator; } else { //if (ch == 10 || ch == 13) // 如果是换行或回车符，将它拷贝给s2输出 if (ch == 10 || ch == 13 || ch == 32) //谢谢读者mces89的指正 { s2 += s1.substr(0, i); } } s1 = s1.substr(i,dd); continue; } else { if (ch < 176) // 中文标点等非汉字字符 { i = 0; dd = (int)s1.length(); while(i < dd && ((unsigned char)s1[i] < 176) && ((unsigned char)s1[i] >= 161) && (!((unsigned char)s1[i] == 161 && ((unsigned char)s1[i+1] >= 162 && (unsigned char)s1[i+1] <= 168))) && (!((unsigned char)s1[i] == 161 && ((unsigned char)s1[i+1] >= 171 && (unsigned char)s1[i+1] <= 191))) && (!((unsigned char)s1[i] == 163 && ((unsigned char)s1[i+1] == 172 || (unsigned char)s1[i+1] == 161) || (unsigned char)s1[i+1] == 168 || (unsigned char)s1[i+1] == 169 || (unsigned char)s1[i+1] == 186 || (unsigned char)s1[i+1] == 187 || (unsigned char)s1[i+1] == 191))) { i = i + 2; // 假定没有半个汉字 } if (i == 0) { i = i + 2; } if (!(ch == 161 && (unsigned char)s1[1] == 161)) // 不处理中文空格 { s2+=s1.substr(0, i) + Separator; // 其他的非汉字双字节字符可能连续输出 } s1 = s1.substr(i, dd); continue; } } // 以下处理汉字串 i = 2; dd = (int)s1.length(); while(i < dd && (unsigned char)s1[i] >= 176) { i += 2; } s2 += SegmentSentence(s1.substr(0, i)); s1 = s1.substr(i,dd); } return s2; } int main(int argc, char *argv[]) { string strtmp; //用于保存从语料库中读入的每一行 string line; //用于输出每一行的结果 ifstream infile(argv[1]); // 打开输入文件 if (!infile.is_open()) // 打开输入文件失败则退出程序 { cerr << \"Unable to open input file: \" << argv[1] << \" -- bailing out!\" << endl; exit(-1); } ofstream outfile1(\"SegmentResult.txt\"); //确定输出文件 if (!outfile1.is_open()) { cerr << \"Unable to open file：SegmentResult.txt\" << \"--bailing out!\" << endl; exit(-1); } while (getline(infile, strtmp, 'n')) //读入语料库中的每一行并用最大匹配法处理 { line = strtmp; line = SegmentSentenceMM(line); // 调用分词函数进行分词处理 outfile1 << line << endl; // 将分词结果写入目标文件 } return 0; } 补充说明：如果使用正向匹配法，请将源代码中的相关注释 “//\"互换。 注：原创文章，转载请注明出处“我爱自然语言处理”：www.52nlp.cn 本文链接地址： http://www.52nlp.cn/maximum-matching-method-of-chinese-word-segmentation/","title":"中文分词入门之最大匹配法"},{"content":" 本文节选自黄昌宁老师和赵海博士在07年第3期《中文信息学报》上发表的《中文分词十年回顾》，旨在介绍目前比较流行的基于字标注的中文分词方法。 　　在2002年之前，自动分词方法基本上是基于词(或词典)的，在此基础上可进一步分成基于规则和基于统计的两大类。第一篇基于字标注(Character-based Tagging)的分词论文发表在2002年第一届SIGHAN研讨会上，当时并未引起学界的重视。一年后，Xue在最大熵(Maximum Entropy，ME)模型上实现的基于字的分词系统参加了Bakeoff-2003的评测，在As语料库的封闭测试项目上获得第二名)，然而其OOV 召回率Roov(0.729)却位居榜首。Xue还在CityU语料库的封闭测试中获得第三名，其Roov(0.670)仍然是该项比赛中最高的。尽管在Bakeoff2003中各种分词技术的优劣尚难分仲伯，但既然未登录词对分词精度的影响比分词歧义至少大5倍以上，我们自然看好这种能获致最高OOV召回的分词方法。这一预测果然在Bakeoff2005上得到了证实。 　　基于字标注的分词系统在Bakeoff-2005上崭露头角。其中Low的系统采用最大熵模型，在四项开放测试中夺得三项冠军(AS，CityU，PKU)和一项亚军(MSRA)。Tseng的系统采用条件随机场模型，在四项封闭测试中取得两项冠军(CityU， MSRA)、一项亚军(PKU)和一项季军(AS)。到了Bakeoff-2006，基于字的分词系统已遍地开花。其中，笔者用条件随机场模型实现的基于字标注的分词系统，在参加的六项分词评测中，夺得四个第一(CityU开放，As开放，As封闭，CTB封闭)和两个第三(CTB开放，CityU封闭)。 　　以往的分词方法，无论是基于规则的还是基于统计的，一般都依赖于一个事先编制的词表(词典)。自动分词过程就是通过词表和相关信息来做出词语切分的决策。与此相反，基于字标注的分词方法实际上是构词方法。即把分词过程视为字在字串中的标注问题。由于每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)，假如规定每个字最多只有四个构词位置：即B(词首)，M (词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式： 　　(甲)分词结果：／上海／计划／N／本／世纪／末／实现／人均／国内／生产／总值／五千美元／。 　　(乙)字标注形式：上／B海／E计／B划／E N／S 本／s世／B 纪／E 末／S 实／B 现／E 人／B 均／E 国／B 内／E生／B产／E总／B值／E 五／B千／M 美／M 元／E 。／S 　　首先需要说明，这里说到的“字”不只限于汉字。考虑到中文真实文本中不可避免地会包含一定数量的非汉字字符，本文所说的“字”，也包括外文字母、阿拉伯数字和标点符号等字符。所有这些字符都是构词的基本单元。当然，汉字依然是这个单元集合中数量最多的一类字符。 　　把分词过程视为字的标注问题的一个重要优势在于，它能够平衡地看待词表词和未登录词的识别问题。在这种分词技术中，文本中的词表词和未登录词都是用统一的字标注过程来实现的。在学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词(如人名、地名、机构名)识别模块。这使得分词系统的设计大大简化。在字标注过程中，所有的字根据预定义的特征进行词位特性的学习，获得一个概率模型。然后，在待分字串上，根据字与字之间的结合紧密程度，得到一个词位的标注结果。最后，根据词位定义直接获得最终的分词结果。总而言之，在这样一个分词过程中，分词成为字重组的简单过程。然而这一简单处理带来的分词结果却是令人满意的。 注：转载请注明出处“我爱自然语言处理”：www.52nlp.cn","title":"基于字标注的中文分词方法"},{"content":" 了解nutch的人基本上对这个开源的系统都是比较欣赏的，起码在国内是这样的，也很有多搜索网站是基于这个系统修改过来的，不过要做得好，做得真正是一个商业化的搜索，这个修改就不是一朝一夕的事情，也不是修修剪剪那么简单了。 作为一个通用的全网级别的搜索引擎架构，nutch(lucene)确实为广大人民群众提供了一块大大的蛋糕，为进入搜索这个行业大大降低了门槛。那么它距商业的搜索到底有多远呢？以我的个人观点来谈一下。   一、总体功能  一个专业的网络搜索引擎至少包含3部分即抓取、处理和搜索。下面是它们的一般功能： 抓取：抓取（蜘蛛、爬虫、crawler、spider等）程序负责爬行特定网络（也可能是整个网络），把网络上的页面和其它需要的文件下载到本地来。目前的难点是web2.0的普及导致的js分析和身份认证等问题。 处理：处理（分类、信息抽取、数据挖掘、classify、information extraction、data mining等）程序对抓回来的页面进行分析，比如，对网站的内容进行分类、对新闻页面的新闻信息进行提取、页面模版生成、对各个网站之间的关系进行计算等等。 搜索：搜索（information retrieve）程序负责把文档填充到数据库，然后根据查询字符串到数据库中寻找最相关的文档展现给用户。   二、信息抓取  网络信息抓取包含了见面抓取、文本文件抓取和其它文件地抓取。普通的信息抓取利用基本的html页面分析器(htmlParser、NeckoHtml、 JTidy等)来解析页面，得到其中的信息。基本上两点，一个是抓取，一个是分析。 抓取这一步要处理身份论证、要支持多种协议等等的，nutch在这里默认的插件使用的是nekohtml，效果还可以。但是nutch对html分析的结果的文本是把页面里所有的文本都合在一起(其中有一个开关来控制内层锚文本是否加上)作为总文本输出，所以这样页面上所有的噪音都没有去除。 另一个是分析，分析一个html，最强的要数ie、firefox等浏览器了，在这一步上，nutch默认的htmlParser的处理能力是不可同日而语的。现在ajax盛行，对于js的处理也是一个重大的问题，现在nutch对js是视而不见的。    三、信息处理  对于信息的处理是nutch最薄弱的环节了，同时也是这个行业里的“宝地”，胜败决定就在这里。这里包括分类、信息抽取、数据挖掘、classify、information extraction、data mining等等。 在默认的nutch组件里有cluster这个包，是用来为搜索结果进行聚类用的，nutch默认的聚类是用开源Carrot2 的后缀树算法做Web文本聚类 还有ontology (本体),是人工智能范畴内的概念。Ontology研究热点的出现与Semantic Web的提出和发展直接相关，借助Ontology中的推理规则，使应用系统具有一定的推理能力。默认的nutch也带了一个简单的ontology的应用系统--HP的jena。但是对于一个商业应用来说这些仅仅是一个模具性质的。 nutch为这方面准备了最最基本的接口，其它的就得自己搞定了，比如机器学习(ML)、自然语言处理(NLP)、数据分析(DA)。。而   四、搜索  nutch其实从功能上来讲是由爬虫和搜索两大部分组成的，搜索是lucene来挑梁的。所以这部分的局限其实就是lucene的局限了。lucene也可能从功能上分为两大部分，一个是索引，一个是查询。对于这部分的研究已经很久了，就是把用户最想要的文档返回给用户，对于搜索引擎而言，速度是非常重要的。索引，专业点说，包含2种：前向索引和反向索引（倒排索引，inverted index）。前者表示的是某个文档里面的所有词语，后者表示的是包含某个词语的所有文档。对应到Lucene上面，它的前向索引可以认为是Term Vectors（词语向量）相关文件，包含.tvx、.tvd和.tvf这3种文件。前向索引没有什么好评论的，它一般只是做为重组原始数据时候的依据，其构建十分简单明了。反向索引对应到Lucene上就是index（索引）。 Lucene把索引划分成一个一个的segment（块，其实是一个小索引），直观的说，当有一批新数据到达的时候，我们一般给其构建成一个新的segment，这是因为修改原来的segment的代价很高（并不是说一定很高，只是lucene采用的文件结构无法简单的加入新的文档）。当一个index包含的segment太多的时候，查找性能就很差了（因为一次查询需要查询多个segment），需要进行segment的合并。在搜索方面nutch对lucene作了外部的处理，一是可以进行分布式搜索，每个节点只返回最高分值的结果，最近再合并；另一方面是对查询进行缓冲，不过只有一级缓冲--LRU(nutch的cache策略及cache策略研究)。   五、结论  仅仅从搜索引擎的构架来看，Lucene(Nutch)缺失的一环是信息的处理。信息的处理恰恰是整个搜索引擎中最核心的技术。所以说对于现在这个行业化、垂直化的搜索时代，nutch 的先天不足就已经是致命的了，但是这并不是不可挽回的，nutch的插件式架构，开放的系统逻辑等等特点，已经为开发者打开了窗户。nutch比较通用的处理逻辑，加上灵活的插件式架构，给我们定制它插上了翅膀。但是它也仅仅是一个框架，里面的任何一个细节都会让你头痛不已(比如ML，NLP)!所以真正的难点也就是这些让人头痛的地方。","title":"Nutch距离一个商业应用的搜索引擎还有多远"},{"content":"  序号 会议名称 会议介绍 代表领域 1 ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu.  http://hpdc13.cs.ucsb.edu 高性能计算 42  NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/  高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications  高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing  该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems  由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing  This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下半年排名。 高性能计算 48 ACM International Conference on Supercomputing  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing  IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 　 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57  FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59  SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60  IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62  IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等概念。 自主计算 63  Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64  International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65  [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66  IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67  USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68  IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69  International Symposium on Computer Architecture（ISCA ） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 70  International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72  IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73  Annual ACM International Conference on Supercomputing（ICS）  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。 高性能计算 74  Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难 操作系统 75  ACM Symposium on Operating Systems Principles （SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难 操作系统 76  Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困难 操作系统，程序语言 77  Workshop on Hot Topics in Operating Systems （HOTOS） 操作系统最好的会议之一 操作系统 78  Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79  Annual IEEE Conference on Local Computer Networks（LCN） 　 网络 80  International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术 121 ECOOP - European Conference on Object-Oriented Programming 关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。 编译技术 122 ESOP - European Symposium on Programming ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。 编译技术 123 Euro-Par - European Conference on Parallel Computing 关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。 　 124 SAS - International Static Analysis Symposium 关注程序的静态分析的权威会议。 编译技术 125 CAV - Computer Aided Verification Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。 编译技术 126 FASE - Fundamental Approaches to Software Engineering ETAPS的member conference，主要关注Software Science，影响因子0.91。 编译技术 127 TACAS - Tools and Algorithms for the Construction and Analysis of Systems ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24 编译技术 128 VMCAI - Verification, Model Checking and Abstract Interpretation Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。 编译技术 129  ACL: The Association for Computational Linguistics 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。 人工智能 计算语言学 130 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索方面最好的会议, ACM 主办, 每年开。19％左右 信息检索技术 131 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining 数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右 　 132 WWW: The ACM International World Wide Web Conference 应用和媒体领域顶级国际会议 万维网 133 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库领域顶级国际 数据管理 134 CIKM: The ACM Conference on Information and Knowledge Management 数据库领域知名国际会议 数据管理 135 COLING: International Conference on Computational Linguistics 计算语言学知名国际会议 计算语言学 136 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 137 IEEE ICDM: International Conference on Data Mining 数据挖掘领域顶级国际会议 　 138 IJCAI: International Joint Conference on Artificial Intelligence 人工智能领域顶级国际会议，论文接受率18％左右 人工智能 139 VLDB: The ACM International Conference on Very Large Data Bases 数据库领域顶级国际 数据库 140 SIGGRAPH: ACM SIGGRAPH Conference 计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％ 计算机图形学 141 EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics 欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20% 计算机图形学 142 AAAI: American Association for Artificial Intelligence 美国人工智能学会AAAI的年会，使该领域的顶级会议 人工智能 143 ACM Conference　on　Computer and　Communications Security ACM通信和计算健全领域顶级学术会议 信息安全 144 ACM SIGCOMM: Special Interest Group on Data Communications 　 数据通信 145 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索领域的重要会议 信息检索 146 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。 通信与网络 147 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems ACM性能建模与评价领域顶级学术会议 通信与网络 148 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。 数据管理 149 ASPLOS: Architectural Support for Programming Languages and Operating Systems 编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。 体系结构 150 CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid 网格计算国际会议，网格平台、中间件 　 151 CIKM: The ACM Conference on Information and Knowledge Management 信息检索领域的会议，录用率为15% 信息检索 152 CLUSTER - IEEE International Conference on Cluster Computing 集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。 集群计算 153 CPM: Combinatorial Pattern Matching Symposium 组合模式匹配年会，是字符串匹配、模式匹配较好的会议。 模式匹配 154 FAST4: Third USENIX Conference on File and Storage Technologies, USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。 文件与存储 155 Grid : IEEE/ACM International Workshop on Grid Computing 网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。 网格计算 156 HPC: IEEE International Conference for High Performance Computing 　 　 157 HPDC: International Symposium on High Performance Distributed Computing 高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。 高性能计算 158 ICDCS: IEEE International Conference on Distributed Computing Systems IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS 举办了26届，录用率为15%。 分布式计算 159 ICML: International Conference on Machine Learning 机器学习领域中的顶级会议 机器学习 160 ICWS: IEEE International Conference on Web Services Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。 Web服务 161 IEEE CSB: Computer Society Bioinformatics 　 　 162 IEEE ICDM: International Conference on Data Mining 数据挖掘领域的著名会议，率用率为14%。 数据挖掘 163 IEEE ICNP: International Conference on Network Protocols IEEE 网络通信领域顶级学术会议，录用率在10%左右。 网络 164 IEEE ICON: IEEE International Conference on Networks 　 　 165 IEEE INFOCOM: conference on computer communications IEEE网络通信领域著名会议，领域广泛。 网络 166 IEEE IPCCC: International Performance Computing and Communications Conference IEEE性能领域著名学术会议，主要关注性能评价。 网络性能 167 IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval 字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。 字符串处理信息检索 168 IJCAI: International Joint Conference on AI 人工智能领域的顶级会议。 人工智能 169 IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。 170 International Symposium on High Performance Computer Architecture(HPCA) 　 服务计算 171 IPDPS: IEEE International Parallel and Distributed Processing Symposium IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%， 会议的优秀论文可能发表于JDPC杂志上。 并行计算分布式计算 172 ISMB: International conference on Intelligent Systems for Molecular Biology 　 无线 173 MobiCom: ACM/IEEE Conference on Mobile Computing and Networking 始于1995，无线、移动计算方面比较有历史和重要的会议。 安全 174 MobiSys: The International Conference on Mobile Systems, Applications, and Services 无线方面，2006年第4名。 无线 175 OSDI: USENIX Symposium on Operating Systems Design and Implementation USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。 操作系统 176 PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining 　 　 177 PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies 关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。 并行计算分布式计算 178 PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases 数据挖掘领域的重要会议，录用率为14%。 数据挖掘 179 SCC: IEEE International Conference on Services Computing 服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。 服务计算 180 SDM: SIAM International Conference on Data Mining 数据挖掘领域的重要会议，录用率为14% 数据挖据 181 SOSE: IEEE International Workshop on Service-Oriented System Engineering 　 　 182 USENIX Sec: USENIX Security Symposium USENIX安全领域重要会议，侧重安全技术。 安全 183 USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI) USENIX网络领域重要会议，设计网络涉及各方面内容。 网络 184 VLDB: The ACM International Conference on Very Large Data Bases 　 数据管理 185 WWW: The ACM International World Wide Web Conference ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。 Internet 186 RAID International Symposium on Recent Advances in Intrusion Detection 数据库顶级国际会议 　 187 IJCAI: International Joint Conference on Artificial Intelligence 人工智能顶级国际会议 人工智能 188 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据库 189 ICML: International Conference on Machine Learning 机器学习顶级国际会议 机器学习 190 PRICAI: Pacific Rim International Conference on Artificial Intelligence 亚太人工智能国际会议 人工智能 191 IFIP ICIIP: IFIP International Conference on Intelligent Information Processing IFIP智能信息处理国际会议 智能信息处理 192 NIPS: Neural Information Processing Systems 神经信息处理领域顶级国际会议 神经计算，机器学习 193 ISCA: International Symposium on Computer Architecture   体系结构领域的顶级会议 微处理器设计 194 International Symposium on Microarchitecture 体系结构领域的顶级会议 微处理器设计 195 HPCA：International Symposium on High Performance Computer Architecture 体系结构领域的顶级会议 微处理器设计 196 APCSAC: Asia-Pacific Computer Systems Architecture Conference  体系结构方面的重要会议 微处理器设计 197 ISLPED: International Symposium on Low Power Electronics and Design 低功耗设计的重要会议 微处理器设计 198 OSDI: Operation System Design & Implementation  操作系统方面的重要会议 微处理器设计 199 ASPLOS: Architecture Support for Programming Languages and Operation  体系结构方面的顶尖会议 微处理器设计 200 ICCD: IEEE International Conference on Computer Design 体系结构方面的顶尖会议 微处理器设计 201 DAC: Design Automation Conference  设计自动化领域的顶级会议 微处理器设计 202 IEEE/ACM International Conference on Computer Aided Design(ICCAD) 集成电路设计自动化方面的顶尖会议 微处理器设计 203 ASP-DAC: Asia and South Pacific Design Automation Conference  设计自动化领域的重要会议 微处理器设计 204 ISSCC: IEEE International Solid-State Circuits Conference 设计自动化领域的重要会议 微处理器设计 205 CICC: Custom Integrated Circuits Conference 集成电路设计方面的顶尖会议（公认排名第二） 微处理器设计 206 ESSCIRC: European Conference on Solid-State Circuits 集成电路设计方面的顶尖会议 微处理器设计 207 Symposium on VLSI Circuits 集成电路设计方面的顶尖会议 微处理器设计 208 IEEE International ASIC/SOC Conference 集成电路设计方面的重要会议 微处理器设计 209 Symposium on VLSI Technology 集成电路设计方面的重要会议 微处理器设计 210 ASSCC: Asian Conference on Solid-State Circuits 集成电路领域重要会议 微处理器设计 211 MWSCAS: Midwest Symposium on Circuits and Systems 集成电路领域重要会议 微处理器设计 212 ICECS: IEEE International Conference on Electronics, Circuits and Systems： 集成电路领域重要会议 微处理器设计 213 ISCAS: International Symposium Circuit and System  电路与系统方面的重要会议 微处理器设计 214 RFIC: IEEE Symposium on Radio Frequency Integrated Circuits  射频集成电路领域顶尖会议 微处理器设计 215 ACM RECOMB: Int. Conference on Research in Computational Molecular Biology RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在20%左右。 计算生物学 216 IEEE CSB: Computer Society Bioinformatics CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主办。 计算生物学 217 International Conference of the IEEE Engineering in Medicine and Biology Society （EMBC） 生物信息学与计算生物学国际研讨会 计算生物学 218 PSB: Pacific Symposium on Biocomputing  PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举办。 计算生物学 219 WABI:Workshop on Algorithms in Bioinformatics WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方面。 计算生物学 220 CSB: IEEE Computational Systems Bioinformatics Conference  CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主办 计算生物学 221 ISMB：Annual International Conference on Intelligent Systems for Molecular Biology ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为Bioinformatics杂志的专刊发表，近年来录取率为15%左右。 生物信息 222 ECCB：European Conference on Computational Biology ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为Bioinformatics杂志的专刊发表，近年录取率在20%左右。 生物信息 223 APBC：Asia Pacific Bioinformatics Conference APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。 生物信息 224 COCOON：Annual International Computing and Combinatorics Conference COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左右。 生物信息 225 CPM：Annual Symposium on Combinatorial Pattern Matching CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左右。 生物信息 226 EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千计。 生物信息 227 Geospatial Information and Technology Association（GITA） Annual Conference 地球空间信息与技术协会年会 遥感与空间信息处理 228 International Geoscience and Remote Sensing Symposium （IGRSS） 地球科学与遥感国际会议 遥感与空间信息处理 229 International Society for Photogrammetry and Remote Sensing （ISPRS）Technical Commission Symposium 国际摄影测量与遥感学会专业委员会会议 遥感与空间信息处理 230 International Conference on Geoinformatics 地球信息国际会议 遥感与空间信息处理 231 IEEE SKG (Semantics, Knowledge and Grid) 由计算所发起的IEEE国际会议，每年有100人参加。 知识网格 232 WWW: The ACM International World Wide Web Conference Internet领域顶级国际会议 Internet 233 International Semantic Web Conference Semantic Web领域顶级会议，录用率17% Semantic Web 234 ACM SIGMOD: ACM SIGMOD Conf on Management of Data ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。 数据管理 235 ACM PODS Conference ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。 数据管理 236 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据管理 237 IEEE ICDE - International Conference on Data Engineering 数据库顶级国际会议 数据管理","title":"计算机学科会议排名"},{"content":"说明：由于我近期工作涉及生物本体，然而国内有关本体的时新文章较少，因此翻译ICBO2011会议的部分论文，与本体领域的研究者和爱好者分享。本人翻译水平有限，如果大家英文水平不错，还是推荐直接看原文。 摘要：尽管人类血液对于临床实践与研究的重要性众所周知，然而血液学与血液注入数据仍然分散在众多的异构资源中。由于对术语的使用与定义缺乏系统化的关注，这样就给医生和生物医学专业人员带来问题。我们这里介绍了血液本体，它是正在进行的一项计划，被设计用来作为可控词汇以组织血液相关的信息。本论文描述了血液本体的使用范围，开发阶段以及一些预期的用途。   关键词：血液学，血液注入，人类血液，人类体液，本体 1 背景 生物医学领域是广阔而又复杂的，医学事实的表达也是一项复杂的任务。医学领域的复杂性与重要性要求表达尽可能与本体提供的表示保持一致。近年出现的丰富的医学本体与开放数据资源的出现，例如Open Biomedical Ontologies Foundry，都证明了该方法对于生命科学的可行性。 在本文中，我们介绍了血液本体（BLO），它是设计用来允许对科学研究及人类血液操作相关信息进行探索的完整架构。BLO是正在长期进行的对血液学领域和血液注入实现知识管理的项目的一部分，它的结构包括下面三个部分： 1）基于本体原则的知识组织 2）来自专家和文本的知识获取 3）可视化工具 在这篇论文中，我们描述了BLO和它当前的开发阶段。   2 方法 BLO包括一个联合相关的本体集合，每一个本体代表一组血液学与血液注入领域相关的条目。这些子本体分别是： 1）BLO-Core：血液学基础的本体 2）BLO-Management：管理血液相关过程的本体 3）BLO-Products：代表血液操作产物的本体 4）BLO-Administrative：规则文档的本体 在当前阶段，BLO-Core的主要工作是： 1）收集血液学领域的术语 2）复用其他本体中可用的数据 3）组织一个有层次的预期的关系研究 知识获取由生物和医学领域的专家来进行，他们都是巴西第二大血库——Hemominas Foundation的成员。我们已经采用了下面的方法： 1）根据Protégé-Frames（基于OGMS）创建的表格进行的测试 2）使用语义wiki保证知识获取的有效性 3）将有效的术语由wiki转换为Protégé-OWL 出于培养本体间协同工作能力的目的，BLO依赖于良好统一的计划，它们主要来自OBO Foundry framework。在OBO的使用范围内，基因本体、蛋白质本体、细胞类型本体等等都是很重要的。BLO还依赖于用作支持科研的顶级本体Basic Formal Ontology所辖基础范围。为了能够聚合这些本体，我们采用了被称为Minimal Information to ReferenceExternal Ontology Terms（MIREOT）的实验方法。   3 结果 这一部分描述了开发每一个BLO子本体的当前阶段，以及未来开发的计划。 BLO-Core焦点集中在血液的生理学方面，它代表了用来进行血液学研究与实践的基本信息。这一本体提供了化学要素、血液的分子免疫细胞基础以及血液紊乱与迁移等基本内容。当前，800多个术语已经被定义并合并入Core子集。 BLO-Management本体涉及血液操作和相关服务等一系列过程。血液操作主要包括下面的工作： 1）质量管理 2）血液利用管理 3）捐赠人选择 4）血液收集 5）注入控制 6）非重读字首省略控制 7）血液检测 这些活动依次涉及设计BLO-Management的一系列过程。BLO的这一分支在开发中涉及了自然语言处理技术，用于处理来自美国联合血库的一小部分文档。 BLO-Products本体旨在面向世界范围内血液操作带来的可能的多种派生产物所引发的挑战。它主要基于对国际定义的标签系统ISBT-128的研究。ISBT-128标准化了血液产物的象征编码，允许它们在全世界的血库和注射服务中是可读的。 像ISBT这样的标准已经证明了实践的重要性，但是它允许了自然语言中常见的模糊性。BLO的这一分支正在开发中，结果是部分的。该研究涉及了用于创建IBST术语和描述的规则的评估，用来检验该标准下的本体决策。 BLO-Administrative本体旨在覆盖互联网上与血库和注射服务相关的所有官方文档中的条目。我们这里说的文档指的是政策，即管理机构、专业联合会、法律、规章制度、官方认可的分类系统和标准。BLO-Administrative被设计符合IAO。这里的结果只是一部分，并涉及到尝试基于注重实效的方法创建一个文档描述。正在被评估的这种文档的例子有：血液捐赠顺序，同意信，质量需求等等。   4 讨论 BLO服务于多个目的，例如：作为开发协同系统的核心词汇，作为计算推论的基础，作为教育目的的知识基础，作为诊断帮助信息的工具。 基于血液成分的诊断的重要性依赖于这样一个事实，即血细胞是源器官中可访问的干扰指示器。患病状态下，血液中任何细胞都可能发展为畸形，对这些细胞的检测会为诊断带来帮助，也对病人有益。 BLO也考虑了人类体液更广阔的角度。值得一提的是BLO与SALO之间的协作。SALO是基于多数同意的与salivaomics域相关的术语与关系的可控词汇。它依赖于由UCLA Salivaomics Research Group开发的唾液诊断技术的研究。 BLO正在被专门研究。例如，Hemominas Foundation研究组对它的使用，该研究组主要研究血液传输的疾病（HIV-1/2，hepatitis B和C等等）、血友病、Von-Willebrand disease、Sickle Cell Anemia and Human T-cell Lymphotropic Virus。   5 结论 在本文中，我们介绍了一项正在进行的计划BLO，它的开发旨在促进对血液数据的访问、使用和分析。无论对医药科学还是临床实践捐献，这一医学领域都是一个广阔并且高度开发的系统。BLO的重要性在于实现它，因为尽管有近年来研究的进步，许多对于人类血液的医学过程仍有许多无法完全理解之处。","title":"血液本体：血液学领域的本体(The Blood Ontology-An Ontology in the Domain of Hematology)"},{"content":"我为什么需要一个个人网站？ 因为个人网站有如下用途： 它可以作为日记与文集。 它可以作为备忘录，记录一些哪怕很琐碎的事情，以便日后查询。 它可以作为较为正式笔记本，记录学习工作中遇到的问题以及解决方法，并且可以与人分享。 它可以作为一个发布机构，告诉世界你的最新动向等等。 它可以作为你的简历（cv）的承载者。 它可以作为你的项目经历的记录，从项目的主页到项目的过程都能用它承载。 它可以作为与世界沟通的渠道，通过留言簿，任何想跟你联系的人都能跟你联系。 它可以做为观察与数据挖掘的材料，你（或其他人）可以通过它了解你是一个怎样的人、一段时间内都做了哪些事情等等。 这么多的作用，使得个人网站的存在非常必要，或许你会问，大多数人都没有个人网站但都活得好好的，不是吗？是的，但是请不要忘记，上面描述的个人网站能做的事情，没有个人网站的人也在做，只不过以相对较糟的方式，比如用纸质日记本来记日记，十年后你可能已经找不到了，或者就算找到了，你也不太可能去仔细看，更别提从中搜索你想要的东西（比如看看某段时间内我都在做些什么想写什么），因此，个人网站比没有个人网站来完成上面那些任务大体上是要好得多。但是个人网站也存在问题，也许也是大家疑惑的地方，比如怎么搭（虽然现在有很多诸如wordpress扥模板，但是技术门槛还是存在的），另外，如果用一些大网站提供的博客（如新浪博客），也存在各种不同的问题（下面提到），更何况，计算机从业者以外的人，还是有很多人并不是很频繁或很喜欢跟电脑打交道的，还有很多包括信息连接阻碍等问题，都是存在的，所以个人网站并不是完美的解决方案。但是现在这种将整个人信息电子化的趋势已经极端明显了，个人网站的类似者就是博客、facebook个人主页等，我建立了个人网站，是因为我能让它做更多。 马上谈到的一个问题是，为什么我不用新浪博客、人人日志、CSDN博客等这种东西，而非要自己创建一个网站？ 其实上面提到的我都是用的，但是它们都存在一些非常致命的问题，包括主题很单一（比如新浪博客、人人日志更偏向于生活与社会，CSDN更偏向于技术），不能满足上面提到的所有需求且难以定制（这一点太大了难以具体描述，也是非常致命的原因），当然，还包括的原因如我是CS出身的，难道还要用它们提供的这种劣质服务吗？然而，在这些大网站提供的平台上发文章的好处在于更容易得到更多的关注（我在CSDN博客上面写的某文章不到一周就达到六千多的访问量，而且用搜索引擎来搜索的时候更容易被排在较高的位置），当然，你的个人网站如果质量高，经过一定时间的积累，还是能够获得搜索引擎的“青睐”的。 是的，有技术背景的同学会问为什么不用wordpress呢，多好的东西，性能、安全做得肯定都比你自己做的好。没错，但是，主要问题还是功能问题，正经的解释是： 1.历史原因，我想做网站的时候对网站制作一无所知，我需要的功能正常的wordpress不能满足，并且，难以通过插件的方式来满足（因为我需要改它数据库的表和属性等），如果一上来就用wordpress来大幅地改，难度太大，费时太多（因为我那时候查到的源码分析的文章太少，而我连html都不会），但如果我自己做一个网站，就能够顺便把这些相关的东西都学一遍，事实也证明，这个目的被很好的达到了。 2.我自己做的chentingpc.me这一个人网站，在文章管理上比wordpress有优势，体现在： 第一，权限管理功能更强，文章是分隐私等级的，wordpress只分“公开”、“密码”、“私人”三个等级，但是我分五个等级，并且，能够区分访客的级别，一定级别的访客可以访问到一定级别隐私程度的文章，从而让诸如日记之类的东西可以被记录在上面； 第二，重要等级做了区分，显然不同的文章有不同的重要等级，但是wordpress数据库中没做这种区分，这种区分的意义不仅在于个人对文章的管理，而且，当别人访问你的网站的时候，你不希望一个公开但不重要的备份的发布映入他的眼帘，在chentingpc.me上面通过文章等级区分并且主页与按钮的存在，可以做到在主页上只看到你推荐到主页的重要文章，在文章版块上既可以看到重要文章，也可以看全部文章，另外，在RSS订阅上也有这样的区分，而wordpress（以及我所知道的所有博客）对此束手无策，从而使得博客的作用变得很单一，一般不重要的东西你都不好意思发上去； 第三，自定义的多级标签机制，wordpress有标签这个概念，但是没有多级标签，多级标签可以便于管理，使得内容的语义划分更灵活、更贴近真实，如果用单层标签，那势必照成很多不同的东西都被柔和在一起（就比如转载、原创与链接），自定义标签还能做更多，比如自然语言处理等等； 第四，从标签中生成目录，wordpress把目录作为我上面所说的第二级标签了，我不仅是这么做的，当然它是二级标签，但是呢，它还可以继续下分，作为一个关键词的挖掘机，比如哲学是一个二级标签，作为一个大目录，但是哲学还可以分为好多类，比如人生哲学、社会哲学等等，你爱怎么分怎么分，只要把在二级标签（如哲学）下分的标签（如人生哲学）与相关的一级标签（如人生哲学、人生等等）关联起来，那么，目录能够非常灵活的被自定义以及生成（现在已经接近凌晨一点，马上笔记本电池耗尽了，所以我不能在此详细描述这个我认为非常漂亮的标签机制了，有兴趣者可以留言交流）； 第五，做链接管理器，当然，链接管理器蛮多的，如google bookmarks、delicious.com等等，但由于网速以及自定义不强等原有，我还是用自己做的这个，并且，wordpress没有这个功能；第六，自己做的网站潜在扩展能力比wordpress强； 第七第八第九。。。。。 其实，我想通过个人网站做（集成）更多的东西，但是我的竞争对手是很强大的，把我的网站的最大用户——我自己——给抢走了，我想做一些个人图片、音乐，乃至个人书籍、电影等等，但是，一方面，这需要大量的时间，而我从自学到写这个网站的0.1版就花了一周时间，而0.22版本出来也就是十来天，另一方面，有许多优秀的网站做的很好而且思路很对、服务也很不错，比如豆瓣的读书、电影、音乐、图片等，比如evernote的私人笔记、草稿等，都把我本来想做的功能抢去做了。但是这我也很高兴，我就直接用他们优秀的免费服务好了，直到他们开始做的很糟糕为止。 总的来说，这个网站还在进化，可能我现在没有多少时间来更新它（指的是框架与代码，内容肯定是经常更新），然而，它总体代表的是一个叫做“个人电子化”的东西，这个趋势现在还在迅猛蔓延（比如教育行业就尚未很好的电子化，APPLE公司就在虎视眈眈中），所以网站以后的趋势还是变化。虽然我可能认为个人战网不会是这个行业未来的前途，但是它们蕴含在chentingpc.me中，两方面都是。","title":"我为什么创建个人网站chentingpc.me"},{"content":"﻿﻿第一章：python自然语言处理 我们可以很容易的获得成千上万的文本。假设我们写一些简单的python程序，能用它做什么？在这个章节中，我们将提出以下问题： 1.我们使用简单的程序设计能对大量的文本做什么？ 2.怎样才能自动的提取关键字和短语来确定一个文本的类型和内容？ 3.python提供什么样的工具和技术来完成像这样的工作？ 4.自然语言处理面临着什么有趣的挑战？ 这一章将分成零碎的完全不相同风格的两部分。在\"计算机语言\"部分，我们将布置一些激发语言兴趣的任务，而不是解释它们如何工作；在“亲近python”部分，会有条理的讲速python的重要概念。我们将会把题目分为两个部分，但是稍后的章节则会混合不同的部分。希望这种介绍方式能让你真正体会到语言学和计算学领域的味道。如果你在这两个领域有良好的基础，可以跳过1.5节。在后面的章节中，我们将会重复一些重要的知识点，如果错过了一些知识点可以在http://www.nltk.org/在线查阅一些资料。这些资料如果对你来说是完全陌生的，这个章节将会带来更多的疑问，而这些问题的答案在本书中可以找到。 1.1计算机语言：文本和文字 因为我们每天都会对文本进行读和写，对文本非常的熟悉。我们可以通过各种有趣的方式来操作和解释这些文本数据。但是在做这些之前，我们不得不先开始了解python. 开始进入python Python使用友好的界面，能够让你直接进入交互式编译器来运行你的程序。Python使用一种叫作IDLE（InteractiveDeveLopmentEnvironment）的简单图形界面。在Unix系统下你可以直接用shell命令来打开IDLE(如果没有安装，现输入python).这时候终端会打印出python的版本，在这里你可以简单的核对一下你的python的版本（这里的是2.5.1）： Python2.5.1 (r251:54863, Apr 15 2008, 22:57:26) [GCC4.0.1 (Apple Inc. build 5465)] on darwin Type\"help\", \"copyright\", \"credits\" or\"license\" for more information. >>> 如果不能运行python解释器，python可能没有被正确的安装。可以访http://python.org/，找到问题的所在。 “>>>”表示python编译器等待输入，书中的样例中的该符号，不需要自己输入。现在开始使用python就像使用计算器那样就可以了： >>>1 + 5 * 2 - 3 8 >>> 编译器立刻完成了计算并先显示了答案，并且再次显示”>>>”,这就意味着python等待输入另一个指令。 你可以自己尝试输入一些表达式，可以使用＊和/进行乘法和除法运算，并且可以输入一些带括号的复数表达式。除法运算的行为可能并不是你意料之中的，当进行整数除法和浮点除法的时候会出现不同的结果，为了获得意料之中的结果，可以输入：from__future__ import division。 通过简单你例子示范了怎样在python解释器中进行工作并通过不同的表达式知道了这种语言能做什么。现在就让我们尝试一下输入一个不完整的表达式，编译器会怎么工作： >>>1 + File \"<stdin>\", line 1 1+ ^ SyntaxError:invalid syntax >>> 这引起了一个syntaxerror。在python中，指令的结尾是一个加号是没有意义的。python编译器会提示出现错误的行数。目前我们已经可以使用python解释器了，就已经做好了用该语言处理数据工作的准备工作。","title":"python自然语言处理第一章1.1"},{"content":"最近要做文本分类相关的课程project，因此上网找了一下文本分类的资料，下面这个感觉比较通俗易懂，收录在这里。 来源  http://www.blogjava.net/zhenandaci/category/31868.html?Show=All 文本分类入 门(一)文本分类问题的定义 文本分类系列文章，从文本分类问题的定义开始，主要讲解文本分类系统的构成，主流的统计学习方法以及较为优秀的SVM算法及其改进。       一个文本（以下基本不区分“文本”和“文档”两个词的含义）分类问题就是将一篇文档归入预先定义的几个类别中的一个或几个，而文本的自动分类则是使用计算机程序来实现这样的分类。通俗点说，就好比你拿一篇文章，问计算机这文章要说的究竟是体育，经济还是教育，计算机答不上就打它的屁屁（……）。 注意这个定义当中着重强调的两个事实。 第一，用于分类所需要的类别体系是预先确定的。例如新浪新闻的分类体系，Yahoo!网页导航的分类层次。这种分类层次一旦确定，在相当长的时间内都是不可变的，或者即使要变更，也要付出相当大的代价（基本不亚于推倒并重建一个分类系统）。 第二，一篇文档并没有严格规定只能被分配给一个类别。这与分类这个问题的主观性有关，例如找10个人判断一篇文章所陈述的主题究竟属于金融，银行还是财政政策领域，10个人可能会给出11个不同的答案（聪明的读者，您应该能看出来并没有11个答案，这只是一种修辞方法，笑），因此一篇文章很可能被分配到多个类别当中，只不过分给某些类别让人信服，而有些让人感觉模棱两可罢了（说的专业点，置信度不一样）。 八股是一种写文章的格式，过去用于科举，现在用于科研，总之，和科学有点关系的文章就得八股，鉴于我正锻炼自己写论文的能力，所以按照标准的格式，陈述了文本分类问题的定义之后，我要说说它的应用范围。 现在一说到文本分类，大部分人想当然的将这个问题简化为判断一篇文章说的是什么，这只是文本分类的一小部分应用，我们可以称之为“依据主题的分类”。实际上，文本分类还可以用于判断文章的写作风格，作者态度（积极？消极？），甚至判断作者真伪（例如看看《红楼梦》最后二十回到底是不是曹雪芹写的）。总而言之，凡是与文本有关，与分类有关，不管从什么角度出发，依据的是何特征，都可以叫做文本分类。 当然，目前真正大量使用文本分类技术的，仍是依据文章主题的分类，而据此构建最多的系统，当属搜索引擎。内里的原因当然不言自明，我只是想给大家提个醒，文本分类还不完全等同于网页分类。网页所包含的信息远比含于其中的文字（文本）信息多得多，对一个网页的分类，除了考虑文本内容的分类以外，链入链出的链接信息，页面文件本身的元数据，甚至是包含此网页的网站结构和主题，都能给分类提供莫大的帮助（比如新浪体育专栏里的网页毫无疑问都是关于体育的），因此说文本分类实际上是网页分类的一个子集也毫不为过。当然，纯粹的文本分类系统与网页分类也不是一点区别都没有。文本分类有个重要前提：即只能根据文章的文字内容进行分类，而不应借助诸如文件的编码格式，文章作者，发布日期等信息。而这些信息对网页来说常常是可用的，有时起到的作用还很巨大！因此纯粹的文本分类系统要想达到相当的分类效果，必须在本身的理论基础和技术含量上下功夫。 除了搜索引擎，诸如数字图书馆，档案管理等等要和海量文字信息打交道的系统，都用得上文本分类。另外，我的硕士论文也用得上（笑）。 下一章和大家侃侃与文本分类有关的具体方法概览，有事您说话。 文本分类入门(二)文本分类的方法 文本分类问题与其它分类问题没有本质上的区别，其方法可以归结为根据待分类数据的某些特征来进行匹配，当然完全的匹配是不太可能的，因此必须（根据某种评价标准）选择最优的匹配结果，从而完成分类。 因此核心的问题便转化为用哪些特征表示一个文本才能保证有效和快速的分类（注意这两方面的需求往往是互相矛盾的）。因此自有文本分类系统的那天起，就一直是对特征的不同选择主导着方法派别的不同。 最早的词匹配法仅仅根据文档中是否出现了与类名相同的词（顶多再加入同义词的处理）来判断文档是否属于某个类别。很显然，这种过于简单的方法无法带来良好的分类效果。 后来兴起过一段时间的知识工程的方法则借助于专业人员的帮助，为每个类别定义大量的推理规则，如果一篇文档能满足这些推理规则，则可以判定属于该类别。这里与特定规则的匹配程度成为了文本的特征。由于在系统中加入了人为判断的因素，准确度比词匹配法大为提高。但这种方法的缺点仍然明显，例如分类的质量严重依赖于这些规则的好坏，也就是依赖于制定规则的“人”的好坏；再比如制定规则的人都是专家级别，人力成本大幅上升常常令人难以承受；而知识工程最致命的弱点是完全不具备可推广性，一个针对金融领域构建的分类系统，如果要扩充到医疗或社会保险等相关领域，则除了完全推倒重来以外没有其他办法，常常造成巨大的知识和资金浪费。 后来人们意识到，究竟依据什么特征来判断文本应当隶属的类别这个问题，就连人类自己都不太回答得清楚，有太多所谓“只可意会，不能言传”的东西在里面。人类的判断大多依据经验以及直觉，因此自然而然的会有人想到何让机器像人类一样自己来通过对大量同类文档的观察来自己总结经验，作为今后分类的依据。 这便是统计学习方法的基本思想（也有人把这一大类方法称为机器学习，两种叫法只是涵盖范围大小有些区别，均无不妥）。 统计学习方法需要一批由人工进行了准确分类的文档作为学习的材料（称为训练集，注意由人分类一批文档比从这些文档中总结出准确的规则成本要低得多），计算机从这些文档重挖掘出一些能够有效分类的规则，这个过程被形象的称为训练，而总结出的规则集合常常被称为分类器。训练完成之后，需要对计算机从来没有见过的文档进行分类时，便使用这些分类器来进行。 现如今，统计学习方法已经成为了文本分类领域绝对的主流。主要的原因在于其中的很多技术拥有坚实的理论基础（相比之下，知识工程方法中专家的主观因素居多），存在明确的评价标准，以及实际表现良好。 下一章就深入统计学习方法，看看这种方法的前提，相关理论和具体实现。 文本分类入门(三)统计学习方法 前文说到使用统计学习方法进行文本分类就是让计算机自己来观察由人提供的训练文档集，自己总结出用于判别文档类别的规则和依据。理想的结果当然是让计算机在理解文章内容的基础上进行这样的分类，然而遗憾的是，我们所说的“理解”往往指的是文章的语义甚至是语用信息，这一类信息极其复杂，抽象，而且存在上下文相关性，对这类信息如何在计算机中表示都是尚未解决的问题（往大里说，这是一个“知识表示”的问题，完全可以另写一系列文章来说了），更不要说让计算机来理解。 利用计算机来解决问题的标准思路应该是：为这种问题寻找一种计算机可以理解的表示方法，或曰建立一个模型（一个文档表示模型）；然后基于这个模型，选择各方面满足要求的算法来解决。用谭浩强的话说，程序，就是数据+算法。（啥？你不知道谭浩强是谁？上过学么？学过C么？这捣什么乱？） 既然文本的语义和语用信息很难转换成计算机能够理解的表示形式，接下来顺理成章的，人们开始用文章中所包含的较低级别的词汇信息来表示文档，一试之下，效果居然还不错。 统计学习方法进行文本分类（以下就简称为“统计学习方法”，虽然这个方法也可以应用到除文本分类以外的多个领域）的一个重要前提由此产生，那就是认为：文档的内容与其中所包含的词有着必然的联系，同一类文档之间总存在多个共同的词，而不同类的文档所包含的词之间差异很大[1]。 进一步的，不光是包含哪些词很重要，这些词出现的次数对分类也很重要。 这一前提使得向量模型（俗称的VSM，向量空间模型）成了适合文本分类问题的文档表示模型。在这种模型中，一篇文章被看作特征项集合来看，利用加权特征项构成向量进行文本表示，利用词频信息对文本特征进行加权。它实现起来比较简单，并且分类准确度也高，能够满足一般应用的要求。[5] 而实际上，文本是一种信息载体，其所携带的信息由几部分组成：如组成元素本身的信息（词的信息）、组成元素之间顺序关系带来的信息以及上下文信息（更严格的说，还包括阅读者本身的背景和理解）[12]。 而VSM这种文档表示模型，基本上完全忽略了除词的信息以外所有的部分，这使得它能表达的信息量存在上限[12]，也直接导致了基于这种模型构建的文本分类系统（虽然这是目前绝对主流的做法），几乎永远也不可能达到人类的分类能力。后面我们也会谈到，相比于所谓的分类算法，对特征的选择，也就是使用哪些特征来代表一篇文档，往往更能影响分类的效果。 对于扩充文档表示模型所包含的信息量，人们也做过有益的尝试，例如被称为LSI（Latent Semantic Index潜在语义索引）的方法，就被实验证明保留了一定的语义信息（之所以说被实验证明了，是因为人们还无法在形式上严格地证明它确实保留了语义信息，而且这种语义信息并非以人可以理解的方式被保留下来），此为后话。 前文说到（就不能不用这种老旧的说法？换换新的，比如Previously on \"Prison Break\"，噢，不对，是Previously on Text Categorizaiton……）统计学习方法其实就是一个两阶段的解决方案，（1）训练阶段，由计算机来总结分类的规则；（2）分类阶段，给计算机一些它从来没见过的文档，让它分类（分不对就打屁屁）。 下一章就专门说说训练阶段的二三事。 文本分类入门(四)训练Part 1 训练，顾名思义，就是training（汗，这解释），简单的说就是让计算机从给定的一堆文档中自己学习分类的规则（如果学不对的话，还要，打屁屁？）。 开始训练之前，再多说几句关于VSM这种文档表示模型的话。 举个例子，假设说把我正在写的“文本分类入门”系列文章的第二篇抽出来当作一个需要分类的文本，则可以用如下的向量来表示这个文本，以便于计算机理解和处理。 w2=（文本，5，统计学习，4，模型，0，……） 这个向量表示在w2所代表的文本中，“文本”这个词出现了5次（这个信息就叫做词频），“统计学习”这个词出现了4次，而“模型”这个词出现了0次，依此类推，后面的词没有列出。 而系列的第三篇文章可以表示为 w3=（文本，9，统计学习，4，模型，10，……） 其含义同上。如果还有更多的文档需要表示，我们都可以使用这种方式。 只通过观察w2和w3我们就可以看出实际上有更方便的表示文本向量的方法，那就是把所有文档都要用到的词从向量中抽离出来，形成共用的数据结构（也可以仍是向量的形式），这个数据结构就叫做词典，或者特征项集合。 例如我们的问题就可以抽离出一个词典向量 D=（文本，统计学习，模型，……） 所有的文档向量均可在参考这个词典向量的基础上简化成诸如 w2=（5，4，0，……） w3=（9，4，10，……） 的形式，其含义没有改变。      5，4，10这些数字分别叫做各个词在某个文档中的权重，实际上单单使用词频作为权重并不多见，也不十分有用，更常见的做法是使用地球人都知道的TF/IDF值作为权重。（关于TF/IDF的详细解释，Google的吴军研究员写了非常通俗易懂的文章，发布于Google黑板报，链接地址是http://googlechinablog.com/2006/06/blog-post_27.html，有兴趣不妨一读）TF/IDF作为一个词对所属文档主题的贡献程度来说，是非常重要的度量标准，也是将文档转化为向量表示过程中的重要一环。       在这个转化过程中隐含了一个很严重的问题。注意看看词典向量D，你觉得它会有多大？或者说，你觉得它会包含多少个词？ 假设我们的系统仅仅处理汉语文本，如果不做任何处理，这个词典向量会包含汉语中所有的词汇，我手头有一本商务印书馆出版的《现代汉语词典》第5版（2005年5月出版），其中收录了65，000个词，D大致也应该有这么大，也就是说，D是一个65，000维的向量，而所有的文本向量w2,w3,wn也全都是65，000维的！（这是文本分类这一问题本身的一个特性，称为“高维性”）想一想，大部分文章仅仅千余字，包含的词至多几百，为了表示这样一个文本，却要使用65，000维的向量，这是对存储资源和计算能力多大的浪费呀！（这又是文本分类问题的另一个特性，称为“向量稀疏性”，后面会专门有一章讨论这些特性，并指出解决的方法，至少是努力的方向） 中国是一个人口众多而资源稀少的国家，我们不提倡一味发展粗放型的经济，我们所需要的可持续发展是指资源消耗少，生产效率高，环境污染少……跑题了…… 这么多的词汇当中，诸如“体育”，“经济”，“金融”，“处理器”等等，都是极其能够代表文章主题的，但另外很多词，像“我们”，“在”，“事情”，“里面”等等，在任何主题的文章中都很常见，根本无法指望通过这些词来对文本类别的归属作个判断。这一事实首先引发了对文本进行被称为“去停止词”的预处理步骤（对英文来说还有词根还原，但这些与训练阶段无关，不赘述，会在以后讲述中英文文本分类方法区别的章节中讨论），与此同时，我们也从词典向量D中把这些词去掉。        但经过停止词处理后剩下的词汇仍然太多，使用了太多的特征来表示文本，就是常说的特征集过大，不仅耗费计算资源，也因为会引起“过拟合问题”而影响分类效果[22]。 这个问题是训练阶段要解决的第一个问题，即如何选取那些最具代表性的词汇（更严格的说法应该是，那些最具代表性的特征，为了便于理解，可以把特征暂时当成词汇来想象）。对这个问题的解决，有人叫它特征提取，也有人叫它降维。 特征提取实际上有两大类方法。一类称为特征选择（Term Selection），指的是从原有的特征（那许多有用无用混在一起的词汇）中提取出少量的，具有代表性的特征，但特征的类型没有变化（原来是一堆词，特征提取后仍是一堆词，数量大大减少了而已）。另一类称为特征抽取（Term Extraction）的方法则有所不同，它从原有的特征中重构出新的特征（原来是一堆词，重构后变成了别的，例如LSI将其转为矩阵，文档生成模型将其转化为某个概率分布的一些参数），新的特征具有更强的代表性，并耗费更少的计算资源。（特征提取的各种算法会有专门章节讨论） 训练阶段，计算机根据训练集中的文档，使用特征提取找出最具代表性的词典向量（仍然是不太严格的说法），然后参照这个词典向量把这些训练集文档转化为向量表示，之后的所有运算便都使用这些向量进行，不再理会原始的文本形式的文档了（换言之，失宠了，后后）。 下一章继续训练，咱们之间还没完。（怎么听着像要找人寻仇似的） 文本分类入门(五)训练Part 2 将样本数据成功转化为向量表示之后，计算机才算开始真正意义上的“学习”过程。 再重复一次，所谓样本，也叫训练数据，是由人工进行分类处理过的文档集合，计算机认为这些数据的分类是绝对正确的，可以信赖的（但某些方法也有针对训练数据可能有错误而应对的措施）。接下来的一步便是由计算机来观察这些训练数据的特点，来猜测一个可能的分类规则（这个分类规则也可以叫做分类器，在机器学习的理论著作中也叫做一个“假设”，因为毕竟是对真实分类规则的一个猜测），一旦这个分类满足一些条件，我们就认为这个分类规则大致正确并且足够好了，便成为训练阶段的最终产品——分类器！再遇到新的，计算机没有见过的文档时，便使用这个分类器来判断新文档的类别。 举一个现实中的例子，人们评价一辆车是否是“好车”的时候，可以看作一个分类问题。我们也可以把一辆车的所有特征提取出来转化为向量形式。在这个问题中词典向量可以为： D=（价格，最高时速，外观得分，性价比，稀有程度） 则一辆保时捷的向量表示就可以写成 vp=（200万，320，9.5，3，9） 而一辆丰田花冠则可以写成 vt=（15万，220，6.0，8，3） 找不同的人来评价哪辆车算好车，很可能会得出不同的结论。务实的人认为性价比才是评判的指标，他会认为丰田花冠是好车而保时捷不是；喜欢奢华的有钱人可能以稀有程度来评判，得出相反的结论；喜欢综合考量的人很可能把各项指标都加权考虑之后才下结论。 可见，对同一个分类问题，用同样的表示形式（同样的文档模型），但因为关注数据不同方面的特性而可能得到不同的结论。这种对文档数据不同方面侧重的不同导致了原理和实现方式都不尽相同的多种方法，每种方法也都对文本分类这个问题本身作了一些有利于自身的假设和简化，这些假设又接下来影响着依据这些方法而得到的分类器最终的表现，可谓环环相连，丝丝入扣，冥冥之中自有天意呀（这都什么词儿……）。          比较常见，家喻户晓，常年被评为国家免检产品（？！）的分类算法有一大堆，什么决策树，Rocchio，朴素贝叶斯，神经网络，支持向量机，线性最小平方拟合，kNN，遗传算法，最大熵，Generalized Instance Set等等等等（这张单子还可以继续列下去）。在这里只挑几个最具代表性的算法侃一侃。 Rocchio算法 Rocchio算法应该算是人们思考文本分类问题时最先能想到，也最符合直觉的解决方法。基本的思路是把一个类别里的样本文档各项取个平均值（例如把所有“体育”类文档中词汇“篮球”出现的次数取个平均值，再把“裁判”取个平均值，依次做下去），可以得到一个新的向量，形象的称之为“质心”，质心就成了这个类别最具代表性的向量表示。再有新文档需要判断的时候，比较新文档和质心有多么相像（八股点说，判断他们之间的距离）就可以确定新文档属不属于这个类。稍微改进一点的Rocchio算法不尽考虑属于这个类别的文档（称为正样本），也考虑不属于这个类别的文档数据（称为负样本），计算出来的质心尽量靠近正样本同时尽量远离负样本。Rocchio算法做了两个很致命的假设，使得它的性能出奇的差。一是它认为一个类别的文档仅仅聚集在一个质心的周围，实际情况往往不是如此（这样的数据称为线性不可分的）；二是它假设训练数据是绝对正确的，因为它没有任何定量衡量样本是否含有噪声的机制，因而也就对错误数据毫无抵抗力。 不过Rocchio产生的分类器很直观，很容易被人类理解，算法也简单，还是有一定的利用价值的（做汉奸状），常常被用来做科研中比较不同算法优劣的基线系统（Base Line）。 朴素贝叶斯算法（Naive Bayes） 贝叶斯算法关注的是文档属于某类别概率。文档属于某个类别的概率等于文档中每个词属于该类别的概率的综合表达式。而每个词属于该类别的概率又在一定程度上可以用这个词在该类别训练文档中出现的次数（词频信息）来粗略估计，因而使得整个计算过程成为可行的。使用朴素贝叶斯算法时，在训练阶段的主要任务就是估计这些值。       朴素贝叶斯算法的公式只有一个 其中P(d| Ci)=P(w1|Ci) P(w2|Ci) …P(wi|Ci) P(w1|Ci) …P(wm|Ci) （式1）       P(wi|Ci)就代表词汇wi属于类别Ci的概率。 这其中就蕴含着朴素贝叶斯算法最大的两个缺陷。 首先，P(d| Ci)之所以能展开成（式1）的连乘积形式，就是假设一篇文章中的各个词之间是彼此独立的，其中一个词的出现丝毫不受另一个词的影响（回忆一下概率论中变量彼此独立的概念就可以知道），但这显然不对，即使不是语言学专家的我们也知道，词语之间有明显的所谓“共现”关系，在不同主题的文章中，可能共现的次数或频率有变化，但彼此间绝对谈不上独立。 其二，使用某个词在某个类别训练文档中出现的次数来估计P(wi|Ci)时，只在训练样本数量非常多的情况下才比较准确（考虑扔硬币的问题，得通过大量观察才能基本得出正反面出现的概率都是二分之一的结论，观察次数太少时很可能得到错误的答案），而需要大量样本的要求不仅给前期人工分类的工作带来更高要求（从而成本上升），在后期由计算机处理的时候也对存储和计算资源提出了更高的要求。 kNN算法       kNN算法则又有所不同，在kNN算法看来，训练样本就代表了类别的准确信息（因此此算法产生的分类器也叫做“基于实例”的分类器），而不管样本是使用什么特征表示的。其基本思想是在给定新文档后，计算新文档特征向量和训练文档集中各个文档的向量的相似度，得到K篇与该新文档距离最近最相似的文档，根据这K篇文档所属的类别判定新文档所属的类别（注意这也意味着kNN算法根本没有真正意义上的“训练”阶段）。这种判断方法很好的克服了Rocchio算法中无法处理线性不可分问题的缺陷，也很适用于分类标准随时会产生变化的需求（只要删除旧训练文档，添加新训练文档，就改变了分类的准则）。        kNN唯一的也可以说最致命的缺点就是判断一篇新文档的类别时，需要把它与现存的所有训练文档全都比较一遍，这个计算代价并不是每个系统都能够承受的（比如我将要构建的一个文本分类系统，上万个类，每个类即便只有20个训练样本，为了判断一个新文档的类别，也要做20万次的向量比较！）。一些基于kNN的改良方法比如Generalized Instance Set就在试图解决这个问题。 下一节继续讲和训练阶段有关的话题，包括概述已知性能最好的SVM算法。明儿见！（北京人儿，呵呵） 文本分类入门(六)训练Part 3 SVM算法 支持向量机(Support Vector Machine)是Cortes和Vapnik于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中[10]。 支持向量机方法是建立在统计学习理论的VC 维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力[14]（或称泛化能力）。       SVM 方法有很坚实的理论基础，SVM 训练的本质是解决一个二次规划问题（Quadruple Programming，指目标函数为二次函数，约束条件为线性约束的最优化问题），得到的是全局最优解，这使它有着其他统计学习技术难以比拟的优越性。SVM 分类器的文本分类效果很好，是最好的分类器之一。同时使用核函数将原始的样本空间向高维空间进行变换，能够解决原始样本线性不可分的问题。其缺点是核函数的选择缺乏指导，难以针对具体问题选择最佳的核函数；另外SVM 训练速度极大地受到训练集规模的影响，计算开销比较大，针对SVM 的训练速度问题，研究者提出了很多改进方法，包括Chunking 方法、Osuna 算法、SMO 算法和交互SVM 等等[14]。       SVM分类器的优点在于通用性较好，且分类精度高、分类速度快、分类速度与训练样本个数无关，在查准和查全率方面都优于kNN及朴素贝叶斯方法[8]。 与其它算法相比，SVM算法的理论基础较为复杂，但应用前景很广，我打算专门写一个系列的文章，详细的讨论SVM算法，stay tuned！ 介绍过了几个很具代表性的算法之后，不妨用国内外的几组实验数据来比较一下他们的优劣。 在中文语料上的试验，文献[6]使用了复旦大学自然语言处理实验室提供的基准语料对当前的基于词向量空间文本模型的几种分类算法进行了测试，这一基准语料分为20个类别，共有9804篇训练文档，以及9833篇测试文档。在经过统一的分词处理、噪声词消除等预处理之后，各个分类方法的性能指标如下。 其中F1 测度是一种综合了查准率与召回率的指标，只有当两个值均比较大的时候，对应的F1测度才比较大，因此是比单一的查准或召回率更加具有代表性的指标。 由比较结果不难看出，SVM和kNN明显优于朴素贝叶斯方法（但他们也都优于Rocchio方法，这种方法已经很少再参加评测了）。 在英文语料上，路透社的Reuters-21578 “ModApt´e”是比较常用的测试集，在这个测试集上的测试由很多人做过，Sebastiani在文献[23]中做了总结，相关算法的结果摘录如下： 分类算法在Reuters-21578 “ModApt´e”上的F1测度:       Rocchio  0.776 朴素贝叶斯  0.795       kNN  0.823       SVM  0.864 仅以F1测度来看，kNN是相当接近SVM算法的，但F1只反映了分类效果（即分类分得准不准），而没有考虑性能（即分类分得快不快）。综合而论，SVM是效果和性能均不错的算法。 前面也提到过，训练阶段的最终产物就是分类器，分类阶段仅仅是使用这些分类器对新来的文档分类而已，没有过多可说的东西。 下一章节是对到目前为止出现过的概念的列表及简单的解释，也会引入一些后面会用到的概念。再之后会谈及分类问题本身的分类（绕口），中英文分类问题的相似与不同之处以及几种特征提取算法的概述和比较，路漫漫…… 文本分类入门(七)相关概念总结 学习方法：使用样例（或称样本，训练集）来合成计算机程序的过程称为学习方法[22]。 监督学习：学习过程中使用的样例是由输入/输出对给出时，称为监督学习[22]。最典型的监督学习例子就是文本分类问题，训练集是一些已经明确分好了类别文档组成，文档就是输入，对应的类别就是输出。 非监督学习：学习过程中使用的样例不包含输入/输出对，学习的任务是理解数据产生的过程 [22]。典型的非监督学习例子是聚类，类别的数量，名称，事先全都没有确定，由计算机自己观察样例来总结得出。       TSR（Term Space Reduction）：特征空间的压缩，即降维，也可以叫做特征提取。包括特征选择和特征抽取两大类方法。 分类状态得分（CSV，Categorization Status Value)：用于描述将文档归于某个类别下有多大的可信度。 准确率（Precision）：在所有被判断为正确的文档中，有多大比例是确实正确的。 召回率（Recall）：在所有确实正确的文档中，有多大比例被我们判为正确。 假设：计算机对训练集背后的真实模型（真实的分类规则）的猜测称为假设。可以把真实的分类规则想像为一个目标函数，我们的假设则是另一个函数，假设函数在所有的训练数据上都得出与真实函数相同（或足够接近）的结果。 泛化性：一个假设能够正确分类训练集之外数据（即新的，未知的数据）的能力称为该假设的泛化性[22]。 一致假设：一个假设能够对所有训练数据正确分类，则称这个假设是一致的[22]。 过拟合：为了得到一致假设而使假设变得过度复杂称为过拟合[22]。想像某种学习算法产生了一个过拟合的分类器，这个分类器能够百分之百的正确分类样本数据（即再拿样本中的文档来给它，它绝对不会分错），但也就为了能够对样本完全正确的分类，使得它的构造如此精细复杂，规则如此严格，以至于任何与样本数据稍有不同的文档它全都认为不属于这个类别！ 超平面（Hyper Plane）：n维空间中的线性函数唯一确定了一个超平面。一些较直观的例子，在二维空间中，一条直线就是一个超平面；在三维空间中，一个平面就是一个超平面。 线性可分和不可分：如果存在一个超平面能够正确分类训练数据，并且这个程序保证收敛，这种情况称为线形可分。如果这样的超平面不存在，则称数据是线性不可分的[22]。 正样本和负样本：对某个类别来说，属于这个类别的样本文档称为正样本；不属于这个类别的文档称为负样本。 规划：对于目标函数，等式或不等式约束都是线性函数的问题称为线性规划问题。对于目标函数是二次的，而约束都是线性函数的最优化问题称为二次规划问题[22]。 对偶问题： 给定一个带约束的优化问题 目标函数：min f(x) 约束条件：C(x) ≥0 可以通过拉格朗日乘子构造拉格朗日函数       L(x,λ)=f(x)- λTC(x) 令g(λ)= f(x)- λTC(x) 则原问题可以转化为 目标函数：max g(λ) 约束条件：λ≥0 这个新的优化问题就称为原问题的对偶问题（两个问题在取得最优解时达到的条件相同）。 文本分类入门(八)中英文文本分类的异同 从文本分类系统的处理流程来看，无论待分类的文本是中文还是英文，在训练阶段之前都要经过一个预处理的步骤，去除无用的信息，减少后续步骤的复杂度和计算负担。 对中文文本来说，首先要经历一个分词的过程，就是把连续的文字流切分成一个一个单独的词汇（因为词汇将作为训练阶段“特征”的最基本单位），例如原文是“中华人民共和国今天成立了”的文本就要被切分成“中华／人民／共和国／今天／成立／了”这样的形式。而对英文来说，没有这个步骤（更严格的说，并不是没有这个步骤，而是英文只需要通过空格和标点便很容易将一个一个独立的词从原文中区分出来）。中文分词的效果对文本分类系统的表现影响很大，因为在后面的流程中，全都使用预处理之后的文本信息，不再参考原始文本，因此分词的效果不好，等同于引入了错误的训练数据。分词本身也是一个值得大书特书的问题，目前比较常用的方法有词典法，隐马尔科夫模型和新兴的CRF方法。 预处理中在分词之后的“去停止词”一步对两者来说是相同的，都是要把语言中一些表意能力很差的辅助性文字从原始文本中去除，对中文文本来说，类似“我们”，“在”，“了”，“的”这样的词汇都会被去除，英文中的“ an”，“in”，“the”等也一样。这一步骤会参照一个被称为“停止词表”的数据（里面记录了应该被去除的词，有可能是以文件形式存储在硬盘上，也有可能是以数据结构形式放在内存中）来进行。 对中文文本来说，到此就已初审合格，可以参加训练了（笑）。而英文文本还有进一步简化和压缩的空间。我们都知道，英文中同一个词有所谓词形的变化（相对的，词义本身却并没有变），例如名词有单复数的变化，动词有时态的变化，形容词有比较级的变化等等，还包括这些变化形式的某种组合。而正因为词义本身没有变化，仅仅词形不同的词就不应该作为独立的词来存储和和参与分类计算。去除这些词形不同，但词义相同的词，仅保留一个副本的步骤就称为“词根还原”，例如在一篇英文文档中，经过词根还原后，“computer”，“compute”，“computing”，“computational”这些词全都被处理成“compute”（大小写转换也在这一步完成，当然，还要记下这些词的数目作为compute的词频信息）。 经过预处理步骤之后，原始文档转换成了非常节省资源，也便于计算的形式，后面的训练阶段大同小异（仅仅抽取出的特征不同而已，毕竟，一个是中文词汇的集合，一个是英文词汇的集合嘛）。 下一章节侃侃分类问题本身的分类。 文本分类入门(九)文本分类问题的分类 开始之前首先说说分类体系。回忆一下，分类体系是指事先确定的类别的层次结构以及文档与这些类别间的关系。 其中包含着两方面的内容： 一，类别之间的关系。一般来说类别之间的关系都是可以表示成树形结构，这意味着一个类有多个子类，而一个子类唯一的属于一个父类。这种类别体系很常用，却并不代表它在现实世界中也是符合常识的，举个例子，“临床心理学”这个类别应该即属于“临床医学”的范畴，同时也属于“心理学”，但在分类系统中却不便于使用这样的结构。想象一下，这相当于类别的层次结构是一个有环图，无论遍历还是今后类别的合并，比较，都会带来无数的麻烦。 二，文档与类别间的关系。一般来说，在分类系统中，我们倾向于让一篇文档唯一的属于一个类别（更严格的说，是在同一层次中仅属于一个类别，因为属于一个类别的时候，显然也属于这个类别的父类别），这使得我们只适用一个标签就可以标记这个文档的类别，而一旦允许文档属于多个类别，标签的数目便成为大小不定的变量，难于设计成高效的数据结构。这种“属于多个”类的想法更糟的地方在于文档类别表示的语义方面，试想，如果姚明给灾区捐款的新闻即属于灾区新闻，也属于体育新闻的话（这在现实中倒确实是合情合理的），当用户使用这个系统来查找文档，指定的条件是要所有“属于灾区新闻但不属于体育新闻的新闻”（有点拗口，不过正好练嘴皮子啦，笑）的时候，这篇姚明的报道是否应该包含在查询结果中呢？这是一个矛盾的问题。 文本分类问题牵涉到如此多的主题，本身又含有如此多的属性，因此可以从多个角度对文本分类问题本身进行一下分类。 分类系统使用何种分类算法是分类系统的核心属性。如果一个分类算法在一次分类判断时，仅仅输出一个真假值用来表示待分类的文档是否属于当前类别的话，这样的系统就可以叫做基于二元分类器的分类系统。有些分类算法天然就是独立二元的，例如支持向量机，它只能回答这个文档是或不是这个类别的。这种分类算法也常常被称为“硬分类”的算法（Hard Categorization）。而有的算法在一次判断后就可以输出文档属于多个类别的得分（假设说，得分越大，则说明越有可能属于这个类别），这类算法称为“排序分类”的算法（Ranking Categorization），也叫做m元分类算法。kNN就是典型的m元分类算法（因为kNN会找出与待分类文档最相近的训练样本，并记录下这些样本所属的分类）。 文本分类入门（十）特征选择算法之开方检验 前文提到过，除了分类算法以外，为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种，这次先介绍特征选择算法中效果比较好的开方检验方法。 大家应该还记得，开方检验其实是数理统计中一种常用的检验两个变量独立性的方法。（什么？你是文史类专业的学生，没有学过数理统计？那你做什么文本分类？在这捣什么乱？） 开方检验最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。具体做的时候常常先假设两个变量确实是独立的（行话就叫做“原假设”），然后观察实际值（也可以叫做观察值）与理论值（这个理论值是指“如果两者确实独立”的情况下应该有的值）的偏差程度，如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。 那么用什么来衡量偏差程度呢？假设理论值为E（这也是数学期望的符号哦），实际值为x，如果仅仅使用所有样本的观察值与理论值的差值x-E之和 来衡量，单个的观察值还好说，当有多个观察值x1，x2，x3的时候，很可能x1-E，x2-E，x3-E的值有正有负，因而互相抵消，使得最终的结果看上好像偏差为0，但实际上每个都有偏差，而且都还不小！此时很直接的想法便是使用方差代替均值，这样就解决了正负抵消的问题，即使用 这时又引来了新的问题，对于500的均值来说，相差5其实是很小的（相差1%），而对20的均值来说，5相当于25%的差异，这是使用方差也无法体现的。因此应该考虑改进上面的式子，让均值的大小不影响我们对差异程度的判断 式（1） 上面这个式子已经相当好了。实际上这个式子就是开方检验使用的差值衡量公式。当提供了数个样本的观察值x1，x2，……xi ，……xn之后，代入到式（1）中就可以求得开方值，用这个值与事先设定的阈值比较，如果大于阈值（即偏差很大），就认为原假设不成立，反之则认为原假设成立。 在文本分类问题的特征选择阶段，我们主要关心一个词t（一个随机变量）与一个类别c（另一个随机变量）之间是否相互独立？如果独立，就可以说词t对类别c完全没有表征作用，即我们根本无法根据t出现与否来判断一篇文档是否属于c这个分类。但与最普通的开方检验不同，我们不需要设定阈值，因为很难说词t和类别c关联到什么程度才算是有表征作用，我们只想借用这个方法来选出一些最最相关的即可。 此时我们仍然需要明白对特征选择来说原假设是什么，因为计算出的开方值越大，说明对原假设的偏离越大，我们越倾向于认为原假设的反面情况是正确的。我们能不能把原假设定为“词t与类别c相关“？原则上说当然可以，这也是一个健全的民主主义社会赋予每个公民的权利（笑），但此时你会发现根本不知道此时的理论值该是多少！你会把自己绕进死胡同。所以我们一般都使用”词t与类别c不相关“来做原假设。选择的过程也变成了为每个词计算它与类别c的开方值，从大到小排个序（此时开方值越大越相关），取前k个就可以（k值可以根据自己的需要选，这也是一个健全的民主主义社会赋予每个公民的权利）。 好，原理有了，该来个例子说说到底怎么算了。 比如说现在有N篇文档，其中有M篇是关于体育的，我们想考察一个词“篮球”与类别“体育”之间的相关性（任谁都看得出来两者很相关，但很遗憾，我们是智慧生物，计算机不是，它一点也看不出来，想让它认识到这一点，只能让它算算看）。我们有四个观察值可以使用：      1. 包含“篮球”且属于“体育”类别的文档数，命名为A      2. 包含“篮球”但不属于“体育”类别的文档数，命名为B      3. 不包含“篮球”但却属于“体育”类别的文档数，命名为C      4.  既不包含“篮球”也不属于“体育”类别的文档数，命名为D 如果有些特点你没看出来，那我说一说，首先，A+B+C+D=N（这，这不废话嘛）。其次，A+C的意思其实就是说“属于体育类的文章数量”，因此，它就等于M，同时，B+D就等于N-M。 好，那么理论值是什么呢？以包含“篮球”且属于“体育”类别的文档数为例。如果原假设是成立的，即“篮球”和体育类文章没什么关联性，那么在所有的文章中，“篮球”这个词都应该是等概率出现，而不管文章是不是体育类的。这个概率具体是多少，我们并不知道，但他应该体现在观察结果中（就好比抛硬币的概率是二分之一，可以通过观察多次抛的结果来大致确定），因此我们可以说这个概率接近 （因为A+B是包含“篮球”的文章数，除以总文档数就是“篮球”出现的概率，当然，这里认为在一篇文章中出现即可，而不管出现了几次）而属于体育类的文章数为A+C，在这些个文档中，应该有 篇包含“篮球”这个词（数量乘以概率嘛）。 但实际有多少呢？考考你（读者：切，当然是A啦，表格里写着嘛……）。 此时对这种情况的差值就得出了（套用式（1）的公式），应该是 同样，我们还可以计算剩下三种情况的差值D12，D21，D22，聪明的读者一定能自己算出来（读者：切，明明是自己懒得写了……）。有了所有观察值的差值，就可以计算“篮球”与“体育”类文章的开方值 把D11，D12，D21，D22的值分别代入并化简，可以得到 词t与类别c的开方值更一般的形式可以写成   式（2） 接下来我们就可以计算其他词如“排球”，“产品”，“银行”等等与体育类别的开方值，然后根据大小来排序，选择我们需要的最大的数个词汇作为特征项就可以了。 实际上式（2）还可以进一步化简，注意如果给定了一个文档集合（例如我们的训练集）和一个类别，则N，M，N-M（即A+C和B+D）对同一类别文档中的所有词来说都是一样的，而我们只关心一堆词对某个类别的开方值的大小顺序，而并不关心具体的值，因此把它们从式（2）中去掉是完全可以的，故实际计算的时候我们都使用   式（3） 好啦，并不高深对不对？ 针对英文纯文本的实验结果表明：作为特征选择方法时，开方检验和信息增益的效果最佳（相同的分类算法，使用不同的特征选择算法来得到比较结果）；文档频率方法的性能同前两者大体相当，术语强度方法性能一般；互信息方法的性能最差（文献[17]）。 但开方检验也并非就十全十美了。回头想想A和B的值是怎么得出来的，它统计文档中是否出现词t，却不管t在该文档中出现了几次，这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。甚至会出现有些情况，一个词在一类文章的每篇文档中都只出现了一次，其开方值却大过了在该类文章99%的文档中出现了10次的词，其实后面的词才是更具代表性的，但只因为它出现的文档数比前面的词少了“1”，特征选择的时候就可能筛掉后面的词而保留了前者。这就是开方检验著名的“低频词缺陷“。因此开方检验也经常同其他因素如词频综合考虑来扬长避短。 好啦，关于开方检验先说这么多，有机会还将介绍其他的特征选择算法。 附：给精通统计学的同学多说几句，式（1）实际上是对连续型的随机变量的差值计算公式，而我们这里统计的“文档数量“显然是离散的数值（全是整数），因此真正在统计学中计算的时候，是有修正过程的，但这种修正仍然是只影响具体的开方值，而不影响大小的顺序，故文本分类中不做这种修正。 文本分类入门（十一）特征选择方法之信息增益 从以上讨论中可以看出，信息增益也是考虑了特征出现和不出现两种情况，与开方检验一样，是比较全面的，因而效果不错。但信息增益最大的问题还在于它只能考察特征对整个系统的贡献，而不能具体到某个类别上，这就使得它只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而无法做“本地”的特征选择（每个类别有自己的特征集合，因为有的词，对这个类别很有区分度，对另一个类别则无足轻重）。 看看，导出的过程其实很简单，没有什么神秘的对不对。可有的学术论文里就喜欢把这种本来很直白的东西写得很晦涩，仿佛只有读者看不懂才是作者的真正成功。 咱们是新一代的学者，咱们没有知识不怕被别人看出来，咱们有知识也不怕教给别人。所以咱都把事情说简单点，说明白点，大家好，才是真的好。 文本分类入门（十一）特征选择方法之信息增益 前文提到过，除了开方检验（CHI）以外，信息增益（IG，Information Gain）也是很有效的特征选择方法。但凡是特征选择，总是在将特征的重要程度量化之后再进行选择，而如何量化特征的重要性，就成了各种方法间最大的不同。开方检验中使用特征与类别间的关联性来进行这个量化，关联性越强，特征得分越高，该特征越应该被保留。 在信息增益中，重要性的衡量标准就是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。 因此先回忆一下信息论中有关信息量（就是“熵”）的定义。说有这么一个变量X，它可能的取值有n多种，分别是x1，x2，……，xn，每一种取到的概率分别是P1，P2，……，Pn，那么X的熵就定义为： 意思就是一个变量可能的变化越多（反而跟变量具体的取值没有任何关系，只和值的种类多少以及发生概率有关），它携带的信息量就越大（因此我一直觉得我们的政策法规信息量非常大，因为它变化很多，基本朝令夕改，笑）。 对分类系统来说，类别C是变量，它可能的取值是C1，C2，……，Cn，而每一个类别出现的概率是P(C1)，P(C2)，……，P(Cn)，因此n就是类别的总数。此时分类系统的熵就可以表示为： 有同学说不好理解呀，这样想就好了，文本分类系统的作用就是输出一个表示文本属于哪个类别的值，而这个值可能是C1，C2，……，Cn，因此这个值所携带的信息量就是上式中的这么多。 信息增益是针对一个一个的特征而言的，就是看一个特征t，系统有它和没它的时候信息量各是多少，两者的差值就是这个特征给系统带来的信息量，即增益。系统含有特征t的时候信息量很好计算，就是刚才的式子，它表示的是包含所有特征时系统的信息量。 问题是当系统不包含t时，信息量如何计算？我们换个角度想问题，把系统要做的事情想象成这样：说教室里有很多座位，学生们每次上课进来的时候可以随便坐，因而变化是很大的（无数种可能的座次情况）；但是现在有一个座位，看黑板很清楚，听老师讲也很清楚，于是校长的小舅子的姐姐的女儿托关系（真辗转啊），把这个座位定下来了，每次只能给她坐，别人不行，此时情况怎样？对于座次的可能情况来说，我们很容易看出以下两种情况是等价的：（1）教室里没有这个座位；（2）教室里虽然有这个座位，但其他人不能坐（因为反正它也不能参与到变化中来，它是不变的）。 对应到我们的系统中，就是下面的等价：（1）系统不包含特征t；（2）系统虽然包含特征t，但是t已经固定了，不能变化。 我们计算分类系统不包含特征t的时候，就使用情况（2）来代替，就是计算当一个特征t不能变化时，系统的信息量是多少。这个信息量其实也有专门的名称，就叫做“条件熵”，条件嘛，自然就是指“t已经固定“这个条件。 但是问题接踵而至，例如一个特征X，它可能的取值有n多种（x1，x2，……，xn），当计算条件熵而需要把它固定的时候，要把它固定在哪一个值上呢？答案是每一种可能都要固定一下，计算n个值，然后取均值才是条件熵。而取均值也不是简单的加一加然后除以n，而是要用每个值出现的概率来算平均（简单理解，就是一个值出现的可能性比较大，固定在它上面时算出来的信息量占的比重就要多一些）。 因此有这样两个条件熵的表达式： 这是指特征X被固定为值xi时的条件熵， 这是指特征X被固定时的条件熵，注意与上式在意义上的区别。从刚才计算均值的讨论可以看出来，第二个式子与第一个式子的关系就是： 具体到我们文本分类系统中的特征t，t有几个可能的值呢？注意t是指一个固定的特征，比如他就是指关键词“经济”或者“体育”，当我们说特征“经济”可能的取值时，实际上只有两个，“经济”要么出现，要么不出现。一般的，t的取值只有t（代表t出现）和（代表t不出现），注意系统包含t但t 不出现与系统根本不包含t可是两回事。 因此固定t时系统的条件熵就有了，为了区别t出现时的符号与特征t本身的符号，我们用T代表特征，而用t代表T出现，那么： 与刚才的式子对照一下，含义很清楚对吧，P(t)就是T出现的概率，就是T不出现的概率。这个式子可以进一步展开，其中的 另一半就可以展开为： 因此特征T给系统带来的信息增益就可以写成系统原本的熵与固定特征T后的条件熵之差： 公式中的东西看上去很多，其实也都很好计算。比如P(Ci)，表示类别Ci出现的概率，其实只要用1除以类别总数就得到了（这是说你平等的看待每个类别而忽略它们的大小时这样算，如果考虑了大小就要把大小的影响加进去）。再比如P(t)，就是特征T出现的概率，只要用出现过T的文档数除以总文档数就可以了，再比如P(Ci|t)表示出现T的时候，类别Ci出现的概率，只要用出现了T并且属于类别Ci的文档数除以出现了T的文档数就可以了。 从以上讨论中可以看出，信息增益也是考虑了特征出现和不出现两种情况，与开方检验一样，是比较全面的，因而效果不错。但信息增益最大的问题还在于它只能考察特征对整个系统的贡献，而不能具体到某个类别上，这就使得它只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而无法做“本地”的特征选择（每个类别有自己的特征集合，因为有的词，对这个类别很有区分度，对另一个类别则无足轻重）。 看看，导出的过程其实很简单，没有什么神秘的对不对。可有的学术论文里就喜欢把这种本来很直白的东西写得很晦涩，仿佛只有读者看不懂才是作者的真正成功。 咱们是新一代的学者，咱们没有知识不怕被别人看出来，咱们有知识也不怕教给别人。所以咱都把事情说简单点，说明白点，大家好，才是真的好。 文本分类入门（番外篇）特征选择与特征权重计算的区别 在文本分类的过程中，特征（也可以简单的理解为“词”）从人类能够理解的形式转换为计算机能够理解的形式时，实际上经过了两步骤的量化——特征选择阶段的重要程度量化和将具体文本转化为向量时的特征权重量化。初次接触文本分类的人很容易混淆这两个步骤使用的方法和各自的目的，因而我经常听到读者有类似“如何使用TFIDF做特征选择”或者“卡方检验量化权重后每篇文章都一样”等等困惑。 文本分类本质上也是一个模式识别的问题，因此我想借用一个更直观的例子来说说特征选择和权重量化到底各自是什么东西，当然，一旦解释清楚，你马上就会觉得文本分类这东西实在白痴，实在没什么技术含量，你也就不会再继续看我的技术博客，不过我不担心，因为你已经踏上了更光明的道路（笑），我高兴还来不及。 想想通过指纹来识别一个人的身份，只看一个人的指纹，当然说不出他姓甚名谁，识别的过程实际上是比对的过程，要与已有的指纹库比较，找出相同的，或者说相似到一定程度的那一个。 首要的问题是，人的指纹太复杂，包含太多的位置和几何形状，要完全重现一个人的指纹，存储和计算都是大麻烦。因此第一步总是一个特征选择的问题，我们把全人类的指纹都统计一下，看看哪几个位置能够最好的区分不同的人。显然不同的位置效果很不一样，在有的位置上，我的指纹是是什么形状，其他人也大都是这个形状，这个位置就不具有区分度，或者说不具有表征性，或者说，对分类问题来说，它的重要程度低。这样的位置我们就倾向于在识别的时候根本不看它，不考虑它。 那怎么看谁重要谁不重要呢？这就依赖于具体的选择方法如何来量化重要程度，对卡方检验和信息增益这类方法来说，量化以后的得分越大的特征就越重要（也就是说，有可能有些方法，是得分越小的越重要）。 比如说你看10个位置，他们的重要程度分别是：    1 2   3   4   5 6   7 8 9  10 （20，5，10，20，30，15，4，3，7， 3） 显然第1，第3，4，5，6个位置比其他位置更重要，而相对的，第1个位置又比第3个位置更重要。 识别时，我们只在那些重要的位置上采样。当今的指纹识别系统，大都只用到人指纹的5个位置（惊讶么？只要5个位置的信息就可以区分60亿人），这5个位置就是经过特征选择过程而得以保留的系统特征集合。假设这个就是刚才的例子，那么该集合应该是： （第1个位置，第3个位置，第4个位置，第5个位置，第6个位置） 当然，具体的第3个位置是指纹中的哪个位置你自己总得清楚。 确定了这5个位置之后，就可以把一个人的指纹映射到这个只有5个维度的空间中，我们就把他在5个位置上的几何形状分别转换成一个具体的值，这就是特征权重的计算。依据什么来转换，就是你选择的特征权重量化方法，在文本分类中，最常用的就是TFIDF。 我想一定是“权重“这个词误导了所有人，让大家以为TFIDF计算出的值代表的是特征的重要程度，其实完全不是。例如我们有一位男同学，他的指纹向量是： （10，3，4，20，5） 你注意到他第1个位置的得分（10）比第3个位置的得分（3）高，那么能说第1个位置比第3个位置重要么？如果再有一位女同学，她的指纹向量是： （10，20，4，20，5） 看看，第1个位置得分（10）又比第3个位置（20）低了，那这两个位置到底哪个更重要呢？答案是第1个位置更重要，但这不是在特征权重计算这一步体现出来的，而是在我们特征选择的时候就确定了，第1个位置比第3个位置更重要。 因此要记住，通过TFIDF计算一个特征的权重时，该权重体现出的根本不是特征的重要程度！ 那它代表什么？再看看两位同学的指纹，放到一起： （10， 3，4，20，5） （10，20，4，20，5） 在第三个位置上女同学的权重高于男同学，这不代表该女同学在指纹的这个位置上更“优秀“（毕竟，指纹还有什么优秀不优秀的分别么，笑），也不代表她的这个位置比男同学的这个位置更重要，3和20这两个得分，仅仅代表他们的”不同“。 在文本分类中也是如此，比如我们的系统特征集合只有两个词： （经济，发展） 这两个词是使用卡方检验（特征选择）选出来的，有一篇文章的向量形式是 （2，5） 另一篇 （3，4） 这两个向量形式就是用TFIDF算出来的，很容易看出两篇文章不是同一篇，为什么？因为他们的特征权重根本不一样，所以说权重代表的是差别，而不是优劣。想想你说“经济这个词在第二篇文章中得分高，因此它在第二篇文章中比在第一篇文章中更重要“，这句话代表什么意义呢？你自己都不知道吧（笑）。 所以，当再说起使用TFIDF来计算特征权重时，最好把“权重“这个字眼忘掉，我们就把它说成计算得分好了（甚至”得分“也不太好，因为人总会不自觉的认为，得分高的就更重要），或者就仅仅说成是量化。 如此，你就再也不会拿TFIDF去做特征选择了。 小Tips：为什么有的论文里确实使用了TFIDF作特征选择呢？ 严格说来并不是不可以，而且严格说来只要有一种方法能够从一堆特征中挑出少数的一些，它就可以叫做一种特征选择方法，就连“随机选取一部分“都算是一种，而且效果并没有差到惊人的地步哦！还是可以分对一大半的哦！所以有的人就用TFIDF的得分来把特征排排序，取得分最大的几个进入系统特征集合，效果也还行（毕竟，连随机选取效果也都还行），怎么说呢，他们愿意这么干就这么干吧。就像咱国家非得实行户口制度，这个制度说不出任何道理，也不见他带来任何好处，但不也没影响二十一世纪成为中国的世纪么，呵呵。","title":"文本分类入门"},{"content":"终于把书看到传说中重要的第六章了。。。 看完第六章开始后悔之前花那么多时间看前四章内容了。。看了也忘掉了。。什么困惑度啊什么的一堆概念还是要翻一翻。。之前总怕不仔细看后面的看不懂。。结果重要的第五章第六章反而看的比前面愉快了许多~主要还是概率的知识，几乎都是条件概率，条件概率公式、全概率公式和贝叶斯公式用的挺多，当然还有独立性。。下面不废话了。。具体说一下。。 【心得及问题】 1、N元语法的n体现了该词间的独立性，n越小独立性越强。则可根据不同语料的独立性特点选择不同的模型了。通常n=3。直观上讲，第i位置的词与前面多少个词的相关性并不一定，另外，“词”是一个笼统的概念（可以代表字、词短语等），它的选取也不确定，而一个模型直接赋予n一个确定的值，这本身是一种近似。所以说，模型不可能精确表达，根据这种局限性，一个好的模型的重要性就可想而知了。模型提出后也要检验，至少要满足已知定理，比如n元语法模型就加上了<BOS> 和<EOS>，以使i-1有意义并满足概率的归一性。 2、P75：刚开始没看懂计算P(Wi|Wi-1)的公式。是因为求和符号下面的Wi理解为施加给i了，思维定势，而Wi-1是不变的。其实就是条件概率公式P(A|B) = P(AB)/P(B)约掉了样本空间里元素的个数而已。 3、P77：之前看平滑方法的时候想比较一下，看看优缺点，但是一堆公式看起来很费劲。计算了一下，发现使用加法平滑算法时，概率的变大还是变小取决于（b-a|V|）的正负（a为原来概率的分子，b为原来概率的分母）。感觉词汇表的容量|V|通常应该很大，即使a比b小也有可能大多数情况该值为负，即概率变小，那么概率的归一性怎么保证？这点没有想明白。另外关于平滑还有一个问题，说平滑的基本思想是“劫富济贫”，不明白为什么。不让概率为零的实际意义书上有解释，而为什么要让概率分布均匀就不晓得了，没百度出来。。后面的平滑方法看不懂也看不下去了，想着将来用到再看，毕竟我发现纯理论的东西看了跟没看一样，知道有这么回事就行了。况且最后有人做过实验比较，具有指导意义。 4、P95：P(q1,q2,……,qt)的第二个等式刚开始感觉应该用约等。后来想想，感觉它的意思应该是基于马尔可夫模型吧，模型已经是近似了，而在该模型中就是假定该模型是准确的吧，然后与前面其他的状态无关，所以就写等号了。 5、隐马尔可夫模型不过是一个双重的随机过程，书上小球的例子举得很直观，理解没有困难，将马尔可夫模型的应用范围扩大到潜在事件随即生成表面时间的情况。 6、前向、后向（这个还要实现以下，之后附上代码。。）包括维特比算法，个人认为关键是动态规划的思想，所以总结在下面。之前连动态规划是什么都不知道。。汗颜。。这也体现了算法的重要性啊！ 【思想小结】 1、动态规划：就是为了避免重复计算浪费时间，通常把计算过的量给保留起来，下次则直接查找，无需计算。就是一种空间换时间的感觉。。当然计算量要是很小就不必要这样啦。。那个维特比算法让我想起来Dijkstra的单源最短路径那个算法，有点像呢。而且都是设了一个变量来进行路径的记录。（而路径记录要是我刚开始肯定会想到链表或者数组。。效率啊。。人家一个变量就解决了，最后递归输出一个逆序就行了~）所以说，算法有很多，也有很多是相通的，算法思想才是最重要的。 2、模型思想：呵呵，刚才正好说到算法思想，模型思想也是如此，感觉这是一种抽象的能力。这世界上万事万物那么多，谁有时间和精力都去研究一遍呢，况且研究难度也很大。而模型的思想就是一种抽象思想，把一个问题抽象成一类问题，一个好的模型就能解决一系列问题了，也是一种对一系列问题的归类和总结，是透过现象看本质的能力。咦。。扯到哲学了。。言归正传，这同时也是理论与实际的完美结合。一个模型建立的时候，你不能不考虑理论因素，比如不能与公理定理等客观真理相违背（感受第一点加<EOS>就是这样）。而建立之后，又要经得起检验，不能与人的基本常识（当然是可证明的客观常识）或者实际经验和需求等相违背（比如平滑方法的提出，概率为零导致语音识别错误）。所以提出和提出之后都要经过检验和优化，提出新的方法甚至新的模型，这便是研究一个模型的价值吧。 附前向算法（注意代码风格呀！！那个*写成+让我Debug了一天啊有木有！！）： #include <stdio.h>#include <stdlib.h>int main(){    int i, n, j;    int array[100]= {0};    double result=0;    float init[5] = {0.1,0.3,0.2,0.2,0.2};    float state[5][5] = { {0,0.2,0.4,0.1,0.3}, {0.2,0.8,0,0,0}, {0.5,0.1,0.1,0.2,0.1}, \\        {0.2,0.3,0.1,0.3,0.1}, {0,0.1,0.2,0.4,0.3}    };    double forward[100][5];    float signal[5][3] = { {0.5,0.3,0.2}, {0.1,0.1,0.8}, {0,0,1}, {0.4,0.5,0.1}, {0.7,0,0.3}};    printf(\"请输入观察序列的长度：\\n\");    scanf(\"%d\", &n);    for(i=0; i<n; i++)    {        printf(\"请输入第%d个观察到的颜色序号并按回车结束\\n\", i+1);        scanf(\"%d\", &array[i]);        if(array[i]<1 || array[i]>3)        {            printf(\"颜色只有1、2、3！请重新输入！\\n\\n\");            i--;        }    }    system(\"cls\");    printf(\"您输入的序列为：\\n\");    for(i=0; i<n; i++)    {        printf(\"%d  \", array[i]);    }    for(i=0; i<5; i++)    {        forward[0][i] = init[i]*signal[i][array[0]-1];        printf(\"forward[0][%d]: %lf  \", i, forward[0][i]);    }    for(i=1; i<n; i++)    {        for(j=0; j<5; j++)        {            forward[i][j] = (forward[i-1][0]*state[0][j]+forward[i-1][1]*state[1][j]+ \\                             forward[i-1][2]*state[2][j]+forward[i-1][3]*state[3][j]+ \\                             forward[i-1][4]|*state[4][j])*signal[j][array[i]-1];//            printf(\"\\nforward[%d][%d]: %lf\\n\", i, j, forward[i][j]);        }    }    for(i=0; i<5; i++)    {//        printf(\"%lf  \", forward[n-1][i]);        result += forward[n-1][i];    }    printf(\"\\n该观察序列的概率为：%lf\\n\", result);    return 0;} 后向算法（感觉书上算法第三步写错了。。害我苦思冥想到底和第一个状态有什么关系想了好久。。。）： #include <stdio.h>#include <stdlib.h>int main(){    int i, j, n;    int array[100] = {0};    double result=0;    float init[5] = {0.1,0.3,0.2,0.2,0.2};    float state[5][5] = { {0,0.2,0.4,0.1,0.3}, {0.2,0.8,0,0,0}, {0.5,0.1,0.1,0.2,0.1}, \\        {0.2,0.3,0.1,0.3,0.1}, {0,0.1,0.2,0.4,0.3}    };    double back[100][5];    float signal[5][3] = { {0.5,0.3,0.2}, {0.1,0.1,0.8}, {0,0,1}, {0.4,0.5,0.1}, {0.7,0,0.3}};    printf(\"请输入观察序列的长度：\\n\");    scanf(\"%d\", &n);    for(i=0; i<n; i++)    {        printf(\"请输入第%d个观察到的颜色序号并按回车结束\\n\", i+1);        scanf(\"%d\", &array[i]);        if(array[i]<1 || array[i]>3)        {            printf(\"颜色只有1、2、3！请重新输入！\\n\\n\");            i--;        }    }    system(\"cls\");    printf(\"您输入的序列为：\\n\");    for(i=0; i<n; i++)    {        printf(\"%d  \", array[i]);    }    for(i=0; i<5; i++)    {        back[n][i] = 1;    }    for(i=n-1; i>0; i--)    {        for(j=0; j<5; j++)        {            back[i][j] = state[j][0]*signal[0][array[i]-1]*back[i+1][0] + \\            state[j][1]*signal[1][array[i]-1]*back[i+1][1] + \\            state[j][2]*signal[2][array[i]-1]*back[i+1][2] + \\            state[j][3]*signal[3][array[i]-1]*back[i+1][3] + \\            state[j][4]*signal[4][array[i]-1]*back[i+1][4];        }    }    for(i=0; i<5; i++)    {        result += back[1][i]*init[i]*signal[i][array[0]-1];    }        printf(\"\\n该观察序列的概率为：%lf\\n\", result);    return 0;}","title":"统计自然语言处理——n元语法（马尔可夫模型）小结"},{"content":"翻译自：http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html 转载自： http://www.52nlp.cn/hmm-learn-best-practices-one-introduction 隐马尔科夫模型（HMM）依然是读者访问“我爱自然语言处理”的一个热门相关关键词，我曾在《HMM学习最佳范例与崔晓源的博客》中介绍过国外的一个不错的HMM学习教程，并且国内崔晓源师兄有一个相应的翻译版本，不过这个版本比较简化和粗略，有些地方只是概况性的翻译了一下，省去了一些内容，所以从今天开始计划在52nlp上系统的重新翻译这个学习教程，希望对大家有点用。 一、介绍（Introduction） 　　我们通常都习惯寻找一个事物在一段时间里的变化模式（规律）。这些模式发生在很多领域，比如计算机中的指令序列，句子中的词语顺序和口语单词中的音素序列等等，事实上任何领域中的一系列事件都有可能产生有用的模式。 　　考虑一个简单的例子，有人试图通过一片海藻推断天气——民间传说告诉我们‘湿透的’海藻意味着潮湿阴雨，而‘干燥的’海藻则意味着阳光灿烂。如果它处于一个中间状态（‘有湿气’），我们就无法确定天气如何。然而，天气的状态并没有受限于海藻的状态，所以我们可以在观察的基础上预测天气是雨天或晴天的可能性。另一个有用的线索是前一天的天气状态（或者，至少是它的可能状态）——通过综合昨天的天气及相应观察到的海藻状态，我们有可能更好的预测今天的天气。 　　这是本教程中我们将考虑的一个典型的系统类型。 　　首先，我们将介绍产生概率模式的系统，如晴天及雨天间的天气波动。 　　然后，我们将会看到这样一个系统，我们希望预测的状态并不是观察到的——其底层系统是隐藏的。在上面的例子中，观察到的序列将是海藻而隐藏的系统将是实际的天气。 　　最后，我们会利用已经建立的模型解决一些实际的问题。对于上述例子，我们想知道： 　　1. 给出一个星期每天的海藻观察状态，之后的天气将会是什么? 　　2. 给定一个海藻的观察状态序列，预测一下此时是冬季还是夏季？直观地，如果一段时间内海藻都是干燥的，那么这段时间很可能是夏季，反之，如果一段时间内海藻都是潮湿的，那么这段时间可能是冬季。 二、生成模式（Generating Patterns） 1、确定性模式（Deterministic Patterns） 　　考虑一套交通信号灯，灯的颜色变化序列依次是红色-红色/黄色-绿色-黄色-红色。这个序列可以作为一个状态机器，交通信号灯的不同状态都紧跟着上一个状态。 　　　　 　　注意每一个状态都是唯一的依赖于前一个状态，所以，如果交通灯为绿色，那么下一个颜色状态将始终是黄色——也就是说，该系统是确定性的。确定性系统相对比较容易理解和分析，因为状态间的转移是完全已知的。 2、非确定性模式（Non-deterministic patterns） 　　为了使天气那个例子更符合实际，加入第三个状态——多云。与交通信号灯例子不同，我们并不期望这三个天气状态之间的变化是确定性的，但是我们依然希望对这个系统建模以便生成一个天气变化模式（规律）。 　　一种做法是假设模型的当前状态仅仅依赖于前面的几个状态，这被称为马尔科夫假设，它极大地简化了问题。显然，这可能是一种粗糙的假设，并且因此可能将一些非常重要的信息丢失。 　　当考虑天气问题时，马尔科夫假设假定今天的天气只能通过过去几天已知的天气情况进行预测——而对于其他因素，譬如风力、气压等则没有考虑。在这个例子以及其他相似的例子中，这样的假设显然是不现实的。然而，由于这样经过简化的系统可以用来分析，我们常常接受这样的知识假设，虽然它产生的某些信息不完全准确。 　　　　　　　　　　　　 　　一个马尔科夫过程是状态间的转移仅依赖于前n个状态的过程。这个过程被称之为n阶马尔科夫模型，其中n是影响下一个状态选择的（前）n个状态。最简单的马尔科夫过程是一阶模型，它的状态选择仅与前一个状态有关。这里要注意它与确定性系统并不相同，因为下一个状态的选择由相应的概率决定，并不是确定性的。 　　下图是天气例子中状态间所有可能的一阶状态转移情况： 　　　　 　　对于有M个状态的一阶马尔科夫模型，共有个状态转移，因为任何一个状态都有可能是所有状态的下一个转移状态。每一个状态转移都有一个概率值，称为状态转移概率——这是从一个状态转移到另一个状态的概率。所有的个概率可以用一个状态转移矩阵表示。注意这些概率并不随时间变化而不同——这是一个非常重要（但常常不符合实际）的假设。 　　下面的状态转移矩阵显示的是天气例子中可能的状态转移概率： 　　　　 　　-也就是说，如果昨天是晴天，那么今天是晴天的概率为0.5，是多云的概率为0.375。注意，每一行的概率之和为1。 　　要初始化这样一个系统，我们需要确定起始日天气的（或可能的）情况，定义其为一个初始概率向量，称为向量。 　　　　　　　　　　 　　-也就是说，第一天为晴天的概率为1。 　　现在我们定义一个一阶马尔科夫过程如下： 　　　状态：三个状态——晴天，多云，雨天。 　　　向量：定义系统初始化时每一个状态的概率。 　　　状态转移矩阵：给定前一天天气情况下的当前天气概率。 　　任何一个可以用这种方式描述的系统都是一个马尔科夫过程。 3、总结 　　我们尝试识别时间变化中的模式，并且为了达到这个目的我们试图对这个过程建模以便产生这样的模式。我们使用了离散时间点、离散状态以及做了马尔科夫假设。在采用了这些假设之后，系统产生了这个被描述为马尔科夫过程的模式，它包含了一个向量（初始概率）和一个状态转移矩阵。关于假设，重要的一点是状态转移矩阵并不随时间的改变而改变——这个矩阵在整个系统的生命周期中是固定不变的。 三、隐藏模式（Hidden Patterns） 1、马尔科夫过程的局限性 　　在某些情况下，我们希望找到的模式用马尔科夫过程描述还显得不充分。回顾一下天气那个例子，一个隐士也许不能够直接获取到天气的观察情况，但是他有一些水藻。民间传说告诉我们水藻的状态与天气状态有一定的概率关系——天气和水藻的状态是紧密相关的。在这个例子中我们有两组状态，观察的状态（水藻的状态）和隐藏的状态（天气的状态）。我们希望为隐士设计一种算法，在不能够直接观察天气的情况下，通过水藻和马尔科夫假设来预测天气。 　　一个更实际的问题是语音识别，我们听到的声音是来自于声带、喉咙大小、舌头位置以及其他一些东西的组合结果。所有这些因素相互作用产生一个单词的声音，一套语音识别系统检测的声音就是来自于个人发音时身体内部物理变化所引起的不断改变的声音。 　　一些语音识别装置工作的原理是将内部的语音产出看作是隐藏的状态，而将声音结果作为一系列观察的状态，这些由语音过程生成并且最好的近似了实际（隐藏）的状态。在这两个例子中，需要着重指出的是，隐藏状态的数目与观察状态的数目可以是不同的。一个包含三个状态的天气系统（晴天、多云、雨天）中，可以观察到4个等级的海藻湿润情况（干、稍干、潮湿、湿润）；纯粹的语音可以由80个音素描述，而身体的发音系统会产生出不同数目的声音，或者比80多，或者比80少。 　　在这种情况下，观察到的状态序列与隐藏过程有一定的概率关系。我们使用隐马尔科夫模型对这样的过程建模，这个模型包含了一个底层隐藏的随时间改变的马尔科夫过程，以及一个与隐藏状态某种程度相关的可观察到的状态集合。 2、隐马尔科夫模型（Hidden Markov Models） 　　下图显示的是天气例子中的隐藏状态和观察状态。假设隐藏状态（实际的天气）由一个简单的一阶马尔科夫过程描述，那么它们之间都相互连接。 　　 　　隐藏状态和观察状态之间的连接表示：在给定的马尔科夫过程中，一个特定的隐藏状态生成特定的观察状态的概率。这很清晰的表示了‘进入’一个观察状态的所有概率之和为1，在上面这个例子中就是Pr(Obs|Sun), Pr(Obs|Cloud) 及 Pr(Obs|Rain)之和。（对这句话我有点疑惑？） 　　除了定义了马尔科夫过程的概率关系，我们还有另一个矩阵，定义为混淆矩阵（confusion matrix），它包含了给定一个隐藏状态后得到的观察状态的概率。对于天气例子，混淆矩阵是： 　　 　　注意矩阵的每一行之和是1。 3、总结（Summary） 　　我们已经看到在一些过程中一个观察序列与一个底层马尔科夫过程是概率相关的。在这些例子中，观察状态的数目可以和隐藏状态的数码不同。 　　我们使用一个隐马尔科夫模型（HMM）对这些例子建模。这个模型包含两组状态集合和三组概率集合： 　　* 隐藏状态：一个系统的（真实）状态，可以由一个马尔科夫过程进行描述（例如，天气）。 　　* 观察状态：在这个过程中‘可视’的状态（例如，海藻的湿度）。 　　* 向量：包含了（隐）模型在时间t=1时一个特殊的隐藏状态的概率（初始概率）。 　　* 状态转移矩阵：包含了一个隐藏状态到另一个隐藏状态的概率 　　* 混淆矩阵：包含了给定隐马尔科夫模型的某一个特殊的隐藏状态，观察到的某个观察状态的概率。 　　因此一个隐马尔科夫模型是在一个标准的马尔科夫过程中引入一组观察状态，以及其与隐藏状态间的一些概率关系。 四、隐马尔科夫模型（Hidden Markov Models） 1、定义（Definition of a hidden Markov model） 　　一个隐马尔科夫模型是一个三元组（, A, B）。 　　：初始化概率向量； 　　：状态转移矩阵； 　　：混淆矩阵； 　　在状态转移矩阵及混淆矩阵中的每一个概率都是时间无关的——也就是说，当系统演化时这些矩阵并不随时间改变。实际上，这是马尔科夫模型关于真实世界最不现实的一个假设。 2、应用（Uses associated with HMMs） 　　一旦一个系统可以作为HMM被描述，就可以用来解决三个基本问题。其中前两个是模式识别的问题：给定HMM求一个观察序列的概率（评估）；搜索最有可能生成一个观察序列的隐藏状态序列（解码）。第三个问题是给定观察序列生成一个HMM（学习）。 　a) 评估（Evaluation） 　　考虑这样的问题，我们有一些描述不同系统的隐马尔科夫模型（也就是一些( ,A,B)三元组的集合）及一个观察序列。我们想知道哪一个HMM最有可能产生了这个给定的观察序列。例如，对于海藻来说，我们也许会有一个“夏季”模型和一个“冬季”模型，因为不同季节之间的情况是不同的——我们也许想根据海藻湿度的观察序列来确定当前的季节。 　　我们使用前向算法（forward algorithm）来计算给定隐马尔科夫模型（HMM）后的一个观察序列的概率，并因此选择最合适的隐马尔科夫模型(HMM)。 　　在语音识别中这种类型的问题发生在当一大堆数目的马尔科夫模型被使用，并且每一个模型都对一个特殊的单词进行建模时。一个观察序列从一个发音单词中形成，并且通过寻找对于此观察序列最有可能的隐马尔科夫模型（HMM）识别这个单词。 　b) 解码（ Decoding） 　　给定观察序列搜索最可能的隐藏状态序列。 　　另一个相关问题，也是最感兴趣的一个，就是搜索生成输出序列的隐藏状态序列。在许多情况下我们对于模型中的隐藏状态更感兴趣，因为它们代表了一些更有价值的东西，而这些东西通常不能直接观察到。 　　考虑海藻和天气这个例子，一个盲人隐士只能感觉到海藻的状态，但是他更想知道天气的情况，天气状态在这里就是隐藏状态。 　　我们使用Viterbi 算法（Viterbi algorithm）确定（搜索）已知观察序列及HMM下最可能的隐藏状态序列。 　　Viterbi算法（Viterbi algorithm）的另一广泛应用是自然语言处理中的词性标注。在词性标注中，句子中的单词是观察状态，词性（语法类别）是隐藏状态（注意对于许多单词，如wind，fish拥有不止一个词性）。对于每句话中的单词，通过搜索其最可能的隐藏状态，我们就可以在给定的上下文中找到每个单词最可能的词性标注。 　C）学习（Learning） 　　根据观察序列生成隐马尔科夫模型。 　　第三个问题，也是与HMM相关的问题中最难的，根据一个观察序列（来自于已知的集合），以及与其有关的一个隐藏状态集，估计一个最合适的隐马尔科夫模型（HMM），也就是确定对已知序列描述的最合适的（,A,B）三元组。 　　当矩阵A和B不能够直接被（估计）测量时，前向-后向算法（forward-backward algorithm）被用来进行学习（参数估计），这也是实际应用中常见的情况。 3、总结（Summary） 　　由一个向量和两个矩阵(,A,B)描述的隐马尔科夫模型对于实际系统有着巨大的价值，虽然经常只是一种近似，但它们却是经得起分析的。隐马尔科夫模型通常解决的问题包括： 　　1. 对于一个观察序列匹配最可能的系统——评估，使用前向算法（forward algorithm）解决； 　　2. 对于已生成的一个观察序列，确定最可能的隐藏状态序列——解码，使用Viterbi 算法（Viterbi algorithm）解决； 　　3. 对于已生成的观察序列，决定最可能的模型参数——学习，使用前向-后向算法（forward-backward algorithm）解决。 五、前向算法（Forward Algorithm） 计算观察序列的概率（Finding the probability of an observed sequence） 1.穷举搜索（ Exhaustive search for solution） 　　给定隐马尔科夫模型，也就是在模型参数（, A, B)已知的情况下，我们想找到观察序列的概率。还是考虑天气这个例子，我们有一个用来描述天气及与它密切相关的海藻湿度状态的隐马尔科夫模型(HMM)，另外我们还有一个海藻的湿度状态观察序列。假设连续3天海藻湿度的观察结果是（干燥、湿润、湿透）——而这三天每一天都可能是晴天、多云或下雨，对于观察序列以及隐藏的状态，可以将其视为网格： 　　网格中的每一列都显示了可能的的天气状态，并且每一列中的每个状态都与相邻列中的每一个状态相连。而其状态间的转移都由状态转移矩阵提供一个概率。在每一列下面都是某个时间点上的观察状态，给定任一个隐藏状态所得到的观察状态的概率由混淆矩阵提供。 　　可以看出，一种计算观察序列概率的方法是找到每一个可能的隐藏状态，并且将这些隐藏状态下的观察序列概率相加。对于上面那个（天气）例子，将有3^3 = 27种不同的天气序列可能性，因此，观察序列的概率是： 　　Pr(dry,damp,soggy | HMM) = Pr(dry,damp,soggy | sunny,sunny,sunny) + Pr(dry,damp,soggy | sunny,sunny ,cloudy) + Pr(dry,damp,soggy | sunny,sunny ,rainy) + . . . . Pr(dry,damp,soggy | rainy,rainy ,rainy) 　　用这种方式计算观察序列概率极为昂贵，特别对于大的模型或较长的序列，因此我们可以利用这些概率的时间不变性来减少问题的复杂度。 2.使用递归降低问题复杂度 　　给定一个隐马尔科夫模型（HMM），我们将考虑递归地计算一个观察序列的概率。我们首先定义局部概率（partial probability）,它是到达网格中的某个中间状态时的概率。然后，我们将介绍如何在t=1和t=n(>1)时计算这些局部概率。 　　假设一个T-长观察序列是： 　　　　　 　　 　2a.局部概率(‘s) 　　考虑下面这个网格，它显示的是天气状态及对于观察序列干燥，湿润及湿透的一阶状态转移情况： 　　　 　　我们可以将计算到达网格中某个中间状态的概率作为所有到达这个状态的可能路径的概率求和问题。 　　例如，t=2时位于“多云”状态的局部概率通过如下路径计算得出： 　　　 　　我们定义t时刻位于状态j的局部概率为at(j)——这个局部概率计算如下： 　　t ( j )= Pr( 观察状态 | 隐藏状态j ) x Pr(t时刻所有指向j状态的路径） 　　对于最后的观察状态，其局部概率包括了通过所有可能的路径到达这些状态的概率——例如，对于上述网格，最终的局部概率通过如下路径计算得出： 　　　 　　由此可见，对于这些最终局部概率求和等价于对于网格中所有可能的路径概率求和，也就求出了给定隐马尔科夫模型(HMM)后的观察序列概率。 　　第3节给出了一个计算这些概率的动态示例。 五、前向算法（Forward Algorithm） 计算观察序列的概率（Finding the probability of an observed sequence） 2b.计算t=1时的局部概率‘s 　　我们按如下公式计算局部概率： 　　t ( j )= Pr( 观察状态 | 隐藏状态j ) x Pr(t时刻所有指向j状态的路径） 　　特别当t=1时，没有任何指向当前状态的路径。故t=1时位于当前状态的概率是初始概率，即Pr(state|t=1)=P(state)，因此，t=1时的局部概率等于当前状态的初始概率乘以相关的观察概率： 　　　　　　　　　 　　所以初始时刻状态j的局部概率依赖于此状态的初始概率及相应时刻我们所见的观察概率。 2c.计算t>1时的局部概率‘s 　　我们再次回顾局部概率的计算公式如下： 　　t ( j )= Pr( 观察状态 | 隐藏状态j ) x Pr(t时刻所有指向j状态的路径） 　　我们可以假设（递归地），乘号左边项“Pr( 观察状态 | 隐藏状态j )”已经有了，现在考虑其右边项“Pr(t时刻所有指向j状态的路径）”。 　　为了计算到达某个状态的所有路径的概率，我们可以计算到达此状态的每条路径的概率并对它们求和，例如： 　　　　　　 　　计算所需要的路径数目随着观察序列的增加而指数级递增，但是t-1时刻‘s给出了所有到达此状态的前一路径概率，因此，我们可以通过t-1时刻的局部概率定义t时刻的‘s，即： 　　　　　 　　故我们所计算的这个概率等于相应的观察概率（亦即，t+1时在状态j所观察到的符号的概率）与该时刻到达此状态的概率总和——这来自于上一步每一个局部概率的计算结果与相应的状态转移概率乘积后再相加——的乘积。 　　注意我们已经有了一个仅利用t时刻局部概率计算t+1时刻局部概率的表达式。 　　现在我们就可以递归地计算给定隐马尔科夫模型(HMM)后一个观察序列的概率了——即通过t=1时刻的局部概率‘s计算t=2时刻的‘s，通过t=2时刻的‘s计算t=3时刻的‘s等等直到t=T。给定隐马尔科夫模型(HMM)的观察序列的概率就等于t=T时刻的局部概率之和。 2d.降低计算复杂度 　　我们可以比较通过穷举搜索（评估）和通过递归前向算法计算观察序列概率的时间复杂度。 　　我们有一个长度为T的观察序列O以及一个含有n个隐藏状态的隐马尔科夫模型l=(,A,B)。 　　穷举搜索将包括计算所有可能的序列： 　　　 　　公式 　　　　 　　对我们所观察到的概率求和——注意其复杂度与T成指数级关系。相反的，使用前向算法我们可以利用上一步计算的信息，相应地，其时间复杂度与T成线性关系。 注：穷举搜索的时间复杂度是，前向算法的时间复杂度是，其中T指的是观察序列长度，N指的是隐藏状态数目。 3.总结 　　我们的目标是计算给定隐马尔科夫模型HMM下的观察序列的概率——Pr(observations |)。 　　我们首先通过计算局部概率（‘s）降低计算整个概率的复杂度，局部概率表示的是t时刻到达某个状态s的概率。 　　t=1时，可以利用初始概率(来自于P向量）和观察概率Pr(observation|state)（来自于混淆矩阵）计算局部概率；而t>1时的局部概率可以利用t-时的局部概率计算。 　　因此，这个问题是递归定义的，观察序列的概率就是通过依次计算t=1,2,…,T时的局部概率，并且对于t=T时所有局部概率‘s相加得到的。 　　注意，用这种方式计算观察序列概率的时间复杂度远远小于计算所有序列的概率并对其相加（穷举搜索）的时间复杂度。 五、前向算法（Forward Algorithm） 前向算法定义（Forward algorithm definition） 　　我们使用前向算法计算T长观察序列的概率: 　　　　　 　　其中y的每一个是观察集合之一。局部（中间）概率(‘s)是递归计算的，首先通过计算t=1时刻所有状态的局部概率： 　　　　　 　　然后在每个时间点，t=2，… ，T时，对于每个状态的局部概率，由下式计算局部概率: 　　　　　 　　也就是当前状态相应的观察概率与所有到达该状态的路径概率之积，其递归地利用了上一个时间点已经计算好的一些值。 　　最后，给定HMM,,观察序列的概率等于T时刻所有局部概率之和： 　　　　　 　　再重复说明一下，每一个局部概率（t > 2 时）都由前一时刻的结果计算得出。 　　对于“天气”那个例子，下面的图表显示了t = 2为状态为多云时局部概率的计算过程。这是相应的观察概率b与前一时刻的局部概率与状态转移概率a相乘后的总和再求积的结果： 　　　 （注：本图及维特比算法4中的相似图存在问题，具体请见文后评论，非常感谢读者YaseenTA的指正） 总结（Summary） 　　我们使用前向算法来计算给定隐马尔科夫模型（HMM）后的一个观察序列的概率。它在计算中利用递归避免对网格所有路径进行穷举计算。 　　给定这种算法，可以直接用来确定对于已知的一个观察序列，在一些隐马尔科夫模型（HMMs）中哪一个HMM最好的描述了它——先用前向算法评估每一个（HMM），再选取其中概率最高的一个。 首先需要说明的是，本节不是这个系列的翻译，而是作为前向算法这一章的补充，希望能从实践的角度来说明前向算法。除了用程序来解读hmm的前向算法外，还希望将原文所举例子的问题拿出来和大家探讨。 　　文中所举的程序来自于UMDHMM这个C语言版本的HMM工具包，具体见《几种不同程序语言的HMM版本》。先说明一下UMDHMM这个包的基本情况，在linux环境下，进入umdhmm-v1.02目录，“make all”之后会产生4个可执行文件，分别是： 　　genseq: 利用一个给定的隐马尔科夫模型产生一个符号序列（Generates a symbol sequence using the specified model sequence using the specified model） 　　testfor: 利用前向算法计算log Prob(观察序列| HMM模型)（Computes log Prob(observation|model) using the Forward algorithm.） 　　testvit: 对于给定的观察符号序列及HMM，利用Viterbi 算法生成最可能的隐藏状态序列（Generates the most like state sequence for a given symbol sequence, given the HMM, using Viterbi） 　　esthmm: 对于给定的观察符号序列，利用BaumWelch算法学习隐马尔科夫模型HMM（Estimates the HMM from a given symbol sequence using BaumWelch）。 　　这些可执行文件需要读入有固定格式的HMM文件及观察符号序列文件，格式要求及举例如下： 　　HMM 文件格式： ——————————————————————– 　　　　M= number of symbols 　　　　N= number of states 　　　　A: 　　　　a11 a12 … a1N 　　　　a21 a22 … a2N 　　　　. . . . 　　　　 . . . . 　　　　 . . . . 　　　　aN1 aN2 … aNN 　　　　B: 　　　　b11 b12 … b1M 　　　　b21 b22 … b2M 　　　　. . . . 　　　　. . . . 　　　　 . . . . 　　　　bN1 bN2 … bNM 　　　　pi: 　　　　pi1 pi2 … piN ——————————————————————– 　　HMM文件举例： ——————————————————————– 　　　　M= 2 　　　　N= 3 　　　　A: 　　　　0.333 0.333 0.333 　　　　0.333 0.333 0.333 　　　　0.333 0.333 0.333 　　　　B: 　　　　0.5 0.5 　　　　0.75 0.25 　　　　0.25 0.75 　　　　pi: 　　　　0.333 0.333 0.333 ——————————————————————– 　　观察序列文件格式： ——————————————————————– 　　　　T=seqence length 　　　　o1 o2 o3 . . . oT ——————————————————————– 　　观察序列文件举例： ——————————————————————– 　　　　T= 10 　　　　1 1 1 1 2 1 2 2 2 2 ——————————————————————– 　　对于前向算法的测试程序testfor来说，运行： 　　　testfor model.hmm（HMM文件） obs.seq（观察序列文件） 　　就可以得到观察序列的概率结果的对数值，这里我们在testfor.c的第58行对数结果的输出下再加一行输出： 　　　fprintf(stdout, “prob(O| model) = %fn”, proba); 　　就可以输出运用前向算法计算观察序列所得到的概率值。至此，所有的准备工作已结束，接下来，我们将进入具体的程序解读。 　　首先，需要定义HMM的数据结构，也就是HMM的五个基本要素，在UMDHMM中是如下定义的（在hmm.h中）： typedef struct { int N; /* 隐藏状态数目;Q={1,2,…,N} */ int M; /* 观察符号数目; V={1,2,…,M}*/ double **A; /* 状态转移矩阵A[1..N][1..N]. a[i][j] 是从t时刻状态i到t+1时刻状态j的转移概率 */ double **B; /* 混淆矩阵B[1..N][1..M]. b[j][k]在状态j时观察到符合k的概率。*/ double *pi; /* 初始向量pi[1..N]，pi[i] 是初始状态概率分布 */ } HMM; 前向算法程序示例如下（在forward.c中）： /* 　函数参数说明： 　*phmm：已知的HMM模型；T：观察符号序列长度； 　*O：观察序列；**alpha：局部概率；*pprob：最终的观察概率 */ void Forward(HMM *phmm, int T, int *O, double **alpha, double *pprob) { 　　int i, j; 　　/* 状态索引 */ 　　int t; 　　 /* 时间索引 */ 　　double sum; /*求局部概率时的中间值 */ 　　/* 1. 初始化：计算t=1时刻所有状态的局部概率： */ 　　for (i = 1; i <= phmm->N; i++) 　　　　alpha[1][i] = phmm->pi[i]* phmm->B[i][O[1]]; 　　 　　/* 2. 归纳：递归计算每个时间点，t=2，… ，T时的局部概率 */ 　　for (t = 1; t < T; t++) 　　{ 　　　　for (j = 1; j <= phmm->N; j++) 　　　　{ 　　　　　　sum = 0.0; 　　　　　　for (i = 1; i <= phmm->N; i++) 　　　　　　　　sum += alpha[t][i]* (phmm->A[i][j]); 　　　　　　alpha[t+1][j] = sum*(phmm->B[j][O[t+1]]); 　　　　} 　　} 　　/* 3. 终止：观察序列的概率等于T时刻所有局部概率之和*/ 　　*pprob = 0.0; 　　for (i = 1; i <= phmm->N; i++) 　　　　*pprob += alpha[T][i]; } 　　下一节我将用这个程序来验证英文原文中所举前向算法演示例子的问题。 在HMM这个翻译系列的原文中，作者举了一个前向算法的交互例子，这也是这个系列中比较出彩的地方，但是，在具体运行这个例子的时候，却发现其似乎有点问题。 　　先说一下如何使用这个交互例子，运行时需要浏览器支持java，我用的是firefox。首先在Set按钮前面的对话框里上观察序列，如“Dry,Damp, Soggy” 或“Dry Damp Soggy”，观察符号间用逗号或空格隔开；然后再点击Set按钮，这样就初始化了观察矩阵；如果想得到一个总的结果，即Pr(观察序列|隐马尔科夫模型)，就点旁边的Run按钮；如果想一步一步观察计算过程，即每个节点的局部概率，就单击旁边的Step按钮。 　　原文交互例子（即天气这个例子）中所定义的已知隐马尔科夫模型如下： 　　1、隐藏状态 (天气)：Sunny，Cloudy，Rainy； 　　2、观察状态（海藻湿度）：Dry，Dryish，Damp，Soggy； 　　3、初始状态概率： Sunny（0.63）， Cloudy（0.17）， Rainy（0.20）； 　　4、状态转移矩阵： 　　　　　　　　　　　　 weather today 　　　　　　　　　　　　 Sunny Cloudy Rainy 　　　 　weather　Sunny 0.500 0.375 0.125 　　　　yesterday Cloudy 0.250 0.125 0.625 　　　　　　　　　 Rainy 　0.250 0.375 0.375 　　5、混淆矩阵： 　　　　　　　　　　　　observed states 　　　　　　　　　　　Dry Dryish Damp Soggy 　　　　　　　　　Sunny 0.60 0.20 0.15 0.05 　　　　hidden　 Cloudy 0.25 0.25 0.25 0.25 　　　　states　　Rainy 0.05 0.10 0.35 0.50 　　为了UMDHMM也能运行这个例子，我们将上述天气例子中的隐马尔科夫模型转化为如下的UMDHMM可读的HMM文件weather.hmm： ——————————————————————– 　　　　M= 4 　　　　N= 3　 　　　　A: 　　　　0.500 0.375 0.125 　　　　0.250 0.125 0.625 　　　　0.250 0.375 0.375 　　　　B: 　　　　0.60 0.20 0.15 0.05 　　　　0.25 0.25 0.25 0.25 　　　　0.05 0.10 0.35 0.50 　　　　pi: 　　　　0.63 0.17 0.20 ——————————————————————– 　　在运行例子之前，如果读者也想观察每一步的运算结果，可以将umdhmm-v1.02目录下forward.c中的void Forward(…)函数替换如下： ——————————————————————– void Forward(HMM *phmm, int T, int *O, double **alpha, double *pprob) { 　　int i, j; /* state indices */ 　　int t; /* time index */ 　　double sum; /* partial sum */ 　　 　　/* 1. Initialization */ 　　for (i = 1; i <= phmm->N; i++) 　　{ 　　　　alpha[1][i] = phmm->pi[i]* phmm->B[i][O[1]]; 　　　　printf( “a[1][%d] = pi[%d] * b[%d][%d] = %f * %f = %f\\n”,i, i, i, O[i], phmm->pi[i], phmm->B[i][O[1]], alpha[1][i] ); 　　} 　　 　　/* 2. Induction */ 　　for (t = 1; t < T; t++) 　　{ 　　　　for (j = 1; j <= phmm->N; j++) 　　　　{ 　　　　　　sum = 0.0; 　　　　　　for (i = 1; i <= phmm->N; i++) 　　　　　　{ 　　　　　　　　sum += alpha[t][i]* (phmm->A[i][j]); 　　　　　　　　printf( “a[%d][%d] * A[%d][%d] = %f * %f = %f\\n”, t, i, i, j, alpha[t][i], phmm->A[i][j], alpha[t][i]* (phmm->A[i][j])); 　　　　　　　　printf( “sum = %f\\n”, sum ); 　　　　　　} 　　　　　　alpha[t+1][j] = sum*(phmm->B[j][O[t+1]]); 　　　　　　printf( “a[%d][%d] = sum * b[%d][%d]] = %f * %f = %f\\n”,t+1, j, j, O[t+1], sum, phmm->B[j][O[t+1]], alpha[t+1][j] ); 　　　　} 　　} 　　/* 3. Termination */ 　　*pprob = 0.0; 　　for (i = 1; i <= phmm->N; i++) 　　{ 　　　　*pprob += alpha[T][i]; 　　　　printf( “alpha[%d][%d] = %f\\n”, T, i, alpha[T][i] ); 　　　　printf( “pprob = %f\\n”, *pprob ); 　　} } ——————————————————————– 　　替换完毕之后，重新“make clean”，“make all”，这样新的testfor可执行程序就可以输出前向算法每一步的计算结果。 　　现在我们就用testfor来运行原文中默认给出的观察序列“Dry,Damp,Soggy”，其所对应的UMDHMM可读的观察序列文件test1.seq： ——————————————————————– 　　　　T=3 　　　　1 3 4 ——————————————————————– 　　好了，一切准备工作就绪，现在就输入如下命令： 　　　　testfor weather.hmm test1.seq > result1 　　result1就包含了所有的结果细节： ——————————————————————– Forward without scaling a[1][1] = pi[1] * b[1][1] = 0.630000 * 0.600000 = 0.378000 a[1][2] = pi[2] * b[2][3] = 0.170000 * 0.250000 = 0.042500 a[1][3] = pi[3] * b[3][4] = 0.200000 * 0.050000 = 0.010000 … pprob = 0.026901 log prob(O| model) = -3.615577E+00 prob(O| model) = 0.026901 … ——————————————————————– 　　黑体部分是最终的观察序列的概率结果，即本例中的Pr(观察序列|HMM) = 0.026901。 　　但是，在原文中点Run按钮后，结果却是：Probability of this model = 0.027386915。 　　这其中的差别到底在哪里？我们来仔细观察一下中间运行过程： 　　在初始化亦t=1时刻的局部概率计算两个是一致的，没有问题。但是，t=2时，在隐藏状态“Sunny”的局部概率是不一致的。英文原文给出的例子的运行结果是： 　　Alpha = (((0.37800002*0.5) + (0.0425*0.375) + (0.010000001*0.125)) * 0.15) = 0.03092813 　　而UMDHMM给出的结果是： ——————————————————————– 　　a[1][1] * A[1][1] = 0.378000 * 0.500000 = 0.189000 　　sum = 0.189000 　　a[1][2] * A[2][1] = 0.042500 * 0.250000 = 0.010625 　　sum = 0.199625 　　a[1][3] * A[3][1] = 0.010000 * 0.250000 = 0.002500 　　sum = 0.202125 　　a[2][1] = sum * b[1][3]] = 0.202125 * 0.150000 = 0.030319 ——————————————————————– 　　区别就在于状态转移概率的选择上，原文选择的是状态转移矩阵中的第一行，而UMDHMM选择的则是状态转移矩阵中的第一列。如果从原文给出的状态转移矩阵来看，第一行代表的是从前一时刻的状态“Sunny”分别到当前时刻的状态“Sunny”，“Cloudy”，“Rainy”的概率；而第一列代表的是从前一时刻的状态“Sunny”，“Cloudy”，“Rainy”分别到当前时刻状态“Sunny”的概率。这样看来似乎原文的计算过程有误，读者不妨多试几个例子看看，前向算法这一章就到此为止了。 六、维特比算法（Viterbi Algorithm） 寻找最可能的隐藏状态序列(Finding most probable sequence of hidden states) 　　对于一个特殊的隐马尔科夫模型(HMM)及一个相应的观察序列，我们常常希望能找到生成此序列最可能的隐藏状态序列。 1.穷举搜索 　　我们使用下面这张网格图片来形象化的说明隐藏状态和观察状态之间的关系： 　　我们可以通过列出所有可能的隐藏状态序列并且计算对于每个组合相应的观察序列的概率来找到最可能的隐藏状态序列。最可能的隐藏状态序列是使下面这个概率最大的组合： 　　　　　　Pr（观察序列|隐藏状态的组合） 　　例如，对于网格中所显示的观察序列，最可能的隐藏状态序列是下面这些概率中最大概率所对应的那个隐藏状态序列： 　　Pr(dry,damp,soggy | sunny,sunny,sunny), Pr(dry,damp,soggy | sunny,sunny,cloudy), Pr(dry,damp,soggy | sunny,sunny,rainy), . . . . Pr(dry,damp,soggy | rainy,rainy,rainy) 　　这种方法是可行的，但是通过穷举计算每一个组合的概率找到最可能的序列是极为昂贵的。与前向算法类似，我们可以利用这些概率的时间不变性来降低计算复杂度。 2.使用递归降低复杂度 　　给定一个观察序列和一个隐马尔科夫模型（HMM），我们将考虑递归地寻找最有可能的隐藏状态序列。我们首先定义局部概率,它是到达网格中的某个特殊的中间状态时的概率。然后，我们将介绍如何在t=1和t=n(>1)时计算这些局部概率。 　　这些局部概率与前向算法中所计算的局部概率是不同的，因为它们表示的是时刻t时到达某个状态最可能的路径的概率，而不是所有路径概率的总和。 　2a.局部概率‘s和局部最佳途径 　　考虑下面这个网格，它显示的是天气状态及对于观察序列干燥，湿润及湿透的一阶状态转移情况： 　　　 　　对于网格中的每一个中间及终止状态，都有一个到达该状态的最可能路径。举例来说，在t=3时刻的3个状态中的每一个都有一个到达此状态的最可能路径，或许是这样的： 　　 　　我们称这些路径局部最佳路径(partial best paths)。其中每个局部最佳路径都有一个相关联的概率，即局部概率或。与前向算法中的局部概率不同，是到达该状态（最可能）的一条路径的概率。 　　因而(i,t)是t时刻到达状态i的所有序列概率中最大的概率，而局部最佳路径是得到此最大概率的隐藏状态序列。对于每一个可能的i和t值来说，这一类概率（及局部路径）均存在。 　　特别地，在t=T时每一个状态都有一个局部概率和一个局部最佳路径。这样我们就可以通过选择此时刻包含最大局部概率的状态及其相应的局部最佳路径来确定全局最佳路径（最佳隐藏状态序列）。 六、维特比算法（Viterbi Algorithm） 寻找最可能的隐藏状态序列(Finding most probable sequence of hidden states) 2b.计算t=1时刻的局部概率‘s 　　我们计算的局部概率是作为最可能到达我们当前位置的路径的概率（已知的特殊知识如观察概率及前一个状态的概率）。当t=1的时候，到达某状态的最可能路径明显是不存在的；但是，我们使用t=1时的所处状态的初始概率及相应的观察状态k1的观察概率计算局部概率；即 　　　　　　　　　　 　　——与前向算法类似，这个结果是通过初始概率和相应的观察概率相乘得出的。 2c.计算t>1时刻的局部概率‘s 　　现在我们来展示如何利用t-1时刻的局部概率计算t时刻的局部概率。 　　考虑如下的网格： 　　　　 　　我们考虑计算t时刻到达状态X的最可能的路径；这条到达状态X的路径将通过t-1时刻的状态A，B或C中的某一个。 　　因此，最可能的到达状态X的路径将是下面这些路径的某一个 　　　　　　　（状态序列），…，A，X 　　　　　　　（状态序列），…，B，X 或　　　　　　（状态序列），…，C，X 　　我们想找到路径末端是AX,BX或CX并且拥有最大概率的路径。 　　回顾一下马尔科夫假设：给定一个状态序列，一个状态发生的概率只依赖于前n个状态。特别地，在一阶马尔可夫假设下，状态X在一个状态序列后发生的概率只取决于之前的一个状态，即 　　　Pr (到达状态A最可能的路径) .Pr (X | A) . Pr (观察状态 | X) 　　与此相同，路径末端是AX的最可能的路径将是到达A的最可能路径再紧跟X。相似地，这条路径的概率将是： 　　　Pr (到达状态A最可能的路径) .Pr (X | A) . Pr (观察状态 | X) 　　因此，到达状态X的最可能路径概率是： 　　 　　其中第一项是t-1时刻的局部概率，第二项是状态转移概率以及第三项是观察概率。 　　泛化上述公式，就是在t时刻，观察状态是kt，到达隐藏状态i的最佳局部路径的概率是： 　　　　　 　　这里，我们假设前一个状态的知识（局部概率）是已知的，同时利用了状态转移概率和相应的观察概率之积。然后，我们就可以在其中选择最大的概率了（局部概率）。 六、维特比算法（Viterbi Algorithm） 寻找最可能的隐藏状态序列(Finding most probable sequence of hidden states) 2d.反向指针，‘s 　　考虑下面这个网格 　　　 　　在每一个中间及终止状态我们都知道了局部概率，(i,t)。然而我们的目标是在给定一个观察序列的情况下寻找网格中最可能的隐藏状态序列——因此，我们需要一些方法来记住网格中的局部最佳路径。 　　回顾一下我们是如何计算局部概率的，计算t时刻的‘s我们仅仅需要知道t-1时刻的‘s。在这个局部概率计算之后，就有可能记录前一时刻哪个状态生成了(i,t)——也就是说，在t-1时刻系统必须处于某个状态，该状态导致了系统在t时刻到达状态i是最优的。这种记录（记忆）是通过对每一个状态赋予一个反向指针完成的，这个指针指向最优的引发当前状态的前一时刻的某个状态。 　　形式上，我们可以写成如下的公式 　　　　 　　其中argmax运算符是用来计算使括号中表达式的值最大的索引j的。 　　请注意这个表达式是通过前一个时间步骤的局部概率‘s和转移概率计算的，并不包括观察概率（与计算局部概率‘s本身不同）。这是因为我们希望这些‘s能回答这个问题“如果我在这里，最可能通过哪条路径到达下一个状态？”——这个问题与隐藏状态有关，因此与观察概率有关的混淆（矩阵）因子是可以被忽略的。 2e.维特比算法的优点 　　使用Viterbi算法对观察序列进行解码有两个重要的优点： 　　1. 通过使用递归减少计算复杂度——这一点和前向算法使用递归减少计算复杂度是完全类似的。 　　2.维特比算法有一个非常有用的性质，就是对于观察序列的整个上下文进行了最好的解释（考虑）。事实上，寻找最可能的隐藏状态序列不止这一种方法，其他替代方法也可以，譬如，可以这样确定如下的隐藏状态序列： 　　　　 其中 　　　　 　　这里，采用了“自左向右”的决策方式进行一种近似的判断，其对于每个隐藏状态的判断是建立在前一个步骤的判断的基础之上（而第一步从隐藏状态的初始向量开始）。 　　这种做法，如果在整个观察序列的中部发生“噪音干扰”时，其最终的结果将与正确的答案严重偏离。 　　相反， 维特比算法在确定最可能的终止状态前将考虑整个观察序列，然后通过指针“回溯”以确定某个隐藏状态是否是最可能的隐藏状态序列中的一员。这是非常有用的，因为这样就可以孤立序列中的“噪音”，而这些“噪音”在实时数据中是很常见的。 3.小结 　　维特比算法提供了一种有效的计算方法来分析隐马尔科夫模型的观察序列，并捕获最可能的隐藏状态序列。它利用递归减少计算量，并使用整个序列的上下文来做判断，从而对包含“噪音”的序列也能进行良好的分析。 　　在使用时，维特比算法对于网格中的每一个单元(cell)都计算一个局部概率，同时包括一个反向指针用来指示最可能的到达该单元的路径。当完成整个计算过程后，首先在终止时刻找到最可能的状态，然后通过反向指针回溯到t=1时刻，这样回溯路径上的状态序列就是最可能的隐藏状态序列了。 六、维特比算法（Viterbi Algorithm） 维特比算法定义(Viterbi algorithm definition) 1、维特比算法的形式化定义 　　维特比算法可以形式化的概括为： 　　对于每一个i，i = 1，… ，n，令： 　　　　　 　　——这一步是通过隐藏状态的初始概率和相应的观察概率之积计算了t=1时刻的局部概率。 　　对于t=2，…，T和i=1，…，n,令： 　　　　　 　　——这样就确定了到达下一个状态的最可能路径，并对如何到达下一个状态做了记录。具体来说首先通过考察所有的转移概率与上一步获得的最大的局部概率之积，然后记录下其中最大的一个，同时也包含了上一步触发此概率的状态。 　　令： 　　　　　 　　——这样就确定了系统完成时(t=T)最可能的隐藏状态。 　　对于t=T-1，…，1 　　令： 　　　　　 　　——这样就可以按最可能的状态路径在整个网格回溯。回溯完成时，对于观察序列来说，序列i1 … iT就是生成此观察序列的最可能的隐藏状态序列。 　　2.计算单独的‘s和‘s 　　维特比算法中的局部概率‘s的计算与前向算法中的局部概率‘s的很相似。下面这幅图表显示了‘s和‘s的计算细节，可以对比一下前向算法3中的计算局部概率‘s的那幅图表： 　　 （注：本图及前向算法3中的相似图存在问题，具体请见前向算法3文后评论，非常感谢读者YaseenTA的指正） 　　唯一不同的是前向算法中计算局部概率‘s时的求和符号（）在维特比算法中计算局部概率‘s时被替换为max——这一个重要的不同也说明了在维特比算法中我们选择的是到达当前状态的最可能路径，而不是总的概率。我们在维特比算法中维护了一个“反向指针”记录了到达当前状态的最佳路径，即在计算‘s时通过argmax运算符获得。 总结(Summary) 　　对于一个特定的隐马尔科夫模型，维特比算法被用来寻找生成一个观察序列的最可能的隐藏状态序列。我们利用概率的时间不变性，通过避免计算网格中每一条路径的概率来降低问题的复杂度。维特比算法对于每一个状态(t>1)都保存了一个反向指针()，并在每一个状态中存储了一个局部概率()。 　　局部概率是由反向指针指示的路径到达某个状态的概率。 　　当t=T时，维特比算法所到达的这些终止状态的局部概率‘s是按照最优（最可能）的路径到达该状态的概率。因此，选择其中最大的一个，并回溯找出所隐藏的状态路径，就是这个问题的最好答案。 　　关于维特比算法，需要着重强调的一点是它不是简单的对于某个给定的时间点选择最可能的隐藏状态，而是基于全局序列做决策——因此，如果在观察序列中有一个“非寻常”的事件发生，对于维特比算法的结果也影响不大。 　　这在语音处理中是特别有价值的，譬如当某个单词发音的一个中间音素出现失真或丢失的情况时，该单词也可以被识别出来。 六、维特比算法（Viterbi Algorithm） 维特比算法程序示例 　　仍然需要说明的是，本节不是这个系列的翻译，而是作为维特比算法这一章的补充，将UMDHMM这个C语言版本的HMM工具包中的维特比算法程序展示给大家，并运行包中所附带的例子。关于UMDHMM这个工具包的介绍，大家可以参考前向算法4中的介绍。 维特比算法程序示例如下（在viterbi.c中): void Viterbi(HMM *phmm, int T, int *O, double **delta, int **psi,int *q, double *pprob) { 　　int i, j; /* state indices */ 　　int t; /* time index */ 　　int maxvalind; 　　double maxval, val; 　　/* 1. Initialization */ 　　for (i = 1; i <= phmm->N; i++) 　　{ 　　　　delta[1][i] = phmm->pi[i] * (phmm->B[i][O[1]]); 　　　　psi[1][i] = 0; 　　} 　　/* 2. Recursion */ 　　for (t = 2; t <= T; t++) 　　{ 　　　　for (j = 1; j <= phmm->N; j++) 　　　　{ 　　　　　　maxval = 0.0; 　　　　　　maxvalind = 1; 　　　　　　for (i = 1; i <= phmm->N; i++) 　　　　　　{ 　　　　　　　　val = delta[t-1][i]*(phmm->A[i][j]); 　　　　　　　　if (val > maxval) 　　　　　　　　{ 　　　　　　　　　　maxval = val; 　　　　　　　　　　maxvalind = i; 　　　　　　　　} 　　　　　　} 　　　　　　delta[t][j] = maxval*(phmm->B[j][O[t]]); 　　　　　　psi[t][j] = maxvalind; 　　　　} 　　} 　　/* 3. Termination */ 　　*pprob = 0.0; 　　q[T] = 1; 　　for (i = 1; i <= phmm->N; i++) 　　{ 　　　　if (delta[T][i] > *pprob) 　　　　{ 　　　　　　*pprob = delta[T][i]; 　　　　　　q[T] = i; 　　　　} 　　} 　　/* 4. Path (state sequence) backtracking */ 　　for (t = T – 1; t >= 1; t–) 　　　　q[t] = psi[t+1][q[t+1]]; } 　　在UMDHMM包中所生成的4个可执行程序中，testvit是用来测试维特比算法的， 对于给定的观察符号序列及HMM，利用Viterbi 算法生成最可能的隐藏状态序列。这里我们利用UMDHMM包中test.hmm和test.seq来测试维特比算法，关于这两个文件，具体如下： 　　test.hmm： ——————————————————————– 　　　　M= 2 　　　　N= 3 　　　　A: 　　　　0.333 0.333 0.333 　　　　0.333 0.333 0.333 　　　　0.333 0.333 0.333 　　　　B: 　　　　0.5 0.5 　　　　0.75 0.25 　　　　0.25 0.75 　　　　pi: 　　　　0.333 0.333 0.333 ——————————————————————– 　　test.seq： ——————————————————————– 　　　　T= 10 　　　　1 1 1 1 2 1 2 2 2 2 ——————————————————————– 　　对于维特比算法的测试程序testvit来说，运行： 　　　testvit test.hmm test.seq 　　结果如下： 　　———————————— 　　Viterbi using direct probabilities 　　Viterbi MLE log prob = -1.387295E+01 　　Optimal state sequence: 　　T= 10 　　2 2 2 2 3 2 3 3 3 3 　　———————————— 　　Viterbi using log probabilities 　　Viterbi MLE log prob = -1.387295E+01 　　Optimal state sequence: 　　T= 10 　　2 2 2 2 3 2 3 3 3 3 　　———————————— 　　The two log probabilites and optimal state sequences 　　should identical (within numerical precision). 　　序列“2 2 2 2 3 2 3 3 3 3”就是最终所找到的隐藏状态序列。好了，维特比算法这一章就到此为止了。 七、前向-后向算法(Forward-backward algorithm) 根据观察序列生成隐马尔科夫模型(Generating a HMM from a sequence of obersvations) 　　与HMM模型相关的“有用”的问题是评估（前向算法）和解码（维特比算法）——它们一个被用来测量一个模型的相对适用性，另一个被用来推测模型隐藏的部分在做什么（“到底发生了”什么）。可以看出它们都依赖于隐马尔科夫模型（HMM）参数这一先验知识——状态转移矩阵，混淆（观察）矩阵，以及向量（初始化概率向量）。 　　然而，在许多实际问题的情况下这些参数都不能直接计算的，而要需要进行估计——这就是隐马尔科夫模型中的学习问题。前向-后向算法就可以以一个观察序列为基础来进行这样的估计，而这个观察序列来自于一个给定的集合，它所代表的是一个隐马尔科夫模型中的一个已知的隐藏集合。 　　一个例子可能是一个庞大的语音处理数据库，其底层的语音可能由一个马尔可夫过程基于已知的音素建模的，而其可以观察的部分可能由可识别的状态（可能通过一些矢量数据表示）建模的，但是没有（直接）的方式来获取隐马尔科夫模型（HMM）参数。 　　前向-后向算法并非特别难以理解，但自然地比前向算法和维特比算法更复杂。由于这个原因，这里就不详细讲解前向-后向算法了（任何有关HMM模型的参考文献都会提供这方面的资料的）。 　　总之，前向-后向算法首先对于隐马尔科夫模型的参数进行一个初始的估计（这很可能是完全错误的），然后通过对于给定的数据评估这些参数的的价值并减少它们所引起的错误来重新修订这些HMM参数。从这个意义上讲，它是以一种梯度下降的形式寻找一种错误测度的最小值。 　　之所以称其为前向-后向算法，主要是因为对于网格中的每一个状态，它既计算到达此状态的“前向”概率（给定当前模型的近似估计），又计算生成此模型最终状态的“后向”概率（给定当前模型的近似估计）。 这些都可以通过利用递归进行有利地计算，就像我们已经看到的。可以通过利用近似的HMM模型参数来提高这些中间概率进行调整，而这些调整又形成了前向-后向算法迭代的基础。 注：关于前向-后向算法，原文只讲了这么多，后继我将按自己的理解补充一些内容。 七、前向-后向算法(Forward-backward algorithm) 　　要理解前向-后向算法，首先需要了解两个算法：后向算法和EM算法。后向算法是必须的，因为前向-后向算法就是利用了前向算法与后向算法中的变量因子，其得名也因于此；而EM算法不是必须的，不过由于前向-后向算法是EM算法的一个特例，因此了解一下EM算法也是有好处的，说实话，对于EM算法，我也是云里雾里的。好了，废话少说，我们先谈谈后向算法。 1、后向算法（Backward algorithm） 　　其实如果理解了前向算法，后向算法也是比较好理解的，这里首先重新定义一下前向算法中的局部概率at(i)，称其为前向变量，这也是为前向-后向算法做点准备： 　　　 　　相似地，我们也可以定义一个后向变量Bt(i)（同样可以理解为一个局部概率）： 　　　 　　后向变量(局部概率)表示的是已知隐马尔科夫模型及t时刻位于隐藏状态Si这一事实，从t+1时刻到终止时刻的局部观察序列的概率。同样与前向算法相似，我们可以从后向前（故称之为后向算法）递归地计算后向变量： 　　1）初始化，令t=T时刻所有状态的后向变量为1： 　　　　　 　　2）归纳，递归计算每个时间点，t=T-1,T-2,…,1时的后向变量： 　　 　　这样就可以计算每个时间点上所有的隐藏状态所对应的后向变量，如果需要利用后向算法计算观察序列的概率，只需将t=1时刻的后向变量（局部概率）相加即可。下图显示的是t+1时刻与t时刻的后向变量之间的关系： 　　　 　　上述主要参考自HMM经典论文《A tutorial on Hidden Markov Models and selected applications in speech recognition》。下面我们给出利用后向算法计算观察序列概率的程序示例，这个程序仍然来自于UMDHMM。 后向算法程序示例如下（在backward.c中): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 void Backward(HMM *phmm, int T, int *O, double **beta, double *pprob) { 　　int     i, j;   /* state indices */ 　　int     t;      /* time index */ 　　double sum;   　　/* 1. Initialization */ 　　for (i = 1; i <= phmm->N; i++) 　　　　beta[T][i] = 1.0;   　　/* 2. Induction */ 　　for (t = T - 1; t >= 1; t--)  　　{ 　　　　for (i = 1; i <= phmm->N; i++)  　　　　{ 　　　　　　sum = 0.0; 　　　　　　for (j = 1; j <= phmm->N; j++) 　　　　　　　　sum += phmm->A[i][j] *  　　　　　　　　　　　　　　(phmm->B[j][O[t+1]])*beta[t+1][j]; 　　　　　　beta[t][i] = sum; 　　　　} 　　}   　　/* 3. Termination */ 　　*pprob = 0.0; 　　for (i = 1; i <= phmm->N; i++) 　　　　*pprob += beta[1][i]; } 　　好了，后向算法就到此为止了，下一节我们粗略的谈谈EM算法。 七、前向-后向算法(Forward-backward algorithm) 　　前向-后向算法是Baum于1972年提出来的，又称之为Baum-Welch算法，虽然它是EM(Expectation-Maximization)算法的一个特例，但EM算法却是于1977年提出的。那么为什么说前向-后向算法是EM算法的一个特例呢？这里有两点需要说明一下。 　　第一，1977年A. P. Dempster、N. M. Laird、 D. B. Rubin在其论文“Maximum Likelihood from Incomplete Data via the EM Algorithm”中首次提出了EM算法的概念，但是他们也在论文的介绍中提到了在此之前就有一些学者利用了EM算法的思想解决了一些特殊问题，其中就包括了Baum在70年代初期的相关工作，只是这类方法没有被总结而已，他们的工作就是对这类解决问题的方法在更高的层次上定义了一个完整的EM算法框架。 　　第二，对于前向-后向算法与EM算法的关系，此后在许多与HMM或EM相关的论文里都被提及，其中贾里尼克（Jelinek）老先生在1997所著的书“Statistical Methods for Speech Recognition”中对于前向-后向算法与EM算法的关系进行了完整的描述，读者有兴趣的话可以找来这本书读读。 　　关于EM算法的讲解，网上有很多，这里我就不献丑了，直接拿目前搜索“EM算法”在Google排名第一的文章做了参考，希望读者不要拍砖： 　　EM 算法是 Dempster，Laind，Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 MLE 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有讨厌数据等所谓的不完全数据(incomplete data)。 　　假定集合Z = (X,Y)由观测数据 X 和未观测数据Y 组成，Z = (X,Y)和 X 分别称为完整数据和不完整数据。假设Z的联合概率密度被参数化地定义为P(X，Y|Θ)，其中Θ 表示要被估计的参数。Θ 的最大似然估计是求不完整数据的对数似然函数L(X;Θ)的最大值而得到的： 　　　L(Θ; X )= log p(X |Θ) = ∫log p(X ,Y |Θ)dY ；(1) 　　EM算法包括两个步骤：由E步和M步组成，它是通过迭代地最大化完整数据的对数似然函数Lc( X;Θ )的期望来最大化不完整数据的对数似然函数，其中： 　　　Lc(X;Θ) =log p(X，Y |Θ) ； (2) 　　假设在算法第t次迭代后Θ 获得的估计记为Θ(t ) ，则在（t+1）次迭代时， 　　E-步：计算完整数据的对数似然函数的期望，记为： 　　　Q(Θ |Θ (t) ) = E{Lc(Θ;Z)|X;Θ(t) }； (3) 　　M-步：通过最大化Q(Θ |Θ(t) ) 来获得新的Θ 。 　　通过交替使用这两个步骤，EM算法逐步改进模型的参数，使参数和训练样本的似然概率逐渐增大，最后终止于一个极大点。 　　直观地理解EM算法，它也可被看作为一个逐次逼近算法：事先并不知道模型的参数，可以随机的选择一套参数或者事先粗略地给定某个初始参数λ0 ，确定出对应于这组参数的最可能的状态，计算每个训练样本的可能结果的概率，在当前的状态下再由样本对参数修正，重新估计参数λ ，并在新的参数下重新确定模型的状态，这样，通过多次的迭代，循环直至某个收敛条件满足为止，就可以使得模型的参数逐渐逼近真实参数。 　　EM算法的主要目的是提供一个简单的迭代算法计算后验密度函数，它的最大优点是简单和稳定，但容易陷入局部最优。 　　参考原文见：http://49805085.spaces.live.com/Blog/cns!145C40DDDB4C6E5!670.entry 　　注意上面那段粗体字，读者如果觉得EM算法不好理解的话，就记住这段粗体字的意思吧！ 　　有了后向算法和EM算法的预备知识，下一节我们就正式的谈一谈前向-后向算法。 七、前向-后向算法(Forward-backward algorithm) 　　隐马尔科夫模型（HMM）的三个基本问题中，第三个HMM参数学习的问题是最难的，因为对于给定的观察序列O，没有任何一种方法可以精确地找到一组最优的隐马尔科夫模型参数（A、B、）使P(O|)最大。因而，学者们退而求其次，不能使P(O|)全局最优，就寻求使其局部最优（最大化）的解决方法，而前向-后向算法（又称之为Baum-Welch算法）就成了隐马尔科夫模型学习问题的一种替代（近似）解决方法。 　　我们首先定义两个变量。给定观察序列O及隐马尔科夫模型，定义t时刻位于隐藏状态Si的概率变量为： 　　　　　　　　 　　回顾一下第二节中关于前向变量at(i)及后向变量Bt(i)的定义，我们可以很容易地将上式用前向、后向变量表示为： 　　　 　　其中分母的作用是确保： 　　给定观察序列O及隐马尔科夫模型，定义t时刻位于隐藏状态Si及t+1时刻位于隐藏状态Sj的概率变量为： 　　　　 　　该变量在网格中所代表的关系如下图所示： 　 　　同样，该变量也可以由前向、后向变量表示： 　　　 　　而上述定义的两个变量间也存在着如下关系： 　　　　　　　　　　　　 　　如果对于时间轴t上的所有相加，我们可以得到一个总和，它可以被解释为从其他隐藏状态访问Si的期望值（网格中的所有时间的期望），或者，如果我们求和时不包括时间轴上的t=T时刻，那么它可以被解释为从隐藏状态Si出发的状态转移期望值。相似地，如果对在时间轴t上求和（从t=1到t=T-1），那么该和可以被解释为从状态Si到状态Sj的状态转移期望值。即： 　　　 　　　 七、前向-后向算法(Forward-backward algorithm) 　　上一节我们定义了两个变量及相应的期望值，本节我们利用这两个变量及其期望值来重新估计隐马尔科夫模型（HMM）的参数，A及B： 　　如果我们定义当前的HMM模型为，那么可以利用该模型计算上面三个式子的右端；我们再定义重新估计的HMM模型为，那么上面三个式子的左端就是重估的HMM模型参数。Baum及他的同事在70年代证明了因此如果我们迭代地的计算上面三个式子，由此不断地重新估计HMM的参数，那么在多次迭代后可以得到的HMM模型的一个最大似然估计。不过需要注意的是，前向-后向算法所得的这个结果（最大似然估计）是一个局部最优解。 　　关于前向-后向算法和EM算法的具体关系的解释，大家可以参考HMM经典论文《A tutorial on Hidden Markov Models and selected applications in speech recognition》，这里就不详述了。下面我们给出UMDHMM中的前向-后向算法示例，这个算法比较复杂，这里只取主函数，其中所引用的函数大家如果感兴趣的话可以自行参考UMDHMM。 前向-后向算法程序示例如下（在baum.c中): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 void BaumWelch(HMM *phmm, int T, int *O, double **alpha, double **beta, double **gamma, int *pniter, double *plogprobinit, double *plogprobfinal) { 　　int   i, j, k; 　　int   t, l = 0; 　　double    logprobf, logprobb,  threshold; 　　double    numeratorA, denominatorA; 　　double    numeratorB, denominatorB; 　　double ***xi, *scale; 　　double delta, deltaprev, logprobprev; 　　deltaprev = 10e-70; 　　xi = AllocXi(T, phmm->N); 　　scale = dvector(1, T); 　　ForwardWithScale(phmm, T, O, alpha, scale, &logprobf); 　　*plogprobinit = logprobf; /* log P(O |intial model) */ 　　BackwardWithScale(phmm, T, O, beta, scale, &logprobb); 　　ComputeGamma(phmm, T, alpha, beta, gamma); 　　ComputeXi(phmm, T, O, alpha, beta, xi); 　　logprobprev = logprobf; 　　do   　　{  　　　　/* reestimate frequency of state i in time t=1 */ 　　　　for (i = 1; i <= phmm->N; i++)  　　　　　　phmm->pi[i] = .001 + .999*gamma[1][i]; 　　　　/* reestimate transition matrix  and symbol prob in 　　　　　　　　each state */ 　　　　for (i = 1; i <= phmm->N; i++)  　　　　{  　　　　　　denominatorA = 0.0; 　　　　　　for (t = 1; t <= T - 1; t++)  　　　　　　　　denominatorA += gamma[t][i]; 　　　　　　for (j = 1; j <= phmm->N; j++)  　　　　　　{ 　　　　　　　　numeratorA = 0.0; 　　　　　　　　for (t = 1; t <= T - 1; t++)  　　　　　　　　　　numeratorA += xi[t][i][j]; 　　　　　　　　phmm->A[i][j] = .001 + 　　　　　　　　　　　　　　　　　.999*numeratorA/denominatorA; 　　　　　　} 　　　　　　denominatorB = denominatorA + gamma[T][i];  　　　　　　for (k = 1; k <= phmm->M; k++)  　　　　　　{ 　　　　　　　　numeratorB = 0.0; 　　　　　　　　for (t = 1; t <= T; t++)  　　　　　　　　{ 　　　　　　　　　　if (O[t] == k)  　　　　　　　　　　　　numeratorB += gamma[t][i]; 　　　　　　　　} 　　　　　　　　phmm->B[i][k] = .001 + 　　　　　　　　　　　　　　　　　.999*numeratorB/denominatorB; 　　　　　　} 　　　　} 　　　　ForwardWithScale(phmm, T, O, alpha, scale, &logprobf); 　　　　BackwardWithScale(phmm, T, O, beta, scale, &logprobb); 　　　　ComputeGamma(phmm, T, alpha, beta, gamma); 　　　　ComputeXi(phmm, T, O, alpha, beta, xi); 　　　　/* compute difference between log probability of  　　　　　　two iterations */ 　　　　delta = logprobf - logprobprev;  　　　　logprobprev = logprobf; 　　　　l++; 　　} 　　while (delta > DELTA); /* if log probability does not  　　　　　　　　　　　　　　change much, exit */    　　*pniter = l; 　　*plogprobfinal = logprobf; /* log P(O|estimated model) */ 　　FreeXi(xi, T, phmm->N); 　　free_dvector(scale, 1, T); } 　　 　　前向-后向算法就到此为止了。 八、总结(Summary) 　　通常，模式并不是单独的出现，而是作为时间序列中的一个部分——这个过程有时候可以被辅助用来对它们进行识别。在基于时间的进程中，通常都会使用一些假设——一个最常用的假设是进程的状态只依赖于前面N个状态——这样我们就有了一个N阶马尔科夫模型。最简单的例子是N = 1。 　　存在很多例子，在这些例子中进程的状态（模式）是不能够被直接观察的，但是可以非直接地，或者概率地被观察为模式的另外一种集合——这样我们就可以定义一类隐马尔科夫模型——这些模型已被证明在当前许多研究领域，尤其是语音识别领域具有非常大的价值。 　　在实际的过程中这些模型提出了三个问题都可以得到立即有效的解决，分别是： 　　* 评估：对于一个给定的隐马尔科夫模型其生成一个给定的观察序列的概率是多少。前向算法可以有效的解决此问题。 　　* 解码：什么样的隐藏（底层）状态序列最有可能生成一个给定的观察序列。维特比算法可以有效的解决此问题。 　　* 学习：对于一个给定的观察序列样本，什么样的模型最可能生成该序列——也就是说，该模型的参数是什么。这个问题可以通过使用前向-后向算法解决。 　　隐马尔科夫模型（HMM）在分析实际系统中已被证明有很大的价值；它们通常的缺点是过于简化的假设，这与马尔可夫假设相关——即一个状态只依赖于前一个状态，并且这种依赖关系是独立于时间之外的（与时间无关）。 　　关于隐马尔科夫模型的完整论述，可参阅： 　　L R Rabiner and B H Juang, `An introduction to HMMs’, iEEE ASSP Magazine, 3, 4-16. 　　全文完！ 　　后记：这个翻译系列终于可以告一段落了，从6月2日起至今，历史四个多月，期间断断续续翻译并夹杂些自己个人的理解，希望这个系列对于HMM的学习者能有些用处，我个人也就很满足了。接下来，我会结合HMM在自然语言处理中的一些典型应用，譬如词性标注、中文分词等，从实践的角度讲讲自己的理解，欢迎大家继续关注52nlp。","title":"HMM学习范例"},{"content":"大数据时代 http://www.36kr.com/p/83147.html 对数字在行？对数据着迷？那么你听到的是机遇的敲门声。 周墨（音译，Mo Zhou）去年夏天刚刚完成耶鲁大学的MBA学业就被IBM抢走，加入了这家技术公司快速发展的数据顾问的队伍当中。他们帮助企业弄清楚数据爆炸的意义—Web流量、社交网络上的评论，以及监控货物、供应商及客户的软件和传感器上的数据，以提供决策指南、削减成本、拉动销售。“我一直对数字情有独钟，”周小姐说。她是数据分析师，这个职位跟她的技能很配。 为了开发利用好这股数据洪流，美国需要大量像她这样的人。去年，咨询公司麦肯锡的的研究机构麦肯锡全球研究所进行了一项调查，调查预计，美国需要14万至19万名以上具备“深度分析”专长的人员，而对具备数据知识的经理的需求超过150万，无论是招聘的还是再培训的都行。 数据丰富的影响延伸到商业之外。比如说Justin Grimmer就是新生代的政治学者中的一员。作为斯坦福大学的一名28岁的助理教授，他看到了“一个机遇，因为学科正变得越来越趋于数据密集”，所以在自己的大学及研究生研究当中，他把数学运用到了政治科学里面。他的研究包括对博客发文、国会演讲以及新闻发布、新闻内容的自动计算机分析，以便深入了解政治观念是如何被传播出去的。 其他领域，如科学、体育、广告及公共卫生，发生的故事也类似—即数据驱动发现和决策的趋势。“这是一次革命，” 哈佛量化社会科学研究所主任Gary King说：“我们的确正在起航。不过，在庞大的新数据来源的支持下，量化的前进步伐将会踏遍学术、商业和政府领域。没有一个领域可以不被触及。” 欢迎来到大数据时代。硅谷的新宠，前有Google，后有Facebook，都是驾驭Web数据的大师—它们都擅长于给在线搜索、文章和消息披上互联网广告的外套。上个月，在瑞士达沃斯举行的世界经济论坛上，大数据是框定的主题之一。该论坛的一份报告，《大数据，大影响》，宣告了数据成为一种新型的经济资产，就像货币或者黄金一样。 里克·斯莫兰（Rick Smolan），《生活中的一天（Day in the Life）》系列摄影的作者，正计划在今年晚些时候启动一个名为《大数据的人类面孔》的项目。斯莫兰先生是一位狂热份子，称大数据有可能成为“人类的仪表盘”，能够作为一项智能工具帮助与贫穷、犯罪以及污染作战。隐私的倡导者则持怀疑的态度，警告说大数据就是老大哥（注：Big Data is Big Brother，看过乔治·奥威尔的《1984》的诸位对‘Big Brother’应该不会感到陌生），只不过是披上了企业的外衣。 什么是大数据？这是一种文化基因（meme），一个营销术语，确实如此，不过也是技术领域发展趋势的一个概括，这一趋势打开了理解世界和制定决策的新办法之门。根据技术研究机构IDC的预计，大量新数据无时不刻不在涌现，它们以每年50%的速度在增长，或者说每两年就要翻一番多。并不仅仅是数据的洪流越来越大，而且全新的支流也会越来越多。比方说，现在全球就有无数的数字传感器依附在工业设备、汽车、电表和板条箱上。它们能够测定方位、运动、振动、温度、湿度、甚至大气中的化学变化，并可以通信。 将这些通信传感器与计算智能连接在一起，你就能够看到所谓的物联网（Internet of Things）或者工业互联网（Industrial Internet）的崛起。对信息访问的改善也为大数据趋势推波助澜。比如说，政府数据—就业数字等其他信息正在稳步移植到Web上。2009年，华盛顿通过启动Data.gov进一步打开了数据之门，该网站令各种政府数据向公众开放。 数据不仅变得越来越普遍，而且对于计算机来说也变得更加可读。这股大数据浪潮当中大部分都是桀骜不驯的—都是一些像Web和那些传感数据流的文字、图像、视频那样难以控制的东西。这被称为是非结构数据，通常都不是传统数据库的腹中物。 不过，从互联网时代浩瀚的非结构数据宝藏中收获知识和洞察的计算机工具正在快速普及。处在一线的是正在迅速发展的人工智能技术，像自然语言处理、模式识别以及机器学习。 那些人工智能技术可以被应用到多个领域。比方说，Google的搜索及广告业务，还有它那已经在加州驰骋了数千英里的实验性机器人汽车， 这些都使用了一大堆的人工智能技巧。这些都是令人怯步的大数据挑战，需要解析大量的数据，并要马上做出决策。 反过来，新数据的充裕又加速了计算的进展—这就是大数据的良性循环。比方说，机器学习算法就是从数据中学习的，数据越多，机器学得就越多。我们就拿Siri这款苹果去年秋季引入的iPhone对话及问答应用作为例子吧。该应用的起源还要追溯到一个五角大楼的研究项目，并在随后拆分出了一家硅谷的初创企业。苹果于2010年收购了Siri，然后不断地给它喂数据。现在，随着人们提供了数以百万计的问题，Siri正变成一位越来越老练的个人助手，为iPhone用户提供了提醒、天气预报、饭店建议等服务，其回答的问题数如宇宙般不断膨胀。 麻省理工学院斯隆管理学院的经济学家Erik Brynjolfsson说，要想领会大数据的潜在影响，你得看看显微镜。发明于4个世纪之前的显微镜，使得人们以前所未有的水平观看和测量事物—细胞级。这是测量的一次革命。 Brynjolfsson教授解释说，数据的测量正是显微镜的现代等价物。比如说，Google的搜索，Facebook的文章以及Twitter的消息，使得在产生行为和情绪时对其进行精细地衡量成为可能。 Brynjolfsson说，在商业、经济等其他领域，决策将会越来越以数据和分析为基础，而非靠经验和直觉。“我们可以开始科学化很多了”，他评论道。 数据优先的思考是有回报的，这方面存在着大量的轶事证据。最出名的仍属《点球成金（Moneyball）》，这本迈克尔·路易斯（Michael Lewis）2003年出的书，记录了预算很少的奥克兰运动家队（Oakland A）如何利用数据和晦涩难懂的棒球统计识别出被低估的球员的故事。大量的数据分析不仅已成为棒球的标准，在其他体育运动中亦然，包括英式足球在内，且在去年由布拉德·皮特（Brad Pitt）主演的同名电影上映之前老早就这么做了。 零售商，如沃尔玛和Kohl’s，则分析销售、定价和经济、人口、天气方面的数据来为特定的门店选择合适的产品，并确定降价的时机。物流公司，如UPS，挖掘货车交付时间和交通模式方面的数据以调整路线。 而在线约会服务，像Match.com，则不断仔细查看其上个人特点、反应以及沟通的Web列表以便改进男女配对约会的算法。在纽约警察局的领导之下，美国全国的警察局都在使用计算机化的地图，并对诸如历史犯罪模式、发薪日、体育活动、降雨及假日等变量进行分析，以期预测出有可能的犯罪“热点”，并在那些地方预先部署警力。 Brynjolfsson教授与另外两位同事一道进行的研究于去年公布，研究认为，由数据来指导管理正在美国的整个企业界扩散并开始取得成效。他们研究了179家大型的公司后发现，那些采用“数据驱动决策制定”者其获得的生产力要比通过其他因素进行解释所获得的高出5到6个百分点。 大数据的预测能力也正在被探索中，并在公共卫生、经济发展及经济预测等领域有获得成功的希望。研究人员已发现，Google搜索请求中诸如“流感症状”和“流感治疗”之类的关键词出现的高峰要比一个地区医院急诊室流感患者增加出现的时间早两三个星期（而急诊室的报告往往要比浏览慢两个星期左右）。 全球脉动（Global Pulse），这项由联合国新发起的行动计划，希望大数据能对全球的发展起到杠杆作用。该组织将会用自然语言破译软件对社交网络中的消息以及短信进行所谓的情绪分析—以帮助预测出特定地区失业、开支缩减或疾病爆发的情况。其目标是使用数字化的预警信号来预先指导援助计划，比方说，预防一个地区出现倒退回贫困的情况。 研究表明，在经济预测方面，Google上房产相关搜索量的增减趋势相对于地产经济学家的预测而言是一个更加准确的预言者。美联储，还有其他者均注意到了这一点。去年7月，美国国家经济研究局主持了一个题为“大数据的机遇”的研讨会，探讨其对经济专业的影响。  大数据已经转变了对社会网络如何运转的研究。在上世纪六十年代，在一次著名的社会关系实验中，哈佛大学的米尔格兰姆（Stanley Milgram）利用包裹作为其研究媒介。他把包裹发往美国中西部的志愿者，指导他们将包裹发给波士顿的陌生人，但不是直接发过去；参与者只能将包裹发给自己认识的某个人。包裹易手的次数平均值少得不同寻常，大概只有6次。这就是“小世界现象”的一个经典体现，由此也形成了一个流行语“六度分隔”。 今天，社交网络研究包括了发掘巨量的在线集体行为的数字数据集。其中的发现包括：你认得但不常联系的人—也即社会学上称为“弱联系”的人，是职位空缺内部消息的最佳来源。他们在一个略微不同于你的密友圈的社交世界中穿梭，所以能够看到一些你和自己最好的朋友看不到的机会。 研究人员能够看出影响的模式，可以知道某个主题的交流什么时候最热—就拿跟踪Twitter的标签趋势来说吧。这个在线的透明玻璃鱼缸就是观摩巨量人群实时行为的一扇窗口。“我需要理解某项活动的爆发，我在数据中寻找热点，”康奈尔大学的Jon Kleinberg教授说：“你只能通过大数据才能做到这一点。” 诚然，大数据自身也存在风险。统计学者和计算机科学家指出，巨量数据集和细颗粒度的测量会导致出现“错误发现”的风险增加。斯坦福大学的统计学教授特Trevor Hastie说，在大规模的数据干草堆中寻找一根有意义的针，其麻烦在于“许多稻草看起来也像针（注：呵呵，看起来这比大海捞针还要困难，因为千人一面）”。 大数据还为恶搞统计和带偏见的实情调查研究提供了更多的原材料。这就是老花招—事实我已经知道了，现在让我们来把它们给找出来吧，的新诀窍—高科技。乔治梅森大学的数学家Rebecca Goldin说，这就是“数据利用最有害的方式之一。” 在利用计算机及数学模型的情况下，我们已经驯服和理解了数据。这些模型，正如文学之隐喻，是一种解释的简化。它们对于理解是很有用的，不过也存在局限性。隐私倡导者警告说，根据在线调查，模型有可能推导出一种不公平或带歧视性的相关性及统计推断，从而影响到某人的产品、银行贷款及医疗保险。 尽管存在这些告诫，但大势似乎已经不可逆转。数据已在驾驶位就坐。它就在那里，它是有用的，是有价值的，甚至还很时尚。 资深数据分析师，长久以来朋友一听到他们谈自己工作就感到厌烦的人，现在却突然变得对他们好奇起来。这些分析师称，此乃拜《点球成金》之所赐，不过实际情况远非如此。“文化改变了”，哥伦比亚大学的统计及政治科学家Andrew Gelman说：“大家认为数字和统计有趣，好玩。现在它是很酷的东西了。”","title":"大数据时代"},{"content":"1) NFC近场通讯: NFC是Near Field Communication缩写，即近距离无线通讯技术，它是一种非接触式识别和互联技术，能够在移动设备、消费类电子产品、PC 和智能控件工具间进行近距离无线通信。NFC技术的出现为人们提供了一种更加简单、触控式的解决方案，能够让消费者更加简单直观地交换信息、访问内容与服务。   2) 裸眼3D技术 3）AR技术： Augmented Reality(中文翻成增强实境)，这个词近来在网上出现的越来越多，Augmented Reality可以算是Virtual Reality〈虚拟实境〉当中的一支，不过略为不同的是，Virtual Reality是创造一个全新的虚拟世界出来，而Augmented Reality则是强调『虚实结合』 http://baike.baidu.com/view/4993017.html?fromTaglist 4）4G技术 如果手机移动网速可以达到100M，OMG手机就会很强大 以上原文参考：http://mobile.yesky.com/10/30427510.shtml 5)虚拟现实    防真技术，AR都可以看做虚拟现实的简化版，虚拟试衣是其中一个应用。虚拟现实的技术出现已经很久，在大学的时候有一段时间对这个比较感兴趣。10年前在跟同事们讨论技术发展的时候，也做过这方面的应用想法，当时想做的几个软件或设备：电子书，试衣，发型等。由于技术和行动力不够作罢，很回忆那段可以憧憬未来的日子。 可参考http://mobile.csdn.net/a/20120216/311921.html文章了解。 ============== 个人感觉以后手机将是一个超大应用集成系统，很多现有的卡功能，证件功能都可以用手机搞定。远程健康检查，个人健康追踪也可以用手机搞定，结合物联网技术，你拿着手机购物就可以识别真伪....等.当然这需要芯片，感应器等设备的集成和微型化。 广告语：拥有手机就可拥抱一切！ 广告语：没有做不到，只有想不到！   CSDN就应该多发些这类文章。跨领域技术融合，应用方面的。 以后接着加，当做学习记录. 6)Siri不是什么以及Siri是什么 • Siri不只是 – 语音识别系统； – 持续自学习系统； – 上下文感知系统； – 传统的AI基础架构； • Siri是什么 – 集成上述优点的服务导向的新型AI架构 – 本质上是个通过多轮对话的用户意图识别系统 – 集成了语音识别、自然语言处理、用户意图分析、任务控制系统等多种技术的综合AI框架 – 目前我看到最像人类大脑运行机制的系统  7)Personalized Assistant That Learns Program（PAL)； – Cognitive Assistant That Learns and Organizes(CALO)； 8)Redis是什么 REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。Redis提供了一些丰富的数据结构，包括lists, sets, ordered sets 以及hashes ，当然还有和Memcached一样的strings结构.Redis当然还包括了对这些数据结构的丰富操作。 Redis的优点 性能极高– Redis能支持超过 100K+ 每秒的读写频率。 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。 丰富的特性– Redis还支持 publish/subscribe, 通知, key 过期等等特性。  9) DSL领域专用语言(domain specific language / DSL)      所谓DSL领域专用语言(domain specific language / DSL)，其基本思想是“求专不求全”，不像通用目的语言那样目标范围涵盖一切软件问题，而是专门针对某一特定问题的计算机语言. 业务自然语言（Business Nature Language）是DSL的一个重要分支。它的产生是基于这样的一些事实：对于大多数企业应用而言，使用一些类似自然语言的语法和结构构造DSL是不错的选择；通过业务自然语言，可以推动和促进业务人员和程序员之间的沟通；类自然语言的DSL相较其他形式的DSL重用起来较为容易。正是由于上述这些特点，BNL类DSL在DSL的实践中是最流行的。我个人就曾在三个不同的项目里实现了针对不同领域的BNL类DSL，我甚至在Smalltalk语法的基础上修改提炼，得到了一种具有通用语法表达的脚本语言。利用它可以方便地构造DSL。    DSM:Domain SpecificModeling 领域规范模型   ","title":"新概念新技术收集系列(1)"},{"content":"A Maximum Entropy Approach to Natural Language Processing （自然语言处理的最大熵方法 ）         最大熵的方法可以追溯到圣经时期（Biblical times）。但是，到了目前计算机已经变得足够的强大，在现实世界中的实际问题，比如统计估计、模式识别等问题可以用到最大熵的概念来处理。在本文，我们介绍一个基于最大熵的统计方法。展示了一个用最大似然方法来来构造的最大熵模型，以及描述了怎么样使这个方法更加有效，以上的例子是自然语言处理中的几个问题。 1 简介        统计模型是用来解决构造预测随机过程行为（the behavior of a random process）的随机模型（stochastic model）的问题。在构造这个随机模型中，经常会得到一个行为产生的结果样本知识。给定了这个样本，它是随机行为的不完整的知识状态，建模问题是把这些知识变成行为的表现。然后，我们可以利用这些表现知识来预测未来的过程的行为。 棒球经理采用平均击球次数，这些数据从以前的有效击球中收集到，来计算一个球员在下一场比赛胜利的概率。因为，相应的可以知道他们操纵了那些首发阵容。华尔街的投机者（他们是报酬最高的统计建模师）通过对以往的股票走势建模，来预测明天的波动情况。利用预测到的未来走势，以此来改变他们的投资组合。自然语言研究者设计了语言和声学模型，用在语音识别系统和相关的应用上。         在过去的几十年中，人们目睹了自然语言统计模型的预测能力的显著进步。以语言模型为例Bahl et al. (1989)用决策树模型，Della Pietra et al. (1994)用自动推断链接语法（automatically inferred link grammars）来对语言中的长距离联系（long range correlations）进行建模。在句法分析上，Black et al. (1992)描述了如何自动从标注的文本中抽取语法规则，并把这些规则用在语法统计模型上。在语音识别上，Lucassen and Mercer (1984)介绍了自动发现单词发音中单词拼写转换相关特征的技术。        虽然在变动中，但是所有的统计模型都面临两两个基本的任务。第一是决定一组统计数据来捕捉随机过程的行为。给定了一组数据，第二个任务是建立这些过程的准确模型，这些模型能够预测未来过程的结果。第一个任务称为特征选择；第二个任务称为模型选择。在后面我们用最大熵的概念来统一的表示这两个任务。","title":"自然语言处理的最大熵方法 之1"},{"content":"  /*版权声明：可以任意转载，转载时请务必标明文章原始出处和作者信息 .*/                                               自然语言处理领域的两种创新观念                              张俊林                       timestamp:2006年11月26日        自然语言处理作为一个研究领域，曾经是一个颇为冷门的方向，但是现在随着互联网搜索概念股的疯狂被投资人追捧，搜索和自然语言处理逐渐成为学术领域的显学。借着感恩节的当口，让我们这些靠自然语言处理技术混饭吃的兄弟们也表达一下感激之情：感谢CCTV,感谢CHANNEL V....不对，排错马屁了。应该是：国际主义阵营感谢GOOGLE，民族主义阵营感谢百度，是你们在纳斯达克上市给了我们这些人混口饭吃的机会，使得我们从吃不饱饭的非洲难民阵营进化到勉强能吃饱的社会主义初级阶段那群人的阵营，顺便还带给我们跨入吃得更好的资本主义阵营的梦想。    其实，真正应该感谢的是互联网，现在互联网的数据实在是太多了，所以现在大家上网面临的问题不是没有信息的问题，而是信息太多找不到自己所需要的信息，这个时候搜索和语言处理就体现出用武之地了。我们需要采取技术手段把过多的嘈杂的信息整理的头头是道，这样网民才能便捷地找到自己想要的东西。所以我个人乐观的认为，随着互联网的发展，搜索和自然语言处理会成为越来越重要的工具。     自然语言处理作为一个研究领域，其成长历程应该说是比较坎坷的。很早的时候，也曾风光过，通俗的说就是：咱也阔过。那时候研究人员都采用规则的方法，就是大家想一些处理规则，然后计算机按照人想的规则去处理文本。开始大家都还是很乐观的，期望自然语言处理能够大师拳脚，很快应用到各行各业。但是现实的残酷很快打碎了人们的美梦，发现现实世界的复杂不是人想出一些规则就能搞定，而且规则多了还会出现规则之间打架的问题。总而言之，自然语言处理(NLP)成为了一个鸡肋方向，食指无味，气质可惜。直到统计方法破石而出，NLP才见到了一丝曙光，并且有渐渐光大门楣，光宗耀祖的趋势。现在统计方法基本上占了所有NLP子领域的山头， 漫山遍插统计大王旗，统计方法应用效果也确实不错。基本上可以进入实用阶段了。    但是，目前NLP学术研究基本上处于发展平台期，就是说大局已定，能做的就是在一些细枝末节的方向上做些修修补补的工作，你去看ACL/COLING这些最高级别的国际会议的论文就知道所言非虚，一个研究领域进入平台期 的标志是：假设你几年不看论文，等想起来去看最新的论文，发现大家还是在一个圈子里面绕来绕去的。现在的研究圈子模式已经变成了： 各种数学模型是一个万能工具箱，研究人员从这个工具箱里面取出不同的工具，然后用这些工具来进行修修补补的工作。场景基本上如下：   A博士说了：你用隐马尔科夫分词？那我用隐马尔科夫标注词性；此时又跳出来一位B博士：你们太落后了，居然还在玩隐马尔科夫？我都玩到最大熵了。话音未落，C博士飞起一脚把B博士踢下台去：瞧你那熊样，还最大熵呢?你以为现在才是二十一世纪初啊(B博士敬佩而又无辜的眼光望着台上的C博士，挠着头想：难道现在不是二十一世纪初么），听说过CRF么？我不仅CRF了，我都CRF好几年了。     总而言之，现在NLP研究基本上和补鞋匠的工作有的一拼。就好像用不同型号的胶水来补不同牌子的鞋子一样，看着挺热闹，其实没啥意思在补也不能把一双布鞋补成一双运动鞋，顶多是把一双破布鞋补成看上去不那么破的布鞋而已。有时候，补完一个小洞后又露出一个大洞，只是布鞋匠不说而已。     说说我理解的NLP的两种创新。其实，其他领域估计也差不多，而且，我的看法看起来相当象废话，其实基本上就是废话，世上废话本来就很多， 在多两句也无妨。     一种创新是研究模式的颠覆，这需要大智慧，是所谓的大创新。就像刚开始的规则方法的出现，后来统计方法的一枝独秀，再到最近的大家都嚷嚷要把统计和规则结合起来搞。当然，我个人对两者结合的效果持怀疑态度，因为以我愚钝的智力看不出两者到底有多大的互补性，至于是否真有效那就走着瞧吧。现在需要的是一种完全不同的处理思路，至于是什么，估计谁也不知道，NLP呼唤爱因斯坦。    另外一种创新是应用创新，就是说大家采用的核心技术其实差不多，都那么点货，其实你也不用藏着掖着，你怎么做的外人不知道，内人 还不知道么？这个时候最好的方法是用同样的核心技术做不一样的应用。应用创新可能是目前更加值得关注的创新方法。          至于搜索研究领域，跟NLP处境差不多，基本上是难兄难弟的关系。从最初的内容匹配到后一阶段的链接分析，在之后基本上停留在链接分析上没怎么动过窝，大家一样在从事补鞋的工作。         说道搜索，就顺便谈谈国内的搜索公司，其实百度也好，雅虎也好，包括后起之秀搜狗，奇虎也好。大家用的什么技术估计自己心理都有数，哪个敢跳出来说我有独门秘笈？如果真跳出来了，只能问候一声：骗子你好。除此之外，无话可谈。大家技术上其实都差不多，可能闻道有先后，但是道就是那些道。        ","title":"自然语言处理领域的两种创新观念"},{"content":"访问控制列表（Access Control List，ACL） 是路由器和交换机接口的指令列表，用来控制端口进出的数据包。ACL适用于所有的被路由协议，如IP、IPX、AppleTalk等。这张表中包含了匹配关系、条件和查询语句，表只是一个框架结构，其目的是为了对某种访问进行控制。 ACL介绍 　　信息点间通信和内外网络的通信都是企业网络中必不可少的业务需求，但是为了保证内网的安全性，需要通过安全策略来保障非授权用户只能访问特定的网络资源，从而达到对访问进行控制的目的。简而言之，ACL可以过滤网络中的流量，是控制访问的一种网络技术手段。 　　ACL的定义也是基于每一种协议的。如果路由器接口配置成为支持三种协议（IP、AppleTalk以及IPX）的情况，那么，用户必须定义三种ACL来分别控制这三种协议的数据包。 　　[1] ACL的作用 　　ACL可以限制网络流量、提高网络性能。例如，ACL可以根据数据包的协议，指定数据包的优先级。 　　ACL提供对通信流量的控制手段。例如，ACL可以限定或简化路由更新信息的长度，从而限制通过路由器某一网段的通信流量。 　　ACL是提供网络安全访问的基本手段。ACL允许主机A访问人力资源网络，而拒绝主机B访问。 　　ACL可以在路由器端口处决定哪种类型的通信流量被转发或被阻塞。例如，用户可以允许E-mail通信流量被路由，拒绝所有的Telnet通信流量。 　　例如：某部门要求只能使用 WWW 这个功能，就可以通过ACL实现； 又例如，为了某部门的保密性，不允许其访问外网，也不允许外网访问它，就可以通过ACL实现。 ACL 3p原则 　　记住 3P 原则，您便记住了在路由器上应用 ACL 的一般规则。您可以为每种协议 (per protocol)、每个方向 (per direction)、每个接口 (per interface) 配置一个 ACL： 　　每种协议一个 ACL 要控制接口上的流量，必须为接口上启用的每种协议定义相应的 ACL。 　　每个方向一个 ACL 一个 ACL 只能控制接口上一个方向的流量。要控制入站流量和出站流量，必须分别定义两个 ACL。 　　每个接口一个 ACL 一个 ACL 只能控制一个接口（例如快速以太网 0/0）上的流量。 　　ACL 的编写可能相当复杂而且极具挑战性。每个接口上都可以针对多种协议和各个方向进行定义。示例中的路由器有两个接口配置了 IP、AppleTalk 和 IPX。该路由器可能需要 12 个不同的 ACL — 协议数 (3) 乘以方向数 (2)，再乘以端口数 (2)。 ACL的执行过程 　　一个端口执行哪条ACL，这需要按照列表中的条件语句执行顺序来判断。如果一个数据包的报头跟表中某个条件判断语句相匹配，那么后面的语句就将被忽略，不再进行检查。 　　数据包只有在跟第一个判断条件不匹配时，它才被交给ACL中的下一个条件判断语句进行比较。如果匹配（假设为允许发送），则不管是第一条还是最后一条语句，数据都会立即发送到目的接口。如果所有的ACL判断语句都检测完毕，仍没有匹配的语句出口，则该数据包将视为被拒绝而被丢弃。这里要注意，ACL不能对本路由器产生的数据包进行控制。 ACL的分类 　　目前有两种主要的ACL:标准ACL和扩展ACL。其他的还有标准MAC ACL、时间控制ACL、以太协议 ACL 、IPv6 ACL等。 　　标准的ACL使用 1 ~ 99 以及1300~1999之间的数字作为表号，扩展的ACL使用 100 ~ 199以及2000~2699之间的数字作为表号。 　　标准ACL可以阻止来自某一网络的所有通信流量，或者允许来自某一特定网络的所有通信流量，或者拒绝某一协议簇（比如IP）的所有通信流量。 　　扩展ACL比标准ACL提供了更广泛的控制范围。例如，网络管理员如果希望做到“允许外来的Web通信流量通过，拒绝外来的FTP和Telnet等通信流量”，那么，他可以使用扩展ACL来达到目的，标准ACL不能控制这么精确。 　　在标准与扩展访问控制列表中均要使用表号，而在命名访问控制列表中使用一个字母或数字组合的字符串来代替前面所使用的数字。使用命名访问控制列表可以用来删除某一条特定的控制条目，这样可以让我们在使用过程中方便地进行修改。 在使用命名访问控制列表时，要求路由器的IOS在11.2以上的版本，并且不能以同一名字命名多个ACL，不同类型的ACL也不能使用相同的名字。 　　随着网络的发展和用户要求的变化，从IOS 12.0开始，思科（CISCO）路由器新增加了一种基于时间的访问列表。通过它，可以根据一天中的不同时间，或者根据一星期中的不同日期，或二者相结合来控制网络数据包的转发。这种基于时间的访问列表，就是在原来的标准访问列表和扩展访问列表中，加入有效的时间范围来更合理有效地控制网络。首先定义一个时间范围，然后在原来的各种访问列表的基础上应用它。 　　基于时间访问列表的设计中，用time-range 命令来指定时间范围的名称，然后用absolute命令，或者一个或多个periodic命令来具体定义时间范围。[2] 正确放置ACL 　　ACL通过过滤数据包并且丢弃不希望抵达目的地的数据包来控制通信流量。然而，网络能否有效地减少不必要的通信流量，这还要取决于网络管理员把ACL放置在哪个地方。 　　假设在的一个运行TCP/IP协议的网络环境中，网络只想拒绝从RouterA的T0接口连接的网络到RouterD的E1接口连接的网络的访问，即禁止从网络1到网络2的访问。 　　根据减少不必要通信流量的通行准则，网管员应该尽可能地把ACL放置在靠近被拒绝的通信流量的来源处，即RouterA上。如果网管员使用标准ACL来进行网络流量限制，因为标准ACL只能检查源IP地址，所以实际执行情况为：凡是检查到源IP地址和网络1匹配的数据包将会被丢掉，即网络1到网络2、网络3和网络4的访问都将被禁止。由此可见，这个ACL控制方法不能达到网管员的目的。同理，将ACL放在RouterB和RouterC上也存在同样的问题。只有将ACL放在连接目标网络的RouterD上（E0接口），网络才能准确实现网管员的目标。由此可以得出一个结论: 标准ACL要尽量靠近目的端。 　　网管员如果使用扩展ACL来进行上述控制，则完全可以把ACL放在RouterA上，因为扩展ACL能控制源地址（网络1），也能控制目的地址（网络2），这样从网络1到网络2访问的数据包在RouterA上就被丢弃，不会传到RouterB、RouterC和RouterD上，从而减少不必要的网络流量。因此，我们可以得出另一个结论：扩展ACL要尽量靠近源端。ACL的主要的命令　命令描述access-list 定义访问控制列表参数ip access-group 指派一个访问控制列表到一个接口ip access-list extended 定义一个扩展访问控制列表Remark 注释一个访问控制列表show ip access-list 显示已配置的访问控制列表 定义ACL时所应遵循的规范 　　（1）ACL的列表号指出了是哪种协议的ACL。各种协议有自己的ACL，而每个协议的ACL又分为标准ACL和扩展ACL。这些ACL是通过ACL列表号区别的。如果在使用一种访问ACL时用错了列表号，那么就会出错误。 　　（2）一个ACL的配置是每协议、每接口、每方向的。路由器的一个接口上每一种协议可以配置进方向和出方向两个ACL。也就是说，如果路由器上启用了IP和IPX两种协议栈，那么路由器的一个接口上可以配置IP、IPX两种协议，每种协议进出两个方向，共四个ACL。 　　（3）ACL的语句顺序决定了对数据包的控制顺序。在ACL中各描述语句的放置顺序是很重要的。当路由器决定某一数据包是被转发还是被阻塞时，会按照各项描述语句在ACL中的顺序，根据各描述语句的判断条件，对数据报进行检查，一旦找到了某一匹配条件就结束比较过程，不再检查以后的其他条件判断语句。 　　（4）最有限制性的语句应该放在ACL语句的首行。把最有限制性的语句放在ACL语句的首行或者语句中靠近前面的位置上，把“全部允许”或者“全部拒绝”这样的语句放在末行或接近末行，可以防止出现诸如本该拒绝(放过)的数据包被放过(拒绝)的情况。 　　（5）新的表项只能被添加到ACL的末尾，这意味着不可能改变已有访问控制列表的功能。如果必须改变，只有先删除已存在的ACL，然后创建一个新ACL，将新ACL应用到相应的接口上。 　　（6）在将ACL应用到接口之前，一定要先建立ACL。首先在全局模式下建立ACL，然后把它应用在接口的出方向或进方向上。在接口上应用一个不存在的ACL是不可能的。 　　（7）ACL语句不能被逐条的删除，只能一次性删除整个ACL。 　　（8）在ACL的最后，有一条隐含的“全部拒绝”的命令，所以在 ACL里一定至少有一条“允许”的语句。 　　（9）ACL只能过滤穿过路由器的数据流量，不能过滤由本路由器上发出的数据包。 　　（10）在路由器选择进行以前，应用在接口进入方向的ACL起作用。 　　（11）在路由器选择决定以后，应用在接口离开方向的ACL起作用。　 　　ACL会议(Annual Meeting of the Association for Computational Linguistics) 　　ACL会议是自然语言处理与计算语言学领域最高级别的学术会议，由计算语言学协会主办，每年一届。涉及对话(Dialogue)篇章(Discourse)评测( Eval)信息抽取( IE) 信息检索( IR) 语言生成(LanguageGen) 语言资源(LanguageRes) 机器翻译(MT) 多模态(Multimodal) 音韵学/ 形态学( Phon/ Morph) 自动问答(QA) 语义(Semantics) 情感(Sentiment) 语音(Speech) 统计机器学习(Stat ML) 文摘(Summarisation) 句法(Syntax) 等多个方面。 ACL 常见问题 　　1) “ACL 的最后一条语句都是隐式拒绝语句” 是什么意思？ 　　每个 ACL 的末尾都会自动插入一条隐含的 deny 语句，虽然ACL中看不到这条语句，它仍起作用。隐含的 deny 语句会阻止所有流量，以防不受欢迎的流量意外进入网络。 　　2) 配置ACL后为什么没有生效？ 　　在创建访问控制列表之后，必须将其应用到某个接口才可开始生效。ACL 控制的对象是进出接口的流量。 参考资料 1 轻松学习理解ACL访问控制列表   http://technic.xkq.com/20090220/5710.html 2 用ACL构建防火墙体系打造安全的管理体系   http://technic.xkq.com/20090430/95904.html 扩展阅读： 1 用动画演示 ACL 的作用及过滤数据包的方法： 2 http://www.visualland.net/view.php?cid=1384&protocol=ACL&title=1.%20ACL%20basic&ctype=2","title":"ACL 简介"},{"content":"【摘要】 - 生成模型：无穷样本==》概率密度模型 = 产生模型==》预测 - 判别模型：有限样本==》判别函数 = 预测模型==》预测 【简介】 简单的说，假设o是观察值，q是模型。 如果对P(o|q)建模，就是Generative模型。其基本思想是首先建立样本的概率密度模型，再利用模型进行推理预测。要求已知样本无穷或尽可能的大限制。 这种方法一般建立在统计力学和bayes理论的基础之上。 如果对条件概率(后验概率) P(q|o)建模，就是Discrminative模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。代表性理论为统计学习理论。 这两种方法目前交叉较多。 【判别模型Discriminative Model】——inter-class probabilistic description 又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)， p(class|context)。 利用正负例和分类标签，focus在判别模型的边缘分布。目标函数直接对应于分类准确率。 - 主要特点： 寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。 - 优点: 分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。 能清晰的分辨出多类或某一类与其他类之间的差异特征 在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好 适用于较多类别的识别 判别模型的性能比生成模型要简单，比较容易学习 - 缺点： 不能反映训练数据本身的特性。能力有限，可以告诉你的是1还是2，但没有办法把整个场景描述出来。 Lack elegance of generative: Priors, 结构, 不确定性 Alternative notions of penalty functions, regularization, 核函数 黑盒操作: 变量间的关系不清楚，不可视 - 常见的主要有： logistic regression SVMs traditional neural networks Nearest neighbor Conditional random fields(CRF): 目前最新提出的热门模型，从NLP领域产生的，正在向ASR和CV上发展。 - 主要应用： Image and document classification Biosequence analysis Time series prediction 【生成模型Generative Model】——intra-class probabilistic description 又叫产生式模型。估计的是联合概率分布（joint probability distribution），p(class, context)=p(class|context)*p(context)。 用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。在机器学习中，或用于直接对数据建模（用概率密度函数对观察到的draw建模），或作为生成条件概率密度函数的中间步骤。通过使用贝叶斯rule可以从生成模型中得到条件分布。 如果观察到的数据是完全由生成模型所生成的，那么就可以fitting生成模型的参数，从而仅可能的增加数据相似度。但数据很少能由生成模型完全得到，所以比较准确的方式是直接对条件密度函数建模，即使用分类或回归分析。 与描述模型的不同是，描述模型中所有变量都是直接测量得到。 - 主要特点： 一般主要是对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。 只关注自己的inclass本身（即点左下角区域内的概率），不关心到底 decision boundary在哪。 - 优点: 实际上带的信息要比判别模型丰富， 研究单类问题比判别模型灵活性强 模型可以通过增量学习得到 能用于数据不完整（missing data）情况 modular construction of composed solutions to complex problems prior knowledge can be easily taken into account robust to partial occlusion and viewpoint changes can tolerate significant intra-class variation of object appearance - 缺点： tend to produce a significant number of false positives. This is particularly true for object classes which share a high visual similarity such as horses and cows 学习和计算过程比较复杂 - 常见的主要有： Gaussians, Naive Bayes, Mixtures of multinomials Mixtures of Gaussians, Mixtures of experts, HMMs Sigmoidal belief networks, Bayesian networks Markov random fields 所列举的Generative model也可以用disriminative方法来训练，比如GMM或HMM，训练的方法有EBW(Extended Baum Welch),或最近Fei Sha提出的Large         Margin方法。 - 主要应用： NLP: Traditional rule-based or Boolean logic systems (Dialog and Lexis-Nexis) are giving way to statistical approaches (Markov models and stochastic context grammars) Medical Diagnosis: QMR knowledge base, initially a heuristic expert systems for reasoning about diseases and symptoms been augmented with decision theoretic formulation Genomics and Bioinformatics Sequences represented as generative HMMs 【两者之间的关系】 由生成模型可以得到判别模型，但由判别模型得不到生成模型。 Can performance of SVMs be combined elegantly with flexible Bayesian statistics? Maximum Entropy Discrimination marries both methods: Solve over a distribution of parameters (a distribution over solutions) 【参考网址】 http://prfans.com/forum/viewthread.php?tid=80 http://hi.baidu.com/cat_ng/blog/item/5e59c3cea730270593457e1d.html http://en.wikipedia.org/wiki/Generative_model http://blog.csdn.net/yangleecool/archive/2009/04/05/4051029.aspx ================== 比较三种模型：HMMs and MRF and CRF http://blog.sina.com.cn/s/blog_4cdaefce010082rm.html HMMs(隐马尔科夫模型): 状态序列不能直接被观测到(hidden)； 每一个观测被认为是状态序列的随机函数； 状态转移矩阵是随机函数，根据转移概率矩阵来改变状态。 HMMs与MRF的区别是只包含标号场变量，不包括观测场变量。 MRF(马尔科夫随机场) 将图像模拟成一个随机变量组成的网格。 其中的每一个变量具有明确的对由其自身之外的随机变量组成的近邻的依赖性(马尔科夫性)。 CRF(条件随机场),又称为马尔可夫随机域 一种用于标注和切分有序数据的条件概率模型。 从形式上来说CRF可以看做是一种无向图模型，考察给定输入序列的标注序列的条件概率。 在视觉问题的应用： HMMs:图像去噪、图像纹理分割、模糊图像复原、纹理图像检索、自动目标识别等 MRF: 图像恢复、图像分割、边缘检测、纹理分析、目标匹配和识别等 CRF: 目标检测、识别、序列图像中的目标分割 P.S. 标号场为隐随机场，它描述像素的局部相关属性，采用的模型应根据人们对图像的结构与特征的认识程度，具有相当大的灵活性。 空域标号场的先验模型主要有非因果马尔可夫模型和因果马尔可夫模型。 参考自：link ================================================== 概率图模型之生成模型与判别模型 自然语言处理中，经常要处理序列标注问题（分词、词性标注、组快分析等），为给定的观察序列标注标记序列。 令o和s分别代表观察序列和标记序列， 根据贝叶斯公式， 1 生成模型和判别模型的定义 对o和s进行统计建模，通常有两种方式： (1)生成模型 构建o和s的联合分布p(s,o) (2)判别模型 构建o和s的条件分布p(s|o) 2 判别模型和生成模型的对比 （1）训练时，二者优化准则不同 生成模型优化训练数据的联合分布概率； 判别模型优化训练数据的条件分布概率，判别模型与序列标记问题有较好的对应性。 （2）对于观察序列的处理不同 生成模型中，观察序列作为模型的一部分； 判别模型中，观察序列只作为条件，因此可以针对观察序列设计灵活的特征。 （3）训练复杂度不同 判别模型训练复杂度较高。 （4）是否支持无指导训练 生成模型支持无指导训练。 From：link ==================================================== 一个通俗易懂的解释，摘录如下： Let’s say you have input data x and you want to classify the data into labels y. A generative model learns the joint probability distribution p(x,y) and a discriminative model learns the conditional probability distribution p(y|x) – which you should read as ‘the probability of y given x’. Here’s a really simple example. Suppose you have the following data in the form (x,y):        (1,0), (1,0), (2,0), (2, 1) p(x,y) is              y=0   y=1            -----------       x=1 | 1/2   0       x=2 | 1/4   1/4 p(y|x) is              y=0   y=1            -----------       x=1 | 1     0       x=2 | 1/2   1/2 If you take a few minutes to stare at those two matrices, you will understand the difference between the two probability distributions. The distribution p(y|x) is the natural distribution for classifying a given example x into a class y, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model p(x,y), which can be tranformed into p(y|x) by applying Bayes rule and then used for classification. However, the distribution p(x,y) can also be used for other purposes. For example you could use p(x,y) to generate likely (x,y) pairs. From the description above you might be thinking that generative models are more generally useful and therefore better, but it’s not as simple as that. This paper is a very popular reference on the subject of discriminative vs. generative classifiers, but it’s pretty heavy going. The overall gist is that discriminative models generally outperform generative models in classification tasks. 另一个解释，摘录如下： 判别模型Discriminative Model，又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)， p(class|context)。 生成模型Generative Model，又叫产生式模型。估计的是联合概率分布（joint probability distribution），p(class, context)=p(class|context)*p(context)。","title":"判别模型和生成模型"},{"content":"摘要：概述了英文文本自动校对技术的产生背景，分析了英文文本的特点，并对英文文本校对的技术难点和解决方法以及国内外的研究现状进行了阐述。在此基础上重点讨论了基于贝叶斯方法的英文单词自动校对技术的实现。 关键字：贝叶斯方法；模糊自动校对；非词错误(单词错误)   English words based on the Bayesianapproach to fuzzy automatic proofreading technology and its application Abstract: An overview of theEnglish text ,automatic proofreading technology Background ,and analysis of the characteristics of the Englishtext ,and English text ,proofreading ,technical difficulties and solutions ,aswell as domestic and international research status are described .On this basis,focused on the automatic proofreading technology to achieve the English word based onBayesian methods. Key words:  Bayesian methods ; Fuzzy Automatic Proofreading;Non-word error (word error)     1 引言及相关研究          英文文本自动校对是自然语言处理的主要应用领域之一。对英文文本自动校对的已经经历了相当长的时间。早期的研究方向主要是针对光学字符识别(Optical Character Recognition)、语音识别以及程序代码中的拼写错误。而现今英文文本自动校对技术主要应用于机读文本(Machine-Readable Text)的自动校对。对于英文文本自动校对的研究最早应用于IBM Thomas J.Watson 研究中心研制的IBM/360和IBM/370，即使用UNIX实现了一个TYPO英文拼写检查器；之后在1971年，斯坦福大小的Ralph Gorin在DEC-10机上实现了一个英文拼写检查程序Spell.     近些年，随着文本自动校对的不断改善和进步，一些成果已经慢慢走近平常人的生活：如Word中自带的英文拼写检查功能、Google等搜索引擎中的文本自动校对和提示功能等。     在应用键盘录入英文字符时，常见的错误有以下几种：非词错误、真词错误和句法语义错误。非词错误是指文本中那些被词边界分隔出得字符串，根本就不是词典中的词。如以下错误：raech→reach，thew→they等就是非词错误。造成这种错误的原因是由于粗心地输入造成的，这些错误可以概括为替换错误、易位错误、丢失错误和插入错误等。真词错误是由于输入人员粗心地输入所形成的可以在字典中查找到的单词，但却不是想要的单词。如在输入from时发生了真词错误是from变成了form,而form也是字典中的单词。真诚错误虽说不像非词错误那样导致输入的单词不存在，但是往往会导致所输入的单词与上下文搭配不当，并不是当前语境所需要的单词。句法语义错误则是由于真词错误造成的或在输入时丢失某个单词甚至一整行文本。一般而言，可以将非词错误称为单词错误，而将真词错误称为上下文相关的文本错误。     本文主要针对的是非词错误的自动校对的研究。非词错误的纠错方法主要有误拼词典法、词形距离法、最小编辑距离法、相似键法、骨架键法、N-gram法、基于规则的技术、词典及神经网络技术等。     误拼字典法：收集大规模真实文本中拼写出错的英文单词，并给出相应的正确拼写，建造一个无歧义的误拼字典。在进行英文单词拼写检查时，查找误拼字典。若命中则说明该单词拼写有误，该单词的正确拼写为纠错建议。该方法的特点是侦错和纠错一体化，效率高。但是英文拼写错误具有随机性，很难保证误拼字典的无歧义性和全面性。因此查准率低、校对效果差。     词形距离法：是一种基于最大相似度和最小串间距离的英文校对法。核心思想是构造单向的似然性函数，若该单词在词典中则单词拼写正确；否则，按照似然性函数在词典中找到一个与误拼单词最相似的词作为纠错候选词。该方法的特点是节省存储空间，能反应一定的常见拼写错误统计规律，是一种模糊校对的方法。     最小编辑距离法：通过计算误拼字符串与词典中某个词间的最小编辑距离来确定纠错候选词。所谓最小编辑距离是指将一个词串转换为另一词串所需的最少的编辑操作次数，而该编辑操作则包括：插入、删除、易位和替换等。     相似键法：相似键技术是将每个字符串与一个键相对应。使那些拼写相似的字符串具有相同或相似的键。当计算出某个误拼字符串的键值之后，它将给出一个指针，指向所有与该误拼字符串相似的单词，并将它们作为给误拼字符串的纠错建议。     骨架键法：通过构建骨架键词典，在英文单词出现错误时，先抽取出该错误单词的骨架键，然后再去查骨架键词典，将词典中与该词具有相同骨架键的正确单词作为该单词的纠错建议。     N-gram法：基于n元文法，通过对大规模英文文本的统计得到单词与单词间的转移概率矩阵。当检测到某英文单词不在词典中时，查转移概率矩阵，取转移概率大于某给定阈值的单词作为纠错建议。     基于规则的技术：利用规则的形式将通常的拼写错误模式进行表示，这些规则可用来将拼写错误变为有效的单词。对于一个误拼字符串，应用所有合适的规则从词典中找到一些与之相对应的单词作为结果，并对每个结果根据事先赋予生成它的规则的概率估计计算一个数值，根据这个数值对所有候选结果排序。   2.贝叶斯方法介绍          贝叶斯方法是概率论中的一个重要的方法，其应用延伸到各个问题领域，特别是需要做出概率测试的地方，同时贝叶斯方法也是机器学习的核心方法之一。究其主要原因是：现实世界本身是不确定的，人类的观察能力是有限的，某些时候我们需要提供一个猜测或者假设。     现在给出一个具体的例子来了解一下贝叶斯方法：一所学校里面有60%的男生，40%的女生。男生总是选择穿长裤，而女生则一半穿长裤一半穿裙子。根据以上信息我们可以计算出随机选择一个学生，该学生穿长裤的概率和穿裙子的概率各有多大？     假设学校里面人的总数是 U 个。60% 的男生都穿长裤，于是我们得到了 U * P(Boy) *P(Pants|Boy) 个穿长裤的（男生）（其中 P(Boy) 是男生的概率 = 60%，这里可以简单的理解为男生的比例；P(Pants|Boy) 是条件概率，即在 Boy 这个条件下穿长裤的概率是多大，这里是 100% ，因为所有男生都穿长裤）。40% 的女生里面又有一半（50%）是穿长裤的，于是我们又得到了 U * P(Girl) * P(Pants|Girl) 个穿长裤的（女生）。加起来一共是 U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl) 个穿长裤的，其中有 U * P(Girl) * P(Pants|Girl) 个女生。形式化的结果为：P(Girl|Pants)= P(Girl) * P(Pants|Girl) / [P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)]     贝叶斯公式的一般形式是：P(B|A) =P(A|B)*P(B)/[P(A|B)*P(B) + P(A|~B)*P(~B)],实质即：P(B|A)*P(A)=P(AB).   3.拼写纠正理论          问题：argmaxP(猜测想输入的单词|实际输入的单词)，即给定一个单词，选择和它最相似的拼写正确的单词。如果单词拼写正确那么无须进一步的操作。     分析：上述问题可以形式化为：argmaxcP(c|w) ,按照上文贝叶斯公式可以变形为argmaxcP(w|c)*P(c)/P(w) ，即用户可以输错任何词，因此对于任何c而言，出现w的概率P(w)都是一样的。     P(c):文章中出现一个正确拼写单词c的概率。这个概率完全由英语这种语言特点所决定，可以称其为语言模型。     P(w|c):用户想输入c，但是却敲成w的概率。代表用户会以多大的概率把c敲成w,可以称其为误差模型。     argmaxc：用来枚举所有可能的c并且选择概率最大的。   4.英文单词模糊自动校对器的实现     第一步：首先计算P(c),可以读入一个巨大的文本文件big.txt.其中包含大约几百万个单词，相当于语料库。     第二步：如果输入的单词就是语料库中的单词则不进行后续操作。如果用户输入的单词不在词典中，则产生编辑距离(Edit Distance)为2的所有可能单词。所谓编辑距离为1就是对用户输入的单词进行删除1个字符、添加一个字符、交换相邻字符、替换一个字符所产生的所有单词。因此编辑距离为2即对编辑距离为1的这些单词再进行一次编辑操作。最后产生的单词集可能会很大，但是只保留语料库中存在的单词。对于语料库中不存在的单词不予显示。     第三步：假设事件c是我们猜测用户可能想要输入的单词，而事件w是用户实际输入的错误单词，根据贝叶斯公式可知： P(c|w) = P(w|c) * P(c) / P(w)。这里的P(w)对于每个单词都是一样的，可以忽略。而P(w|c)是误差模型(Error Model)，是用户想要输入w却输入c的概率，这是需要大量样本数据和事实依据来得到的，为了简单起见也忽略掉。因此，我们可以找出编辑距离为2的单词集中P(c)概率最大的几个来提示用户。产生编辑距离为2的单词时，应该让编辑距离为1的单词具有更高的优先级。并且当用户输入的单词长度较长时，产生编辑距离为2的单词可能会花费一些时间。所以可以优化为首先产生编辑距离为1的单词，如果与词典做差集后为空，再产生编辑距离为2的单词。     开发工具是eclipse。CPU是Inter i5处理器，内存：3G，系统是Windows xp Java代码： package seu.wjm; import java.io.BufferedReader; import java.io.FileReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.Collections; import java.util.Comparator; import java.util.HashMap; import java.util.HashSet; import java.util.LinkedList; import java.util.List; import java.util.Map; import java.util.Map.Entry; import java.util.Set; import java.util.regex.Pattern;     /**  * 针对英文单词模糊自动校对器的实现  * @author wjm  *  */ public class SpellCheckWithEnglish {          privatestatic final char[] ALPHABET =\"abcdefghijklmnopqrstuvwxyz\".toCharArray();         public staticvoid main(String[] args) throws Exception {            newSpellCheckWithEnglish().start();     }         public voidstart() throws IOException {            //1.Build language model                 //langModel为语言模型对象，是HashMap<String,Double>类型                 //dictionary为langModel中key的集合,即单词的集合（语料库）           Map<String, Double> langModel =buildLanguageModel(\"d:\\\\spellCheck\\\\big.txt\");           Set<String> dictionary = langModel.keySet();                                 System.out.println(\"Please input a word:\");            //2.Read user input loop           BufferedReader reader = new BufferedReader(newInputStreamReader(System.in));            Stringinput;            while((input = reader.readLine()) != null) {                  input = input.trim().toLowerCase();                  //当程序输入bye，那么程序结束                  if (\"bye\".equals(input))                        break;                  //若语料库中包含输入的单词，那么程序继续                  if (dictionary.contains(input)){                            System.out.println(\"语料库中包含\"+ input + \"\");                            continue;                   }                                                                    long startTime = System.currentTimeMillis();                                  /*                   // 3.Build set for word in edit distance and remove inexistent in dictionary                  Set<String> wordsInEditDistance2 =buildEditDistance2Set(langModel, input);                  wordsInEditDistance2.retainAll(dictionary);*/                   //3 wordsInEditDistance代表的是编辑距离                  //buildEditDistance1Set是编辑距离为1的字符的集合                  //buildEditDistance2Set是编辑距离为2的字符的集合                  Set<String> wordsInEditDistance = buildEditDistance1Set(langModel,input);                  //保留仅在语料库中出现的编辑距离为1的字符集                  wordsInEditDistance.retainAll(dictionary);                  if (wordsInEditDistance.isEmpty()) {                        wordsInEditDistance = buildEditDistance2Set(langModel, input);                        wordsInEditDistance.retainAll(dictionary);                        if (wordsInEditDistance.isEmpty()) {                               System.out.println(\"Failed to check this spell\");                                continue;                        }                   }                    // 4.Calculate Bayes's probability                   // c - correct word we guess, w - wrongword user input in reality                  // argmax P(c|w) = argmax P(w|c) * P(c) / P(w)                  // we ignore P(w) here, because it's the same for all words                  List<String> guessWords = guessCorrectWord(langModel,wordsInEditDistance);                  System.out.printf(\"Do you mean %s ? Cost time: %.3fsecond(s)\\n\",                               guessWords.toString(), (System.currentTimeMillis() - startTime) /1000D);            }                }         /**      *buildLanguageModel方法建立语言模型      * @paramsample      * @return      * @throwsIOException      */     privateMap<String, Double> buildLanguageModel(String sample)                  throws IOException {            Map<String,Double> langModel = new HashMap<String, Double>();           BufferedReader reader = new BufferedReader(new FileReader(sample));            //正则表达式：+一次或多次匹配前面的字符或子表达式            //*零次或多次匹配前面的字符或子表达式            //？零次或一次匹配前面的字符或子表达式            Patternpattern = Pattern.compile(\"[a-zA-Z]+\");            Stringline;           //totalCnt：单词出现的频数            inttotalCnt = 0;            while((line = reader.readLine()) != null) {                                 //按照空格进行单词分隔                  String[] words = line.split(\" \");                  for (String word : words) {                        if (pattern.matcher(word).matches()) {                                           //toLowerCase()转换为小写字母                                word =word.toLowerCase();                                //langModel为语言模型对象，是HashMap<String,Double>类型                                //其中get(word)，word为关键字。                                //通过get(key)获取相应的value，即概率                                Double wordCnt =langModel.get(word);                                if (wordCnt ==null)                                      langModel.put(word, 1D);                                else                                      langModel.put(word, wordCnt + 1D);                                totalCnt++;                        }                   }            }           reader.close();                      //setValue设置value的值，即通过除法求出相应的概率            for(Entry<String, Double> entry : langModel.entrySet())                  entry.setValue(entry.getValue() / totalCnt);                       returnlangModel;     }         /**      * 建立编辑距离为1的字符集合，返回类型为Set<String>      * @paramlangModel      * @param input      * @return      */     privateSet<String> buildEditDistance1Set(                  Map<String, Double> langModel,                  String input) {           Set<String> wordsInEditDistance = new HashSet<String>();            char[]characters = input.toCharArray();                       //Deletion: delete letter[i]            //删除一个字符            for (int i = 0; i < input.length();i++)                  wordsInEditDistance.add(input.substring(0,i) + input.substring(i+1));                       //Transposition: swap letter[i] and letter[i+1]            //交换相邻的两个字符            for (inti = 0; i < input.length()-1; i++)                  wordsInEditDistance.add(input.substring(0,i) + characters[i+1] +                                characters[i] +input.substring(i+2));                       //Alteration: change letter[i] to a-z            //用其他字符代替一个字符            for (inti = 0; i < input.length(); i++)                  for (char c : ALPHABET)                        wordsInEditDistance.add(input.substring(0,i) + c +input.substring(i+1));                       // Insertion:insert new letter a-z            //插入一个新的字符            for (inti = 0; i < input.length()+1; i++)                  for (char c : ALPHABET)                        wordsInEditDistance.add(input.substring(0,i) + c + input.substring(i));                        returnwordsInEditDistance;     }         /**      * 建立编辑距离为2的字符集合，返回类型为Set<String>      * 在编辑距离为1的字符集合的基础上再次进行建立编辑距离为1的字符集合的操作      * @paramlangModel      * @param input      * @return      */     privateSet<String> buildEditDistance2Set(                  Map<String, Double> langModel,                  String input) {           Set<String> wordsInEditDistance1 =buildEditDistance1Set(langModel, input);           Set<String> wordsInEditDistance2 = new HashSet<String>();            for(String editDistance1 : wordsInEditDistance1)                  wordsInEditDistance2.addAll(buildEditDistance1Set(langModel,editDistance1));           wordsInEditDistance2.addAll(wordsInEditDistance1);            returnwordsInEditDistance2;     }         /**      * 返回猜测的可能符合要求的单词      * @paramlangModel      * @paramwordsInEditDistance      * @return      */     privateList<String> guessCorrectWord(                  final Map<String, Double> langModel,                  Set<String> wordsInEditDistance) {           List<String> words = newLinkedList<String>(wordsInEditDistance);            //将满足要求的单词进行排序显示输出           Collections.sort(words, new Comparator<String>() {                  @Override                  public int compare(String word1, String word2) {                            //按照概率的大小，从大到小进行输出                        return langModel.get(word2).compareTo(langModel.get(word1));                   }            });            //若符合条件的集合中元素个数大于5个，则仅输出概率最高的前五个单词            returnwords.size() > 5 ? words.subList(0, 5) : words;     }   } 5.实验中存在的问题以及待继续的工作     经过1000次的统计，得出以下结论：达到了简洁, 快速开发和运行速度这三个目标, 不过准确率不算太好.该算法用于英文单词的模糊自动校对，基本上达到了预期的效果。平均匹配率可以达到80%以上。但是对于较短的并且较为常用的单词，该算法所得的匹配率明显不符合要求。同时对于较长的单词，可能返回的结果并不是预期的值。虽然结果不是特别准确，但是却刺激了我的兴趣，希望通过对算法进行适当地改进可以提高算法结果的准确率。该算法仅仅解决了单个单词的自动校对问题，尚且没有达到例如Google输入栏中输入若干个单词可以进行整个句子或者文本的匹配的程度。若想达到文本自动匹配，需要针对更大的语料库进行训练，利用文本上下文的同现与搭配特征或者利用规则或语言学知识进行自动匹配。总的来说，还是通过训练-测试的方法，依据的理论基础仍然是贝叶斯公式。     参考文献： [1].Spellcheckingby computer   by Roger Mitton [2].SPEECHand LANGUAGE PROCESSING   by DanielJurafsky and James H.Martin [3].张仰森，俞士汶. 文本自动校对技术研究综述.计算机应用研究 2005-08-24 [4].AUTOMATICSPELLING CORRECTION IN SCIENTIFIC AND SCHOLARLY TEXT by JOSEPH J. POLLOCKand ANTONIO ZAMORA   Communications ofthe ACM 1984 [5].Techniquesfor Automatically Correcting Words in Text by KAREN KUKICH  ACM 1992 [6].汪维嘉，陈芙蓉，秦进，陆汝占.一种基于窗口技术的中文文本自动校对方法贵州大学学报  2003年5月第20卷第2期  ","title":"基于贝叶斯方法的英文单词模糊自动校对技术及其应用研究"},{"content":" 社交网站的数据挖掘与分析 基本信息 原书名： Mining the Social Web 原出版社： O'Reilly 作者： (美)Matthew A.Russell    [作译者介绍] 译者： 师蓉 丛书名： O'Reilly 精品图书系列 出版社：机械工业出版社 ISBN：9787111369608 上架时间：2012-2-24 出版日期：2012 年2月   http://product.china-pub.com/199037     内容简介 　　《社交网站的数据挖掘与分析》介绍组合社交网络数据、分析技术，如何通过可视化帮助你找到你一直在社交世界中寻找的内容，以及那些你都不知道存在的有用信息。每个独立章节介绍了在社交网络的不同领域挖掘数据的技术，这些领域包括博客和电子邮件。你所需要具备的就是一定的编程经验和学习基本的python工具的意愿。主要内容包括：获得社交网络世界里的直观概要，使用github上灵活的脚本来获取社交网络api中的数据，学习如何应用便捷的python工具来交叉分析你所收集的数据，通过xhtml朋友网络探索基于微格式的社交联系，通过基于html5和javascript工具集的网络技术建立交互式可视化等。 　　 facebook、twitter和linkedln产生了大量宝贵的社交数据，但是怎样才能找出谁通过社交媒介正在进行联系?他们在讨论些什么?或者他们在哪儿?本书简洁而且具有可操作性，它将揭示如何回答这些问题甚至更多的问题。你将学到如何组合社交网站数据、分析技术，如何通过可视化找到你一直在社交世界中寻找的内容，以及你闻所未闻的有用信息。 　　《社交网站的数据挖掘与分析》每章都介绍了在社交网络的不同领域挖掘数据的技术，这些领域包括博客和电子邮件。你所需要具备的就是一定的编程经验和学习基本的python工具的意愿。     目录 《社交网站的数据挖掘与分析》 前言 1 第1章绪论：twitter 数据的处理 9 python 开发工具的安装 9 twitter 数据的收集和处理 11 小结 24 第2章微格式：语义标记和常识碰撞 26 xfn 和朋友 27 使用xfn 来探讨社交关系 29 地理坐标：兴趣爱好的共同主线 37 （以健康的名义）对菜谱进行交叉分析 41 对餐厅评论的搜集 43 小结 45 第3章邮箱：虽然老套却很好用 47 mbox：unix 的入门级邮箱 48 mbox+couchdb= 随意的email 分析 54 将对话线程化到一起 70 使用simile timeline 将邮件“事件”可视化 79 分析你自己的邮件数据 82 小结 84 .第4章twitter ：朋友、关注者和setwise 操作 85 rest 风格的和oauth-cladded api 86 干练而中肯的数据采集器 90 友谊图的构建 108 小结 116 第5章twitter：tweet ，所有的tweet ，只有tweet 118 笔pk 剑：和tweet pk 机枪（?!?） 118 对tweet 的分析（每次一个实体） 121 并置潜在的社交网站（或#justinbieber vs #teaparty） 144 对大量tweet 的可视化 155 小结 163 第6章linkedin ：为了乐趣（和利润？）将职业网络聚类 164 聚类的动机 165 按职位将联系人聚类 167 获取补充个人信息 183 从地理上聚类网络 188 小结 192 第7章google buzz：tf-idf 、余弦相似性和搭配 194 buzz=twitter+ 博客（???） 195 使用nltk 处理数据 198 文本挖掘的基本原则 201 查找相似文档 208 在二元语法中发buzz 215 利用gmail 221 在中断之前试着创建一个搜索引擎…… 225 小结 226 第8章博客及其他：自然语言处理（等） 228 nlp ：帕累托式介绍 228 使用nltk 的典型nlp 管线 231 使用nltk 检测博客中的句子 234 对文件的总结 237 以实体为中心的分析：对数据的深层了解 245 小结 256 第9章facebook ：一体化的奇迹 257 利用社交网络数据 258 对facebook 数据的可视化 274 小结 294 第10 章语义网：简短的讨论 296 发展中的变革 296 人不可能只靠事实生活 297 期望 301  ","title":"社交网站的数据挖掘与分析"},{"content":"转自：http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html 算法：Viterbi algorithm 和Forward-Backward Algorithm。 小实验程序：http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg3.html 隐马尔科夫模型HMM自学 （1） 介绍 崔晓源 翻译 我们通常都习惯寻找一个事物在一段时间里的变化规律。在很多领域我们都希望找到这个规律，比如计算机中的指令顺序，句子中的词顺序和语音中的词顺序等等。一个最适用的例子就是天气的预测。 首先，本文会介绍声称概率模式的系统，用来预测天气的变化 然后，我们会分析这样一个系统，我们希望预测的状态是隐藏在表象之后的，并不是我们观察到的现象。比如，我们会根据观察到的植物海藻的表象来预测天气的状态变化。 最后，我们会利用已经建立的模型解决一些实际的问题，比如根据一些列海藻的观察记录，分析出这几天的天气状态。 Generating Patterns 有两种生成模式：确定性的和非确定性的。 确定性的生成模式：就好比日常生活中的红绿灯，我们知道每个灯的变化规律是固定的。我们可以轻松的根据当前的灯的状态，判断出下一状态。 非确定性的生成模式：比如说天气晴、多云、和雨。与红绿灯不同，我们不能确 定下一时刻的天气状态，但是我们希望能够生成一个模式来得出天气的变化规律。我们可以简单的假设当前的天气只与以前的天气情况有关，这被称为马尔科夫假 设。虽然这是一个大概的估计，会丢失一些信息。但是这个方法非常适于分析。 马尔科夫过程就是当前的状态只与前n个状态有关。这被称作n阶马尔科夫模型。最简单的模型就当n=1时的一阶模型。就当前的状态只与前一状态有关。（这里要注意它和确定性生成模式的区别，这里我们得到的是一个概率模型）。下图是所有可能的天气转变情况： 对于有M个状态的一阶马尔科夫模型，共有M*M个状态转移。每一个状态转移都有其一定的概率，我们叫做转移概率，所有的转移概率可以用一个矩阵表示。在整个建模的过程中，我们假设这个转移矩阵是不变的。 该矩阵的意义是：如果昨天是晴，那么今天是晴的概率为0.5，多云的概率是0.25，雨的概率是0.25。注意每一行和每一列的概率之和为1。 另外，在一个系统开始的时候，我们需要知道一个初始概率，称为 向量。 到现在，我们定义了一个一阶马尔科夫模型，包括如下概念： 状态：晴、多云、雨 状态转移概率 初始概率 隐马尔科夫模型HMM自学 （2） 马尔科夫模型也需要改进！ 崔晓源 翻译 当一个隐士不能通过直接观察天气状态来预测天气时，但他有一些水藻。民间的传说告诉我们水藻的状态与天气有一定的概率关系。也就是说，水藻 的状态与天气时紧密相关的。此时，我们就有两组状态：观察状态（水藻的状态）和隐含状态（天气状态）。因此，我们希望得到一个算法可以为隐士通过水藻和马 尔科夫过程，在没有直接观察天气的情况下得到天气的变化情况。 更容易理解的一个应用就是语音识别，我们的问题定义就是如何通过给出的语音信号预测出原来的文字信息。在这里，语音信号就是观察状态，识别出的文字就是隐含状态。 这里需要注意的是，在任何一种应用中，观察状态的个数与隐含状态的个数有可能不一样的。下面我们就用隐马尔科夫模型HMM来解决这类问题。 HMM 下图是天气例子中两类状态的转移图，我们假设隐状态是由一阶马尔科夫过程描述，因此他们相互连接。 隐状态和观察状态之间的连线表示：在给定的马尔科夫过程中，一个特定的隐状态对应的观察状态的概率。我们同样可以得到一个矩阵： 注意每一行（隐状态对应的所有观察状态）之和为1。 到此，我们可以得到HMM的所有要素：两类状态和三组概率 两类状态：观察状态和隐状态； 三组概率：初始概率、状态转移概率和两态对应概率（confusion matrix） 隐马尔科夫模型HMM自学 （3） HMM 定义 崔晓源 翻译 HMM是一个三元组 (,A,B). the vector of the initial state probabilities; the state transition matrix; the confusion matrix; 这其中，所有的状态转移概率和混淆概率在整个系统中都是一成不变的。这也是HMM中最不切实际的假设。 HMM的应用 有三个主要的应用：前两个是模式识别后一个作为参数估计 (1) 评估 根据已知的HMM找出一个观察序列的概率。 这类问题是假设我们有一系列的HMM模型，来描述不同的系统（比如夏天的天气变化规律和冬天的天气变化规律）， 我们想知道哪个系统生成观察状态序列的概率最大。反过来说，把不同季节的天气系统应用到一个给定的观察状态序列上，得到概率最大的哪个系统所对应的季节就 是最有可能出现的季节。（也就是根据观察状态序列，如何判断季节）。在语音识别中也有同样的应用。 我们会用forward algorithm 算法来得到观察状态序列对应于一个HMM的概率。 (2) 解码 根据观察序列找到最有可能出现的隐状态序列 回想水藻和天气的例子，一个盲人隐士只能通过感受水藻的状态来判断天气状况，这就显得尤为重要。我们使用viterbi algorithm来解决这类问题。 viterbi算法也被广泛的应用在自然语言处理领域。比如词性标注。字面上的文字信息就是观察状态，而词性就是隐状态。通过HMM我们就可以找到一句话上下文中最有可能出现的句法结构。 (3) 学习 从观察序列中得出HMM 这是最难的HMM应用。也就是根据观察序列和其代表的隐状态，生成一个三元组HMM (,A,B)。使这个三元组能够最好的描述我们所见的一个现象规律。 我们用forward-backward algorithm来解决在现实中经常出现的问题--转移矩阵和混淆矩阵不能直接得到的情况。 总结 HMM可以解决的三类问题 Matching the most likely system to a sequence of observations -evaluation, solved using the forward algorithm; determining the hidden sequence most likely to have generated a sequence of observations - decoding, solved using the Viterbi algorithm; determining the model parameters most likely to have generated a sequence of observations - learning, solved using the forward-backward algorithm. 隐马尔科夫模型HMM自学 （4-1）Forward Algorithm 找到观察序列的概率 崔晓源 翻译 Finding the probability of an observed sequence 1、穷举搜索方法 对于水藻和天气的关系，我们可以用穷举搜索方法的到下面的状态转移图（trellis）： 图中，每一列于相邻列的连线由状态转移概率决定，而观察状态和每一列的隐状态则由混淆矩阵决定。如果用穷举的方法的到某一观察状态序列的概率，就要求所有可能的天气状态序列下的概率之和，这个trellis中共有3*3=27个可能的序列。 Pr(dry,damp,soggy | HMM) = Pr(dry,damp,soggy | sunny,sunny,sunny) + Pr(dry,damp,soggy | sunny,sunny ,cloudy) + Pr(dry,damp,soggy | sunny,sunny ,rainy) + . . . . Pr(dry,damp,soggy | rainy,rainy ,rainy) 可见计算复杂度是很大，特别是当状态空间很大，观察序列很长时。我们可以利用概率的时间不变性解决复杂度。 2、采用递归方法降低复杂度 我们采用递归的方式计算观察序列的概率，首先定义部分概率为到达trellis中某一中间状态的概率。在后面的文章里，我们把长度为T的观察状态序列表示为： 2a. Partial probabilities, ('s) 在计算trellis中某一中间状态的概率时，用所有可能到达该状态的路径之和表示。 比如在t=2时间，状态为cloudy的概率可以用下面的路径计算： 用t ( j ) 表示在时间t时 状态j的部分概率。计算方法如下： t ( j )= Pr( observation | hidden state is j ) * Pr(all paths to state j at time t) 最后的观察状态的部分概率表示，这些状态所经过的所有可能路径的概率。比如： 这表示最后的部分概率的和即为trellis中所有可能路径的和，也就是当前HMM下观察序列的概率。 Section 3 会给出一个动态效果介绍如何计算概率。 2b.计算初始状态的部分概率 我们计算部分概率的公式为: t ( j )= Pr( observation | hidden state is j ) x Pr(all paths to state j at time t) 但是在初始状态，没有路径到达这些状态。那么我们就用probability乘以associated observation probability计算： 这样初始时刻的状态的部分概率就只与其自身的概率和该时刻观察状态的概率有关。 隐马尔科夫模型HMM自学 （4-2）Forward Algorithm 崔晓源 翻译 书接上文，前一话我们讲到了Forward Algorithm中初始状态的部分概率的计算方法。这次我们继续介绍。 2c.如何计算t>1时刻的部分概率 回忆一下我们如何计算部分概率： t ( j )= Pr( observation | hidden state is j ) * Pr(all paths to state j at time t) 我们可知（通过递归）乘积中第一项是可用的。那么如何得到Pr(all paths to state j at time t) 呢？ 为了计算到达一个状态的所有路径的概率，就等于每一个到达这个状态的路径之和： 随着序列数的增长，所要计算的路径数呈指数增长。但是在t时刻我们已经计算出所有到达某一状态的部分概率，因此在计算t+1时刻的某一状态的部分概率时只和t时刻有关。 这个式子的含义就是恰当的观察概率（状态j下，时刻t+1所真正看到的观察状态的概率）乘以此时所有到达该状态的概 率和（前一时刻所有状态的概率与相应的转移概率的积）。因此，我们说在计算t+1时刻的概率时，只用到了t时刻的概率。这样我们就可以计算出整个观察序列 的概率。 2d.复杂度比较 对于观察序列长度T，穷举法的复杂度为T的指数级；而Forward Algorithm的复杂度为T的线性。 ======================================================= 最后我们给出Forward Algorithm的完整定义 We use the forward algorithm to calculate the probability of a T long observation sequence; where each of the y is one of the observable set. Intermediate probabilities ('s) are calculated recursively by first calculating for all states at t=1. Then for each time step, t = 2, ..., T, the partial probability is calculated for each state; that is, the product of the appropriate observation probability and the sum over all possible routes to that state, exploiting recursion by knowing these values already for the previous time step. Finally the sum of all partial probabilities gives the probability of the observation, given the HMM, . ======================================================= 我们还用天气的例子来说明如何计算t=2时刻，状态CLOUDY的部分概率 怎么样？看到这里豁然开朗了吧。要是还不明白，我就.....................还有办法，看个动画效果： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/forward_algorithm/s3_pg3.html 参数定义： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/forward_algorithm/s3_pg4.html http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/forward_algorithm/s3_pg5.html 最后记住我们使用这个算法的目的（没有应用任何算法都是垃圾），从若干个HMM模型中选出一个最能够体现给定的观察状态序列的模型（概率最大的那个）。 Forward Algorithm （Done） 隐马尔科夫模型HMM自学 （5-1）Viterbi Algorithm 本来想明天再把后面的部分写好，可是睡觉今天是节日呢？一时情不自禁就有打开电脑.......... 找到可能性最大的隐含状态序列 崔晓源 翻译 多数情况下，我们都希望能够根据一个给定的HMM模型，根据观察状态序列找到产生这一序列的潜在的隐含状态序列。 1、穷举搜索方法 我们可以通过穷举的方式列出所有可能隐含状态序列，并算出每一种隐状态序列组合对应的观察状态序列的概率。概率最大的那个组合对应的就是最可能的隐状态序列组合。 Pr(observed sequence | hidden state combination). 比如说上图中的trellis中，最有可能的隐状态序列是使得概率： Pr(dry,damp,soggy | sunny,sunny,sunny), Pr(dry,damp,soggy | sunny,sunny,cloudy), Pr(dry,damp,soggy | sunny,sunny,rainy), . . . . Pr(dry,damp,soggy | rainy,rainy,rainy) 得到最大值的序列。 同样这种穷举法的计算量太大了。为了解决这个问题，我们可以利用和Forward algorithm一样的原理--概率的时间不变性来减少计算量。 2.用递归方式减少复杂度 在给定的观察序列和HMM模型下，我们用一种递归的方式找到最有可能的隐状态序列。同样我们滴定部分概率，即在trellis中到达某一中间状态的概率。然后介绍如何在初始时刻t=1和t>1的时刻分别求解这个部分概率。但要注意，这里的部分概率是到达某一中间状态的概率最大的路径而不是所有概率之和。 2.1部分概率和部分最优路径 看如下trellis 对于trellis中的每个中间状态和结束状态，都存在一条到达它的最优路径。他可能是下图这样： 我们这些路径为部分最优路径，每一条 部分最优路径都对应一个关联概率--部分概率。与Forward algorithm不同是最有可能到达该状态的一条路径的概率。 (i,t)是所有序列中在t时刻以状态i终止的最大概率。当然它所对应那条路径就是部分最优路径。 (i,t)对于每个i,t都是存在的。这样我们就可以在时间T（序列的最后一个状态）找到整个序列的最优路径。 2b. 计算 's 在t = 1的初始值 由于在t=1不存在任何部分最优路径，因此可以用初始状态 向量协助计算。 这一点与Forward Algorithm相同 2c. 计算 's 在t > 1 的部分概率 同样我们只用t-1时刻的信息来得到t时刻的部分概率。 由此图可以看出到达X的最优路径是下面中的一条： (sequence of states), . . ., A, X                                (sequence of states), . . ., B, X or (sequence of states), . . ., C, X 我们希望找到一条概率最大的。回想马尔科夫一阶模型的假设，一个状态之和它前一时刻的状态有关。 Pr (most probable path to A) . Pr (X | A) . Pr (observation | X) 因此到达X的最大概率就是： 其中第一部分由t-1时刻的部分概率得到，第二部分是状态转移概率，第三部分是混淆矩阵中对应的概率。 隐马尔科夫模型HMM自学 （5-2）Viterbi Algorithm 书接前文，viterbi算法已经基本成形...... 崔晓源 翻译 一般化上一篇最后得到的公式我们可以把概率的求解写成： 2d. 反向指针, 's 考虑下面trellis 现在我们可以得到到达每一个中间或者终点状态的概率最大的路径。但是我们需要采取一些方法来记录这条路径。这就需要在每个状态记录得到该状态最优路径的前一状态。记为： 这样argmax操作符就会选择使得括号中式子最大的索引j。 如果有人问，为什么没有乘以混淆矩阵中的观察概率因子。这是因为我们关心的是在到达当前状态的最优路径中，前一状态的信息，而与他对应的观察状态无关。 2e. viterbi算法的两个优点 1）与Forward算法一样，它极大的降低了计算复杂度 2）viterbi会根据输入的观察序列，“自左向右”的根据上下文给出最优的理解。由于viterbi会在给出最终选择前考虑所有的观察序列因素，这样就避免了由于突然的噪声使得决策原理正确答案。这种情况在真实的数据中经常出现。 ================================================== 下面给出viterbi算法完整的定义 1. Formal definition of algorithm The algorithm may be summarised formally as: For each i,, i = 1, ... , n, let : - this intialises the probability calculations by taking the product of the intitial hidden state probabilities with the associated observation probabilities. For t = 2, ..., T, and i = 1, ... , n let : - thus determining the most probable route to the next state, and remembering how to get there. This is done by considering all products of transition probabilities with the maximal probabilities already derived for the preceding step. The largest such is remembered, together with what provoked it. Let : - thus determining which state at system completion (t=T) is the most probable. For t = T - 1, ..., 1 Let : - thus backtracking through the trellis, following the most probable route. On completion, the sequence i1 ... iT will hold the most probable sequence of hidden states for the observation sequence in hand. ================================================== 我们还用天气的例子来说明如何计算状态CLOUDY的部分概率，注意它与Forward算法的区别 还是那句话： 怎么样？看到这里豁然开朗了吧。要是还不明白，我就.....................还有办法，看个动画效果： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg3.html 参数定义： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg4.html http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg5.html 别忘了，viterbi算法的目的是根据给定的观察状态序列找出最有可能的隐含状态序列，别忘了viterbi算法不会被中间的噪音所干扰。 隐马尔科夫模型HMM自学 （6）尾声 崔晓源 翻译 HMM的第三个应用就是learning，这个算法就不再这里详述了，并不是因为他难于理解，而是它比前两个算法要复杂很多。这个方向在语 音处理数据库上有重要的地位。因为它可以帮助我们在状态空间很大，观察序列很长的环境下找到合适HMM模型参数：初始状态、转移概率、混淆矩阵等。  ","title":"隐马尔科夫模型HMM"},{"content":"网络是社会创造的而不是技术创造的。我设计她是为了社会效益--使得大家能协同工作，而不是作为一个技术玩具。网络的终极目标是支持和改善我们在世界上象网络一样的存在。我门使家庭，社团，公司各自更紧密的联系在一起。我们使得千里之外的人互信互利，同时消除躲在人们心中黑暗角落的猜疑。--Tim Berners-Lee  谁适合读这本书？ 如果你有基本的编程背景，并且对挖掘和分析社交网络数据感兴趣，想要把握住随之而来的机会，那么你来对地方了。在读完这本书的前几页后，我们就将开始动手实践。我是个直性子，然而，要事先声明的是，读者最可能要抱怨的是，本书的所有章节都太短。但是在象这种日新月异且充满机遇的领域，这种情况往往是必然的。也就是说，我相信80比20理论，我深信这本书很好的呈现了你想探索的这个领域20％有趣的知识，但需要你花80％的可用的时间。 这本书很短，但它的覆盖面很广。一般来讲，宽度较深度来说要多一些， 这就导致书中一些主题需要更详细的探讨，当然，也有一些深入的部分去探讨有趣的数据挖掘和数据分析技术。你既可以一章一章左右为难的读完本书，来拓宽你在处理社交网络数据的视野，也可以选择一些特别感兴趣的章节来读。换句话来说，每一章节都是一口大小的且完全独立， 但是我特别花心思来按一定顺序编排这些材料，，使得这本书作为一个整体来读会很有趣。 最近几年，社交网站，例如Facebook, Twitter 和 LinkedIn, 从流行，到主流，已经转变成世界普遍现象了。在2010年第一季度， 最流行的社交网站Facebook已经超过Google,成为最经常访问的网站，正好说明了人们上网习惯的改变。断言网络更多是一个社会环境而不是搜索信息的工具，可能还有些言过其实。然而，社交网络正在满足人们很多基本需求，而搜索引擎却不是为这些方面设计的。社交网络正在改变我们线上和线下的生活，它们使得技术能给我们带来最好的（有时候是最坏的）东西。由于社交网络的爆发，现实世界和虚拟世界的界线进一步变窄了。 基本上，本书的每一章都会针对社交网络，以数据挖掘，数据分析，可视化等技术来回答以下几种问题： 谁可能认识谁，他们有哪些共同的朋友？  人们之间交流的频度是怎么样的？ 人们交流在多大程度上是对称的呢? 在网络上，谁是最安静的人，而谁又是最善谈的人呢？ 在网络上，谁是最有影响力的人，而谁又是最受欢迎的人呢？ 人们正在谈论什么（它是否有趣）？ 回答这些类型的问题的一般方法是将两个或多个的人联系在一起，找出表明联系存在的上下文。而回答这些类型的问题仅仅是复杂处理过程的开始， 但是你必须从某处开始着手，毕竟低处的水果容易采摘嘛， 我们要感谢那些设计良好的社交网络APIs,以及众多开源的工具。 粗略的来讲，本书将社交网络看作人，活动，事件，概念等组成的图。作为行业领袖，Google和Facebook都在开始积极推进以图为中心的术语而不是以网络为中心，他们几乎是同时推出以图为基础的API。事实上，Tim Berners-Lee已经暗示他本应该用Giant Global Graph (GGG)这个词而不是World Wide Web(WWW)，因为在定义因特网的拓扑结构时，“网”和“图”两个词很容易互换。无论Lee对网络的最初观点被怎么看待，但随着社交数据的发展，网络正变得越来越丰富。当我们回顾几年前，很容易发现，原有的社交网络所创造的第二和第三级（译：不知指什么）的影响，使得我们认识到真正的语义网络，二者之间的距离正在缩短。 ﻿﻿谁不适合读这本书？ 如果您要从头开发自己的自然语言处理程序，探索新的可视化库， 或者创建任何技术前沿的东西，都不在这本书的范围内。您会感到非常失望，如果您买这本书是因为这些原因之一。然而，用短短的几百页，去阐明文本分析或记录匹配也是不现实的，这也不是我的目标，但是也不意味着，这本书不会帮你获得一些对困难问题的合理解决方案，将这些解决方案运用到社交网站领域，会是一个很有趣的过程。它也不意味着将极大的兴趣投入到上述这些有意思的研究领域不是个好主意。像这样一本简短的书不是不可能满足你很多方面的需求的。 在这个时代是很明显的，这本书默认你是能连上因特网的。这也不是一本假期读物，因为它涉及很多超链接，其中很多在GitHub －－ 一个社会化的git仓库，很多示例代码放在上面了。","title":"前言"},{"content":"看了吴军（http://www.cs.jhu.edu/~junwu/）的数学之美系列， 有些收获。 数学理论之博大精深，需要细细体会，数学的实际应用，更需探索和发掘；学以致用啊。其中的 贝叶斯、最大熵模型、SVD、布尔代数、布隆过滤器、相关历史等感觉非常有意思，可惜最大熵模型、SVD没怎么看懂。 发现Google黑板报有些文章不错， 吴军还有 “浪潮之巅” 有时间好好看看。 相关链接 数学之美 一 统计语言模型 数学之美 二 谈谈中文分词 数学之美 三 隐含马尔可夫模型在语言处理中的应用 数学之美 四 怎样度量信息? 数学之美 五 简单之美：布尔代数和搜索引擎的索引 数学之美 六 图论和网络爬虫 (Web Crawlers) 数学之美 七 信息论在信息处理中的应用 数学之美 八 贾里尼克的故事和现代语言处理 数学之美 九 如何确定网页和查询的相关性 数学之美 十 有限状态机和地址识别 数学之美 十一 Google 阿卡 47 的制造者阿米特.辛格博士 数学之美 十二 余弦定理和新闻的分类 数学之美 十三 信息指纹及其应用 数学之美 十四 谈谈数学模型的重要性 数学之美 十五 繁与简 自然语言处理的几位精英 数学之美 十六（上）不要把所有的鸡蛋放在一个篮子里 最大熵模型 数学之美 十六（下）不要把所有的鸡蛋放在一个篮子里 最大熵模型 数学之美 十七 闪光的不一定是金子 谈谈搜索引擎作弊问题(Search Engine Anti-SPAM) 数学之美 十八 矩阵运算和文本处理中的分类问题 数学之美 十九 马尔可夫链的扩展 贝叶斯网络 (Bayesian Networks) 数学之美 二十 自然语言处理的教父 马库斯 数学之美 二十一 布隆过滤器（Bloom Filter） 数学之美 二十二 由电视剧《暗算》所想到的 — 谈谈密码学的数学原理","title":"数学之美系列（吴军 Google研究员）"},{"content":"相信google大家都非常的熟悉。她是一个互联网搜索引擎。赖以生存的技术是dfs和mapReduce。那么hadoop就是对其开源的一个产物。 目前facebook、linkedin、amazon、emc、ebay、twitter、IBM、microsoft、apple、hp 、taobao、baidu等都在应用hadoop。 行业背景： 信息社会的信息增量在高速发展 •随着互联网/移动互联网、数码设备、物联网/传感器等技术的发展，全球数据生产在高速增长 •信息成为企业战略资产，市场竞争和政策管制要求越来越多的数据被长期保存 •企业越来越需要长期保存各类数据，以进行用户行为分析、市场研究，信息服务企业更是需要积累越来越多的信息资源 •为了遵从萨巴斯、上网日志审计等管制要求，企业需要长期保存越来越多的生产数据 •信息处理技术的发展使很多数据的价值能够被更好地挖掘和利用 •自然语言处理、语音识别、图像处理技术等 •据IDC研究报告，未来10年全球数据量将以40+%的速度增长，2020年全球数据量将达到35ZB（35,000,000PB），为2009年（0.8ZB）的44倍 简单点一句话：这是一个数据时代。 举例： 1）淘宝选择HBASE——HBase是Apache的Hadoop项目的子项目。 ——HBase是一个分布式的、面向列的开源数据库，该技术来源于Chang et al所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。 ——就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。 ——HBase不同于一般的关系数据库,它是一个适合于非结构化数据存储的数据库.另一个不同的是HBase基于列的而不是基于行的模式。 淘宝选择HBASE的原因 •和Hadoop一样的海量数据处理的能力 •易于横向扩展 •随机读写的高性能 •高可靠性和稳定性 •在互联网公司有较多的适用场景 •单行写入的强一致性 •开源，社区活跃并有大公司支持 应用举例 1*淘宝实时传输平台 –数据每天TB级的数据写入应用 –旧的存储模型（内存+硬盘 ） –发布和订阅的使用场景 2•淘宝指数 –倒排索引的属性查询（Redis ->Hbase） –实时/性能 –客户端Join –高冗余，每行百兆级的数据应用 3*交易历史记录查询系统 –百亿行数据表，千亿级二级索引表 –每天千万行更新 –查询场景简单，检索条件较少 –关系型数据库所带来的问题 –基于userId + time + id rowkey设计 –成本考虑","title":"hadoop学习2"},{"content":"  转自http://www.infoq.com/cn/articles/zjl-sns-automatic-mining 一．为何要在大规模SNS中挖掘兴趣圈子 随着国外的facebook、twitter以及国内的人人、新浪微博等SNS及内容分享平台的逐步流行，如何从上亿的海量用户中自动挖掘兴趣圈子成为了一个有趣也非常必要的工作。所谓“兴趣圈子”，指的是在同一分享平台下，有着共同的兴趣爱好的用户群体，比如新浪微博里哪些用户是对云计算感兴趣的？他们是否形成了一个密切交互的圈子？对这些信息的挖掘是很有趣也很有实际用处的。 如果能够从海量用户中通过自动手段挖掘出一个个的兴趣圈子，对于很多具体应用来说是非常重要的基础数据，比如可以利用用户所属兴趣圈子进行感兴趣人物推荐，或者根据所属圈子的群体特性分析用户的个人兴趣点等，所以在SNS平台下，如何对海量数据自动进行兴趣圈子挖掘是个非常有用的基础功能。 二．如何挖掘兴趣圈子 现在的问题是：给定海量用户，如何才能挖掘出具有相似兴趣的圈子？我们基于微博用户的互动信息，构建了一整套兴趣圈子挖掘算法，并取得了较好的挖掘效果。如果把每个用户想象成一个巨大的图中一个节点，如果用户A对用户B有互动行为（转发，评论等），我们可以在用户A和用户B之间建立一条有向边，通过这种方式可以构建出有上亿节点，几十亿边的巨大的有向图。挖掘兴趣圈子就是在这样的巨图中进行的。我们把兴趣圈子挖掘转换为一个图切割问题的具体应用。图1是这个思路的简化图示例。 图1 兴趣图例子 2.1 图切割问题 图切割问题本质上是一个聚类问题，几乎所有聚类算法的基本思想都是相近的：给定一批数据，自动对数据进行聚类，使得聚合到同一类别的数据之间比较相似，而不同类别之间的数据差异较大。图切割问题也符合这个定义，等于是将图中节点进行聚类，把密集相连的一批节点聚合到一起，而连接比较稀疏的节点尽可能划分到不同的类别中。 如果用相对形式化的语言来描述的话，图切割问题就是：给定n个点(x1, x2…,xn)，聚类的目标是将这n个点分成k个簇，使得同一簇中的数据点比较相似，不同簇间的数据点比较相异。如果按照节点之间的兴趣相似度构建关系图G(V, E)，问题就转化为了在图G上做划分，将图G分成k个子图A1,A2,…Ak,使得划分后子图内包含边的总权值尽可能高，而子图之间边的权重尽可能小。在图1所示的例子中，标为相同颜色的节点可被视为聚合到相同子图中，边的权值直观表示为边的长度，即边越长，两个节点距离越远，即其相似性越小，也就是说其边的权值小。 图切割算法有很多，比如min-cut，min-max cut，ratio cut等等，我们采用了谱聚类算法来挖掘用户兴趣圈子。 2.2谱聚类算法 谱聚类算法和很多其他距离算法相比有很多优点，下文会详述此点，同样的，谱聚类也适合解决图切割问题。 谱聚类有个比较有趣的特性，即这个算法可以将图切割问题转换为求由图形成的矩阵的特征值和对应的特征向量问题，这样就把图切割问题转换为矩阵特征值求解及在其基础上的聚类问题。 图2 谱聚类算法流程 图2是利用谱聚类进行兴趣圈子挖掘的算法流程示意图，首先我们获得用户之间的互动数据，由于谱聚类只能处理无向图，而用户之间的互动数据是有向的，所以首先根据一定规则将有向图转换为无向图，之后就形成了所有用户的兴趣相似性图。根据谱聚类算法要求，将这个相似性图转换为拉普拉斯矩阵，然后对这个矩阵求其前K个特征值及其对应的特征向量，求解前K个特征向量s1,s2,…,sk，组成矩阵S[n][k]（n为用户编号)，这样就将一个原本是n*n的矩阵转换为小很多的n*k矩阵，对S按行进行Kmeans聚类，每一行对应相似兴趣图中一个节点。其最终聚类结果就是谱聚类最终的输出结果。 之所以采取谱聚类来解决这个问题，源于这个算法本身具有的一些优点，比如： 谱聚类具有坚实的理论基础：图谱理论 谱聚类不含凸球形数据分布的隐性假设，而常见的很多聚类算法比如KMeans, EM算法都存在这一假设。比如对于图3所示的例子中，谱聚类的聚类效果比较好。 图3 非凸球形数据 由于谱聚类具备独特的优点，所以近来应用非常广泛（语音识别、文本挖掘等），但是谱聚类的计算复杂度还是较高，所以面对海量数据，如何能够快速计算是个问题。 为了能够处理上亿的海量数据，我们主要采取了两项措施来对原始算法进行改造，首先是利用MPI平台构建分布式计算系统，对于这种计算密集型迭代式应用，通常hadoop平台被认为是不太合适的，所以通过构建MPI分布式平台来加快数据的分布以提升计算速度。 第二项主要改进措施是将谱聚类由平面型聚类（flat)改造为层次聚类（hierarchy),其基本思想也很简单，即通过多次谱聚类迭代，首先将一个巨大的图划分为较少数的密集子图，然后针对每个密集子图再次迭代使用谱聚类来递归地将其划分为较小的密集子图，通过几个层级的切割，也可以有效增加分布式计算效果并大大提快整体运行效率。 当然，除了以上两项主要改进措施，还包含一些相对细小的改进，在此就不赘述细节了。 2.3应用谱聚类在SNS中挖掘兴趣圈子 正像上文所述，大规模SNS用户中挖掘兴趣圈子的问题可以进一步抽象为用户兴趣图的一个图切割问题，我们通过对谱聚类处理大规模数据进行了技术改进后，使得这项技术可以在多机并行环境下较快地处理上亿规模数据的图切割，在兴趣圈子自动挖掘方面既实现了较好的挖掘效果，又能够使得算法处理真实世界的大规模数据，使其在现实中可行而非仅仅停留在小规模数据处理的学术研究阶段。 下面给出三个使用上述技术在新浪微博平台挖掘出的兴趣圈子，因为实际的兴趣圈子很大（大部分包含几十到几百个节点），所以只列出了兴趣圈子的一部分，从这些例子可以看出其效果还是比较理想的。 用户微博ID 微博名 身份说明 1197161814 李开复 创新工场董事长兼首席执行官 1656232852 JackF2 创新工场豌豆实验室 联合创始人 1738208940 宓金华 创新工场魔图精灵项目负责人 1652837301 徐磊Ryan 布丁 创始人 CEO 原创新工场战略发展部总经理 1642333010 张亮 创新工场投资经理；Apple4us 发起人 1926746140 许红梅Grace 创新工场人力资源部副总裁 1650741047 cuijin 创新工场市场总监崔瑾 1676705655 裘伯纯Benjamin 创新工场法务负责人裘伯纯 1751792424 dikanggu 创新工场员工 1419563143 zouyu9631 创新工场员工 ......................................     表1 “李开复”所属兴趣圈子 用户微博ID 微博名 身份说明 1656809190 赵薇 著名演员，代表作《画皮》《还珠格格》等 1829847745 一号立井 李亚鹏 1679085395 邓讴歌 太合麦田音乐制作人 1719232542 那英 内地流行乐天后 1629810574 veggieg 王菲 1496813600 老焦爱民 《杜拉拉升职记》制片人 1768955554 张扬张杨 著名导演 1262945510 廖凡 演员廖凡 1919269943 王一涵 北京中艺博文化传播有限公司董事长兼总经理 1497323383 磨刀哎呦霍霍 编剧霍昕 ..............................     表2 \"赵薇\"所属兴趣圈子 用户微博ID 微博名 身份说明 1922397344 白硕sse 上海证券交易所总工程师，IR与NLP专家 1937618377 林鸿飞 大连理工大学电子信息与电气工程学部 副部长 1684953923 关毅的围脖 哈尔滨工业大学计算机学院教授、博士生导师关毅 1936526225 王斌_ICTIR 中国科学院计算技术研究所副研究员，博士生导师王斌 1808067361 ITNLP 哈尔滨工业大学智能技术与自然语言处理(ITNLP)研究室 1970879995 孙茂松 清华大学计算机科学与技术系教授、中国中文信息学会副理事长孙茂松 1788077877 张颖峰 上海载和网络科技有限公司 研发总监 1340489195 韩先培 中国科学院软件所助理研究员 1497035431 梁斌penny 清华大学计算机科学与技术系在读博士；《走进搜索引擎》《深入搜索引擎》作者，THUIRDB的Coder。 1064649941 张俊林say 《这就是搜索引擎：核心技术详解》作者。本文作者。 .............................     表3 “自然语言处理与信息检索”兴趣圈子 通过大量的聚类数据分析，使用互动数据构建用户兴趣图得出的兴趣圈子大部分属于以下两种类型：一种类型是同事朋友圈子，这是因为线下关系迁移到网络的体现；另外一种比较常见的是兴趣类似的微博用户，比如NLP圈子，NOSQL圈子这种根据讨论技术确定的兴趣圈子等，这是由于共同关注相似话题并经常互动形成的。 三．结束语 大规模SNS与内容分享平台中如何自动挖掘兴趣圈子是个很有趣也非常必要的功能，现有公开文献很少提及超大规模数据如何实现自动挖掘的算法，大多数是在10万以下规模数据进行的研究工作，本文简述了在新浪微博平台通过改造的谱聚类进行的大规模兴趣圈子挖掘，实践表明取得了很好的挖掘效果。当然，现有系统还面临一些问题，比如属于硬聚类，即每个用户只能隶属于一个兴趣圈子，而实际上很可能一个用户属于多个兴趣组中，所以我们面对大规模数据的软聚类，也在进行进一步的研发与改进。 关于作者 张俊林，《这就是搜索引擎：核心技术详解》作者、新浪微博研发人员。","title":"大规模SNS中兴趣圈子的自动挖掘"},{"content":" The First Class: 今天先谈谈AI里面tier-1的conferences, 其实基本上就是AI里面大家比较公认的top conference. 下面同分的按字母序排列.    IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象machine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的\"IJCAI Inc.\"主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer   & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的 是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外,   IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位.    AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可 以给到1+, 也可以给到1-或者2+, 总的来说我给它\"1\". 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些,   特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比 IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了.    COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上 可以看成理论计算机科学和机器学习的交叉, 所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: \"一小群数 学家在开会\". 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT.    CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. IEEE一直有个倾向, 要把 会办成\"盛会\", 历史上已经有些会被它从quality很好的会办成\"盛会\"了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了.    ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行.    ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的 介绍.    NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS'05的论文集是06年出. 会议的名字是 \"Advances in Neural Information Processing Systems\", 所以, 与ICML\\ECML这样 的\"标准的\"机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael   Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给\"外 人\"的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML\\ECML\\COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会.    ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开.      KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开.      SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来.    SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了.   另: 参见sir和lucky的介绍.    UAI (1-): 名字叫\"人工智能中的不确定性\", 涉及表示\\推理\\学习等很多方面, AUAI   (Association of UAI) 主办, 每年开.      The Second Class: tier-2的会议列得不全, 我熟悉的领域比较全一些.      AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显.    ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去.    ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已 经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议 的reputation上升非常明显.    ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年 历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大 距离了.    SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚,   但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少 目前还是相当的.    ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并 来的. 因为这个领域逐渐变冷清, 影响比以前已经小了.    ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直 半冷不热, 所以总是停留在2上.    COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比 ICCV-ECCV和ICML-ECML大得多.    ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,   很难往上升.    ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好 的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算 学习理论的内容.      EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING 相当, 但我觉得它还是要弱一点.    ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面 的内容, 所以它只能保住2-的位置了.      PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把 它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.   但因为ICDM和SDM, 这已经不太可能了. 所以今年的PKDD和ECML虽然还是一起开,   但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑,   如果ECML中不了还可以被PKDD接受).    The Third Class: 列得很不全. 另外, 因为AI的相关会议非常多, 所以能列在tier-3也算不错了, 基本上能 进到所有AI会议中的前30%吧    ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了.      DS (3+): 日本人发起的一个接近数据挖掘的会议.    ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议.    ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的 quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在 其实3+已经不太呆得住了.    PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5.    ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域 的人不重视会议,在该领域它的重要性不如IJCNN.    AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.    CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.    CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC/FUZZ-IEEE这三个会议是 计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World   Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和 其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有 quality非常高的论文, 也有入门新手的习作.      FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍.    GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型.    ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议.    ICIP (3): 图像处理方面最著名的会议之一, 盛会型.    ICPR (3): 模式识别方面最著名的会议之一, 盛会型.    IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名 就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个 session做被提名论文报告, 倒是很热闹.    IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍.    IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议.    PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综 合型会议太多, 所以很难上升.         Combined List: 说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的      tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern   Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and   Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and   Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence      tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems   ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling   ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing   ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases    tier-3: ACCV (3+): Asian Conference on Computer Vision   DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial   Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation   FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference   ICASSP (3): International Conference on Acoustics, Speech, and Signal   Processing   ICIP (3): International Conference on Image Processing   ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering   Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence       关于List的补充说明: 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要 说明的是:    1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高.      2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更 容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表1 0篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的 评价和认可程度.      3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的 发表源上. 原因很多, 就不细说了.    4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要 考虑到有不少刚开始做研究的学生在代老板审稿.      5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野 鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑.    6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有 不太重视会议的分支.    7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都 可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后 , 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例 如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读 ).      8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体 系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外 很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投 了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的 会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有 不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说).      9. ... ... 国内不多专注的人，敬仰！   机器学习主要还是ICML和NIPS，ICML相对容易理解（只是相对，没有一定基础还是啃不动的），相对充实；NIPS更加偏理论，数学推导一片一片，数学符号也是稀奇古怪；二者都是精华，因为只有八页，所以超级浓缩，特别是好论文（比如best student papers）甚至字字珠玑，句句经典。    另外除了berkeley的M. Jordan，还有stanford的koller，max-planck的两个所，实力都很强。    CVPR，ICCV的领域是图像，包括从基于图像识别处理的机器学习研究到具体如何应用相应算法解决问题，是纵向的。以前重点人脸，现在渐渐转移开了。中国人（包括华人华侨）在这个领域还是颇有成就的，是本科生想出去的话可以多多考虑这个方向。    SIGIR：信息检索领域的专门会议。开了很多年了，重心也从以前的图书馆检索转移到网络检索，现在也包罗很多了，只要与检索有关的，机器学习、自然语言处理、图像音乐检索、搜索引擎结构及分布式等等。个人感觉会议整体水平还是不如上面所述的会议，也是是由这个领域的特殊性决定的。另外信息检索的文章除了在这里和AI会议还有发在www上的。    ACL，EMNLP都是自然语言处理方面的。但是好像中国人对这个的兴趣不大，是不是都在研究中文信息处理？哈哈。基本上没有深入研究过，不做详细评价。    数据挖掘主要是KDD（SIGKDD）吧。数据挖掘这个词已经滥用了，统计的人说自己的在DM，数据库的人说这是自己的本行，ai的也如是说，这倒也算了，争来争去都是学术界在划地盘。问题是在工业界的，甚至毫不相关的工作行业都说是在挖掘。这其实与数据挖掘的“广泛应用”有关系。我认为，在研究方面数据挖掘kdd是一直不断发展的，早些时候的DM就变成能赚钱能忽悠的DM了，要了解现在DM就要看论文了，呵呵    其他的就是一些杂志了，看多了就都知道了","title":"AI的主要会议评点"},{"content":"    终于走出这一步了，有些激动，有些期待，更多的是喜悦。     06年就申请了csdn的帐号， 很少登录。毕业也4~5年了，一直执着于编程，纯coder一个。这些年各种项目做过，技术的东西接触也不少，但除了脑袋偶尔回想到的一点，啥也没留下。年初想到要开个博客，初衷很简单，将以后的技术积累沉淀下来。     在csdn上开技术博客，其实有蛮多好处的。     1、多一个知己，多一个生活圈子。每当写一篇博文的时候，就如和一个知己在畅谈，一个默默倾听的老朋友在叙旧。这种感觉在日益冷漠的社会多么的难得，弥足的珍贵。把花在看八卦新闻，肥皂剧，垃圾电影的时间腾出来和这么一位知己畅谈又何乐而不为呢? 众所周知，csdn是国内有名的IT圈，这里面有各行各业的大牛，形形色色的IT人士，最新的技术热点，可以说这是个大舞台，大圈子。开博后让自己不再是这个舞台的看客，从而真正的参与进来和这个圈子一起互动。也许将来能结识更多的朋友，让一个人的技术生活变得不再孤单。     2、技术的沉淀与升华。如果没有写博客，我照样会去解决寻找各种技术问题的答案，但如果把不它写下来将来这个问题对还会是个问题；如果没有写博客，我照样能去学习很多新知识，但不过不把它归纳起来这些新知识还只是新知识；如果没写博客，我照样经历IT生活的零零总总而慢慢成长，但如果没有写下来这些都是私有的，没有办法让跟多的人和你分享。如果写博客会让我有更多独立时间去总结，激励自己不断的前行；如果写博客会发现在自己IT的海洋不断的收获了一枚枚贝壳，把这些贝壳装在篮子里就是一笔意外财富，把这些贝壳展示给大家，会换来更多的收获。     3、各种能力的提高。不言而喻，写博客肯定会提高个人的书写能力。此外，在创作过程中无形地锻炼了思维的条理性和组织性。通过写技术博客，还会激励自己对问题或知识的深入掌握，鞭策自己的持之以恒。      有这么多诱人的好处，但是很多人都会说“咱搞IT的，天天加班，为项目没日没夜哪有时间啊？”。我一直坚信，时间是海绵，挤挤总会有的，时间又是流水，不好好抓住只会不回。把写博客当作一个项目吧，一个真正你自己的项目，你将会这个项目的产品经理，项目经历，开发人员，客户。这是一个很好的项目，没有需求变更，没有进度压力和领导的要求，同时你会是这个项目的成果，也不会有诡异的bug跟踪。不积硅步，无以至千里，享受这千里旅途吧。     写到最后，在为本博客做个小小的规划吧，其实也是我个人的规划。     首先，此博客主要用来记录现在或过去工作中的通用技术。比如常见的linux系统编程，C/C++技术，以前做过的自然语言处理技术，Perl语言，这些年玩过的各类开发相关的工具。     其次，个人的读书或学习的笔记。好读书不求甚解，是我目前最真实的写照。家里书买了一大堆，经常看，但从来不计笔记，每次看完书之后，几乎不留任何笔墨，为此还常常被老婆反讽读书不认真。想象本人读书不爱记录，通常是因为遇到好书巴不得一口气读完，而做记录太磨蹭了，爽快读完后，又懒于回头总结，如此恶性循环而使得很读书只是泛读，时间一长又忘了。痛定思痛，这些笔记计划主要包含算法，软件编程思想，web编程，GUI编程，嵌入式，搜索引擎，自然语言处理等等杂七杂八各种爱好啦。     最后，就是一些IT畅谈了。估计没人愿意听怨妇的胡言乱语，我也不想把一些带情绪的生活琐事浪费大家时间。准备主要聊聊程序人生，IT动态之类的，毕竟博客不是学术论文。因为喜欢而自发而为的东西，就没必要搞的公式划，一切只要能让自己快乐，对别人有积极作用就好。     这么点文字，还要周末熬夜，看来确实要多练才行，不过挺开心的，继续加油吧！希望今年能学习至少一种新语言（比如ptyhon,php),掌握一些新的库（比如QT,boost),熟悉一个新领域（比如web），当然更重要的把基本功（算法，系统）掌握的更好。","title":"开博宣言"},{"content":"2012年读书计划：   在读：《C语言宝典》，迟剑，刘艳飞等编著，电子工业出版社          分析——之前学过c语言，现在做个全面复习吧，另外对链表之类的不甚了解，却在面试的时候十分重要，故仔细看！   预读： 《C++宝典》，李鹏程等编著，电子工业出版社          分析：c语言，c++都是工作笔试面试中的基础语言，很是重要 《数据结构(C语言版)》，严蔚敏等，清华大学出版社          分析：经典教材，可惜我之前没上过这门课，而工作中又是非常重要的 《自然语言处理综论》，朱拉斯凯(Daniel Jurafsky)，为OOV语音检索的科研拓展思路；为以后进百度or腾讯做准备          分析：经典之作，值得一读 《这就是搜索引擎：核心技术详解》，张俊林，电子工业出版社，为OOV语音检索的科研拓展思路；为以后进百度or腾讯做准备          分析：本书通俗易懂，适合我阅读   其它预读： java编程语言学习 基于lucene的搜索引擎开发，是由java开发的，所以先得学习java语言","title":"2012读书之工作篇"},{"content":"1、ASSERT与VERIFY的有什么 相同 和不同呢  1 ASSERT与VERIFY宏在Debug模式下作用基本一致，二者都对表达式的值进行计算，如果值为非0，则什么事也不做；如果值为0，则输出诊断信息。 2 ASSERT与VERIFY宏在Release模式下效果完全不一样。ASSERT不计算表达式的值，也不会输出诊断信息；VERIFY计算表达式的值，但不管值为0还是非0都不会输出诊断信息。VERIFY与ASSERT用在程序调试上并无本质上的区别。 此外，TRACE() 宏的编译也受 _DEBUG 控制。所有这些断言都只在 Debug版中才被编译，而在 Release 版中被忽略。唯一的例外是 VERIFY() 。事实上，这些宏都是调用了 assert() 函数，只不过附加了一些与库有关的调试代码。如果你在这些宏中加入了任何程序代码，而不只是布尔表达式（例如赋值、能改变变量值的函数调用 等），那么 Release 版都不会执行这些操作，从而造成错误。初学者很容易犯这类错误，查找的方法也很简单，因为这些宏都已在上面列出，只要利用 VC++ 的 Find in Files 功能在工程所有文件中找到用这些宏的地方再一一检查即可。另外，有些高手可能还会加入 #ifdef _DEBUG 之类的条件编译，也要注意一下。 顺便值得一提的是 VERIFY() 宏，这个宏允许你将程序代码放在布尔表达式里。这个宏通常用来检查 Windows API 的返回值。有些人可能为这个原因而滥用 VERIFY() ，事实上这是危险的，因为 VERIFY() 违反了断言的思想，不能使程序代码和调试代码完全分离，最终可能会带来很多麻烦。因此，专家们建议尽量少用这个宏 2、文件input.data中包含9999990个数字，这些数字都在1-10000000之间并且是未排序的。每个数字仅出现一次并占用一行，请给出一个程序，统计哪十个数字没有出现在input.data中，将结果输出到Output.data文件中（占用内存越小越好，运行时间越快越好）  答：http://blog.csdn.net/v_JULY_v/article/details/6451990 位图方案。熟悉位图的朋友可能会想到用位图来表示这个文件集合。例如正如编程珠玑一书上所述，用一个20位长的字符串来表示一个所有元素都小于20的简单的非负整数集合，边框用如下字符串来表示集合{1,2,3,5,8,13}： 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 上述集合中各数对应的位置则置1，没有对应的数的位置则置0。  参考编程珠玑一书上的位图方案，针对我们的10^7个数据量的磁盘文件排序问题，我们可以这么考虑，由于每个7位十进制整数表示一个小于1000万的整数。我们可以使用一个具有1000万个位的字符串来表示这个文件，其中，当且仅当整数i在文件中存在时，第i位为1。采取这个位图的方案是因为我们面对的这个问题的特殊性：1、输入数据限制在相对较小的范围内，2、数据没有重复，3、其中的每条记录都是单一的整数，没有任何其它与之关联的数据。   所以，此问题用位图的方案分为以下三步进行解决： 第一步，将所有的位都置为0，从而将集合初始化为空。 第二步，通过读入文件中的每个整数来建立集合，将每个对应的位都置为1。 第三步，检验每一位，如果该位为1，就输出对应的整数。     经过以上三步后，产生有序的输出文件。令n为位图向量中的位数（本例中为1000 0000），程序可以用伪代码表示如下：   //磁盘文件排序位图方案的伪代码//copyright@ Jon Bentley//July、updated，2011.05.29。//第一步，将所有的位都初始化为0for i ={0,....n}    bit[i]=0;//第二步，通过读入文件中的每个整数来建立集合，将每个对应的位都置为1。for each i in the input file    bit[i]=1;//第三步，检验每一位，如果该位为1，就输出对应的整数。for i={0...n}    if bit[i]==1      write i on the output file  //copyright@ yansha  //July、2010.05.30。  //位图方案解决10^7个数据量的文件的排序问题  //如果有重复的数据，那么只能显示其中一个 其他的将被忽略  #include <iostream>  #include <bitset>  #include <assert.h>  #include <time.h>  using namespace std;    const int max_each_scan = 5000000;    int main()  {      clock_t begin = clock();      bitset<max_each_scan> bit_map;      bit_map.reset();            // open the file with the unsorted data      FILE *fp_unsort_file = fopen(\"data.txt\", \"r\");      assert(fp_unsort_file);      int num;        // the first time scan to sort the data between 0 - 4999999      while (fscanf(fp_unsort_file, \"%d \", &num) != EOF)      {          if (num < max_each_scan)              bit_map.set(num, 1);      }            FILE *fp_sort_file = fopen(\"sort.txt\", \"w\");      assert(fp_sort_file);      int i;            // write the sorted data into file      for (i = 0; i < max_each_scan; i++)      {          if (bit_map[i] != 1)              fprintf(fp_sort_file, \"%d \", i);      }            // the second time scan to sort the data between 5000000 - 9999999      int result = fseek(fp_unsort_file, 0, SEEK_SET);      if (result)          cout << \"fseek failed!\" << endl;      else      {          bit_map.reset();          while (fscanf(fp_unsort_file, \"%d \", &num) != EOF)          {              if (num >= max_each_scan && num < 10000000)              {                  num -= max_each_scan;                  bit_map.set(num, 1);              }          }          for (i = 0; i < max_each_scan; i++)          {              if (bit_map[i] != 1)                  fprintf(fp_sort_file, \"%d \", i + max_each_scan);          }      }            clock_t end = clock();      cout<<\"用位图的方法，耗时：\"<<endl;      cout << (end - begin) / CLK_TCK << \"s\" << endl;      fclose(fp_sort_file);      fclose(fp_unsort_file);      return 0;  }   产生随即数的函数： //purpose:  生成随机的不重复的测试数据  //copyright@ 2011.04.19 yansha  //1000w数据量，要保证生成不重复的数据量，一般的程序没有做到。  //但，本程序做到了。  //July、2010.05.30。  #include <iostream>  #include <time.h>  #include <assert.h>  using namespace std;    const int size = 10000000;  int num[size];    int main()  {      int n;      FILE *fp = fopen(\"data.txt\", \"w\");      assert(fp);        for (n = 1; n <= size; n++)            //之前此处写成了n=0;n<size。导致下面有一段小程序的测试数据出现了0，特此订正。          num[n] = n;      srand((unsigned)time(NULL));      int i, j;        for (n = 0; n < size; n++)      {          i = (rand() * RAND_MAX + rand()) % 10000000;          j = (rand() * RAND_MAX + rand()) % 10000000;          swap(num[i], num[j]);      }        for (n = 0; n < size; n++)          fprintf(fp, \"%d \", num[n]);      fclose(fp);      return 0;  } 3、现有一个函数，功能是： 将英文字符串S中所有A子串换成B。要求：1、写出这个函数的合理定义形式(不用写出实现)；2、给出白盒测试代码。注意：尽可能在测试代码中包含典型测试用例，实现代码覆盖率高，并满足功能验证  #include <stdio.h>#include <stdlib.h>#include <string.h>int get_next(char* t,int next[]){  int i = 0;  int k = -1;  int len = strlen(t);   next[0] = k;    while(i<len-1)  {   if( k==-1 || t[i]==t[k] )     {       k++;       i++;      if(t[i] != t[k])         next[i] = k;      else         next[i] = next[k];       }    else       k = next[k];    } }int kmp_find(char* s,char* t){  int i = 0;  int j = 0;  int len1 = strlen(s);   int len2 = strlen(t);   int next[len2];   get_next(t,next);    while(i<len1 && j<len2)    {     if( j==-1 || s[i] == t[j])      {        i++;        j++;      }     else       j = next[j];    }      if(j>=len2)     return i-len2;   else   return -1;}char* substr(char* s,char* a,char* b){    int len = strlen(a);        int index = kmp_find(s,a);    //kmp find where is a in s or you can use strstr    char* head = s;    *(head+index) = '\\0';    char* tail = s + index + len;    //把字符串s分为 head a tail三部分    sprintf(s,\"%s%s%s\",head,b,tail);      if(kmp_find(s,a) != -1)//如果替换一个后还含有a继续     return substr(s, a, b);    else     return s;}     int main(int argc, char *argv[]){  char* s = (char*)malloc(100);  memset(s,0,100);  sprintf(s,\"%s\",argv[1]);  printf(\"str:%s\\n\",s);   printf(\"sub:%s by %s\\nafter:%s\\n\",argv[2],argv[3],substr(s, argv[2], argv[3]));  free(s);   s=NULL;  system(\"PAUSE\");      return 0;}  #include   <stdio.h> #include   <string.h> #include   <stdlib.h> char   *   replace(const   char   *   str,   const   char   *   str1,   const   char   *   str2)   {         char     *   pstr1,   *   pstr2,   *   pbuf1,   *   pbuf2;         char         ch;         pbuf1   =   NULL;         pbuf2   =   NULL;         pstr1   =   strstr(str,   str1);         while(pstr1   !=   NULL)    \t{             ch   =   *pstr1;             *pstr1   =   0;             if(pbuf1   ==   NULL)   \t    {                pbuf1   =   (char   *)malloc(strlen(str)   +   strlen(str2)   +   1);                pbuf1[0]   =   0;             }        \t    else   \t   {                pbuf2   =   (char   *)malloc(strlen(pbuf1)   +   strlen(str)   +   strlen(str2)   +   1);                strcpy(pbuf2,   pbuf1);                free(pbuf1);                pbuf1   =   pbuf2;             }             strcat(pbuf1,   str);             strcat(pbuf1,   str2);             *pstr1   =   ch;             str   =   pstr1   +   strlen(str1);             pstr1   =   strstr(str,   str1);         }         if(str&&str[0])   \t{             if(pbuf1   ==   NULL)   \t    {                 pbuf1   =   (char   *)malloc(strlen(str)   +   strlen(str2)   +   1);                 pbuf1[0]   =   0;              }        \t     else    \t     {                pbuf2   =   (char   *)malloc(strlen(pbuf1)   +   strlen(str)   +   strlen(str2)   +   1);                strcpy(pbuf2,   pbuf1);                free(pbuf1);                pbuf1   =   pbuf2;              }             strcat(pbuf1,   str);         }         return   pbuf1; } int   main(int   argc,   char   **   argv)   {         char   *   pstr;         if(argc   !=   4)   \t{                 fprintf(stderr,   \"Error,   parameter\\n \");                 return   1;         }         pstr   =   replace(argv[1],   argv[2],   argv[3]);         fprintf(stdout,   \"replace(\\ \"%s\\ \",   \\ \"%s\\ \",   \\ \"%s\\ \")   =   \\ \"%s\\ \"\\n \",                         argv[1],   argv[2],   argv[3],   pstr);         free(pstr);         return   0; }    4、有一个整数n,写一个函数f(n),返回0到n之间出现的\"1\"的个数。如：f(1)=1     f(11)=4    f(13)=6  请编写程序，实现f(n)。要求：写出核心算法，表达清晰，可用伪代码  # 0～99 中含有一的数字为 01 11(不含十位1) 21 31 41 51 61 71 81 91 和 10 11(不含个位1) 12 13 14 15 16 17 18 19 共20个 # 这样的话，每100 个数之间会有20个1   # 但是 100 ～ 199 是个特例，这中间除了这20个1 还有100个百位开头的1 这和10～19 其实是差不多的 # 其实昨天算错了，不应该算 0～100 是多少 应该算 0～99 为多少，凡是1作为首位的 都需要特殊处理  0~9 之间有1 个 个位1 ，10～19之间也有1个 个位1 也就是说 每10位中有1位个位1 (当个位大于1时也会有1个个位1) # 0~99之间有 1类10位1(10~19) 每百位有一位10位1 # 同理，每千位有1位 百位1   # 可能有点乱，没事 找个实例实验一下 253怎么样？呵呵 # # 按照上面的分析，253 拆分如下 m为总数： # 有25 + 1(这里加1 因为3>1)个 个位1 (01,11,21……251) || m=26 # 有2 + 1(这里加1 因为 5>1)个 十位1 (1*,11*,21*) || m=26 + 3*10 =56 # 有1(因为 2>1)个百位1 (1**) || m=56 + 1*100 = 156 #   # 呵呵 这样就出来了 程序就好写了 但是为1的时候要特殊处理下 将后面所有的数+1(因为是 00～53) 算做1的数量 比如 153 # m = 16 + 2*10 + (53+1) = 90 这个结果对吧。    #include <IOSTREAM>/** */int main(int argc, char* argv[]){\tint n,i,j,k,f,m,s=0;\ti=0;\tm=1;\tprintf(\"Please input a number:\");\tscanf(\"%d\",&n);\tk=n;\tf=n;\twhile(k!=0)\t{\t\tk/=10;\t\ti++;\t}\tif(f<=9&&f>=1)\t{\t\ts=1;\t}\telse\t{\t\tfor(j=0;j<=i;j++)\t\t{\t\t\tif((n/m)%10==0)\t\t\t{\t\t\t\ts=s+(n/(m*10))*m;\t\t\t}\t\t\telse if((n/m)%10==1)\t\t\t{\t\t\t\ts=s+(n/(m*10))*m+n%m+1;\t\t\t}else if((n/m)%10>1)\t\t\t{\t\t\t\ts=s+((n/(m*10))+1)*m;\t\t\t}\t\t\tm*=10;\t\t\t}\t\t}\tprintf(\"The result is:%d\\n\",s);\treturn 0;}   5、1、搜索引擎方向：全文检索、自然语言处理、云计算 2、XML数据库方向：XQuery实现、XML存储、索引 3、跨媒体检索方向：图形、图像、音频、视频检索 ","title":"笔试题总结"},{"content":"                这是现在窝窝团研发副总裁写的关于个性化阅读的文章，很经典，他本人是国内语义网络的先行者，有自己独到的见解，内容都是干货，甚至连做个性化阅读的配置都给出来了。摘抄如下：               以前曾经撰文讲过Topic Engine的过去、现在和未来。Topic Engine是一个生生不息的应用方向，因为从News Group、邮件列表、聊天室、论坛、Google News、博客圈子、群组。。。，人们一直因话题（有人也叫主题，英文为Topic）而聚集而交友，话题一直在生生不息层出不穷，组织形式在不断变异。     现在再讲讲个性化阅读的过去、现在和未来，也算是这个话题的延续。 一、概念定义     泛泛地说，只要是根据用户的历史行为（发言、标签等数据，点击流、分享、收藏、推荐、跳过等动作），动态决定哪些资讯内容（论坛帖子、新闻资讯、博客、微博、等）呈现给用户，都叫个性化阅读。 二、历史阶段 2005年～2007年：     这个阶段还没有Social数据，所以： 首先需要用户选定对哪些分类频道感兴趣，比如历史、人文、明星、体育等。稍微聪明一点的做法，不让用户选分类，而是问用户几个问题，然后就大致匹配出用户的兴趣点。 其次，系统决定给用户展现哪些分类的资讯。 随着用户点击，资讯实时不断变化，点击越多，系统越了解用户的阅读喜好。 这阶段的问题是： 1、利用成熟的协同过滤算法，但由于都在追求实时计算，运算量较大，有一定技术门槛； 2、对用户背景还是不够了解，仅仅通过用户点击流终究太浅。 3、普遍存在冷启动问题。 2008年～2010年：     有了Twitter，有了Facebook，有了Social Graph，个性化阅读器纷纷利用Twitter/Facebook帐号登录，展现的资讯是用户自己好友的Timeline聚合，主要是合并那些被诸多好友推荐的热点链接、图片和视频。不过，这波潮过去之后，像http://thoora.com/ 、http://twittertim.es/等都没有找到足够的用户群，还没有像2005年杀出来的TechMeme那么成功。 这阶段的问题是： 1、依赖于Twitter/Facebook的Social Graph，依赖于好友推送，可供阅读的数据过少，可供计算的数据过少，限制了自身应用的发展； 2、除非与Twitter保持良好的关系，能拿到 Streaming Firehose 接口，提前积累用户数据，否则用户Timeline信息需要积累一段时间，造成大量用户登录后没有可阅读的数据。 2010年：     FlipBoard杀出重围，自动排版技术独步天下。 2011年：     随着国内新浪微博、豆瓣等拥有Interest Graph（兴趣图谱）+Social Graph（社交图谱）海量数据的网站崛起，成为主流数据源，如何把2005年到2010年这些探索择其优点都整合起来，成为一个大课题。     Zite的横空出世，被众人热捧为“Flipboard Killer”，强调的是基于社会化关系的个性化推荐阅读方式。而Flipboard目前的战略重点主要还是集成各种社会化应用及内容源，并以其创造性的阅读体验方式展现出来。国内已经有几家也在Zite的方向上，尤其是iPad应用上，动了起来。 三、热门？还是个性化？     在2009年SXSW大会上，SheGeeks 直言不讳：『 热门内容（Popularity）已经过时了，某种程度上令人讨厌。 我不想知道什么是最流行的，Techmeme已经帮我做到了。我想知道什么东西和我相关。我们需要更多“相关性过滤服务”。』     此时，会有几种做法： 1、以热点资讯为主（先有蛋），以社交图谱为辅（后引入鸡）：将社交图谱引入热点资讯阅读中，像Quora（或中国的知乎）一样按人来隔离不同话题（不同热点）的讨论。Zite的方式类似于此。 2、以社交图谱为主：组织一度好友和二度好友的数据，做好数据挖掘。曾经有人在很久远的年代说过，“建立一个Social Network，每一个用户都推荐出自己喜欢的内容，那么被推荐得最多的，就一定是大多数人最受欢迎的内容。如果把这些推荐内容的用户区分成不同的群体， 就会得到特定群体欢迎的内容。Digg的想法就源于此。不过，这需要用户有足够的动力去推荐自己喜欢的内容，否则，Network也无法形成”。 3、以人为阅读中心：有人很多年前说过“许多人的blog阅读体验和阅读闲谈专栏是相似的，他们选择读什么不读什么的判断依据不是话题，而是作者，因为只有这样才能保证阅读到的内容的质量”。 4、以Topic为中心：用户定义或发掘用户感兴趣的Topic，只要是一篇文章谈及了用户关注的某一个主题，那么就推送给他。或者来自于不同人的文章集中地探讨某个话题，那么把这些文章自动聚合为一个Dialogue（虚拟对话），推送给用户。     除了第一种做法之外，我曾经尝试过其他三种做法。在中国的大环境下，要么数据过少，要么数据质量不高，都不能很好地做到有“发现、探索”、“新鲜、有趣”的冲击力。     当Social能完整地提供三重元素时： 1、 你的身份标识（Indentity）：Who you are； 2、 你的联系人或圈子（Contacts）：Who you know； 3、 你的网际行为（Activities）：What you do 。     那么，Social Graph，Interrest Graph，再联合热点资讯，揉入2005年以来的协同过滤算法，至少能做到make something people want吧。 四、Interest Graph的变化     以前，郑昀针对不同人群做的信息聚合，单纯从内容分类（也就是靠自然语言处理的自动分类算法）做，属于从信息本身下手。这种方式有一个问题：    某一类人群，虽然有一些集中的阅读点，但还有边缘的共同兴趣。举例，如IT人群，虽然共享和推荐的大多数是IT科技文章，但也涌现出很多受欢迎的兴趣点，如韩寒的文章，如冷笑话，如创意趣味产品。     这也就是为何基于 Tag 方式的阅读模式，以及基于指定主题的追踪模式，都不容易持久耐用的原因。一个人群的阅读兴趣点是比较模糊的。对于一个人来说，如果一个信息过滤器供应点科技，供应点娱乐，适当补充些人文历史，就能保证一定的粘度。     所以，郑昀后来觉得从内容分类，由于不引入人工，只靠比较大条的自然语言处理分类，对于博文、微博、论坛帖子等文字质量不稳定的信息会分得很粗糙，所以改变思路，从人群分类开始做。     也就是，划分出目标人群，依靠人群来挑拣信息，NLP算法为辅。这样有一个额外的好处，人群的兴趣点在动态变，短期地变，长期地变，但由于锁定人群，所以筛选出来的信息也在变。而相比之下，自动分类做出的信息，隔几个月或半年后，就要重新训练机器，因为往往信息包含的语言特征变了。     这也是信息聚合中的一个实际考虑点。     现在，中国也有了自己的Interest Graph，比如新浪微博，它的数据天然就表明一个人的兴趣喜好，以及连续波动，都可以跟踪和挖掘出来。以前依靠遍历Twitter、Google Reader、FriendFeed的好友所得到的社群分离，现在通过新浪微博等Social Graph都可以得到类似的。 五、人员配比     一般我对这个领域（Topic Engine啦、个性化阅读啦、Meme Tracker啦），研发人员配比是这么建议的： 爬虫2人， 文本挖掘4人（新词发现+分词+分类一个人，实体识别与发现+情感趋势分析一个人，事件识别与发现一个人，User Interest Profile一个人）， 数据挖掘和分析2人， Web前端展现（包括手持设备）3人， 产品经理1人， 12人是一个比较不错的开局。 待续。敬请期待。                  ","title":"个性化阅读的过去、现在和未来一"},{"content":"既来之，则安之 努力了，命运自由天注定，这条路走不通，另外一条路可能就是上天给你设计的最佳路径！ 2012计划_准备工作篇 ①心态：做下面积极和有意义的事情，就会有个好心情；豁达地看待人和事；真诚待人；与人畅快交流 ②身体： 周二晚上，周四晚上，周日晚上去健身房，如若有事去不了，晚上十点的时候，去操场跑步 早睡早起，睡前不玩手机，早晨醒来，就起床 ③c，c++，java 挨个看，c最重要 ④自然语言处理综论 为去搜索公司做准备，同时也是给自己科研拓展思路 ⑤英语，托福准备 ⑥调研可供职的公司，知己知彼 经常上智联招聘等求职网站，学习面试注意事项 向百度，腾讯，微软等公司看齐   晚上，一有时间，就去教室上自习，看书！  ","title":"2012计划_工作准备篇"},{"content":"数学之美 系列一 -- 统计语言模型 http://www.google.com.hk/ggblog/googlechinablog/2006/04/blog-post_7327.html 2006年4月3日 上午 08:15:00 从本周开始，我们将定期刊登 Google 科学家吴军写的《数学之美》系列文章，介绍数学在信息检索和自然语言处理中的主导作用和奇妙应用。 发表者: 吴军, Google 研究员 前言 也许大家不相信，数学是解决信息检索和自然语言处理的最好工具。它能非常清晰地描述这些领域的实际问题并且给出漂亮的解决办法。每当人们应用数学工具解决一个语言问题时，总会感叹数学之美。我们希望利用 Google 中文黑板报这块园地，介绍一些数学工具，以及我们是如何利用这些工具来开发 Google 产品的。 系列一： 统计语言模型 (Statistical Language Models) Google 的使命是整合全球的信息，所以我们一直致力于研究如何让机器对信息、语言做最好的理解和处理。长期以来，人类一直梦想着能让机器代替人来翻译语言、识别语音、认识文字（不论是印刷体或手写体）和进行海量文献的自动检索，这就需要让机器理解语言。但是人类的语言可以说是信息里最复杂最动态的一部分。为了解决这个问题，人们容易想到的办法就是让机器模拟人类进行学习 - 学习人类的语法、分析语句等等。尤其是在乔姆斯基（Noam Chomsky 有史以来最伟大的语言学家）提出 “形式语言” 以后，人们更坚定了利用语法规则的办法进行文字处理的信念。遗憾的是，几十年过去了，在计算机处理语言领域，基于这个语法规则的方法几乎毫无突破。 其实早在几十年前，数学家兼信息论的祖师爷 香农 (Claude Shannon)就提出了用数学的办法处理自然语言的想法。遗憾的是当时的计算机条件根本无法满足大量信息处理的需要，所以他这个想法当时并没有被人们重视。七十年代初，有了大规模集成电路的快速计算机后，香农的梦想才得以实现。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师贾里尼克 (Fred Jelinek)。当时贾里尼克在 IBM 公司做学术休假 (Sabbatical Leave)，领导了一批杰出的科学家利用大型计算机来处理人类语言问题。统计语言模型就是在那个时候提出的。 给大家举个例子：在很多涉及到自然语言处理的领域，如机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询中，我们都需要知道一个文字序列是否能构成一个大家能理解的句子，显示给使用者。对这个问题，我们可以用一个简单的统计模型来解决这个问题。 如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为： P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1) 其中 P (w1) 表示第一个词w1 出现的概率；P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词wn，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。因此我们假定任意一个词wi的出现概率只同它前面的词 wi-1 有关(即马尔可夫假设），于是问题就变得很简单了。现在，S 出现的概率就变为： P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)… (当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。） 接下来的问题就是如何估计 P (wi|wi-1)。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。 也许很多人不相信用这么简单的数学模型能解决复杂的语音识别、机器翻译等问题。其实不光是常人，就连很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法都有效。比如在 Google 的中英文自动翻译中，用的最重要的就是这个统计语言模型。去年美国标准局(NIST) 对所有的机器翻译系统进行了评测，Google 的系统是不仅是全世界最好的，而且高出所有基于规则的系统很多。 现在，读者也许已经能感受到数学的美妙之处了，它把一些复杂的问题变得如此的简单。当然，真正实现一个好的统计语言模型还有许多细节问题需要解决。贾里尼克和他的同事的贡献在于提出了统计语言模型，而且很漂亮地解决了所有的细节问题。十几年后，李开复用统计语言模型把 997 词语音识别的问题简化成了一个 20 词的识别问题，实现了有史以来第一次大词汇量非特定人连续语音的识别。 我是一名科学研究人员 ，我在工作中经常惊叹于数学语言应用于解决实际问题上时的神奇。我也希望把这种神奇讲解给大家听。当然，归根结底，不管什莫样的科学方法、无论多莫奇妙的解决手段都是为人服务的。我希望 Google 多努力一分，用户就多一分搜索的喜悦。","title":"数学之美 一 统计语言模型"},{"content":"转自：http://www.iteye.com/magazines/43-ik-analyzer  众所周知，全文搜索几乎已经成为每个网站的必须提供的基本功能之一，用Lucene构造一个“索引－查询”的应用是常见的java解决方案，目前由linliangyi2007创立的IK Analyzer是最好的Lucene 中文分词器之一。 首先介绍一下IKAnalyzer：IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。从2006年12月推出1.0版开始，IKAnalyzer 已经推出了3个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。最近刚刚发布了 3.1.1Stable稳定版本，新版本的IKAnalyzer 则发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。 linliangyi2007 (林良益) 是一位资深的Java开发者和优秀开源开发者, JavaEye非常荣幸的采访了他。 linliangyi2007 (林良益)  博客：http://linliangyi2007.iteye.com/ 欢迎大家推荐更多开源项目给我们，支持中国的开源项目发展，发站内短信给JavaEye管理员或者发信到webmaster@iteye.com，谢谢。 目 录 [ - ] 采访IK Analyzer 中文分词器开源项目作者linliangyi2007 linliangyi2007 (林良益) 介绍 采访IK Analyzer 中文分词器开源项目作者linliangyi2007 JavaEye：1. hi，linliangyi2007 你好，非常荣幸能够采访你。你能介绍一下IK Analyzer 中文分词器开源项目是如何创立的吗？ linliangyi2007：好的，这个要从和lucene的结缘开始说起了，大概05年左右，开始是为了完成一个电信的信息管理系统，里面需要用到全文检索的。后来发现对中文搜索，lucene没有很好的分词支持。当时我发现最棒的就是车东的CJK了，应该说，他的blog文章对我的IK Analyzer 诞生起了很大的影响。 后来，我们公司开始做一个基于web gis的本地信息搜索网站的互联网应用，这就促使我萌生了自己写一个中文分词器的想法。最开始是基于对词典的匹配，后来对词典中未出现的词语就有了进一步处理的想法，IK Analyzer 的设计理念也是一步一步形成的。这期间也结合了很多互联网用户的搜索体验的反馈。 有趣的是，我的两位好朋友，也就是paoding分词器的作者和JE-MMAnalyzer分词器的作者，都在基本相同的时期开始了各自的分词器研究。  三个人也就热火朝天的讨论开了，有交流，也有比较。因为有了这样一个圈子和氛围，也使的IK分词器一直从06年底开始，不断的更新到现在。 大家的处理速度，算法的优化，还有词典的整理一直在持续。当然，由于大家都有自己的工作（吃饭问题很实际啊），期间都有一段时间，暂停了项目的发展，IK2.0是在07年初发布的，3.0则到了09年，最近刚刚发布了 3.1.1Stable稳定版本。 JavaEye：2. IK Analyzer 中文分词器项目的特点和应用的主要方面是哪些？新版本做了哪些修正？ linliangyi2007：IK Analyzer 是更多的考虑了互联网用户在产品及名址信息搜索这块的应用，IK特别适用于搜索商家，产品，名址，如商品交易，美食，娱乐，电子地图等，因为它是基于这样的应用诞生的。IK在一开始的设计的时候，它有一个隐形的目标，是对数词，量词，专有名词的增强处理，这是由于它的基于web gis搜索的需求定位决定的。 如果持续使用IK的用户，应该会发现，IK的早期版本对数量词，专有名词的切分，是整体输出的，举个例子：“2009年12月”，在IK1.x版本的时候，是作为一个词元输出的，对未知的路名，人名，商店，公司名称都是如此，因此很多用户说，IK早先版本的分词效果“看起来”特别好，注意，我这里说的是“看起来\", 但搜索起来就未必了。 由于lucene搜索的倒排搜索结构，决定了lucene搜索的速度优势在于“全词匹配”而非like匹配，这就造成了过于粗粒度的输出分词结果好看，但用户经常搜索不到东西，在饱受公司客户“无情的”打击之后，IK后续的版本对此做了很大的改进。后期版本的切分越来越细碎，越来越不“漂亮”了，这点在3.0尤其明显，但保证了用户在分词搜索中的召回率。问题是，这点的改进会带来另一方面的负面影响，词打得太散，搜索的准确度下降了，为此IK3.0从问题的另一角度来提供了相对的解决方案。 JavaEye：3. 能否详细介绍一下这个解决方案？ linliangyi2007：好的，使用lucene搜索的开发者应该注意到，分词器在其中扮演着两个角色：一个是在lucene建立索引库时候，对文档进行切分。这时候，细粒度的切分，保证信息能尽可能的被“查找到”；另一个使用分词器的过程，实在用户输入搜索关键字的过程。分词器要多关键字进行分词，而后同索引匹配。 IK3.0就在这个地方为用户提供了一个相对优化的搜索方式，一个是IKQueryParser，这个也是我在blog中吐血推荐的， 呵呵。对于大多普通的搜索应用，它能为用户提供不错的搜索关键字组合。 举个例子，用户搜索“永和服装饰品”，对于分词器而言，它会切分出“永和”“和服”“服装”“装饰”“饰品”等。但分词器没有判断的能力（实际上，目前所有的分词器，即便有部分排除歧义的功能，也不完善），如果强制分词器进行排歧义处理，则可能会得出完全错误的结果。IK则是尝试给出所有可能的方案，在IKQueryparser 中，它不是简单的返回所有分词结果的组合，而是建立起一个分词树，将有可能的组合放在一起，它的输出会类似于这样：(“永和” && “服装” &&  “饰品”) || (“和服”&& “装饰”)， 通过这个搜索逻辑去索引中进行匹配，在现实中，我们完全可以假设只有合理的词元会搭配在一起， 那么，不合理的搭配，它的就可能不会出现，或者即使出现，但匹配度较低。 因此，IK3.0又给出了一个IKSimilarity的相似度评估器，来提高多词匹配的优先度，这样的搜索，就能形成高匹配度的文档，出现在前面，低匹配度的在后面，不合理的匹配就不出现的结果。这个也是自己的项目实战经历了。 IKSimilarity是实现了lucene Similarity的接口的，在进行搜索前，使用IndexSearch的API进行设置就好，这个在IK3.0的DEMO中有详细的例子说明，至此，IK3.0在尽可能保证文档召回率的前提下，实现了相关文档搜索匹配度的优先。当然，这不能绝对意义上杜绝不正确信息被搜索出来（PS：就目前各大主要搜索引擎的实现也是这样的）。在分词器的设计中，应该说不尽是IK，其他分词器的作者也是绞尽脑汁的想了很多， 但目前还没有特别完美的方案。对IK而言，我也收到了来至各方面用户的反馈，有用在互联网搜索领域的，有用于企业内部搜索的，还有做语言分析的，但就我个人的感觉而言，目前很难在一个分词器中，实现多种目标。因此，我很赞同paoding的作者说的一句话，没有最好的分词器，只有最适合于某个领域的分词器。 JavaEye：4. 你能分析和比较一下类似的其他中文分词器项目吗？ linliangyi2007：简单说一下吧： JE-MManalyzer：它的算法具有歧义分析，比较适合做垂直搜索和信息挖掘。他的中文名称是“极易”，开发者的理念是-简单即是美。 中科院的分词器：中科院的分词器很牛，其切分结果明显基于语义分析。 paoding：paoding的结构设计的非常灵活，适合于对其进行开源改造。 mmseg4j：单从mmseg4j 的项目介绍上看，它是一个很纯粹的基于词典分词的实现，既有细粒度的切分，也有最大长度的切分。应该说，是一个学习词典分词的很好的典范。 JavaEye：5. IK Analyzer 未来的roadmap是什么？ 你对 IK Analyzer的规划和目标是什么？ linliangyi2007：就IK后期的roadmap而言，主要致力于两点，一个是词典的整理优化，这块工作量是巨大的，且是枯燥的，呵呵。3.1.版本后词条是27万，但其中有不少的“不合格”词语，需要被剔除。第二是，有可能引入词频和字频的统计算法，来优化对未知词语的处理，这个还处于理论阶段。 后期的想法，可能会考虑牺牲一定的性能，来换取分词效果，从企业应用和中小型互联网应用而言，10万汉字/秒以上的处理速度，应该都能够满足需求了。   JavaEye：6. 你对整个lucene搜索领域怎么看？能推荐几个你觉得比较关注的搜索领域热点吗？ linliangyi2007：lucene是一个相当优秀的全文检索核心框架，基于它的应用是很多的。就lucene自己而言，已经发展出了nutch（面向互联网），solr（面向企业集群）等多种应用，这些也都是全文索引领域最经常用到的。而实际上，lucene的索引特性还可以用在更多方面，比如，你可以用它了做web gis的地图引擎，这是一个已经成功实现的商用项目。 因此，对lucene的关注，我觉得应该开放自己的思维。因为索引在计算机应用中，领域是非常广泛的，大家应该不拘一格。这点要归功于google对数据搜索应用概念的推广，深入人心。 JavaEye：7. 未来搜索引擎的发展方向会有哪些呢？ linliangyi2007：大型互联网应用，比如google，他们的应该不仅是分词器了，应该是一个自然语言处理系统了，包括了自我学习能力。 先说搜索的内容形式，会多元化，实际上已经有国外的公司在研发了，基于音频的，图像的搜索，如通过歌词搜索音频内容，从搜索的用户体验上，会结合用户的使用习惯给出搜索结果。也就是说，在未来，有可能你和我在google上搜索相同的词语，出来的结果会有不同。 还有，就是搜索形式可能会更多，有针对特定类型信息的垂直搜索，信息挖掘，也可能是针对SNS方式的人际网络搜索。 其实搜索的本质就是按照用户的视角将纷繁的数据进行合理的组织，再呈现在用面前； 从最早的MIS系统的sql搜索，到现在，一直如此。至于搜索领域google这样的大公司，基于新的搜索技术的公司未来有可能有机会超过他们，我想，最终的网络世界一定是大一统的。大家以后应该是买数据赚钱，而不是服务了。服务的方式是有限的，服务的内容（数据）是无限的，是需要人们创造和提供的。 JavaEye：8. 目前IK Analyzer 是你一个人开发？还是有其他合作开发者？现在你平均每天花在IK Analyzer 上的时间大概是多久呢？ linliangyi2007：IK目前就我一人，我也一直在寻找合作者。(JavaEye: 希望什么样的合作者？)最关键的是兴趣和恒心吧。我基本上每天都会花至少30分钟来解答网友的问题，然后如果需要，就进行修订。平时有了新想法，就会进行试验，如果可行，就会发布新版本。有时候一天会有20多封邮件咨询问题，当然有时候一周才一封，呵呵。 JavaEye：9. 目前大概有多少用户在使用IK Analyzer？ linliangyi2007：初步估计从06年底到现在，应该有1万多用户。(JavaEye：很厉害  )  主要都是国内的，毕竟是中文的。 JavaEye：10. 为什么给这个项目起IK Analyzer 这个名字呢？ linliangyi2007：呵呵，这个问题问的好，我很喜欢Diablo，尤其是Diablo II，我玩暗黑7年了。暗黑中有个角色，野蛮人哦，它的终极套装就是“不朽之王Immortal King”，IK诞生的那一天，刚好是我打出一整套套装的那一天，于是就用这个套装的名字做纪念了，呵呵，感谢暴雪，感谢JavaEye，感谢CCAV。。。    听说java也是这么命名的，当时的设计人员正在喝java咖啡来着…… JavaEye：11. 你的开发环境是什么？ 使用什么操作系统，和IDE？ linliangyi2007：我用Eclipse，操作系统多是windows，偶尔用linux， 客户多使用unix。 JavaEye：12. 通过开发IK Analyzer ，你对中国的软件开发人员做开源项目有什么感受和想法吗？ linliangyi2007：现在大家日常用的开发平台，95%以上都是国外的组织贡献的，中国在这块，是需要赶上的。中国的开源比前几年有了明显的发展，但还需要大家一起来参与。希望大家能逐步型成贡献代码的习惯，其实开源不一定每个人都有做一个项目，参与其中就好， 哪怕就一小段的代码。linux能成功，它的很多核心代码都被阅读过，并通过网友们进行了补充和修订。中国的开源氛围薄弱，我觉得更多的是中国传统思想中，对知识保守的因素，师傅对徒弟都要留一手，何况是同业者呢。但我要说，思想的交互是互利的，知识在开放的氛围中增长的速度要远超过封闭的开发， 我经常跟我的同事进行头脑风暴，大家都很有收获的，碰撞后的思想，往往能产生意料之外的好结果。 JavaEye：13. 作为一个JavaEye老会员，你对JavaEye网站有什么建议和意见吗？ linliangyi2007：希望javaeye更专业更有深度。对新人创造更包容，更宽松的咨询空间，对老人们则是思想碰撞，温故知新的场所。","title":"采访IK Analyzer 中文分词器开源项目作者林良益（十三）"},{"content":"                                                                                                              本文节选自《这就是搜索引擎：核心技术详解》第六章       HITS算法也是链接分析中非常基础且重要的算法，目前已被Teoma搜索引擎（www.teoma.com）作为链接分析算法在实际中使用。 6.4.1 Hub页面与Authority页面        Hub页面和Authority页面是HITS算法最基本的两个定义。所谓“Authority”页面，是指与某个领域或者某个话题相关的高质量网页，比如搜索引擎领域，Google和百度首页即该领域的高质量网页，比如视频领域，优酷和土豆首页即该领域的高质量网页。所谓“Hub”页面，指的是包含了很多指向高质量“Authority”页面链接的网页，比如hao123首页可以认为是一个典型的高质量“Hub”网页。       图6-11给出了一个“Hub”页面实例，这个网页是斯坦福大学计算语言学研究组维护的页面，这个网页收集了与统计自然语言处理相关的高质量资源，包括一些著名的开源软件包及语料库等，并通过链接的方式指向这些资源页面。这个页面可以认为是“自然语言处理”这个领域的“Hub”页面，相应的，被这个页面指向的资源页面，大部分是高质量的“Authority”页面。                                                                     图6-11 自然语言处理领域的Hub页面        HITS算法的目的即是通过一定的技术手段，在海量网页中找到与用户查询主题相关的高质量“Authority”页面和“Hub”页面，尤其是“Authority”页面，因为这些页面代表了能够满足用户查询的高质量内容，搜索引擎以此作为搜索结果返回给用户。 6.4.2 相互增强关系     很多算法都是建立在一些假设之上的，HITS算法也不例外。HITS算法隐含并利用了2个基本假设：        基本假设1：一个好的“Authority”页面会被很多好的“Hub”页面指向；        基本假设2：一个好的“Hub”页面会指向很多好的“Authority”页面； 到目前为止，无论是从“Hub”或者“Authority”页面的定义也好，还是从两个基本假设也好，都能看到一个模糊的描述，即“高质量”或者“好的”，那么什么是“好的”Hub页面？什么是“好的”Authority页面？两个基本假设给出了所谓“好”的定义。       基本假设1说明了什么是“好的”Authority页面，即被很多好的Hub页面指向的页面是好的“Authority”页面，这里两个修饰语非常重要：“很多”和“好的”，所谓“很多”，即被越多的Hub页面指向越好，所谓“好的”，意味着指向本页面的“Hub”页面质量越高，则本页面越好。即综合了指向本页面的所有Hub节点的数量和质量因素。       基本假设2则给出了什么是“好的”Hub页面的说明，即指向很多好的Authority页面的网页是好的Hub页面。同样的，“很多”和“好的”两个修饰语很重要，所谓“很多”，即指向的Authority页面数量越多越好；所谓“好的”，即指向的Authority页面质量越高，则本页面越是好的Hub页面。也即综合考虑了该页面有链接指向的所有页面的数量和质量因素。       从以上两个基本假设可以推导出Hub页面和Authority页面之间的相互增强关系（参考图6-12）， 即某个网页的Hub质量越高，则其链接指向的页面的Authority质量越好；反过来也是如此，一个网页的Authority质量越高，则那些有链接指向本网页的页面Hub质量越高。通过这种相互增强关系不断迭代计算，即可找出哪些页面是高质量的Hub页面，哪些页面是高质量的Authority页面。                                                                                                         图6-12 相互增强关系                                                              6.4.3 HITS算法       HITS算法与Pagerank算法一个显著的差异是：HITS算法与用户输入的查询请求密切相关，而Pagerank是与查询无关的全局算法。HITS后续计算步骤都是在接收到用户查询后展开的，即是与查询相关的链接分析算法。       HITS算法接收到了用户查询之后，将查询提交给某个现有的搜索引擎（或者是自己构造的检索系统），并在返回的搜索结果中，提取排名靠前的网页，得到一组与用户查询高度相关的初始网页集合，这个集合被称作为根集（Root Set）。       在根集的基础上，HITS算法对网页集合进行扩充（参考图6-13），扩充原则是：凡是与根集内网页有直接链接指向关系的网页都被扩充进来，无论是有链接指向根集内页面也好，或者是根集页面有链接指向的页面也好，都被扩充进入扩展网页集合。HITS算法在这个扩充网页集合内寻找好的“Hub”页面与好的“Authority”页面。                                                                                                图6-13 根集与扩展集       对于“扩充网页集合”来说，我们并不知道哪些页面是好的“Hub”或者好的“Authority”页面，每个网页都有潜在的可能，所以对于每个页面都设立两个权值，分别来记载这个页面是好的Hub或者Authority页面的可能性。在初始情况下，在没有更多可利用信息前，每个页面的这两个权值都是相同的，可以都设置为1。       之后，即可利用上面提到的两个基本假设，以及相互增强关系等原则进行多轮迭代计算，每轮迭代计算更新每个页面的两个权值，直到权值稳定不再发生明显的变化为止。      图6-14给出了迭代计算过程中，某个页面的Hub权值和Authority权值的更新方式。假设以A(i)代表网页i的Authority权值，以H(i)代表网页i的Hub权值。在图6-14的例子中，“扩充网页集合”有3个网页有链接指向页面1，同时页面1有3个链接指向其它页面。那么，网页1在此轮迭代中的Authority权值即为所有指向网页1页面的Hub权值之和；类似的，网页1的Hub分值即为所指向的页面的Authority权值之和。                                                                                                                    图6-14  Hub与Authority权值计算          “扩充网页集合”内其它页面也以类似的方式对两个权值进行更新，当每个页面的权值都获得了更新，则完成了一轮迭代计算，此时HITS算法会评估上一轮迭代计算中的权值和本轮迭代之后权值的差异，如果发现总体来说权值没有明显变化，说明系统已进入稳定状态，则可以结束计算。将页面根据Authority权值得分由高到低排序，取权值最高的若干页面作为响应用户查询的搜索结果输出。如果比较发现两轮计算总体权值差异较大，则继续进入下一轮迭代计算，直到整个系统权值稳定为止。                       6.4.4  HITS算法存在的问题       HITS算法整体而言是个效果很好的算法，目前不仅应用在搜索引擎领域，而且被“自然语言处理”以及“社交分析”等很多其它计算机领域借鉴使用，并取得了很好的应用效果。尽管如此，最初版本的HITS算法仍然存在一些问题，而后续很多基于HITS算法的链接分析方法，也是立足于改进HITS算法存在的这些问题而提出的。        归纳起来，HITS算法主要在以下几个方面存在不足：         1.计算效率较低            因为HITS算法是与查询相关的算法，所以必须在接收到用户查询后实时进行计算，而HITS算法本身需要进行很多轮迭代计算才能获得最终结果，这导致其计算效率较低，这是实际应用时必须慎重考虑的问题。         2.主题漂移问题           如果在扩展网页集合里包含部分与查询主题无关的页面，而且这些页面之间有较多的相互链接指向，那么使用HITS算法很可能会给予这些无关网页很高的排名，导致搜索结果发生主题漂移，这种现象被称为“紧密链接社区现象”（Tightly-Knit CommunityEffect）。         3.易被作弊者操纵结果           HITS从机制上很容易被作弊者操纵，比如作弊者可以建立一个网页，页面内容增加很多指向高质量网页或者著名网站的网址，这就是一个很好的Hub页面，之后作弊者再将这个网页链接指向作弊网页，于是可以提升作弊网页的Authority得分。         4.结构不稳定          所谓结构不稳定，就是说在原有的“扩充网页集合”内，如果添加删除个别网页或者改变少数链接关系，则HITS算法的排名结果就会有非常大的改变。   6.4.5 HITS算法与PageRank算法比较        HITS算法和PageRank算法可以说是搜索引擎链接分析的两个最基础且最重要的算法。从以上对两个算法的介绍可以看出，两者无论是在基本概念模型还是计算思路以及技术实现细节都有很大的不同，下面对两者之间的差异进行逐一说明。             1.HITS算法是与用户输入的查询请求密切相关的，而PageRank与查询请求无关。所以，HITS算法可以单独作为相似性计算评价标准，而PageRank必须结合内容相似性计算才可以用来对网页相关性进行评价；       2.HITS算法因为与用户查询密切相关，所以必须在接收到用户查询后实时进行计算，计算效率较低；而PageRank则可以在爬虫抓取完成后离线计算，在线直接使用计算结果，计算效率较高；       3.HITS算法的计算对象数量较少，只需计算扩展集合内网页之间的链接关系；而PageRank是全局性算法，对所有互联网页面节点进行处理；       4.从两者的计算效率和处理对象集合大小来比较，PageRank更适合部署在服务器端，而HITS算法更适合部署在客户端；        5.HITS算法存在主题泛化问题，所以更适合处理具体化的用户查询；而PageRank在处理宽泛的用户查询时更有优势；       6.HITS算法在计算时，对于每个页面需要计算两个分值，而PageRank只需计算一个分值即可；在搜索引擎领域，更重视HITS算法计算出的Authority权值，但是在很多应用HITS算法的其它领域，Hub分值也有很重要的作用；       7.从链接反作弊的角度来说，PageRank从机制上优于HITS算法，而HITS算法更易遭受链接作弊的影响。       8.HITS算法结构不稳定，当对“扩充网页集合”内链接关系作出很小改变，则对最终排名有很大影响；而PageRank相对HITS而言表现稳定，其根本原因在于PageRank计算时的“远程跳转”。","title":"搜索引擎链接算法之：HITS算法解析"},{"content":" CRF(Conditional Random Field) 条件随机场是近几年自然语言处理领域常用的算法之一，常用于句法分析、命名实体识别、词性标注等。在我看来，CRF就像一个反向的隐马尔可夫模型(HMM)，两者都是用了马尔科夫链作为隐含变量的概率转移模型，只不过HMM使用隐含变量生成可观测状态，其生成概率有标注集统计得到，是一个生成模型；而CRF反过来通过可观测状态判别隐含变量，其概率亦通过标注集统计得来，是一个判别模型。由于两者模型主干相同，其能够应用的领域往往是重叠的，但在命名实体、句法分析等领域CRF更胜一筹。当然你并不必须学习HMM才能读懂CRF，但通常来说如果做自然语言处理，这两个模型应该都有了解。   >>CRF详细的介绍和公式推导推荐这个PPT教程：http://wenku.baidu.com/view/f32a35d2240c844769eaee55.html。 >>另外推荐一篇HMM应用于中文分词的一篇易读的入门教程，非常形象：http://blog.sina.com.cn/s/blog_68ffc7a40100uebv.html     下文仅针对专门做自然语言处理的同学做一个快速形象的上手简介，并指出最重要的特征。这里假设你已经有基本的自然语言处理概念和马尔科夫链的基本知识。CRF本质上是隐含变量的马尔科夫链+可观测状态到隐含变量的条件概率。说隐含变量和可观测状态很抽象，我们以词性标注为例（如果你不知道什么是词性标注，请百度一下），在词性标注中词性标签就是隐含变量，具体的词语就是可观测状态，词性标注的目的是通过可观测到的一个个单词推断出来每个单词应该被赋予的词性标签。下文将用词性标签和词语代替上述两个名词。   先说马尔科夫链，这里体现了CRF的随机场特征（准确的说是马尔科夫随机场）。这里CRF和HMM都假设词性标签是满足马尔科夫性的，即当前词性仅和上一个词性有概率转移关系而与其它位置的词性无关，比如形容词后面跟形容词的概率是0.5，跟修饰性“的”的概率为0.5，跟动词的概率为0。因此，通过在一个标注集上进行统计，我们很容易得到一个概率转移矩阵，即任意词性A后紧邻任意词性B的概率都可以被统计出来。对HMM来说这部分就结束了，对CRF来说，可以在二维条件转移矩阵基础上再增加一维词语特征，如“当AB相邻，A是动词且B单词长度超过3时，B是名词的概率是xx\"。大家可能注意到了马尔科夫链的窗口为1，即它仅考虑上1个词，这不见得是最合理的。这其实是一个对特征稀疏问题的折中，可以想象仅对两个词性AB统计P(B|A)能够得到很多数据的反馈，而如果统计长度为6的窗口，如P(G | ABCDEF)就会遇到数据稀疏的问题，因为很可能序列ABCDEF根本就没有在数据集中出现过.数据稀疏对机器学习的影响是巨大的，因此马尔科夫链实际以损失一定全局信息的基础上换来了更饱满的数据，实验证明这笔交易在词性标注时是赚的。   再说词性与词语直接的映射概率，这里体现了CRF的条件特征。如果是HMM，这里会直接统计词性-->单词的条件概率矩阵，比如 ”动词“ 生成 ”发射“ 的概率可能为1.5%，而生成”微软“ 的概率为0. 然后对于每一种可能的词性序列结合与条件概率相乘就能得到每一个候选序列的生成概率，然而取概率最高的作为标注结果即可。而CRF正好反过来，CRF通过发掘词语本身的特征（如长度，大小写，匹配特定词表等，也可以包括词语本身），把每个词语转化成为一个一维特征向量(vector)，然后对于每个特征计算特征到词性的条件概率，这样每个词语对候选词性的条件概率即为所有特征条件概率的加和。比如我们假设特征向量只有两个，且P ( ”词语长度>3\" --> 名词词性）的概率为0.9， P(\"词语位于句子末尾“ --> 名词词性）概率为0.4，且一个词恰好满足这两个特征，则其为名词的条件概率为 (0.9 + 0.4) / 2 = 0.65. 这样，CRF根据这个条件转移数值再结合词性的马尔科夫特性，就可以使用与HMM类似的方法寻找最优的词性标注序列了。   为了装得更学术一点本想再贴一个公式搞了半天没贴成功还是算了不过在上面的PPT链接中大家可以找到所以就不写了。总的来说CRF优于HMM的地方在于，它可以引入更多的特征，包括词语本身特征和词语所在上下文的特征，而非单词本身。从某种角度讲，它结合了HMM和最大熵方法。本人也刚刚接触CRF，因此都是从最浅显的角度来介绍的，如果有什么说错的地方欢迎指正啊~ 写这么多不容易，有大牛路过的话请轻拍哈~   转自：http://hi.baidu.com/hehehehello/blog/item/2bc871c66a45c9059d163d94.html","title":"CRF条件随机场简介"},{"content":"信息的飞速增长，使搜索引擎成为人们查找信息的首选工具，Google、百度、中国搜索等大型搜索引擎一直是人们讨论的话题。随着搜索市场价值的不断增加，越来越多的公司开发出自己的搜索引擎，阿里巴巴的商机搜索、8848的购物搜索等也陆续面世，自然，搜索引擎技术也成为技术人员关注的热点。 　　搜索引擎技术的研究，国外比中国要早近十年，从最早的Archie，到后来的Excite，以及altvista、overture、google等搜索引擎面世，搜索引擎发展至今，已经有十几年的历史，而国内开始研究搜索引擎是在上世纪末本世纪初。在许多领域，都是国外的产品和技术一统天下，特别是当某种技术在国外研究多年而国内才开始的情况下。例如操作系统、字处理软件、浏览器等等，但搜索引擎却是个例外。虽然在国外搜索引擎技术早就开始研究，但在国内还是陆续涌现出优秀的搜索引擎，像百度（http://www.baidu.com）等。目前在中文搜索引擎领域，国内的搜索引擎已经和国外的搜索引擎效果上相差不远。之所以能形成这样的局面，有一个重要的原因就在于中文和英文两种语言自身的书写方式不同，这其中对于计算机涉及的技术就是中文分词。 　　什么是中文分词 　　众所周知，英文是以词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。例如，英文句子I am a student，用中文则为：“我是一个学生”。计算机可以很简单通过空格知道student是一个单词，但是不能很容易明白“学”、“生”两个字合起来才表示一个词。把中文的汉字序列切分成有意义的词，就是中文分词，有些人也称为切词。我是一个学生，分词的结果是：我 是 一个 学生。 　　中文分词和搜索引擎 　　中文分词到底对搜索引擎有多大影响？对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。笔者最近替朋友找一些关于日本和服的资料，在搜索引擎上输入“和服”，得到的结果就发现了很多问题。下面就以这个例子来说明分词对搜索结果的影响，在现有三个中文搜索引擎上做测试，测试方法是直接在Google（http://www.google.com）、百度（http://www.baidu.com）上以“和服”为关键词进行搜索： 　　在Google上输入“和服”搜索所有中文简体网页，总共结果507,000条，前20条结果中有14条与和服一点关系都没有。 　　在百度上输入“和服”搜索网页，总共结果为287,000条，前20条结果中有6条与和服一点关系都没有。 　　在中搜上输入“和服”搜索网页，总共结果为26,917条，前20条结果都是与和服相关的网页。 　　这次搜索引擎结果中的错误，就是由于分词的不准确所造成的。通过笔者的了解，Google的中文分词技术采用的是美国一家名叫Basis Technology（http://www.basistech.com）的公司提供的中文分词技术，百度使用的是自己公司开发的分词技术，中搜使用的是国内海量科技（http://www.hylanda.com）提供的分词技术。由此可见，中文分词的准确度，对搜索引擎结果相关性和准确性有相当大的关系。 　　中文分词技术 　　中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。 　　现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 　　1、基于字符串匹配的分词方法 　　这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下： 　　1）正向最大匹配法（由左到右的方向）； 　　2）逆向最大匹配法（由右到左的方向）； 　　3）最少切分（使每一句中切出的词数最小）。 　　还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。 　　一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。 　　对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。 　　2、基于理解的分词方法 　　这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。 　　3、基于统计的分词方法 　　从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 　　到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。笔者了解，海量科技的分词算法就采用“复方分词法”，所谓复方，相当于用中药中的复方概念，即用不同的药才综合起来去医治疾病，同样，对于中文词的识别，需要多种算法来处理不同的问题。 　　分词中的难题 　　有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。 　　1、歧义识别 　　歧义是指同样的一句话，可能有两种或者更多的切分方法。例如：表面的，因为“表面”和“面的”都是词，那么这个短语就可以分成“表面 的”和“表 面的”。这种称为交叉歧义。像这种交叉歧义十分常见，前面举的“和服”的例子，其实就是因为交叉歧义引起的错误。“化妆和服装”可以分成“化妆 和 服装”或者“化妆 和服 装”。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。 　　交叉歧义相对组合歧义来说是还算比较容易处理，组合歧义就必需根据整个句子来判断了。例如，在句子“这个门把手坏了”中，“把手”是个词，但在句子“请把手拿开”中，“把手”就不是一个词；在句子“将军任命了一名中将”中，“中将”是个词，但在句子“产量三年中将增长两倍”中，“中将”就不再是词。这些词计算机又如何去识别? 　　如果交叉歧义和组合歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：“乒乓球拍卖完了”，可以切分成“乒乓 球拍 卖 完 了”、也可切分成“乒乓球 拍卖 完 了”，如果没有上下文其他的句子，恐怕谁也不知道“拍卖”在这里算不算一个词。 　　2、新词识别 　　新词，专业术语称为未登录词。也就是那些在字典中都没有收录过，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解句子“王军虎去广州了”中，“王军虎”是个词，因为是一个人的名字，但要是让计算机去识别就困难了。如果把“王军虎”做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子“王军虎头虎脑的”中，“王军虎”还能不能算词？ 　　新词中除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。 　　中文分词的应用 　　目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。 　　分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有了。科研院校研究的技术，大部分不能很快产品化，而一个专业公司的力量毕竟有限，看来中文分词技术要想更好的服务于更多的产品，还有很长一段路。","title":"中文分词技术"},{"content":"       搜索引擎分为搜索器，索引器，检索器，人机接口四部分。建立索引是个复杂的过程，索引数据库是搜索引擎前端和后端的联系桥梁，可以说起到了管理器的作用。      下面对整个过程做个简要介绍：      (1)搜索器（俗称的网络蜘蛛Robot）从互联网上抓取网页，把网页送入网页数据库，从网页中“提取URL”，把URL送入URL数据库，网络蜘蛛根据得到网页的URL，继续抓取其它网页，反复循环直到把所有的网页抓取完成。      (2)系统对抓取的网页进行分类过滤后存入网页数据库，再对网页内容进行分析，送入索引器模块建立索引，形成“索引数据库”。同时进行链接信息提取，把链接信息（包括锚文本、链接本身等信息）送入索引数据库相关表（链接数据库），为网页评级提供依据。      (3)用户通过查询界面提交查询请求给查询服务器，服务器在“索引数据库”中进行相关网页的查找，同时“网页评级”把查询请求和链接信息结合起来对搜索结果进行相关度的评价，通过查询服务器按照相关度进行排序，并提取关键词的内容摘要，组织最后的页面返回给用户。 在上面的几个环节中，每个部分都可采用不同的技术和模型实现，下面分别做出分析和比较：     一、搜索器-信息采集技术详细内容见我的另一篇文章：NetSpider初探，这里主要强调下信息过滤的重要性，由于在互联网中，存在有大量的无用信息，一个好的搜索引擎应当是尽量减少垃圾内容的数量。这是信息过滤要着重解决的问题。     二、索引器-信息索引技术信息索引就是创建文档信息的特征记录（如标题，作者，关键字，时间等），以使用户能够快速地检索到所需信息。建立索引一般有如下几个处理步骤： (1)识别文档中的词 (2)删除停用词(stop words) (3)提取词干(stemming) (4)用索引项的标号代替词干(stems) (5)统计词干的数量( tf词频率) (6)计算所有单个词项、短语和语义类的权重建立索引的问题：       (1)信息语词切分和语词词法分析语词是信息表达的最小单位。由于语词切分中存在切分歧意，切分需要利用各种上下文知识。语词词法分析是指识别出各个语词的词干，以便根据词千建立信息索引。       (2)进行词性标注及相关的自然语言处理词性标注是指利用基于规则和统计(马尔科夫链)的数学方法对语词进行标注。基于马尔科夫链随机过程的n元语法统计分析方法在词性标注中能达到较高的精度。利用各种语法规则，识别出重要的短语结构。自然语言处理是指将自然语言理解应用在信息检索中，可以提高信息检索的精度和相关性。       (3)建立检索项索引使用倒排文件的方式建立检索项索引。一般应包括“检索项”、“检索项所在文件位置信息”以及“检索项权重”等信息。 三、检索器-信息检索技术信息检索过程大致有如下几个步骤：  (1)给定查询 (2)对查询进行词干提取，算法和对文档的处理相同 (3)用索引编号代替词干 (4)计算查询词干的权重 (5)形成查询向量(VSM) (6)计算查询向量和文档向量之间的相似度 (7)返回排序后的文档集合给用户      搜索引擎所使用的信息检索模型主要有布尔逻辑模型、模糊逻辑模型、向量空间模型以及概率模型等。 (1)布尔逻辑模型 布尔型信息检索模型是最简单的信息检索模型，用户利用布尔逻辑关系构造查询式并提交，搜索引擎根据事先建立的倒排文件确定查询结果。标准布尔逻辑模型为二元逻辑，并可用逻辑符(\"and\". \"or\". \"not\")来组织关键词表达式。布尔型信息检索模型的查全率高，查准率低，为目前大多数搜索引擎所使用。 (2)模糊逻辑模型 这种模型在查询结果处理中加入模糊逻辑运算，将所检索的数据库文档信息与用户的查询要求进行模糊逻辑比较，按照相关的优先次序排列查询结果。模糊逻辑模型可以克服布尔型信息检索模型查询中结果的无序性问题。例如，查询“搜索引擎”，则关键词“搜索引擎”出现次数多的文档将排列在较前的位置上。 (3)向量空间模型向量空间模型用检索项的向量空间来表示用户的查询要求和数据库文档信息。查询结果是根据向量空间的相似性而排列的。向量空间模型可方便地产生有效的查询结果，能提供相关文档的文摘，并对查询结果进行分类，为用户提供准确的信息。 (4)概率模型基干贝叶斯概率论原理的概率模型利用相关反馈的归纳学习方法，获取匹配函数，这是一种较复杂的检索模型。目前，商用信息检索系统主要以布尔模糊逻辑加向量空间模型为主，辅以部分自然语言处理技术来构造自己的检索算法。 四、人机接口-查询组合和结果处理技术查询界面如何更人性化，符合大多数用户的查询习惯是个有待研究的问题。能否提供自然语言的检索，这部分处理过程实际上也会涉及到建立索引过程中的一些技术，比如分词，自然语言处理。还有问答式的搜索引擎，这些都是第三代智能化搜索引擎必须解决的问题。 再就是搜索引擎的检索结果通常包含大量文件，用户不可能一一浏览。搜索引擎一般应按与查询的相关程度对检索结果进行排列，最相关的文件通常排在最前面。搜索引擎确定相关性的方法有概率方法、位置方法、摘要方法、分类或聚类方法等。还有就是对用户行为的分析，利用数据挖掘技术对搜索引擎的日志进行分析，得出用户搜索行为的模式，是提高搜索引擎个性化，人性化的必要手段。下面对最常用也是最重要的技术确定检索网页相关性的方法做个介绍： (1)概率方法概率方法根据关键词在文中出现的频率来判定文件的相关性。这种方法对关键词出现的次数进行统计，关键词出现的次数越多，该文件与查询的相关程度就越高。 (2)位置方法位置方法根据关键词在文中出现的位置来判定文件的相关性。认为关键词在文件中出现得越早，文件的相关程度就越高。 (3)摘要方法摘要方法是指搜索引擎自动地为每个文件生成一份摘要，让用户自己判断结果的相关性，以便用户进行选择。 (4)分类或聚类方法分类或聚类方法是指搜索引擎采用分类或聚类技术，自动把查询结果归入到不同的类别中。 (5)用户反馈方法 对收集的用户反馈信息进行分析，实际上是个自适应的过程，通过对检索行为模型的反复验证，一定会让客户体验的满意度越来越高。 http://www.demix.cn/h?z=26988","title":"搜索引擎原理及其组成部分和功能"},{"content":" 机器学习；数据挖掘；信息检索；文本分类/聚类；自然语言处理；分布式计算；个性化推荐及特征建模。谷歌、bing、百度、雅虎、搜狗、搜搜、宜搜、中搜、有道、人民搜索","title":"。。。"},{"content":"  Patricia Tree  简称PAT tree。 它是 trie 结构的一种特殊形式。是目前信息检索领域应用十分成功的索引方 法，它是1992年由Connel根据《PATRICIA——Patrical Algorithm to Retrieve Information Coded in Alphanumeric》算法发展起来的。   PAT tree 在字符串子串匹配 上有这非常优异的表现，这使得它经常成为一种高效的全文检索算法，在自然语言处理领域也有广泛的应用。其算法中最突出的特点就是采用半无限长字串(semi-infinite string 简称 sistring) 作为字符串的查找结构。   采用半无限长字串(sistring): 一种特殊的子串信息存储方式。 比如一个字符串CUHK。它的子串有C、CU、CUH、CUHK、U、UH、UHK、H、HK、K十种。如果有n个字符的串，就会有n(n+1)/2种子串，其中最长的子串长度为n。因此我们不得不开辟 n(n+1)/2个长度为n的数组来存储它们，那么存储的空间复杂度将达到惊人的O(n^3)级别。   但是我们发现这样一个特点：             CUHK ——  完全可以表示 C、CU、CUH、CUHK             UHK   ——  完全可以表示 U、UH、UHK             HK     ——  完全可以表示 H、HK、             K       ——  完全可以表示 K 这样我们就得到了4个sistring: CUHK、UHK、HK和K。   PAT tree的存储结构 如果直接用单个字符作为存储结点，势必构造出一棵多叉树(如果是中文字符的话，那就完蛋了)。检索起来将会相当不便。事实上，PAT tree是一棵压缩存储的二叉树结构。现在我们用“CUHK”来构造出这样一棵PAT tree 。   开始先介绍一下PAT tree的结点结构(看了后面的过程就再来理解这些概念) * 内部结点：用椭圆形表示，用来存储不同的bit位在整个完整bit sequence中的位置。 * 外部节点(叶子结点)： 用方形表示，用来记录sistring的首字符在完整sistring中的开始位置(字符索引)和sistring出现的频次。 * 左指针：如果 待存储的sistring在 内部结点所存储的bit位置上的数据 是0，则将这个sistring存储在该结点的左子树中。 * 右指针：若数据是1，则存储在右子树中。   (1) 将所有sistring的字符转化成1 bytes的ASCII码值，用二进制位来表示。形成一个bit sequence pattern(没有的空字符我们用0来填充)。                          sistring                           bit sequence  完整sistring  ->   CUHK        010 00011   01010101   01001000   01001011   <- 完整bit sequence                           UHK0        010 10101   01001000   01001011   00000000                                                     HK00         01001000   01001011   00000000   00000000                           K000         01001011   00000000   00000000   00000000 (2) 从第一个bit开始我们发现所有sistring的前3个bit位都相同010，那么相同的这些0/1串对于匹配来说就毫无意义了，因此我们接下来发现第4个bit开始有所不同了。UHK 的第4个bit是1，而CUHK、HK、K的第4个bit是0。则先构造一个内部结点iNode.bitSize=4（第4个bit），然后将UHK的字符索引 cIndex=2(UHK的开始字符U在完整的CUHK的第2位置上)构造成叶子结点插入到iNode的左孩子上，而CUHK、HK、K放在iNode右子树中。(如下图2)   (3) 递归执行第2步，将CUHK、HK、K进一步插入到PAT tree中。流程如下图所示。所有sistring都插入以后结束。 注意：既然PAT tree 是二叉查找树，那么一定要满足二叉查找树的特点。所以，内部结点中的bit 位就需要满足，左孩子的bit 位< 结点bit 位< 右孩子的bit 位。 PAT tree的检索过程   利用PAT tree可以实现对语料的快速检索，检索过程就是根据查询字串在PAT tree中从根结点寻找路径的过程。当比较完查询字串所有位置后，搜索路径达到PAT tree的某一结点。         若该结点为叶子结点，则判断查询字串是否为叶子结点所指的半无限长字串的前缀，如果判断为真，则查询字串在语料中出现的频次即为叶子结点中记录的频次；否则，该查询字串在语料中不存在。         若该结点为内部结点，则判断查询字串是否为该结点所辖子树中任一叶子结点所指的半无限长字串的前缀。如果判断为真，该子树中所有叶子结点记录的频次之和即为查询字串的出现频次。否则，查询字串在语料中不存在。         这样，通过PAT tree可以检索原文中任意长度的字串及其出现频次，所以，PAT tree也是可变长统计语言模型优良的检索结构。     例如：要查找string= “CU ”(bit sequence=010 00 0 1 1 01010101) 是不是在CUHK 中。 (1)   根据“CUHK ”的PAT tree 结构( 如上图) ，根结点r 的bit position=4 ，那么查找bit sequence 的第4 个bit=0 。然后查找R 的左孩子rc 。 (2)    rc 的bit position=5 ，在bit sequence 的第5 个bit=0 。则查找rc 的左孩子rcc 。 (3)   rcc= ” CUHK ” 已经是叶子结点了，则确定一下CU 是不是CUHK 的前缀即可。   PAT tree 的效率         特点：PAT tree查找的时间复杂度和树的深度有关，由于树的构造取决于不同bit位上0,1的分布。因此PAT tree有点像二叉查找树，最坏情况下是单支树(如上图例子)，此时的时间复杂度是O(n-1)，n为字符串的长度。最好情况下是平衡二叉树 结构，时间复杂度是O(log2(N))。另外，作为压缩的二叉查找树，其存储的空间代价大大减少了。       PAT tree的实际应用          PAT tree在子串匹配上有很好的效率，这一点和Suffix Tree(后缀树)，KMP算法的优点相同。因此PAT tree在信息检索和自然语言处理领域是非常常用的工具。比如：关键字提取，新词发现等NLP领域经常使用这种结构。    ","title":"【串和序列处理 1】PAT Tree 子串匹配结构"},{"content":"课程1: Computer Science  http://www.cs101-class.org/ 课程2: Machine Learning ： 机器学习 http://jan2012.ml-class.org/ 课程3: Cryptography ：密码学 http://www.crypto-class.org 课程4: Software Engineering for Software as a Service http://www.saas-class.org/ 课程5: Computer Security  http://www.security-class.org/ 课程6: Design and Analysis of Algorithms I http://www.algo-class.org/ 课程7: Human-Computer Interaction http://www.hci-class.org/ 课程8: Natural Language Processing: 自然语言处理 http://www.nlp-class.org/ 课程9: Probabilistic Graphical Models 概率图模型  http://www.pgm-class.org/ 课程10: Information Theory  http://www.infotheory-class.org/ 课程11: Model Thinking http://www.modelthinker-class.org/ 课程12: Making Green Buildings http://www.greenbuilding-class.org/ 课程13: Technology Entrepreneurship http://www.venture-class.org/ 课程14: The Lean Launchpad http://www.launchpad-class.org/","title":"2012年2月斯坦福大学全球计算机方面公开课报名网址"},{"content":"隐马尔科夫模型HMM自学 介绍 崔晓源 翻译 我们通常都习惯寻找一个事物在一段时间里的变化规律。在很多领域我们都希望找到这个规律，比如计算机中的指令顺序，句子中的词顺序和语音中的词顺序等等。一个最适用的例子就是天气的预测。 首先，本文会介绍声称概率模式的系统，用来预测天气的变化 然后，我们会分析这样一个系统，我们希望预测的状态是隐藏在表象之后的，并不是我们观察到的现象。比如，我们会根据观察到的植物海藻的表象来预测天气的状态变化。 最后，我们会利用已经建立的模型解决一些实际的问题，比如根据一些列海藻的观察记录，分析出这几天的天气状态。 Generating Patterns 有两种生成模式：确定性的和非确定性的。 确定性的生成模式 ：就好比日常生活中的红绿灯，我们知道每个灯的变化规律是固定的。我们可以轻松的根据当前的灯的状态，判断出下一状态。    非确定性的生成模式： 比 如说天气晴、多云、和雨。与红绿灯不同，我们不能确定下一时刻的天气状态，但是我们希望能够生成一个模式来得出天气的变化规律。我们可以简单的假设当前的 天气只与以前的天气情况有关，这被称为马尔科夫假设。虽然这是一个大概的估计，会丢失一些信息。但是这个方法非常适于分析。 马尔科夫过程就是当前的状态只与前n个状态有关。这被称作n阶马尔科夫模型。最简单的模型就当n=1时的一阶模型。就当前的状态只与前一状态有关。（这里要注意它和确定性生成模式的区别，这里我们得到的是一个概率模型）。下图是所有可能的天气转变情况：    对于有M个状态的一阶马尔科夫模型，共有M*M个状态转移。每一个状态转移都有其一定的概率，我们叫做转移概率，所有的转移概率可以用一个矩阵表示。在整个建模的过程中，我们假设这个转移矩阵是不变的。    该矩阵的意义是：如果昨天是晴，那么今天是晴的概率为0.5，多云的概率是0.25，雨的概率是0.25。注意每一行和每一列的概率之和为1。 另外，在一个系统开始的时候，我们需要知道一个初始概率，称为 向量。    到现在，我们定义了一个一阶马尔科夫模型，包括如下概念： 状态：晴、多云、雨 状态转移概率 初始概率   马尔科夫模型也需要改进！ 崔晓源 翻译 当 一个隐士不能通过直接观察天气状态来预测天气时，但他有一些水藻。民间的传说告诉我们水藻的状态与天气有一定的概率关系。也就是说，水藻的状态与天气时紧 密相关的。此时，我们就有两组状态：观察状态（水藻的状态）和隐含状态（天气状态）。因此，我们希望得到一个算法可以为隐士通过水藻和马尔科夫过程，在没 有直接观察天气的情况下得到天气的变化情况。 更容易理解的一个应用就是语音识别，我们的问题定义就是如何通过给出的语音信号预测出原来的文字信息。在这里，语音信号就是观察状态，识别出的文字就是隐含状态。 这里需要注意的是，在任何一种应用中，观察状态的个数与隐含状态的个数有可能不一样的。下面我们就用隐马尔科夫模型HMM来解决这类问题。 HMM 下图是天气例子中两类状态的转移图，我们假设隐状态是由一阶马尔科夫过程描述，因此他们相互连接。 隐状态和观察状态之间的连线表示：在给定的马尔科夫过程中，一个特定的隐状态对应的观察状态的概率。我们同样可以得到一个矩阵： 注意每一行（隐状态对应的所有观察状态）之和为1。 到此，我们可以得到HMM的所有要素：两类状态和三组概率   两类状态：观察状态和隐状态； 三组概率：初始概率、状态转移概率和两态对应概率（confusion matrix） HMM 定义 崔晓源 翻译 HMM是一个三元组 ( ,A,B).  the vector of the initial state probabilities;   the state transition matrix;    the confusion matrix;  这其中，所有的状态转移概率和混淆概率在整个系统中都是一成不变的。这也是HMM中最不切实际的假设。 HMM的应用 有三个主要的应用：前两个是模式识别后一个作为参数估计 (1) 评估 根据已知的HMM找出一个观察序列的概率。 这 类问题是假设我们有一系列的HMM模型，来描述不同的系统（比如夏天的天气变化规律和冬天的天气变化规律），我们想知道哪个系统生成观察状态序列的概率最 大。反过来说，把不同季节的天气系统应用到一个给定的观察状态序列上，得到概率最大的哪个系统所对应的季节就是最有可能出现的季节。（也就是根据观察状态 序列，如何判断季节）。在语音识别中也有同样的应用。 我们会用forward algorithm 算法来得到观察状态序列对应于一个HMM的概率。 (2) 解码 根据观察序列找到最有可能出现的隐状态序列 回想水藻和天气的例子，一个盲人隐士只能通过感受水藻的状态来判断天气状况，这就显得尤为重要。我们使用viterbi algorithm 来解决这类问题。 viterbi算法也被广泛的应用在自然语言处理领域。比如词性标注。字面上的文字信息就是观察状态，而词性就是隐状态。通过HMM我们就可以找到一句话上下文中最有可能出现的句法结构。 (3) 学习 从观察序列中得出HMM 这是最难的HMM应用。也就是根据观察序列和其代表的隐状态，生成一个三元组HMM ( ,A,B)。使这个三元组能够最好的描述我们所见的一个现象规律。 我们用forward-backward algorithm 来解决在现实中经常出现的问题--转移矩阵和混淆矩阵不能直接得到的情况。 总结 HMM可以解决的三类问题 Matching the most likely system to a sequence of observations -evaluation, solved using the forward algorithm; determining the hidden sequence most likely to have generated a sequence of observations - decoding, solved using the Viterbi algorithm; determining the model parameters most likely to have generated a sequence of observations - learning, solved using the forward-backward algorithm. 找到观察序列的概率 崔晓源 翻译 Finding the probability of an observed sequence 1、穷举搜索方法 对于水藻和天气的关系，我们可以用穷举搜索方法的到下面的状态转移图（trellis）： 图中，每一列于相邻列的连线由状态转移概率决定，而观察状态和每一列的隐状态则由混淆矩阵决定。如果用穷举的方法的到某一观察状态序列的概率，就要求所有可能的天气状态序列下的概率之和，这个trellis中共有3*3=27个可能的序列。 Pr(dry,damp,soggy | HMM) = Pr(dry,damp,soggy | sunny,sunny,sunny) + Pr(dry,damp,soggy | sunny,sunny ,cloudy) + Pr(dry,damp,soggy | sunny,sunny ,rainy) + . . . . Pr(dry,damp,soggy | rainy,rainy ,rainy) 可见计算复杂度是很大，特别是当状态空间很大，观察序列很长时。我们可以利用概率的时间不变性 解决复杂度。   2、采用递归方法降低复杂度 我们采用递归的方式计算观察序列的概率，首先定义部分概率 为到达trellis中某一中间状态的概率。在后面的文章里，我们把长度为T的观察状态序列表示为：   2a. Partial probabilities, ( 's) 在计算trellis中某一中间状态的概率时，用所有可能到达该状态的路径之和表示。 比如在t=2时间，状态为cloudy的概率可以用下面的路径计算： 用 t ( j ) 表示在时间t时 状态j的部分概率。计算方法如下：  t ( j )= Pr( observation | hidden state is j ) * Pr(all paths to state j at time t) 最后的观察状态的部分概率表示，这些状态所经过的所有可能路径的概率。比如： 这表示最后的部分概率的和即为trellis中所有可能路径的和，也就是当前HMM下观察序列的概率。 Section 3  会给出一个动态效果介绍如何计算概率。   2b.计算初始状态的部分概率 我们计算部分概率的公式为:  t ( j )= Pr( observation | hidden state is j ) x Pr(all paths to state j at time t) 但是在初始状态，没有路径到达这些状态。那么我们就用probability乘以associated observation probability计算： 这样初始时刻的状态的部分概率就只与其自身的概率和该时刻观察状态的概率有关. ......... 本文转载自崔晓源博客","title":"隐马尔科夫模型"},{"content":"HITS 算法（Hypertext Induced TopicSelection） HITS 算法也是链接分析中非常基础且重要的算法，目前已被Teoma 搜索引擎（www.teoma.com）作为链接分析算法在实际中使用。 Hub 页面与Authority 页面 Hub 页面和Authority 页面是HITS 算法最基本的两个定义。所谓Authority 页面，是指与某个领域或者某个话题相关的高质量网页。比如搜索引擎领域，Google 和百度首页即该领域的高质量网页；比如视频领域，优酷和土豆首页即该领域的高质量网页。所谓的Hub页面，指的是包含了很多指向高质量Authority 页面链接的网页，比如hao123 首页可以认为是一个典型的高质量Hub 网页。 图 6-11 给出了一个Hub 页面实例，这个网页是斯坦福大学计算语言学研究组维护的页面，这个网页收集了与统计自然语言处理相关的高质量资源，包括一些著名的开源软件包及语料库等，并通过链接的方式指向这些资源页面。这个页面可以认为是“自然语言处理”这个领域的Hub 页面，相应地，被这个页面指向的资源页面，大部分是高质量的Authority页面。       HITS 算法的目的是通过一定的技术手段，在海量网页中找到与用户查询主题相关的高质量Authority 页面和Hub 页面，尤其是Authority 页面，因为这些页面代表了能够满足用户查询的高质量内容，搜索引擎以此作为搜索结果返回给用户。 相互增强关系 很多算法都是建立在一些假设之上的，HITS 算法也不例外。HITS 算法隐含并利用了两个基本假设： · 基本假设 1：一个好的Authority页面会被很多好的Hub 页面指向。 · 基本假设 2：一个好的Hub 页面会指向很多好的Authority页面。 到目前为止，无论是从Hub页面或者Authority 页面的定义也好，还是从两个基本假设也好，都能看到一个模糊的描述，即“高质量”或者“好的”，那么什么是“好的”Hub 页面？什么是“好的”Authority 页面？两个基本假设给出了所谓“好”的定义。 基本假设 1 说明了什么是“好的”Authority页面，即被很多好的Hub 页面指向的页面是好的Authority 页面，这里两个修饰语非常重要：“很多”和“好的”，所谓“很多”，即被越多的Hub 页面指向越好，所谓“好的”，意味着指向该页面的Hub 页面质量越高，则页面越好。这综合了指向本页面的所有Hub 节点的数量和质量因素。 基本假设 2 则给出了什么是“好的”Hub页面的说明，即指向很多好的Authority 页面的网页是好的Hub 页面。同样地，“很多”和“好的”两个修饰语很重要，所谓“很多”，即指向的Authority 页面数量越多越好；所谓“好的”，即指向的Authority 页面质量越高，则该页面越是好的Hub 页面。这也综合考虑了该页面有链接指向的所有页面的数量和质量因素。 从以上两个基本假设可以推导出 Hub 页面和Authority 页面之间的相互增强关系，即某个网页的 Hub 质量越高，则其链接指向的页面的Authority 质量越好；反过来也是如此，一个网页的Authority 质量越高，则那些有链接指向本网页的页面Hub 质量越高。通过这种相互增强关系不断迭代计算，即可找出哪些页面是高质量的Hub 页面，哪些页面是高质量的Authority 页面。   HITS算法 HITS 算法与PageRank 算法一个显著的差异是：HITS 算法与用户输入的查询请求密切相关，而PageRank 算法是与查询无关的全局算法。HITS 后续计算步骤都是在接收到用户查询后展开的，即是与查询相关的链接分析算法。 HITS 算法接收到了用户查询之后，将查询提交给某个现有的搜索引擎（或者是自己构造的检索系统），并在返回的搜索结果中，提取排名靠前的网页，得到一组与用户查询高度相关的初始网页集合，这个集合被称做根集（Root Set）。 在根集的基础上，HITS 算法对网页集合进行扩充，扩充原则是：凡是与根集内网页有直接链接指向关系的网页都被扩充进来，无论是有链接指向根集内页面也好，或者是根集页面有链接指向的页面也好，都被扩充进入扩展网页集合。HITS 算法在这个扩展网页集合内寻找好的Hub 页面与好的Authority 页面。   对于扩展网页集合来说，我们并不知道哪些页面是好的Hub 页面或者好的Authority页面，每个网页都有潜在的可能，所以对于每个页面都设立两个权值，分别来记载这个页面是好的Hub 页面或者Authority 页面的可能性。在初始情况下，在没有更多可利用信息前，每个页面的这两个权值都是相同的，可以都设置为1。 之后，即可利用上面提到的两个基本假设，以及相互增强关系等原则进行多轮迭代计算，每轮迭代计算更新每个页面的两个权值，直到权值稳定不再发生明显的变化为止。 图 6-14 给出了迭代计算过程中，某个页面的Hub 权值和Authority 权值的更新方式。假设以A(i)代表网页i 的Authority 权值，以H(i)代表网页i 的Hub 权值。在如图6-14 所示的例子中，扩展网页集合有3 个网页有链接指向页面1，同时页面1 有3 个链接指向其他页面。那么，网页1 在此轮迭代中的Authority 权值即为所有指向网页1 页面的Hub 权值之和；类似地，网页1 的Hub 分值即为所指向的页面的Authority 权值之和。 扩展网页集合内其他页面也以类似的方式对两个权值进行更新，当每个页面的权值都获得了更新，则完成了一轮迭代计算，此时HITS 算法会评估上一轮迭代计算中的权值和本轮迭代之后权值的差异，如果发现总体来说权值没有明显变化，说明系统已进入稳定状态，则可以结束计算。将页面根据Authority 权值得分由高到低排序，取权值最高的若干页面作为响应用户查询的搜索结果输出。如果比较发现两轮计算总体权值差异较大，则继续进入下一轮迭代计算，直到整个系统权值稳定为止。         HITS算法存在的问题 HITS 算法整体而言是个效果很好的算法，目前不仅在搜索引擎领域应用，而且被自然语言处理及社交分析等很多其他计算机领域借鉴使用，并取得了很好的应用效果。尽管如此，最初版本的HITS 算法仍然存在一些问题，而后续很多基于HITS 算法的链接分析方法，也是立足于改进HITS 算法存在的这些问题而提出的。 归纳起来，HITS 算法主要在以下几个方面存在不足。 计算效率较低 因为HITS 算法是与查询相关的算法，所以必须在接收到用户查询后实时进行计算，而HITS 算法本身需要进行很多轮迭代计算才能获得最终结果，这导致其计算效率较低，这是实际应用时必须慎重考虑的问题。 主题漂移问题 如果在扩展网页集合里包含部分与查询主题无关的页面，而且这些页面之间有较多的相互链接指向，那么使用 HITS 算法很可能会给予这些无关网页很高的排名，导致搜索结果发生主题漂移，这种现象被称为紧密链接社区现象（Tightly-Knit Community Effect）。 易被作弊者操纵结果 HITS 算法从机制上很容易被作弊者操纵，比如作弊者可以建立一个网页，页面内容增加很多指向高质量网页或者著名网站的网址，这就是一个很好的Hub 页面，之后作弊者再将这个网页链接指向作弊网页，于是可以提升作弊网页的Authority 得分。 结构不稳定 所谓结构不稳定，就是说在原有的扩展网页集合内，如果添加删除个别网页或者改变少数链接关系，则HITS 算法的排名结果就会有非常大的改变。 HITS算法与PageRank 算法比较 HITS算法和PageRank 算法可以说是搜索引擎链接分析的两个最基础且最重要的算法。从以上对两个算法的介绍可以看出，两者无论是在基本概念模型，还是计算思路及技术实现细节都有很大的不同，下面对两者之间的差异进行逐一说明。 · HITS 算法是与用户输入的查询请求密切相关的，而PageRank 与查询请求无关。所以，HITS 算法可以单独作为相似性计算评价标准，而PageRank 必须结合内容相似性计算才可以用来对网页相关性进行评价。 · HITS 算法因为与用户查询密切相关，所以必须在接收到用户查询后进行实时计算，计算效率较低；而PageRank 则可以在爬虫抓取完成后离线计算，在线直接使用计算结果，计算效率较高。 · HITS 算法的计算对象数量较少，只需计算扩展集合内网页之间的链接关系；而PageRank 是全局性算法，对所有互联网页面节点进行处理。 · 从两者的计算效率和处理对象集合大小来比较，PageRank更适合部署在服务器端，而HITS 算法更适合部署在客户端。 · HITS 算法存在主题泛化问题，所以更适合处理具体的用户查询；而PageRank 算法在处理宽泛的用户查询时更有优势。 · HITS 算法在计算时，对于每个页面需要计算两个分值，而PageRank 算法只需计算一个分值即可；在搜索引擎领域，更重视HITS 算法计算出的Authority 权值，但是在很多应用HITS 算法的其他领域，Hub 分值也有很重要的作用。 · 从链接反作弊的角度来说，PageRank 从机制上优于HITS 算法，而HITS 算法更易遭受链接作弊的影响。 · HITS 算法结构不稳定，当对扩展网页集合内链接关系做出很小改变，则对最终排名有很大影响；而PageRank 算法相对HITS 而言表现稳定，其根本原因在于PageRank 计算时的远程跳转。     ——本段文字节选自《这就是搜索引擎：核心技术详解》 本书详细信息:http://blog.csdn.net/broadview2006/article/details/7179396","title":"HITS 算法（Hypertext Induced TopicSelection）"},{"content":"序号 会议名称 会议介绍 代表领域   1  ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps  ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左右。  网络通信领域   2  IEEE INFOCOM: The Conference on Computer Communications  IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。  网络通信领域   3  IEEE International conference on communications  IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。  网络通信领域   4  IEEE Globecom: Global Telecommunications Conference  IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。  网络通信领域   5  IEEE ITC: International Test Conference  创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千人。  网络通信领域   6  IEEE The International Conference on Dependable Systems and Networks  IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会议。  网络通信领域   7  ACM MobiCom: International Conference on Mobile Computing and Networking  无线网络领域顶级会议，录用率约为10%，每年举行一次。  无线网络领域   8  ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems  偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。  网络通信领域   9  MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing  无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。  无线网络领域   10  IEEE International Conference on Distributed Computing Systems  由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左右。  分布式计算系统领域   11  IMC: Internet Measurement Conference  网络测量领域顶级的专业会议  网络测量   12  ICCV: IEEE International Conference on Computer Vision  领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇  计算机视觉，模式识别，多媒体计算   13  CVPR: IEEE Conf on Comp Vision and Pattern Recognition  领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇  模式识别，计算机视觉，多媒体计算   14  ECCV: European Conference on Computer Vision  领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇  模式识别，计算机视觉，多媒体计算   15  DCC: Data Compression Conference  领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少  数据压缩   16  ICML: International Conference on Machine Learning  领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少  机器学习，模式识别   17  NIPS: Neural Information Processing Systems  领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇）  神经计算，机器学习   18  ACM MM: ACM Multimedia Conference  领域顶级国际会议，全文的录取率极低，但Poster比较容易  多媒体技术，数据压缩   19  IEEE ICIP: International conference on Image Processing  图像处理领域最具影响力国际会议，一年一次  图像处理   20  IEEE ICME: International Conference on Multimedia and Expo  多媒体领域重要国际会议，一年一次  多媒体技术   21  IEEE VR:IEEE Virtual Reality  IEEE虚拟现实会议，每年一次  虚拟现实领域   22  ACM VRST:ACM Virtual Reality Software and Technology  虚拟现实软件与技术ACM年会，一年一次  虚拟现实领域   23  CGI:Computer Graphics International  国际图形学会议，一年一次  图形学领域   24  ACL: The Association for Computational Linguistics  国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次  计算语言学，自然语言处理   25  COLING: International Conference on Computational Linguistics  计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次  计算语言学，自然语言处理   26  IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing  是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一次  信号处理   27  IJCNLP: International Joint Conference on Natural Language Processing  自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一次  自然语言处理   28  IEEE/ACM Design Automation Conference  顶级会议，在美国召开  IC设计领域   29  IEEE VLSI Test Symposium  一级会议，在美国召开  测试领域   30  IEEE/ACM Design, Automation and Test in Europe  一级会议，在欧洲召开  设计和测试领域   31  IEEE Asian Test Symposium  一级会议，在亚洲召开  测试领域   32  Ubicomp: International Conference on Ubiquitous Computing  国际普适计算年会，本领域最权威的学术会议之一，每年一次  普适计算   33  PerCom: IEEE International Conference on Pervasive Computing and Communications  本领域最权威的学术会议之一，每年一次  普适计算   34  EUC: The IFIP International Conference on Embedded And Ubiquitous Computing  普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加  普适计算与嵌入式系统   35  ICPS: IEEE International Conference on Pervasive Services  普适计算与服务会议，一年一次  普适计算   36  SenSys，ACM Conference on Embedded NEtworked Sensor Systems  ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。  传感器网络   37  SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks,  由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％  传感器网络   38  MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems  由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24%  传感器网络   39  The International Conference for High Performance Computing and Communication  每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/  高性能计算   40  CLUSTER 4, IEEE Int’l Conf. on Cluster Computing,  http://grail.sdsc.edu/cluster2004/  高性能计算   41  HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu.   http://hpdc13.cs.ucsb.edu  高性能计算   42   NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)，  每年一次，http://storageconference.org/   高性能计算   43  SuperComputing：The International Conference for High Performance Computing and Communications   高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org  高性能计算   44  IEEE Int'l Conf. on Cluster Computing   该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。  高性能计算   45  [ICDCS] International Conference on Distributed Computing Systems   由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing.  高性能计算   46  [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing   This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定  高性能计算   47  International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference)  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下半年排名。  高性能计算   48  ACM International Conference on Supercomputing   高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。  高性能计算   49  IEEE International Parallel & Distributed Processing Symposium  IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次  高性能计算   50  IEEE International Conference on Parallel Processing   IEEE发起的并行处理国际会议。每年一次  高性能计算   51  IEEE International Conference on High Performance Computing   IEEE发起的高性能计算国际会议。每年一次在印度举行。  高性能计算   52  ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems.  Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems.  高性能计算   53  IEEE Annual Workshops on Workload Characterization.  The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads.  高性能计算   54  International Symposium on Computer Architecture(ISCA)  ISCA is the premier forum for computer architecture research  高性能计算   55  International Symposium on High Performance Computer Architecture(HPCA)  　  高性能计算   56  International Symposium on Microarchitecture (MICRO)  The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism.  高性能计算   57   FAST: USENIX Conference on File and Storage Technologies,  存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶尖的研究小组在上面发表文章。每年举办一届。  存储领域   58  NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)，  存储领域的专业会议，历史很长，在业界比较有影响  存储领域   59   SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French,  存储领域较好的专业会议  存储领域   60   IEEE SC: SC-High Performance Computing, Networking and Storage Conference  高性能计算领域最好会议之一  存储领域   61  IEEE International Workshop on Networking, Architecture, and Storages（IWNAS）  国内办的存储领域的国际会议  存储领域   62   IEEE International Conference on Autonomic Computing（ICAC）  自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等概念。  自主计算   63   Proceedings of the International Conference on Measurements and Modeling of Computer Systems  性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届  性能研究   64   International Symposium on High Performance Computer Architecture(HPCA)  高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章  高性能计算   65   [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing  高性能分布式计算领域的会议，一年一届，已经举办15届  高性能计算   66   IEEE Int'l Conf. on Cluster Computing  集群和高性能计算很有影响的会议  分布式系统   67   USENIX Annual Technical Conference  操作系统、体系结构方面最好的会议之一  计算机系统   68   IEEE/ACM Int'l Symp. on Cluster Computing & the Grid  集群和网格计算领域很好的会议  集群   69   International Symposium on Computer Architecture（ISCA ）  系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难  系统结构   70   International Symposium on Microarchitecture（MICRO）  系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难  系统结构   71  HPCC：The International Conference for High Performance Computing and Communications  高性能计算领域较高的会议  高性能计算   72   IEEE International Conference on High Performance Computing   IEEE发起的高性能计算国际会议。每年一次在印度举行。  高性能计算   73   Annual ACM International Conference on Supercomputing（ICS）   高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。  高性能计算   74   Symposium on Operating System Design and Implementation（OSDI）  操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难  操作系统   75   ACM Symposium on Operating Systems Principles （SOSP）  操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难  操作系统   76   Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS）  操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困难  操作系统，程序语言   77   Workshop on Hot Topics in Operating Systems （HOTOS）  操作系统最好的会议之一  操作系统   78   Proceedings of the International Conference on Parallel Processing（ICPP）  并行计算非常有影响的会议  并行计算   79   Annual IEEE Conference on Local Computer Networks（LCN）  　  网络   80   International Conference on Distributed Computing Systems（ICDCS）  分布式计算非常有影响的会议，每年一次  分布式计算   81  International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT）  分布式计算很好的会议，每年一次，已经举办7届  分布式计算   82  IEEE International Parallel and Distributed Processing Symposium（IPDPS）  并行与分布式计算领域非常有影响的会议，每年一次  并行与分布式计算   83  ASPLOS: Architectural Support for Programming Languages and Operating Systems  ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影响。  编译技术   84  CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems  CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％  编译技术   85  CODES: International Conference on Hardware Software Codesign  CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。  编译技术   86  DAC: Annual ACM IEEE Design Automation Conference  DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。  编译技术   87  ICFP: International Conference on Functional Programming  Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。  编译技术   88  ICS: International Conference on Supercomputing  ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收率30％左右。  编译技术   89  ICSE: International Conference on Software Engineering  软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。  编译技术   90  ISCA: International Conference on Computer Architecture  ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收率20％左右。  编译技术   91  ISMM: International Symposium on Memory Management  内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior  编译技术   92  ISSTA: International Symposium on Software Testing and Analysis  ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。  编译技术   93  LCTES: Language, Compiler and Tool Support for Embedded Systems  关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。  编译技术   94  MICRO: International Symposium on Microarchitecture  计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会议。  编译技术   95  OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications  Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。  编译技术   96  PLDI: Conference on Programming Language Design and Implementation  PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影响。  编译技术   97  PODC: Annual ACM Symposium on Principles of Distributed Computing  关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。  编译技术   98  POPL: Annual Symposium on Principles of Programming Languages  关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％  编译技术   99  PPoPP: Principles and Practice of Parallel Programming  PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收率30％。  编译技术   100  SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems  关注计算机系统性能方面的theory, practice and case studies的国际会议  编译技术   101  SIGSOFT: Foundations of Software Engineering  Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。  编译技术   102  ASE - IEEE International Conference on Automated Software Engineering  关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。  编译技术   103  CGO - International Symposium on Code Generation and Optimization  CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收率30％左右。  编译技术   104  CLUSTER - IEEE International Conference on Cluster Computing  CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。  编译技术   105  DATE - Design, Automation, and Test in Europe  关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议  编译技术   106  EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing  EUROMICRO的一个workshop，关注并行和分布式计算。  编译技术   107  HPCA - International Symposium on High-Performance Computer Architecture  HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。  编译技术   108  HPCS - Annual International Symposium on High Performance Computing Systems and Applications  IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域  编译技术   109  ICDCS - International Conference on Distributed Computing Systems  IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。  编译技术   110  ICPADS - International Conference on Parallel and Distributed Systems  IEEE主办，关注并行和分布式系统的国际会议  编译技术   111  IISWC - IEEE International Symposium on Workload Characterization  关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。  编译技术   112  IPDPS - International Parallel and Distributed Processing Symposium  IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统等。  编译技术   113  ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software  IEEE主办，关注计算机软硬件设计中的性能分析。  编译技术   114  PACT - International Conference on Parallel Architectures and Compilation Techniques  PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影响。  编译技术   115  RTSS - IEEE Real-Time Systems Symposium  实时系统研究的顶级会议，IEEE主办，已经举行了27届。  编译技术   116  RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium  关注实时和嵌入式计算的基础结构，理论，system support的国际会议。  编译技术   117  SC - IEEE/ACM SC Conference  SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。  编译技术   118  LCPC - International Workshop on Languages and Compilers for Parallel Computing  始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。  编译技术   119  CC: International Conference on Compiler Construction  关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83  编译技术   120  HiPEAC - International Conference on High Performance Embedded Architectures & Compilers  关注嵌入式系统的发展，包括处理器设计，编译优化等。  编译技术   121  ECOOP - European Conference on Object-Oriented Programming  关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。  编译技术   122  ESOP - European Symposium on Programming  ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。  编译技术   123  Euro-Par - European Conference on Parallel Computing  关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。  　   124  SAS - International Static Analysis Symposium  关注程序的静态分析的权威会议。  编译技术   125  CAV - Computer Aided Verification  Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。  编译技术   126  FASE - Fundamental Approaches to Software Engineering  ETAPS的member conference，主要关注Software Science，影响因子0.91。  编译技术   127  TACAS - Tools and Algorithms for the Construction and Analysis of Systems  ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24  编译技术   128  VMCAI - Verification, Model Checking and Abstract Interpretation  Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。  编译技术   129   ACL: The Association for Computational Linguistics  计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。  人工智能 计算语言学   130  ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval  信息检索方面最好的会议, ACM 主办, 每年开。19％左右  信息检索技术   131  ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining  数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右  　   132  WWW: The ACM International World Wide Web Conference  应用和媒体领域顶级国际会议  万维网   133  ACM SIGMOD: ACM SIGMOD Conf on Management of Data  数据库领域顶级国际  数据管理   134  CIKM: The ACM Conference on Information and Knowledge Management  数据库领域知名国际会议  数据管理   135  COLING: International Conference on Computational Linguistics  计算语言学知名国际会议  计算语言学   136  ICML: International Conference on Machine Learning  领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少  机器学习，模式识别   137  IEEE ICDM: International Conference on Data Mining  数据挖掘领域顶级国际会议  　   138  IJCAI: International Joint Conference on Artificial Intelligence  人工智能领域顶级国际会议，论文接受率18％左右  人工智能   139  VLDB: The ACM International Conference on Very Large Data Bases  数据库领域顶级国际  数据库   140  SIGGRAPH: ACM SIGGRAPH Conference  计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％  计算机图形学   141  EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics  欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20%  计算机图形学   142  AAAI: American Association for Artificial Intelligence  美国人工智能学会AAAI的年会，使该领域的顶级会议  人工智能   143  ACM Conference　on　Computer and　Communications Security  ACM通信和计算健全领域顶级学术会议  信息安全   144  ACM SIGCOMM: Special Interest Group on Data Communications  　  数据通信   145  ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval  信息检索领域的重要会议  信息检索   146  ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining  ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。  通信与网络   147  ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems  ACM性能建模与评价领域顶级学术会议  通信与网络   148  ACM SIGMOD: ACM SIGMOD Conf on Management of Data  数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。  数据管理   149  ASPLOS: Architectural Support for Programming Languages and Operating Systems  编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。  体系结构   150  CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid  网格计算国际会议，网格平台、中间件  　   151  CIKM: The ACM Conference on Information and Knowledge Management  信息检索领域的会议，录用率为15%  信息检索   152  CLUSTER - IEEE International Conference on Cluster Computing  集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。  集群计算   153  CPM: Combinatorial Pattern Matching Symposium  组合模式匹配年会，是字符串匹配、模式匹配较好的会议。  模式匹配   154  FAST4: Third USENIX Conference on File and Storage Technologies,  USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。  文件与存储   155  Grid : IEEE/ACM International Workshop on Grid Computing  网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。  网格计算   156  HPC: IEEE International Conference for High Performance Computing  　  　   157  HPDC: International Symposium on High Performance Distributed Computing  高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。  高性能计算   158  ICDCS: IEEE International Conference on Distributed Computing Systems  IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS 举办了26届，录用率为15%。  分布式计算   159  ICML: International Conference on Machine Learning  机器学习领域中的顶级会议  机器学习   160  ICWS: IEEE International Conference on Web Services  Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。  Web服务   161  IEEE CSB: Computer Society Bioinformatics  　  　   162  IEEE ICDM: International Conference on Data Mining  数据挖掘领域的著名会议，率用率为14%。  数据挖掘   163  IEEE ICNP: International Conference on Network Protocols  IEEE 网络通信领域顶级学术会议，录用率在10%左右。  网络   164  IEEE ICON: IEEE International Conference on Networks  　  　   165  IEEE INFOCOM: conference on computer communications  IEEE网络通信领域著名会议，领域广泛。  网络   166  IEEE IPCCC: International Performance Computing and Communications Conference  IEEE性能领域著名学术会议，主要关注性能评价。  网络性能   167  IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval  字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。  字符串处理信息检索   168  IJCAI: International Joint Conference on AI  人工智能领域的顶级会议。  人工智能   169  IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference  IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference  ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。   170  International Symposium on High Performance Computer Architecture(HPCA)  　  服务计算   171  IPDPS: IEEE International Parallel and Distributed Processing Symposium  IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%， 会议的优秀论文可能发表于JDPC杂志上。  并行计算分布式计算   172  ISMB: International conference on Intelligent Systems for Molecular Biology  　  无线   173  MobiCom: ACM/IEEE Conference on Mobile Computing and Networking  始于1995，无线、移动计算方面比较有历史和重要的会议。  安全   174  MobiSys: The International Conference on Mobile Systems, Applications, and Services  无线方面，2006年第4名。  无线   175  OSDI: USENIX Symposium on Operating Systems Design and Implementation  USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。  操作系统   176  PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining  　  　   177  PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies  关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。  并行计算分布式计算   178  PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases  数据挖掘领域的重要会议，录用率为14%。  数据挖掘   179  SCC: IEEE International Conference on Services Computing  服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。  服务计算   180  SDM: SIAM International Conference on Data Mining  数据挖掘领域的重要会议，录用率为14%  数据挖据   181  SOSE: IEEE International Workshop on Service-Oriented System Engineering  　  　   182  USENIX Sec: USENIX Security Symposium  USENIX安全领域重要会议，侧重安全技术。  安全   183  USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI)  USENIX网络领域重要会议，设计网络涉及各方面内容。  网络   184  VLDB: The ACM International Conference on Very Large Data Bases  　  数据管理   185  WWW: The ACM International World Wide Web Conference  ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。  Internet   186  RAID International Symposium on Recent Advances in Intrusion Detection  数据库顶级国际会议  　   187  IJCAI: International Joint Conference on Artificial Intelligence  人工智能顶级国际会议  人工智能   188  VLDB: The ACM International Conference on Very Large Data Bases  数据库顶级国际会议  数据库   189  ICML: International Conference on Machine Learning  机器学习顶级国际会议  机器学习   190  PRICAI: Pacific Rim International Conference on Artificial Intelligence  亚太人工智能国际会议  人工智能   191  IFIP ICIIP: IFIP International Conference on Intelligent Information Processing  IFIP智能信息处理国际会议  智能信息处理   192  NIPS: Neural Information Processing Systems  神经信息处理领域顶级国际会议  神经计算，机器学习   193  ISCA: International Symposium on Computer Architecture    体系结构领域的顶级会议  微处理器设计   194  International Symposium on Microarchitecture  体系结构领域的顶级会议  微处理器设计   195  HPCA：International Symposium on High Performance Computer Architecture  体系结构领域的顶级会议  微处理器设计   196  APCSAC: Asia-Pacific Computer Systems Architecture Conference   体系结构方面的重要会议  微处理器设计   197  ISLPED: International Symposium on Low Power Electronics and Design  低功耗设计的重要会议  微处理器设计   198  OSDI: Operation System Design & Implementation   操作系统方面的重要会议  微处理器设计   199  ASPLOS: Architecture Support for Programming Languages and Operation   体系结构方面的顶尖会议  微处理器设计   200  ICCD: IEEE International Conference on Computer Design  体系结构方面的顶尖会议  微处理器设计   201  DAC: Design Automation Conference   设计自动化领域的顶级会议  微处理器设计   202  IEEE/ACM International Conference on Computer Aided Design(ICCAD)  集成电路设计自动化方面的顶尖会议  微处理器设计   203  ASP-DAC: Asia and South Pacific Design Automation Conference   设计自动化领域的重要会议  微处理器设计   204  ISSCC: IEEE International Solid-State Circuits Conference  设计自动化领域的重要会议  微处理器设计   205  CICC: Custom Integrated Circuits Conference  集成电路设计方面的顶尖会议（公认排名第二）  微处理器设计   206  ESSCIRC: European Conference on Solid-State Circuits  集成电路设计方面的顶尖会议  微处理器设计   207  Symposium on VLSI Circuits  集成电路设计方面的顶尖会议  微处理器设计   208  IEEE International ASIC/SOC Conference  集成电路设计方面的重要会议  微处理器设计   209  Symposium on VLSI Technology  集成电路设计方面的重要会议  微处理器设计   210  ASSCC: Asian Conference on Solid-State Circuits  集成电路领域重要会议  微处理器设计   211  MWSCAS: Midwest Symposium on Circuits and Systems  集成电路领域重要会议  微处理器设计   212  ICECS: IEEE International Conference on Electronics, Circuits and Systems：  集成电路领域重要会议  微处理器设计   213  ISCAS: International Symposium Circuit and System   电路与系统方面的重要会议  微处理器设计   214  RFIC: IEEE Symposium on Radio Frequency Integrated Circuits   射频集成电路领域顶尖会议  微处理器设计   215  ACM RECOMB: Int. Conference on Research in Computational Molecular Biology  RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在20%左右。  计算生物学   216  IEEE CSB: Computer Society Bioinformatics  CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主办。  计算生物学   217  International Conference of the IEEE Engineering in Medicine and Biology Society （EMBC）  生物信息学与计算生物学国际研讨会  计算生物学   218  PSB: Pacific Symposium on Biocomputing   PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举办。  计算生物学   219  WABI:Workshop on Algorithms in Bioinformatics  WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方面。  计算生物学   220  CSB: IEEE Computational Systems Bioinformatics Conference   CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主办  计算生物学   221  ISMB：Annual International Conference on Intelligent Systems for Molecular Biology  ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为Bioinformatics杂志的专刊发表，近年来录取率为15%左右。  生物信息   222  ECCB：European Conference on Computational Biology  ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为Bioinformatics杂志的专刊发表，近年录取率在20%左右。  生物信息   223  APBC：Asia Pacific Bioinformatics Conference  APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。  生物信息   224  COCOON：Annual International Computing and Combinatorics Conference  COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左右。  生物信息   225  CPM：Annual Symposium on Combinatorial Pattern Matching  CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左右。  生物信息   226  EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society  EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千计。  生物信息   227  Geospatial Information and Technology Association（GITA） Annual Conference  地球空间信息与技术协会年会  遥感与空间信息处理   228  International Geoscience and Remote Sensing Symposium （IGRSS）  地球科学与遥感国际会议  遥感与空间信息处理   229  International Society for Photogrammetry and Remote Sensing （ISPRS）Technical Commission Symposium  国际摄影测量与遥感学会专业委员会会议  遥感与空间信息处理   230  International Conference on Geoinformatics  地球信息国际会议  遥感与空间信息处理   231  IEEE SKG (Semantics, Knowledge and Grid)  由计算所发起的IEEE国际会议，每年有100人参加。  知识网格   232  WWW: The ACM International World Wide Web Conference  Internet领域顶级国际会议  Internet   233  International Semantic Web Conference  Semantic Web领域顶级会议，录用率17%  Semantic Web   234  ACM SIGMOD: ACM SIGMOD Conf on Management of Data  ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。  数据管理   235  ACM PODS Conference  ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。  数据管理   236  VLDB: The ACM International Conference on Very Large Data Bases  数据库顶级国际会议  数据管理   237  IEEE ICDE - International Conference on Data Engineering  数据库顶级国际会议  数据管理","title":"[置顶] 计算机学科国际会议排名"},{"content":"对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面，这也称为相关度排序。 机器算法和人不一样的地方是人可以直接理解词的意思，文章的意思，机器和算法不能理解。人看到苹果这两个字就知道指的是那个圆圆的，有水的挺好吃的东西，搜索引擎却不能从感性上理解。 中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。 什么是中文分词： 众所周知，英文是以词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。例如，英文句子I am a student，用中文则为：“我是一个学生”。计算机可以很简单通过空格知道student是一个单词，但是不能很容易明白「学」、「生」两个字合起来才表示一个词。把中文的汉字序列切分成有意义的词，就是中文分词，有些人也称为切词。我是一个学生，分词的结果是：我 是 一个 学生。 Google的中文分词技术采用的是美国一家名叫Basis Technology的公司提供的中文分词技术，百度使用的是自己公司开发的分词技术，中搜使用的是国内海量科技提供的分词技术。业界评论海量科技的分词技术目前被认为是国内最好的中文分词技术，其分词准确度超过99%，由此也使得中搜在搜索结果中搜索结果的错误率很低。由此可见，中文分词的准确度，对搜索引擎结果相关性和准确性有相当大的关系。 中文分词技术详解： 中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。 现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 基于字符串匹配的分词方法 这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个「充分大的」机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。 按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下： 正向最大匹配法（由左到右的方向）；逆向最大匹配法（由右到左的方向）；最少切分（使每一句中切出的词数最小）。还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。 但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。 一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。 对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。 基于理解的分词方法 这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。 基于统计的分词方法 从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。 定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。 但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如「这一」、「之一」、「有的」、「我的」、「许多的」等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。笔者了解，海量科技的分词算法就采用「复方分词法」，所谓复方，相当于用中药中的复方概念，即用不同的药才综合起来去医治疾病，同样，对于中文词的识别，需要多种算法来处理不同的问题。 分词中的难题 有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。 歧义识别 歧义是指同样的一句话，可能有两种或者更多的切分方法。例如：表面的，因为「表面」和「面的」都是词，那么这个短语就可以分成「表面 的」和「表 面的」。这种称为交叉歧义。像这种交叉歧义十分常见。「化妆和服装」可以分成「化妆 和 服装」或者「化妆 和服 装」。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。 交叉歧义相对组合歧义来说是还算比较容易处理，组合歧义就必需根据整个句子来判断了。例如，在句子「这个门把手坏了」中，「把手」是个词，但在句子「请把手拿开」中，「把手」就不是一个词；在句子「将军任命了一名中将」中，「中将」是个词，但在句子「产量三年中将增长两倍」中，「中将」就不再是词。这些词计算机又如何去识别? 如果交叉歧义和组合歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：「乒乓球拍卖完了」，可以切分成「乒乓 球拍 卖 完 了」、也可切分成「乒乓球 拍卖 完 了」，如果没有上下文其他的句子，恐怕谁也不知道「拍卖」在这里算不算一个词。 新词识别 新词，专业术语称为未登录词。也就是那些在字典中都没有收录过，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解句子「王军虎去广州了」中，「王军虎」是个词，因为是一个人的名字，但要是让计算机去识别就困难了。 如果把「王军虎」做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子「王军虎头虎脑的」中，「王军虎」还能不能算词？ 新词中除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。 分词技术的应用 目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。 其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。 分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。 因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有了。科研院校研究的技术，大部分不能很快产品化，而一个专业公司的力量毕竟有限，看来中文分词技术要想更好的服务于更多的产品，还有很长一段路。 ================================ 转载自:http://brucehan.com/archives/292","title":"中文分词技术初识"},{"content":"假设有两个句子，我们想知道它们之间是否相关联： 第一个是：“乔布斯离我们而去了。” 第二个是：“苹果价格会不会降？” 如果由人来判断，我们一看就知道，这两个句子之间虽然没有任何公共词语，但仍然是很相关的。这是因为，虽然第二句中的“苹果”可能是指吃的苹果，但是由于第一句里面有了“乔布斯”，我们会很自然的把“苹果”理解为苹果公司的产品。事实上，这种文字语句之间的相关性、相似性问题，在搜索引擎算法中经常遇到。例如，一个用户输入了一个query，我们要从海量的网页库中找出和它最相关的结果。这里就涉及到如何衡量query和网页之间相似度的问题。对于这类问题，人是可以通过上下文语境来判断的。但是，机器可以么？ 在传统信息检索领域里，实际上已经有了很多衡量文档相似性的方法，比如经典的VSM模型。然而这些方法往往基于一个基本假设：文档之间重复的词语越多越可能相似。这一点在实际中并不尽然。很多时候相关程度取决于背后的语义联系，而非表面的词语重复。 那么，这种语义关系应该怎样度量呢？事实上在自然语言处理领域里已经有了很多从词、词组、句子、篇章角度进行衡量的方法。本文要介绍的是其中一个语义挖掘的利器：主题模型。 主题模型是什么？ 主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。还是上面的例子，“苹果”这个词的背后既包含是苹果公司这样一个主题，也包括了水果的主题。当我们和第一句进行比较时，苹果公司这个主题就和“乔布斯”所代表的主题匹配上了，因而我们认为它们是相关的。 在这里，我们先定义一下主题究竟是什么。主题就是一个概念、一个方面。它表现为一系列相关的词语。比如一个文章如果涉及到“百度”这个主题，那么“中文搜索”、“李彦宏”等词语就会以较高的频率出现，而如果涉及到“IBM”这个主题，那么“笔记本”等就会出现的很频繁。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布 。与主题关系越密切的词语，它的条件概率越大，反之则越小。 例如： 通俗来说，一个主题就好像一个“桶”，它装了若干出现概率较高的词语。这些词语和这个主题有很强的相关性，或者说，正是这些词语共同定义了这个主题。对于一段话来说，有些词语可以出自这个“桶”，有些可能来自那个“桶”，一段文本往往是若干个主题的杂合体。我们举个简单的例子，见下图。 以上是从互联网新闻中摘抄下来的一段话。我们划分了4个桶（主题），百度（红色），微软（紫色）、谷歌（蓝色）和市场（绿色）。段落中所包含的每个主题的词语用颜色标识出来了。从颜色分布上我们就可以看出，文字的大意是在讲百度和市场发展。在这里面，谷歌、微软这两个主题也出现了，但不是主要语义。值得注意的是，像“搜索引擎”这样的词语，在百度、微软、谷歌这三个主题上都是很可能出现的，可以认为一个词语放进了多个“桶”。当它在文字中出现的时候，这三个主题均有一定程度的体现。 有了主题的概念，我们不禁要问，究竟如何得到这些主题呢？对文章中的主题又是如何进行分析呢？这正是主题模型要解决的问题。下面我简要介绍一下主题模型是怎样工作的。 主题模型的工作原理 首先，我们用生成模型的视角来看文档和主题这两件事。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为： 上面这个式子，可以矩阵乘法来表示，如下图所示： 左边的矩阵表示每篇文章中每次词语出现的概率；中间的Φ矩阵表示的是每个主题中每个词语出现的概率 ，也就是每个“桶 表示的是每篇文档中各个主题出现的概率 ，可以理解为一段话中每个主题所占的比例。 假如我们有很多的文档，比如大量的网页，我们先对所有文档进行分词，得到一个词汇列表。这样每篇文档就可以表示为一个词语的集合。对于每个词语，我们可以用它在文档中出现的次数除以文档中词语的数目作为它在文档中出现的概率 。这样，对任意一篇文档，左边的矩阵是已知的，右边的两个矩阵未知。而主题模型就是用大量已知的“词语－文档”矩阵 ，通过一系列的训练，推理出右边的“词语－主题”矩阵Φ 和“主题文档”矩阵Θ 。 主题模型训练推理的方法主要有两种，一个是pLSA（Probabilistic Latent Semantic Analysis），另一个是LDA（Latent Dirichlet Allocation）。pLSA主要使用的是EM（期望最大化）算法；LDA采用的是Gibbs sampling方法。由于它们都较为复杂且篇幅有限，这里就只简要地介绍一下pLSA的思想，其他具体方法和公式，读者可以查阅相关资料。 pLSA采用的方法叫做EM（期望最大化）算法，它包含两个不断迭代的过程：E（期望）过程和M（最大化）过程。用一个形象的例子来说吧：比如说食堂的大师傅炒了一盘菜，要等分成两份给两个人吃，显然没有必要拿天平去一点点去精确称量，最简单的办法是先随意的把菜分到两个碗中，然后观察是否一样多，把比较多的那一份取出一点放到另一个碗中，这个过程一直重复下去，直到大家看不出两个碗里的菜有什么差别为止。 对于主题模型训练来说，“计算每个主题里的词语分布”和“计算训练文档中的主题分布”就好比是在往两个人碗里分饭。在E过程中，我们通过贝叶斯公式可以由“词语－主题”矩阵计算出“主题－文档”矩阵。在M过程中，我们再用“主题－文档”矩阵重新计算“词语－主题”矩阵。这个过程一直这样迭代下去。EM算法的神奇之处就在于它可以保证这个迭代过程是收敛的。也就是说，我们在反复迭代之后，就一定可以得到趋向于真实值的 Φ和 Θ。 如何使用主题模型？ 有了主题模型，我们该怎么使用它呢？它有什么优点呢？我总结了以下几点： 1）  它可以衡量文档之间的语义相似性。对于一篇文档，我们求出来的主题分布可以看作是对它的一个抽象表示。对于概率分布，我们可以通过一些距离公式（比如KL距离）来计算出两篇文档的语义距离，从而得到它们之间的相似度。 2)它可以解决多义词的问题。回想最开始的例子，“苹果”可能是水果，也可能指苹果公司。通过我们求出来的“词语－主题”概率分布，我们就可以知道“苹果”都属于哪些主题，就可以通过主题的匹配来计算它与其他文字之间的相似度。 3） 它可以排除文档中噪音的影响。一般来说，文档中的噪音往往处于次要主题中，我们可以把它们忽略掉，只保持文档中最主要的主题。 4） 它是无监督的，完全自动化的。我们只需要提供训练文档，它就可以自动训练出各种概率，无需任何人工标注过程。 5） 它是跟语言无关的。任何语言只要能够对它进行分词，就可以进行训练，得到它的主题分布。 综上所述，主题模型是一个能够挖掘语言背后隐含信息的利器。近些年来各大搜索引擎公司都已经开始重视这方面的研发工作。语义分析的技术正在逐步深入到搜索领域的各个产品中去。在不久的将来，我们的搜索将会变得更加智能，让我们拭目以待吧。","title":"搜索背后的奥秘——浅谈语义主题计算"},{"content":"哈工大社会计算与信息检索研究中心  中文文本分类介绍 概况介绍 中文文本自动分类是自然语言处理的经典研究方向，有着极其重要的应用价值。文本分类的核心技术为构建一个具有高准确度和较高速度的分类器，高效率的分类器才能具有实用性。目前构建分类器的方法有贝叶斯分类算法、K紧邻（K-NN）、决策树、线性最小二乘法估计(LLSF)、支持向量机（SVM）等，其中K-NN和SVM是基于向量空间模型（VSM）的最好的分类器，我们采用的SVM更是具有其他算法所不具备的优点，通过实验也证明了SVM能获得更好的分类性能。分类问题是自然语言处理的一个基本问题，很多相关的研究都可以归结为分类问题。自动分类在信息检索、图书馆管理和网页新闻体系划分都有重要应用。 体系结构 文本分类系统的任务是：在给定的分类体系下，根据文本的内容自动地确定文本关联的类别。从数学角度来看，文本分类是一个映射的过程，它将未标明类别的文本映射到已有的类别中，该映射可以是一一映射，也可以是一对多的映射，因为通常一篇文本可以同多个类别相关联。 我们可以把分类过程分成2个部分：训练过程和分类过程： 在2004年举行的863分类评测中，我们系统在最短时间内最快完成测试，表现出极高的效率和稳定性，在10多家参加单位中处于中游水平。历经2年不断改进，在14类新闻分类体系中准确率达到80%，基本达到实用水平。 系统功能 1）较快的训练和测试过程。 2）根据不同用户需求，可以便捷地更改分类体系。 3）在Windows和Linux下都可以运行。 4）支持单一文本即时分类和成批量文本同时分类。 技术特点 我们使用的方法基于类别特征域的文本分类特征选择方法。该方法首先利用“组合特征抽取”的方法去除原始特征空间中的噪音，从中抽取出候选特征。这里，“组合特征抽取”是指先利用文档频率(DF)的方法去掉一部分低频词，再用互信息的方法选择出候选特征。接下来，本方法为分类体系中的每个类别构建一个类别特征域，对出现在类别特征域中的候选特征进行特征的合并和强化，从而解决数据稀疏的问题。实验表明，这种新的方法较之各种传统方法在特征选择的效果上有着明显改善，并能显著提高文本分类系统的性能。 性能指标 在2004年举行的863分类评测中，我们系统在最短时间内最快完成测试，表现出很高的效率和稳定性，准确率和招回率在10多家参加评测单位中名次位于中游。历经2年不断改进，在11个类别的新闻分类体系中准确率达到80%以上，基本达到实用水平。 目前我们的系统对常用的11个类别进行测试的具体指标： 类别 准确率 招回率 F值 财经 0.725526 0.792208 0.757402 房产 0.911090 0.871912 0.891071 汽车 0.900372 0.925359 0.912695 旅游 0.677174 0.692222 0.684615 体育 0.960980 0.955776 0.958371 教育 0.858624 0.844300 0.851402 生活 0.670659 0.640000 0.654971 科技 0.821463 0.823026 0.822243 游戏 0.930732 0.936212 0.933464 娱乐 0.913690 0.921922 0.917788 军事 0.927310 0.891218 0.908906 微平均值 0.845238 0.844923 0.844811 应用领域 1、信息检索 2、新闻即时分类 3、词义消歧 4、图书馆管理系统 哈工大信息检索研究室推广技术  技术列表 中文分词与词性标注 中文命名实体识别 中文词义消歧 中文依存句法分析 中文自动校对 中文文本分类 单文档自动文摘 中文多文档自动文摘 主页替换自动发现 大规模文本/网页去重系统 中英文例句检索系统 中文信息模糊匹配 简繁/繁简转换 汉语文本自动标注拼音 基于FAQ自动客服系统 中文全文检索系统 英文拼写检查 机构信息抽取系统","title":"中文文本分类"},{"content":"一、计算机科学期刊介绍 　　计算机科学的publication最大特点在于：极度重视会议，而期刊则通常只用来做re-publication。大部分期刊文章都是会议论文的扩展版，首发就在期刊上的相对较少。也正因为如此，计算机期刊的影响因子都低到惊人的程度，顶级刊物往往也只有1到2左右----被引的通常都是会议版论文，而不是很久以后才出版的期刊版。因此，要讨论计算机科学的publication，首先必须强调的一点是totally forget about IF (IF指影响因子)。 　　另外一点要强调的是，计算机科学的绝大多数期刊和大部分的“好”会议都规模非常有限。很多好的期刊一期只登十来篇甚至三四篇论文，有的还是季刊或双月刊。很多好的会议每年只录用三四十篇甚至二十篇左右的论文。所以，当你发现计算机的每个领域都有好几种顶级刊物和好几个顶级会议，不必惊讶。 　　整个计算机科学中最好的期刊为Journal of the ACM(JACM)。此刊物为ACM的官方学刊，受到最广泛的尊敬。但由于该刊宣称它只刊登那些对计算机科学有长远影响的论文，因此其不可避免地具有理论歧视（theory bias）。事实上确实如此：尽管JACM征稿范围包括了计算机的绝大部分领域，然而其刊登的论文大部分都是算法、复杂度、图论、组合数学等纯粹理论的东西，其它领域的论文要想进入则难如登天。 　　另外一份在计算机科学领域有重大影响的刊物为Communications of the ACM (CACM)。从某种意义上来说，CACM比JACM要像Nature/Science很多。JACM上登的全是长篇大论，满纸的定义、定理和证明，别说 一般读者没法看，就连很相近的领域的专家都未必能看懂。而CACM则是magazine，既登高水平的学术论文和综述，也登各种科普性质的文章和新闻。即便是论文，CACM也要求文章必须通俗易懂，不追求数学上的严格证明，而追求易于理解的直觉描述。在十几二十年前，CACM的文章几乎都是经典。但最近几年，由于CACM进一步通俗化，其学术质量稍有下降。 　　IEEE Transaction on Computers为IEEE 在计算机方面最好的刊物。但由于IEEE的特点，其更注重computer engineering而非computer science。换句话说，IEEE Transaction on Computers主要登载systems, architecture, hardware等领域的东西，尽管它的范围已经比大部分刊物要广泛。 　　就刊物的质量而言，ACM Transactions系列总体来讲都高于IEEE Transactions系列，不过也不可一概而论。大部分ACM Transactions都是本领域最好的刊物或最好的刊物之一。大部分IEEE Transactions都是本领域很好的刊物，但也有最好的或者一般的。非ACM/IEEE的刊物中，也有好的甚至最好的。例如，SIAM Journal on Computing被认为是理论方面最好的期刊之一。 　　计算机科学方面的会议论文事实上起着比刊物论文更大的作用。大部分会议都是每年一次，偶尔也有隔年一次的。正规的会议论文需要经过2-4个甚至更多个审稿人的双向或单向匿名评审，并且所有被接收的论文会被结集正式出版。 　　大部分ACM的会议都是本领域顶级的或很好的会议。大部分IEEE的会议都是本领域很好的会议，但也有顶级的或者一般的。 会议的档次通常可以通过论文录用率表现出来。顶级会议通常在20%左右或更低，有时能达到10%左右。我所知道的最低的录用率为7%。很好的会议通常在30%左右。达到40%以上时，会议的名声就很一般了。60%以上的会议通常很难受到尊敬。 　　但也有例外。大名鼎鼎的STOC(ACM Symposium on Theory of Computing)录用率就达到30%以上，但它毫无疑问是理论方面最好的会议。造成这样的情形，主要是因为理论方面的工作者不多，而大部分人对STOC又有一种又敬又怕的心理。 二、国内计算机类三大中文学报投稿体会 　　国内计算机类三大中文学报《计算机学报》《软件学报》《计算机研究与发展》投稿的实际体会。 　　共同点： 　　都是EI核心来源期刊； 　　中国计算机学会参与主办会刊； 　　科学出版社出版发行； 　　可接受8000-10000字左右的长文； 　　稿量大，处理流程大多缓慢，应早投； 　　国内众星捧月。 　　《计算机学报》 http://cjc.ict.ac.cn/ ， 月刊， 中国计算机学会与中国科学院计算技术研究所主办，网站好像最近改版了，但功能有所欠缺。 　　投稿方式： 网页登记然后Email投稿,中英文均可；初审后通知编号，邮寄打印稿二份及投稿声明等， 　　审理费：150元。 　　审稿周期：6个月左右。被拒的或录用的文章给出的意见都比较中肯，感觉审稿人专业啊。（感觉被拒的稿件处理快，可接受的稿件审理较慢） 　　录用率：不详。 　　版面费：不详。 　　发表周期：不详。 　　其它等事宜未接触不加评论。 　　《软件学报》 http://www.jos.org.cn ， 月刊，由中国科学院软件研究所和中国计算机学会联合主办，网站功能齐全，投稿处理流程合理。 　　投稿方式：直接网站在线投稿，中英文均可。初审后通知编号，邮寄审理费。外审通过后再通知其它处理事宜。 　　审理费：150元。 　　审稿周期：6个月左右。（本人前后总共投了二篇，第一篇4个月拒了，第2篇8个月给了通知,根据意见逐一修改，且必须给出修改说明，较为严谨）。 　　录用率：不详。 　　发表周期：录用后的发表周期较长，需要耐心等待，但专刊较快。 　　版面费标准：180.00元/面，收费比较厚道。 　　整个处理流程中，编辑部会及时与作者沟通确认，感觉很受尊重。2007年（以后也会有）该刊组织了很多专刊，投专刊的文章被录用后发表周期相对短，但录用率超低（如今年网络专刊为5%）。专刊反映信息较快，是一种不错的方式。 　　《计算机研究与发展》 http://crad.ict.ac.cn/ ，月刊， 由中国科学院计算技术研究所-中国计算机学会联合主办,网站功能较为齐全。 　　投稿方式：在线投稿或email投稿的同时还要邮寄3份打印稿，投稿声明等。好像不接受英文稿。 　　审理费:200元。 　　审理周期：官方说6个月左右（实际体会，投了一篇稿子，催了三次等了8个月拒了，郁闷。） 　　录用率：不详。 　　发表周期：不详。 　　版面费：260元/页（不超过7页），超出的部分，每页400元。 　　最后必须要提的是：国内最权威的计算机科学与技术类期刊当属英文期刊JCST（中文名：计算机科学技术学报,双月刊,国内外同行评审,中国计算机学会和中科院计算所主办），SCI和EI双收录。 　　附JCST简历：《Journal of Computer Science and Technology》(JCST)是中国计算机科学技术领域唯一个英文学术性期刊。 JCST为中国计算机学会会刊, 由中国科学院计算技术研究所承办。JCST由数十位国际计算机界的著名专家和学者联袂编审，把握世界计算机科学技术最新发展趋势。目前，JCST正在稳步地发展，其影响不断扩大，知名度日益提高。JCST荟萃了国内外计算机科学技术领域中有指导性和开拓性的学术论著，其中部分文章邀请了著名计算机领域的专家撰写。其内容包括: 计算机科学理论，形式化方法，信息安全，算法与计算复杂性，计算机体系结构与高性能计算，模式识别与图像处理， VLSI设计与测试，软件工程，计算机网络与Internet，分布式计算与网格计算，自然语言处理，生物信息学，计算机图形学与人机交互，人工智能等。 　　网站： http://jcst.ict.ac.cn/ ","title":"[转载]计算机专业权威期刊投稿经验总结"},{"content":"之前有一段时间流行一篇标题“你有没有想过，你可能一辈子都是一个小人物？”的日志，看完之后我感觉有点不舒服，倒不是被伤自尊之类的，而是在关于人的追求的问题上的不赞同。但一时想不到一个太好的评论，直到今天在知乎上看到了一句话：我最害怕的不是自己一辈子都是一个小人物，而是每天仍然是昨天的我，没有改变。这就是我全部想说的，写这个总结，也是看看今天的我和去年的我有多大的改变。 上半年还是呆在学校，课程作业外加御姐(导师姓郁，不要误会)的\"加餐\"，时间轴被各种deadline分割，每天在图书馆泡着看ppt或者paper，一度不记得星期，只记得明天XX作业好像要交了。幸运的是，在一大堆烂课中，我还选了一个数据挖掘，旁听了一个信息检索，突然发现原来计算机领域还有这么有意思的事儿。于是课下多投入了点时间，好好做了几个课程作业，看了几篇paper，然后不能自拔，于是有了现在的我。最近时常在想，如果当初我选的是Linux内核开发，现在的我又是什么样呢？这就是缘分吧。御姐那边还是一如既往的无用功，上半学期比较迷茫还算可以忍受，下半学期发现自己感兴趣的领域之后，这种痛苦就以吨为单位了，时常想像《勇敢的心》的主角那样呐喊一声“freedom！”，好在很快就实习了，稍微远离了御姐的魔爪。上面两个事情综合起来，确定了我现在学习的优先级。 课程快要结束，还没开始找实习的时候，和两个同学参加了腾讯soso的比赛，算是小试牛刀。结果还算比较满意，没花多少时间，赚了一百Q币，孝敬岳母了，不过更重要的是，增加了自信，少了很多畏难情绪。 6月27号正式开始了我的实习生涯，到现在已经大半年了，最大的感受就是过得快，一个星期刷刷就过去了，好像也没干什么事儿。大部分时间不是那么累，通常晚上不早回住的地方的主要原因是想在公司上自习。在这大半年中，唯一感觉到累的就是前一段项目进度比较紧的时候，连续一个星期没早于两点睡觉，周六准备要跑的数据，五点才搞完。当然，也就那么一个星期，其他时候工作量还是可以接受的。说到收获，一方面是对自然语言处理这个领域的东西有了一些更加深入的了解，另一方面也对公司的项目流程更熟悉了。当然，最重要的还是认识了很多聪明而且勤奋的朋友，能和他们一起工作真的是一件令人兴奋的事儿。 工作的业余时间也做了一些事儿，这是我觉得我在这一年中比较大的变化，简而言之就是行动力更强了，以前很多停留在嘴皮子上的话开始变为行动了。首先就是爱上了Python，花了一两个星期的课余时间入门之后，我就不可收拾的爱上了这门语言，并且彻底地、完全地、毫不留情地抛弃了Java，虽然Python和Shell的结合不像Perl那么完美，但是我觉得美感上更胜一筹，这半年来大部分的线下代码也都是Shell和Python写的脚本。为了尽可能的使用Python，我还参加了Google赞助的人工智能比赛，一度冲进中国区前三，后来由于要学公开课，就放弃了。另外一大部分时间花在了机器学习的在线公开课上面，虽然课程本身比较简单，不懂也能做对习题，但还是搞清楚了一些基本的概念和算法的本质，另外还认识了一些学友，也算收获不小。为了更好的理解机器学习课程里面的向量化，我还特意利用十一看了MIT的线性代数的公开课视频，弥补了本科数学方面的一大缺陷，同时在这个过程中发现我们的计算机教育和国外的差距不只是科研，更重要的是理念，当然者也可能和师资有关系。这同时也促使我下定决心，今后在学习的过程中，尽量使用英文的原版教材。 生活方面最大的进步就是学会了做菜，哈哈，可惜回来就被老爸鄙视了-。-，不过没关系，他实践的时间数倍于我，我相信长江后浪推前浪。 2012年我只有两个目标：1.学好数学 2.选择好第一份职业。 如果没有末日，希望明年总结的时候，这两个目标能达到。","title":"2012年总结"},{"content":"         自然语言处理系统有很多问题与未登录词识别有关，比如分词、索引、新热点发现、主题词/中心词分析、人物关系统计等等。所以，很多相关系统都号称自己具有未登录词识别功能，也就是内嵌了一个未登录词识别模块。先不论效果好坏，单是这种将未登录词识别作为一个子模块来使用的方法，就值得商榷。   首先，从理论的角度进行分析：   顾名思义，未登录词就是未知的新词。判断一个新字符串是否应作为一个词，是基于世界知识的，需要人参与才能确认。不可能做到完全自动化，除非你的机器系统的智能程度达到了人的水平。如果将未登录词识别作为生产系统的一个子模块，是将系统的整体表现寄托于一个不准确的系统，其效果必然要打折扣。   另外，未登录词是一个总称，并不具有准确的范畴。未登录词几乎涵盖了所有类型的实体名称，包括人名、地名、组织名、产品名、科技名词等等无穷无尽的类别。不能定义边界，也总结不出规律，又如何能作为自动系统的处理对象呢？因此，总的来讲，未登录词识别系统的表现难以评测和控制。没有人的介入，不能放心地使用。   其次，从产品的角度进行分析：   目前，比较可靠的未登录词识别技术只有中文人名识别，还不包括少数民族人名。其他类型的未登录词识别都不成熟，因为做得少，用得少。但技术人员或市场人员经常有意无意地夸大，含糊其辞，这很容易误导产品决策或宣传，使技术研发和产品发展陷入被动。   未登录词识别只是一种技术，往往并不是产品的真正需求。其实，产品需要的是一个可靠的词处理系统，管你未登录词识别还是已登录词识别。比如，如果能高效的人工扩充词表，那也能达到效果。但是由于误导、误解、懒惰等等复杂原因，产品和技术人员往往口径一致地推崇未登录词识别。   再次，从技术的角度进行分析：   未登录词识别的算法复杂度往往很高，需要大量的规则、数据，需要对文本反复扫描，等等。这些会极大地降低系统运行效率。   更进一步，由于算法复杂带来了更高的系统复杂度，系统的可靠性会降低，运行、维护、监控成本都会提高。比如搜索引擎，动辄几百、几千台服务器，系统可靠性每降低一点，其效果和成本的损失都是难以估量的。   最后，从组织管理的角度进行分析：   未登录词识别作为某个系统的一个子模块存在，会造成技术和产品部门的相互推诿。技术部门说，改进需要产品反馈，需要编辑数据；产品部门说，这是技术部门负责的，我们愿意配合，但技术部门没说清楚需要什么数据，如何获得数据。   即便是在技术部门内部，未登录词识别功能分布在各个系统中，其中的诀窍成为每个研发人员的个人财产，不能有效共享。而且大家的结果不尽相同，也会带来麻烦。   所以，未登录词识别应该独立成系统，组织专门的部门研发、运营和改进。将所有研发人员的技术诀窍通过统一的系统集中起来成为公司的整体财产。而直接对外提供线上服务的生产系统，应尽可能使用可靠的、经过确认的词表资源，尽量杜绝不可靠、不可控、难以评估的技术直接对外服务。   当然，这样做的前提，是未登录词识别在公司整体战略中占重要位置。如果仅仅是某些产品、技术人员的小噱头，那就没必要了。   近来跟一些同行聊到未登录词识别的问题，感到这个问题的认识在业内还很模糊。所以整理一下个人想法，记录如上。 转：http://blog.sina.com.cn/s/blog_53dd7ff60100ddg1.html","title":"未登录词识别应该单独成系统"},{"content":"学习计划如下： Oracle数据库优化，Linux系统管理，JDeveloper，及Oracle BI及其他的Fusion Applications的架构设计（3月份，4月份，5月份） 数据结构与算法(Linux C语言) 参考：http://blog.csdn.net/muge0913/article/details/7342977和书籍《程序设计导引及在线实践》（6月份，7月份，8月份） 网络编程，多线程编程，分布式计算（9，10，11月份） 自然语言处理NLP，数据挖掘，机器学习              Oracle 数据挖掘：http://www.oracle.com/technetwork/database/options/odm/index.html","title":"2012年学习计划"},{"content":"转自：http://www.kuqin.com/math/20071204/2790.html 数学之美系列十六（上）－ 不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型 [我们在投资时常常讲不要把所有的鸡蛋放在一个篮子里，这样可以降低风险。在信息处理中，这个原理同样适用。在数学上，这个原理称为最大熵原理(the maximum entropy principle)。这是一个非常有意思的题目，但是把它讲清楚要用两个系列的篇幅。] 前段时间，Google 中国研究院的刘骏总监谈到在网络搜索排名中，用到的信息有上百种。更普遍地讲，在自然语言处理中，我们常常知道各种各样的但是又不完全确定的信息，我们需要用一个统一的模型将这些信息综合起来。如何综合得好，是一门很大的学问。 让我们看一个拼音转汉字的简单的例子。假如输入的拼音是\"wang-xiao-bo\"，利用语言模型，根据有限的上下文(比如前两个词)，我们能给出两个最常见的名字“王小波”和“王晓波”。至于要唯一确定是哪个名字就难了，即使利用较长的上下文也做不到。当然，我们知道如果通篇文章是介绍文学的，作家王小波的可能性就较大；而在讨论两岸关系时，台湾学者王晓波的可能性会较大。在上面的例子中，我们只需要综合两类不同的信息，即主题信息和上下文信息。虽然有不少凑合的办法，比如：分成成千上万种的不同的主题单独处理，或者对每种信息的作用加权平均等等，但都不能准确而圆满地解决问题，这样好比以前我们谈到的行星运动模型中的小圆套大圆打补丁的方法。在很多应用中，我们需要综合几十甚至上百种不同的信息，这种小圆套大圆的方法显然行不通。 数学上最漂亮的办法是最大熵(maximum entropy)模型，它相当于行星运动的椭圆模型。“最大熵”这个名词听起来很深奥，但是它的原理很简单，我们每天都在用。说白了，就是要保留全部的不确定性，将风险降到最小。让我们来看一个实际例子。 有一次，我去 AT&T 实验室作关于最大熵模型的报告，我带去了一个色子。我问听众“每个面朝上的概率分别是多少”，所有人都说是等概率，即各点的概率均为1/6。这种猜测当然是对的。我问听众们为什么，得到的回答是一致的：对这个“一无所知”的色子，假定它每一个朝上概率均等是最安全的做法。（你不应该主观假设它象韦小宝的色子一样灌了铅。）从投资的角度看，就是风险最小的做法。从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。接着，我又告诉听众，我的这个色子被我特殊处理过，已知四点朝上的概率是三分之一，在这种情况下，每个面朝上的概率是多少？这次，大部分人认为除去四点的概率是 1/3，其余的均是 2/15，也就是说已知的条件（四点概率为 1/3）必须满足，而对其余各点的概率因为仍然无从知道，因此只好认为它们均等。注意，在猜测这两种不同情况下的概率分布时，大家都没有添加任何主观的假设，诸如四点的反面一定是三点等等。（事实上，有的色子四点反面不是三点而是一点。）这种基于直觉的猜测之所以准确，是因为它恰好符合了最大熵原理。 最大熵原理指出，当我们需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。（不做主观假设这点很重要。）在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。 回到我们刚才谈到的拼音转汉字的例子，我们已知两种信息，第一，根据语言模型，wang-xiao-bo 可以被转换成王晓波和王小波；第二，根据主题，王小波是作家，《黄金时代》的作者等等，而王晓波是台湾研究两岸关系的学者。因此，我们就可以建立一个最大熵模型，同时满足这两种信息。现在的问题是，这样一个模型是否存在。匈牙利著名数学家、信息论最高奖香农奖得主希萨（Csiszar）证明，对任何一组不自相矛盾的信息，这个最大熵模型不仅存在，而且是唯一的。而且它们都有同一个非常简单的形式 -- 指数函数。下面公式是根据上下文（前两个词）和主题预测下一个词的最大熵模型，其中 w3 是要预测的词（王晓波或者王小波）w1 和 w2 是它的前两个字（比如说它们分别是“出版”，和“”），也就是其上下文的一个大致估计，subject 表示主题。 我们看到，在上面的公式中，有几个参数 lambda 和 Z ，他们需要通过观测数据训练出来。 最大熵模型在形式上是最漂亮的统计模型，而在实现上是最复杂的模型之一。我们在将下一个系列中介绍如何训练最大熵模型的诸多参数，以及最大熵模型在自然语言处理和金融方面很多有趣的应用。 数学之美系列十六（下）－ 不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型  上面用最大熵模型可以将各种信息综合在一起。我们留下一个问题没有回答，就是如何构造最大熵模型。我们已经所有的最大熵模型都是指数函数的形式，现在只需要确定指数函数的参数就可以了，这个过程称为模型的训练。 最原始的最大熵模型的训练方法是一种称为通用迭代算法 GIS(generalized iterative scaling) 的迭代 算法。GIS 的原理并不复杂，大致可以概括为以下几个步骤： 1. 假定第零次迭代的初始模型为等概率的均匀分布。 2. 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，如果超过了实际的，就把相应的模型参数变小；否则，将它们便大。 3. 重复步骤 2 直到收敛。 GIS 最早是由 Darroch 和 Ratcliff 在七十年代提出的。但是，这两人没有能对这种算法的物理含义进行很好地解释。后来是由数学家希萨（Csiszar)解释清楚的，因此，人们在谈到这个算法时，总是同时引用 Darroch 和Ratcliff 以及希萨的两篇论文。GIS 算法每次迭代的时间都很长，需要迭代很多次才能收敛，而且不太稳定，即使在 64 位计算机上都会出现溢出。因此，在实际应用中很少有人真正使用 GIS。大家只是通过它来了解最大熵模型的算法。 八十年代，很有天才的孪生兄弟的达拉皮垂(Della Pietra)在 IBM 对 GIS 算法进行了两方面的改进，提出了改进迭代算法 IIS（improved iterative scaling）。这使得最大熵模型的训练时间缩短了一到两个数量级。这样最大熵模型才有可能变得实用。即使如此，在当时也只有 IBM 有条件是用最大熵模型。 由于最大熵模型在数学上十分完美，对科学家们有很大的诱惑力，因此不少研究者试图把自己的问题用一个类似最大熵的近似模型去套。谁知这一近似，最大熵模型就变得不完美了，结果可想而知，比打补丁的凑合的方法也好不了多少。于是，不少热心人又放弃了这种方法。第一个在实际信息处理应用中验证了最大熵模型的优势的，是宾夕法尼亚大学马库斯的另一个高徒原 IBM 现微软的研究员拉纳帕提(Adwait Ratnaparkhi)。拉纳帕提的聪明之处在于他没有对最大熵模型进行近似，而是找到了几个最适合用最大熵模型、而计算量相对不太大的自然语言处理问题，比如词性标注和句法分析。拉纳帕提成功地将上下文信息、词性（名词、动词和形容词等）、句子成分（主谓宾）通过最大熵模型结合起来，做出了当时世界上最好的词性标识系统和句法分析器。拉纳帕提的论文发表后让人们耳目一新。拉纳帕提的词性标注系统，至今仍然是使用单一方法最好的系统。科学家们从拉纳帕提的成就中，又看到了用最大熵模型解决复杂的文字信息处理的希望。 但是，最大熵模型的计算量仍然是个拦路虎。我在学校时花了很长时间考虑如何简化最大熵模型的计算量。终于有一天，我对我的导师说，我发现一种数学变换，可以将大部分最大熵模型的训练时间在 IIS 的基础上减少两个数量级。我在黑板上推导了一个多小时，他没有找出我的推导中的任何破绽，接着他又回去想了两天，然后告诉我我的算法是对的。从此，我们就建造了一些很大的最大熵模型。这些模型比修修补补的凑合的方法好不少。即使在我找到了快速训练算法以后，为了训练一个包含上下文信息，主题信息和语法信息的文法模型(language model)，我并行使用了 20 台当时最快的 SUN 工作站，仍然计算了三个月。由此可见最大熵模型的复杂的一面。最大熵模型快速算法的实现很复杂，到今天为止，世界上能有效实现这些算法的人也不到一百人。有兴趣实现一个最大熵模型的读者可以阅读我的论文。 最大熵模型，可以说是集简与繁于一体，形式简单，实现复杂。值得一提的是，在Google的很多产品中，比如机器翻译，都直接或间接地用到了最大熵模型。 讲到这里，读者也许会问，当年最早改进最大熵模型算法的达拉皮垂兄弟这些年难道没有做任何事吗？他们在九十年代初贾里尼克离开 IBM 后，也退出了学术界，而到在金融界大显身手。他们两人和很多 IBM 语音识别的同事一同到了一家当时还不大，但现在是世界上最成功对冲基金(hedge fund)公司----文艺复兴技术公司 (Renaissance Technologies)。我们知道，决定股票涨落的因素可能有几十甚至上百种，而最大熵方法恰恰能找到一个同时满足成千上万种不同条件的模型。达拉皮垂兄弟等科学家在那里，用于最大熵模型和其他一些先进的数学工具对股票预测，获得了巨大的成功。从该基金 1988 年创立至今，它的净回报率高达平均每年 34%。也就是说，如果 1988 年你在该基金投入一块钱，今天你能得到 200 块钱。这个业绩，远远超过股神巴菲特的旗舰公司伯克夏哈撒韦（Berkshire Hathaway)。同期，伯克夏哈撒韦的总回报是 16 倍。 值得一提的是，信息处理的很多数学手段，包括隐含马尔可夫模型、子波变换、贝叶斯网络等等，在华尔街多有直接的应用。由此可见，数学模型的作用。","title":"数学之美系列十六（上）－ 不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型"},{"content":"文章来源：漫话中文自动分词和语义识别（上）：中文分词算法 漫话中文自动分词和语义识别（下）：句法结构和语义结构    记得第一次了解中文分词算法是在 Google 黑板报 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进行研究，期间诞生了很多有意思的理论。     中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。     有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。     最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来／应聘”。     维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。     还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如“民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民”并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／服务”。       不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。     当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词数法上，我们就有了一种简明而强大的算法：     对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。     这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分：       他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ）       他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ）       他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ）     正确答案胜出。     需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。     算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。“鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概率，每个词出现的频率也是不同的。     何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。     以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10-9 ，但“有意／见／分歧”的得分只有 1.0×10-11 ，正确方案完胜。     这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话：       这／事／的确／定／不／下来     但是概率算法却会把这个句子分成：       这／事／的／确定／不／下来     原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。     其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。     于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w1 、 w2 ，统计在语料库中词语 w1 后面恰好是 w2 的概率 P(w1, w2) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w1) · P(w1, w2) · … · P(wn-1, wn) ，其中 w1, w2, …, wn 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。     至此，中文自动分词算是有了一个漂亮而实用的算法。         但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。     在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。     可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、“说”、“报道”、“参加”、“访问”、“表示”等动作词。     但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了；要是听说了有人居然敢叫“令计划”，估计直接就崩溃了。     还有那些恰好与上下文组合成词的人名，例如：      费孝通向人大常委会提交书面报告      邓颖超生前使用过的物品     这就是最考验分词算法的句子了。     相比之下，中国地名的用字就分散得多了。北京有一个地方叫“臭泥坑”，网上搜索“臭泥坑”，第一页全是“臭泥坑地图”、“臭泥坑附近酒店”之类的信息。某年《重庆晨报》刊登停电通知，上面赫然印着“停电范围包括沙坪坝区的犀牛屙屎和犀牛屙屎抽水”，读者纷纷去电投诉印刷错误。记者仔细一查，你猜怎么着，印刷并无错误，重庆真的就有叫“犀牛屙屎”和“犀牛屙屎抽水”的地方。     好在，中国地名数量有限，这是可以枚举的。中国地名委员会编写了《中华人民共和国地名录》，收录了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。     真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。     最难识别的未登录词就是缩略语了。“高数”、“抵京”、“女单”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。     汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。     说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。     这篇文章是漫话中文分词算法的续篇。在这里，我们将紧接着上一篇文章的内容继续探讨下去：如果计算机可以对一句话进行自动分词，它还能进一步整理句子的结构，甚至理解句子的意思吗？这两篇文章的关系十分紧密，因此，我把前一篇文章改名为了《漫话中文自动分词和语义识别（上）》，这篇文章自然就是它的下篇。我已经在很多不同的地方做过与这个话题有关的演讲了，在这里我想把它们写下来，和更多的人一同分享。     什么叫做句法结构呢？让我们来看一些例子。“白天鹅在水中游”，这句话是有歧义的，它可能指的是“白天有一只鹅在水中游”，也可能指的是“有一只白天鹅在水中游”。不同的分词方案，产生了不同的意义。有没有什么句子，它的分词方案是唯一的，但也会产生不同的意思呢？有。比如“门没有锁”，它可能是指的“门没有被锁上”，也有可能是指的“门上根本就没有挂锁”。这个句子虽然只能切分成“门／没有／锁”，但由于“锁”这个词既有可能是动词，也有可能是名词，因而让整句话产生了不同的意思。有没有什么句子，它的分词方案是唯一的，并且每个词的词义也都不再变化，但整个句子仍然有歧义呢？有可能。看看这句话：“咬死了猎人的狗”。这句话有可能指的是“把猎人的狗咬死了”，也有可能指的是“一只咬死了猎人的狗”。这个歧义是怎么产生的呢？仔细体会两种不同的意思后，你会发现，句子中最底层的成分可以以不同的顺序组合起来，歧义由此产生。     在前一篇文章中，我们看到了，利用概率转移的方法，我们可以有效地给一句话分词。事实上，利用相同的模型，我们也能给每一个词标注词性。更好的做法则是，我们直接把同一个词不同词性的用法当作是不同的词，从而把分词和词性标注的工作统一起来。但是，所有这样的工作都是对句子进行从左至右线性的分析，而句子结构实际上比这要复杂多了，它是这些词有顺序有层次地组合在一起的。计算机要想正确地解析一个句子，在分词和标注词性后，接下来该做的就是分析句法结构的层次。     在计算机中，怎样描述一个句子的句法结构呢？ 1957 年， Noam Chomsky 出版了《句法结构》一书，把这种语言的层次化结构用形式化的方式清晰地描述了出来，这也就是所谓的“生成语法”模型。这本书是 20 世纪为数不多的几本真正的著作之一，文字非常简练，思路非常明晰，震撼了包括语言学、计算机理论在内的多个领域。记得 Quora 上曾经有人问 Who are the best minds of the world today ，投出来的答案就是 Noam Chomsky 。     随便取一句很长很复杂的话，比如“汽车被开车的师傅修好了”，我们总能至顶向下地一层层分析出它的结构。这个句子最顶层的结构就是“汽车修好了”。汽车怎么修好了呢？汽车被师傅修好了。汽车被什么样的师傅修好了呢？哦，汽车被开车的师傅修好了。当然，我们还可以无限地扩展下去，继续把句子中的每一个最底层的成分替换成更详细更复杂的描述，就好像小学语文中的扩句练习那样。这就是生成语法的核心思想。     熟悉编译原理的朋友们可能知道“上下文无关文法”。其实，上面提到的扩展规则本质上就是一种上下文无关文法。例如，一个句子可以是“什么怎么样”的形式，我们就把这条规则记作       句子 → 名词性短语＋动词性短语     其中，“名词性短语”指的是一个具有名词功能的成分，它有可能就是一个名词，也有可能还有它自己的内部结构。例如，它有可能是一个形容词性短语加上“的”再加上另一个名词性短语构成的，比如“便宜的汽车”；它还有可能是由“动词性短语＋的＋名词性短语”构成的，比如“抛锚了的汽车”；它甚至可能是由“名词性短语＋的＋名词性短语”构成的，比如“老师的汽车”。我们把名词性短语的生成规则也都记下来：       名词性短语 → 名词       名词性短语 → 形容词性短语＋的＋名词性短语       名词性短语 → 动词性短语＋的＋名词性短语       名词性短语 → 名词性短语＋的＋名词性短语       ⋯⋯     类似地，动词性短语也有诸多具体的形式：       动词性短语 → 动词       动词性短语 → 动词性短语＋了       动词性短语 → 介词短语＋动词性短语       ⋯⋯     上面我们涉及到了介词短语，它也有自己的生成规则：       介词短语 → 介词＋名词性短语       ⋯⋯     我们构造句子的任务，也就是从“句子”这个初始结点出发，不断调用规则，产生越来越复杂的句型框架，然后从词库中选择相应词性的单词，填进这个框架里：            而分析句法结构的任务，则是已知一个句子从左到右各词的词性，要反过来求出一棵满足要求的“句法结构树”。这可以用 Earley parser 来实现。     这样看来，句法结构的问题似乎就已经完美的解决了。其实，我们还差得很远。生成语法有两个大问题。首先，句法结构正确的句子不见得都是好句子。 Chomsky 本人给出了一个经典的例子： Colorless green ideas sleep furiously 。形容词加形容词加名词加动词加副词，这是一个完全符合句法要求的序列，但随便拼凑会闹出很多笑话——什么叫做“无色的绿色的想法在狂暴地睡觉”？顺便插播个广告，如果你还挺喜欢这句话的意境的，欢迎去我以前做的 IdeaGenerator 玩玩。不过，如果我们不涉及句子的生成，只关心句子的结构分析，这个缺陷对我们来说影响似乎并不大。生成语法的第二个问题就比较麻烦了：从同一个词性序列出发，可能会构建出不同的句法结构树。比较下面两个例子：       老师 被 迟到 的 学生 逗乐 了       电话 被 窃听 的 房间 找到 了     它们都是“名词＋介词＋动词＋的＋名词＋动词＋了”，但它们的结构并不一样，前者是老师被逗乐了，“迟到”是修饰“学生”的，后者是房间找到了，“电话被窃听”是一起来修饰房间的。但是，纯粹运用前面的模型，我们无法区分出哪句话应该是哪个句法结构树。如何强化句法分析的模型和算法，让计算机构建出一棵正确的句法树，这成了一个大问题。     让我们来看一个更简单的例子吧。同样是“动词＋形容词＋名词”，我们有两种构建句法结构树的方案：            未经过汉语语法训练的朋友可能会问，“点亮蜡烛”和“踢新皮球”的句法结构真的不同吗？我们能证明，这里面真的存在不同。我们造一个句子“踢破皮球”，你会发现对于这个句子来说，两种句法结构都是成立的，于是出现了歧义：把皮球踢破了（结构和“点亮蜡烛”一致），或者是，踢一个破的皮球（结构和“踢新皮球”一致）。     但为什么“点亮蜡烛”只有一种理解方式呢？这是因为我们通常不会把“亮”字直接放在名词前做定语，我们一般不说“一根亮蜡烛”、“一颗亮星星”等等。为什么“踢新皮球”也只有一种理解方式呢？这是因为我们通常不会把“新”直接放在动词后面作补语，不会说“皮球踢新了”，“衣服洗新了”等等。但是“破”既能作定语又能作补语，于是“踢破皮球”就产生了两种不同的意思。如果我们把每个形容词能否作定语，能否作补语都记下来，然后在生成规则中添加限制条件，不就能完美解决这个问题了吗？     基于规则的句法分析器就是这么做的。汉语语言学家们已经列出了所有词的各种特征：       亮：词性 = 形容词，能作补语 = True ，能作定语 = False ⋯⋯       新：词性 = 形容词，能作补语 = False ，能作定语 = True ⋯⋯       ⋯⋯     当然，每个动词也有一大堆属性：       点：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯       踢：词性 = 动词，能带宾语 = True ，能带补语 = True ⋯⋯       污染：词性 = 动词，能带宾语 = True ，能带补语 = False ⋯⋯       排队：词性 = 动词，能带宾语 = False ，能带补语 = False ⋯⋯       ⋯⋯     名词也不例外：       蜡烛：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯       皮球：词性 = 名词，能作主语 = True ，能作宾语 = True ，能受数量词修饰 = True ⋯⋯       ⋯⋯     有人估计会觉得奇怪了：“能作主语”也是一个属性，莫非有些名词不能做主语？哈哈，这样的名词不但有，而且还真不少：剧毒、看头、厉害、正轨、存亡⋯⋯这些词都不放在动词前面。难道有些名词不能做宾语吗？这样的词也有不少：享年、芳龄、心术、浑身、家丑⋯⋯这些词都不放在动词后面。这样说来，存在不受数量词修饰的词也就不奇怪了，事实上上面这些怪异的名词前面基本上都不能加数量词。     另外一个至关重要的就是，这些性质可以“向上传递”。比方说，我们规定，套用规则       名词性短语 → 形容词性短语＋名词性短语     后，整个名词性短语能否作主语、能否作宾语、能否受数量词修饰，这将取决于它的第二个构成成分。通俗地讲就是，如果“皮球”能够作主语，那么“新皮球”也能够作主语。有了“词语知识库”，又确保了这些知识能够在更高层次得到保留，我们就能给语法生成规则添加限制条件了。例如，我们可以规定，套用规则       动词性短语 → 动词性短语＋名词性短语     的前提条件就是，那个动词性短语的“能带宾语”属性为 True ，并且那个名词性短语“能作宾语”的属性为 True 。另外，我们规定       动词性短语 → 动词性短语＋形容词性短语     必须满足动词性短语的“能带补语”属性为 True ，并且形容词性短语“能作补语”属性为 True 。这样便阻止了“踢新皮球”中的“踢”和“新”先结合起来，因为“新”不能作补语。     最后我们规定，套用规则       名词性短语 → 形容词性短语＋名词性短语     时，形容词性短语必须要能作定语。这就避免了“点亮蜡烛”中的“亮”和“蜡烛”先组合起来，因为“亮”通常不作定语。这样，我们便解决了“动词＋形容词＋名词”的结构分析问题。     当然，这只是一个很简单的例子。在这里的问题 6 、 7 、 8 中你可以看到，一条语法生成规则往往有很多限制条件，这些限制条件不光是简单的“功能相符”和“前后一致”，有些复杂的限制条件甚至需要用 IF … THEN … 的方式来描述。你可以在这里看到，汉语中词与词之间还有各种怪异的区别特征，并且哪个词拥有哪些性质纯粹是知识库的问题，完全没有规律可循。一个实用的句法结构分析系统，往往拥有上百种属性标签。北京大学计算语言所编写了《现代汉语语法信息词典》，它里面包含了 579 种属性。我们的理想目标就是，找到汉语中每一种可能会影响句法结构的因素，并据此为词库里的每一个词打上标签；再列出汉语语法中的每一条生成规则，找到每一条生成规则的应用条件，以及应用这条规则之后，整个成分将会以怎样的方式继承哪些子成分的哪些属性，又会在什么样的情况下产生哪些新的属性。按照生成语言学的观点，计算机就应该能正确解析所有的汉语句子了。         那么，这样一来，计算机是否就已经能从句子中获取到理解语义需要的所有信息了呢？答案是否定的。还有这么一些句子，它从分词到词义到结构都没有两可的情况，但整个句子仍然有歧义。考虑这句话“鸡不吃了”，它有两种意思：鸡不吃东西了，或者我们不吃鸡了。但是，这种歧义并不是由分词或者词义或者结构导致的，两种意思所对应的语法结构完全相同，都是“鸡”加上“不吃了”。但为什么歧义仍然产生了呢？这是因为，在句法结构内部，还有更深层次的语义结构，两者并不相同。     汉语就是这么奇怪，位于主语位置上的事物既有可能是动作的发出者，也有可能是动作的承受者。“我吃完了”可以说，“苹果吃完了”也能讲。然而，“鸡”这个东西既能吃，也能被吃，歧义由此产生。     位于宾语位置上的事物也不一定就是动作的承受者，“来客人了”、“住了一个人”都是属于宾语反而是动作发出者的情况。记得某次数理逻辑课上老师感叹，汉语的谓词非常不规范，明明是太阳在晒我，为什么要说成是“我晒太阳”呢？事实上，汉语的动宾搭配范围极其广泛，还有很多更怪异的例子：“写字”是我们真正在写的东西，“写书”是写的结果，“写毛笔”是写的工具，“写楷体”是写的方式，“写地上”是写的场所，“写一只狗”，等等，什么叫做“写一只狗”啊？我们能说“写一只狗”吗？当然可以，这是写的内容嘛，“同学们这周作文写什么啊”，“我写一只狗”。大家可以想像，学中文的老外看了这个会是什么表情。虽然通过句法分析，我们能够判断出句子中的每样东西都和哪个动词相关联，但从语义层面上看这个关联是什么，我们还需要新的模型。     汉语语言学家把事物与动词的语义关系分为了 17 种，叫做 17 种“语义角色”，它们是施事、感事、当事、动力、受事、结果、系事、工具、材料、方式、内容、与事、对象、场所、目标、起点、时间。你可以看到，语义角色的划分非常详细。同样是动作的发出者，施事指的是真正意义上的发出动作，比如“他吃饭”中的“他”；感事则是指某种感知活动的经验者，比如“他知道这件事了”中的“他”；当事则是指性质状态的主体，比如“他病了”中的“他”；动力则是自然力量的发出者，比如“洪水淹没了村庄”中的“洪水”。语义角色的具体划分以及 17 这个数目是有争议的，不过不管怎样，这个模型本身能够非常贴切地回答“什么是语义”这个问题。     汉语有一种“投射理论”，即一个句子的结构是由这个句子中的谓语投射出来的。给定一个动词后，这个动词能够带多少个语义角色，这几个语义角色都是什么，基本上都已经确定了。因而，完整的句子所应有的结构实际上也就已经确定了。比如，说到“休息”这个动词，你就会觉得它缺少一个施事，而且也不缺别的了。我们只会说“老王休息”，不会说“老王休息手”或者“老王休息沙发”。因而我们认为，“休息”只有一个“论元”。它的“论元结构”是：       休息 <施事>     因此，一旦在句子中看到“休息”这个词，我们就需要在句内或者句外寻找“休息”所需要的施事。这个过程有一个很帅的名字，叫做“配价”。“休息”就是一个典型的“一价动词”。我们平时接触的比较多的则是二价动词。不过，它们具体的论元有可能不一样：       吃 <施事，受事>       去 <施事，目标>       淹没 <动力，受事>     三价动词也是有的，例如       送 <施事，受事，与事>     甚至还有零价动词，例如       下雨 <Ф>     下面我们要教计算机做的，就是怎样给动词配价。之前，我们已经给出了解析句法结构的方法，这样计算机便能判断出每个动词究竟在和哪些词发生关系。语义分析的实质，就是确定出它们具体是什么关系。因此，语义识别的问题，也就转化为了“语义角色标注”的问题。然而，语义角色出现的位置并不固定，施事也能出现在动词后面，受事也能出现在动词前面，怎样让计算机识别语义角色呢？在回答这个问题之前，我们不妨问问自己：我们是怎么知道，“我吃完了”中的“我”是“吃”的施事，“苹果吃完了”中的“苹果”是“吃”的受事的呢？大家肯定会说，废话，“我”当然只能是“吃”的施事，因为我显然不会“被吃”；“苹果”当然只能是“吃”的受事，因为苹果显然不能发出“吃”动作。也就是说，“吃”的两个论元都有语义类的要求。我们把“吃”的论元结构写得更详细一些：       吃 <施事[语义类：人|动物]，受事[语义类：食物|药物]> 而“淹没”一词的论元结构则可以补充为：       淹没 <动力[语义类：自然事物]，受事[语义类：建筑物|空间]>     所以，为了完成计算机自动标注语义角色的任务，我们需要人肉建立两个庞大的数据库：语义类词典和论元结构词典。这样的人肉工程早就已经做过了。北京语言大学 1990 年 5 月启动的“九〇￼五语义工程”就是人工构建的一棵规模相当大的语义树。它把词语分成了事物、运动、时空、属性四大类，其中事物类分为事类和物类，物类又分为具体物和抽象物，具体物则再分为生物和非生物，生物之下则分了人类、动物、植物、微生物、生物构件五类，非生物之下则分了天然物、人工物、遗弃物、几何图形和非生物构件五类，其中人工物之下又包括设施物、运载物、器具物、原材料、耗散物、信息物、钱财七类。整棵语义树有 414 个结点，其中叶子结点 309 个，深度最大的地方达到了 9 层。论元结构方面则有清华大学和人民大学共同完成的《现代汉语述语动词机器词典》，词典中包括了各种动词的拼音、释义、分类、论元数、论元的语义角色、论元的语义限制等语法和语义信息。     说到语义工程，不得不提到董振东先生的知网。这是一个综合了语义分类和语义关系的知识库，不但通过语义树反映了词与词的共性，还通过语义关系反映了每个词的个性。它不但能告诉你“医生”和“病人”都是人，还告诉了你“医生”可以对“病人”发出一个“医治”的动作。知网的理念和 WordNet 工程很相似，后者是 Princeton 在 1985 年就已经开始构建的英文单词语义关系词典，背后也是一个语义关系网的概念，词与词的关系涉及同义词、反义词、上下位词、整体与部分、子集与超集、材料与成品等等。如果你装了 Mathematica，你可以通过 WordData 函数获取到 WordNet 的数据。至于前面说的那几个中文知识库嘛，别问我，我也不知道上哪儿取去。       看到这里，想必大家会欢呼，啊，这下子，在中文信息处理领域，从语法到语义都已经漂亮的解决了吧。其实并没有。上面的论元语义角色的模型有很多问题。其中一个很容易想到的就是隐喻的问题，比如“信息淹没了我”、“悲伤淹没了我”。一旦出现动词的新用法，我们只能更新论元结构：       淹没 <动力[语义类：自然事物|抽象事物]，受事[语义类：建筑物|空间|人类]>     但更麻烦的则是下面这些违背语义规则的情况。一个是否定句，比如“张三不可能吃思想”。一个是疑问句，比如“张三怎么可能吃思想”。更麻烦的就是超常现象。随便在新闻网站上一搜，你就会发现各种不符合语义规则的情形。我搜了一个“吃金属”，立即看到某新闻标题《法国一位老人以吃金属为生》。要想解决这些问题，需要给配价模型打上不少补丁。       然而，配价模型也仅仅解决了动词的语义问题。其他词呢？好在，我们也可以为名词发展一套类似的配价理论。我们通常认为“教师”是一个零价名词，而“老师”则是一个一价名词，因为说到“老师”时，我们通常会说“谁的老师”。“态度”则是一个二价的名词，因为我们通常要说“谁对谁的态度”才算完整。事实上，形容词也有配价，“优秀”就是一个一价形容词，“友好”则是一个二价形容词，原因也是类似的。配价理论还有很多更复杂的内容，这里我们就不再详说了。     但还有很多配价理论完全无法解决的问题。比如，语义有指向的问题。“砍光了”、“砍累了”、“砍钝了”、“砍快了”，都是动词后面跟形容词作补语，但实际意义各不相同。“砍光了”指的是“树砍光了”，“砍累了”指的是“人砍累了”，“砍钝了”指的是“斧子砍钝了”，“砍快了”指的是“砍砍快了”。看来，一个动词的每个论元不但有语义类的限制，还有“评价方式”的限制。     两个动词连用，也有语义关系的问题。“抓住不放”中，“抓住”和“不放”这两个动作构成一种反复的关系，抓住就等于不放。“说起来气人”中，“说起来”和“气人”这两个动作构成了一种条件关系，即每次发生了“说起来”这个事件后，都会产生“气人”这个结果。大家或许又会说，这两种情况真的有区别吗？是的，而且我能证明这一点。让我们造一个句子“留着没用”，你会发现它出现了歧义：既可以像“抓住不放”一样理解为反复关系，一直把它留着一直没有使用；又可以像“说起来气人”一样理解为条件关系，留着的话是不会有用的。因此，动词与动词连用确实会产生不同的语义关系，这需要另一套模型来处理。     虚词的语义更麻烦。别以为“了”就是表示完成，“这本书看了三天”表示这本书看完了，“这本书看了三天了”反而表示这本书没看完。“了”到底有多少个义项，现在也没有一个定论。副词也算虚词，副词的语义同样捉摸不定。比较“张三和李四结婚了”与“张三和李四都结婚了”，你会发现描述“都”字的语义没那么简单。       不过，在实际的产品应用中，前面所说的这些问题都不大。这篇文章中讲到的基本上都是基于规则的语言学处理方法。目前更实用的，则是对大规模真实语料的概率统计分析与机器学习算法，这条路子可以无视很多具体的语言学问题，并且效果也相当理想。最大熵模型和条件随机场都是目前非常常用的自然语言处理手段，感兴趣的朋友可以深入研究一下。但是，这些方法也有它们自己的缺点，就是它们的不可预测性。不管哪条路，似乎都离目标还有很远的一段距离。期待在未来的某一日，自然语言处理领域会迎来一套全新的语言模型，一举解决前面提到的所有难题。","title":"漫话中文自动分词和语义识别"},{"content":"泰为上海总公司 泰为信息科技(上海)有限公司是美国telenav,inc.公司(www.telenav.com,纳斯达克上市企业)在中国设立的全资子公司,作为基于手机的无线定位/实时导航应用领域的先驱,它已成功与世界各地众多无线运营商合作,提供多样性的gps(全球定位系统)增值服务。产品覆盖全球众多国家和地区,包括北美、南美/以及欧亚等多个国家。同时,telenav与全球众多的知名公司保持紧密的合作关系,合作伙伴包括sprint、nextel、at&t、verzion、rogers、vivo、rim、palm、navteq、tele atlas、sirf、摩托罗拉、诺基亚、三星、htc、中国移动、台湾远传电信等。 2010年度 5月13日,telenav,inc成功在美国纳斯达克上市。 泰为北京分公司 泰为北京研究中心成立于2006年2月,为追求产品的高效, 智能以及手机后端一体化, 我们立足于开发具有前瞻性的商用产品, 而不从事纯粹理论学术研究. 我们正在寻求有创业精神,编程能力强,有团队精神的人才加入我们的行列。 客户端研发部门:专注于移动/车载平台的应用开发,探索新的移动应用客户端的开发模式,比如跨平台开发,html5 等在移动应用开发上的应用,改善产品的用户体验,提高内容到客户的发布速度。 并且着力于研究车载终端与移动终端的无缝结合,提供更加优良的车内导航、搜索体验。 求路引擎提供导航应用中最基本、最核心的服务,泰为北京研发中心的求路引擎小组研发的业界领先的移动求路引擎提供精准的、基于实时路况/历史路况的高性能求路服务, 为公司开拓北美、欧洲、亚太市场提供坚实基础。在紧跟学术研究前沿的同时,我们也在开展 数据挖掘、ug-map等方面的研发工作,在完善引擎同时,通过加强对数据的理解与掌握,持续提升导航服务的质量。 地址信息的定位和楼堂馆所的查找,是导航应用的第一个步骤。因此,搜索引擎技术是导航领域不可或缺的一部分。北京lab的搜索引擎小组,面向欧美市场,结合手机的特点,为公司研发并维护垂直信息搜索引擎,以及附属的广告系统。同时,我们还在不断探索,力争将自然语言处理与机器学习等一系列人工智能技术,进一步应用到产品中,持续提高服务质量。 2009年度 crackberry评选telenav导航为“2009年度最佳汽车应用伴侣奖” telenav导航产品荣膺blackberry颁发的“2009年度最佳位置服务奖” abi research 评选telenav 为“2009年度世界领先网络版导航产品” telenav荣膺由全球性发展咨询公司frost & sullivan颁发的“2009 north american mobility award”奖项。 telenav与福特签订正式合作协议,全面进军车载导航产品。 telenav成为中国移动独家网络版合作伙伴。 2008年度 telenav荣获“硅谷成长最快公司奖” telenav 荣获“2008移动互联网领域最佳革新奖” abi research 评选telenav为offboard 手机导航第一。 telenav track作为最佳手持设备的mrm解决方案,荣膺由全球著名咨询公司frost & sullivan颁发的“2008 north american mobility award” - telenav track荣膺由全球性发展咨询公司frost & sullivan颁发的“2008 north american mobility award”奖项。 2008年6月1日中国移动telenav手机地图全国正式商用。 2007年度 telenav被美国idc评为2007年度最有潜力的10大无线应用公司之一 telenav荣获由frost & sullivan咨询公司颁发的2007north american mobility award for lbs navigation奖 telenav被评为fiercewireless年度“fierce 15”wireless companies15强企业之一 telenav被alwayson杂志评选为2007年的100强中小型企业(alwayson ao 100 top private companies)之一 2007年2月,荣获中国全球定位系统应用与位置服务产业发展导航论坛颁发的“2006中国通信市场创新企业”奖 2007年9月,telenav荣获“2007中国手机市场发展20周年庆典”之“中国手机市场发展杰出表现奖” 泰为公司成为中国移动集团手机地图/手机导航产品合作伙伴 请提供中、英文简历。 联 系 人:王小姐 地址:上海市长宁区仙霞路333号东方维京大厦10楼 发送到手机 邮政编码:200336 ====================================================== 在最后，我邀请大家参加新浪APP，就是新浪免费送大家的一个空间，支持PHP+MySql，免费二级域名，免费域名绑定 这个是我邀请的地址，您通过这个链接注册即为我的好友，并获赠云豆500个，价值5元哦！短网址是http://t.cn/SXOiLh我创建的小站每天访客已经达到2000+了，每天挂广告赚50+元哦，呵呵，饭钱不愁了，\\(^o^)/","title":"Java Engineer (Content System@Global Map Dept.)"},{"content":"假设有两个句子，我们想知道它们之间是否相关联： 第一个是：“乔布斯离我们而去了。” 第二个是：“苹果价格会不会降？” 如果由人来判断，我们一看就知道，这两个句子之间虽然没有任何公共词语，但仍然是很相关的。这是因为，虽然第二句中的“苹果”可能是指吃的苹果，但是由于第一句里面有了“乔布斯”，我们会很自然的把“苹果”理解为苹果公司的产品。事实上，这种文字语句之间的相关性、相似性问题，在搜索引擎算法中经常遇到。例如，一个用户输入了一个query，我们要从海量的网页库中找出和它最相关的结果。这里就涉及到如何衡量query和网页之间相似度的问题。对于这类问题，人是可以通过上下文语境来判断的。但是，机器可以么？ 在传统信息检索领域里，实际上已经有了很多衡量文档相似性的方法，比如经典的VSM模型。然而这些方法往往基于一个基本假设：文档之间重复的词语越多越可能相似。这一点在实际中并不尽然。很多时候相关程度取决于背后的语义联系，而非表面的词语重复。 那么，这种语义关系应该怎样度量呢？事实上在自然语言处理领域里已经有了很多从词、词组、句子、篇章角度进行衡量的方法。本文要介绍的是其中一个语义挖掘的利器：主题模型。 主题模型是什么？ 主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。还是上面的例子，“苹果”这个词的背后既包含是苹果公司这样一个主题，也包括了水果的主题。当我们和第一句进行比较时，苹果公司这个主题就和“乔布斯”所代表的主题匹配上了，因而我们认为它们是相关的。 在这里，我们先定义一下主题究竟是什么。主题就是一个概念、一个方面。它表现为一系列相关的词语。比如一个文章如果涉及到“百度”这个主题，那么“中文搜索”、“李彦宏”等词语就会以较高的频率出现，而如果涉及到“IBM”这个主题，那么“笔记本”等就会出现的很频繁。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布 。与主题关系越密切的词语，它的条件概率越大，反之则越小。 例如： 通俗来说，一个主题就好像一个“桶”，它装了若干出现概率较高的词语。这些词语和这个主题有很强的相关性，或者说，正是这些词语共同定义了这个主题。对于一段话来说，有些词语可以出自这个“桶”，有些可能来自那个“桶”，一段文本往往是若干个主题的杂合体。我们举个简单的例子，见下图。 以上是从互联网新闻中摘抄下来的一段话。我们划分了4个桶（主题），百度（红色），微软（紫色）、谷歌（蓝色）和市场（绿色）。段落中所包含的每个主题的词语用颜色标识出来了。从颜色分布上我们就可以看出，文字的大意是在讲百度和市场发展。在这里面，谷歌、微软这两个主题也出现了，但不是主要语义。值得注意的是，像“搜索引擎”这样的词语，在百度、微软、谷歌这三个主题上都是很可能出现的，可以认为一个词语放进了多个“桶”。当它在文字中出现的时候，这三个主题均有一定程度的体现。 有了主题的概念，我们不禁要问，究竟如何得到这些主题呢？对文章中的主题又是如何进行分析呢？这正是主题模型要解决的问题。下面我简要介绍一下主题模型是怎样工作的。 主题模型的工作原理 首先，我们用生成模型的视角来看文档和主题这两件事。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为： 上面这个式子，可以矩阵乘法来表示，如下图所示： 左边的矩阵表示每篇文章中每次词语出现的概率；中间的Φ矩阵表示的是每个主题中每个词语出现的概率 ，也就是每个“桶 表示的是每篇文档中各个主题出现的概率 ，可以理解为一段话中每个主题所占的比例。 假如我们有很多的文档，比如大量的网页，我们先对所有文档进行分词，得到一个词汇列表。这样每篇文档就可以表示为一个词语的集合。对于每个词语，我们可以用它在文档中出现的次数除以文档中词语的数目作为它在文档中出现的概率 。这样，对任意一篇文档，左边的矩阵是已知的，右边的两个矩阵未知。而主题模型就是用大量已知的“词语－文档”矩阵 ，通过一系列的训练，推理出右边的“词语－主题”矩阵Φ 和“主题文档”矩阵Θ 。 主题模型训练推理的方法主要有两种，一个是pLSA（Probabilistic Latent Semantic Analysis），另一个是LDA（Latent Dirichlet Allocation）。pLSA主要使用的是EM（期望最大化）算法；LDA采用的是Gibbs sampling方法。由于它们都较为复杂且篇幅有限，这里就只简要地介绍一下pLSA的思想，其他具体方法和公式，读者可以查阅相关资料。 pLSA采用的方法叫做EM（期望最大化）算法，它包含两个不断迭代的过程：E（期望）过程和M（最大化）过程。用一个形象的例子来说吧：比如说食堂的大师傅炒了一盘菜，要等分成两份给两个人吃，显然没有必要拿天平去一点点去精确称量，最简单的办法是先随意的把菜分到两个碗中，然后观察是否一样多，把比较多的那一份取出一点放到另一个碗中，这个过程一直重复下去，直到大家看不出两个碗里的菜有什么差别为止。 对于主题模型训练来说，“计算每个主题里的词语分布”和“计算训练文档中的主题分布”就好比是在往两个人碗里分饭。在E过程中，我们通过贝叶斯公式可以由“词语－主题”矩阵计算出“主题－文档”矩阵。在M过程中，我们再用“主题－文档”矩阵重新计算“词语－主题”矩阵。这个过程一直这样迭代下去。EM算法的神奇之处就在于它可以保证这个迭代过程是收敛的。也就是说，我们在反复迭代之后，就一定可以得到趋向于真实值的 Φ和 Θ。 如何使用主题模型？ 有了主题模型，我们该怎么使用它呢？它有什么优点呢？我总结了以下几点： 1）  它可以衡量文档之间的语义相似性。对于一篇文档，我们求出来的主题分布可以看作是对它的一个抽象表示。对于概率分布，我们可以通过一些距离公式（比如KL距离）来计算出两篇文档的语义距离，从而得到它们之间的相似度。 2)它可以解决多义词的问题。回想最开始的例子，“苹果”可能是水果，也可能指苹果公司。通过我们求出来的“词语－主题”概率分布，我们就可以知道“苹果”都属于哪些主题，就可以通过主题的匹配来计算它与其他文字之间的相似度。 3） 它可以排除文档中噪音的影响。一般来说，文档中的噪音往往处于次要主题中，我们可以把它们忽略掉，只保持文档中最主要的主题。 4） 它是无监督的，完全自动化的。我们只需要提供训练文档，它就可以自动训练出各种概率，无需任何人工标注过程。 5） 它是跟语言无关的。任何语言只要能够对它进行分词，就可以进行训练，得到它的主题分布。 综上所述，主题模型是一个能够挖掘语言背后隐含信息的利器。近些年来各大搜索引擎公司都已经开始重视这方面的研发工作。语义分析的技术正在逐步深入到搜索领域的各个产品中去。在不久的将来，我们的搜索将会变得更加智能，让我们拭目以待吧。","title":"搜索背后的奥秘——浅谈语义主题计算"},{"content":"一、什么是语言模型                  语言模型(Language Model)目的是建立一个能够尽可能客观的描述给定词序列在语言中出现的概率分布。           这句话读起来有点晕，其实就是我们认为在客观的人类语言中，词序列是以一定的概率出现的、有规律的，我们就是要建立一个能尽可能描述这个规律的模型，用其根据已出现的词序列来预测当前要出现的词(以史为鉴)。            语言模型最初用在语音识别领域，识别给定的语音信号对应的词序列(句子)，当前广泛用在分词、信息检索、输入法等NLP相关领域，可以说语言模型是以上这些应用的基石。            从工业角度讲，目前基本上只用一元模型(unigram)、二元模型(bigram)、三元模型(Trigram)，更高元模型的复杂度已经超出了实际应用可以处理的能力。            从学术角度讲，六元以上模型对实际应用的效果不会再有较大的影响(提高)。(没有查阅到相关文献，据说^-^)                      下面就详细说一下应用于实际系统的语言模型的建立过程。 二、建立语言模型的步骤           1、准备语料库与分词工具                        无论做什么产品都需要原材料，原材料的品质是产品质量的基石。语料库就是我们建立语言模型要用到                  的原材料，因此我们一定要保证语料库的品质。                        那么什么样的语料库算好语料呢？这个问题非常难答，因为要看语言模型的具体应用领域，总之，选用                  与应用领域一致的语料永远是一个不错的选择，否则，就让你的语料库尽可能的平衡，例如语料既有新                        闻、又有聊天等等，但是这个平衡(各种类型语料的配比)的掌握可能就要靠经验和实验了。                        有这样一句话“法律应该禁止对分词进行进一步的研究工作”。分词是自然语言处理最基本的工作，目前                  分词的准确率一般可以达到90%以上，但是近年来一直没有显著的提高，而自然语言处理还有更多的问题                    等待大家去做，因此有学者说了上面的玩笑话，其实我是很赞同的，利用有限的时间多做点有意思的事，                    扯远了。                         总之，分词的第一步是准备好适宜的语料库和一个不错的分词程序。                             2、利用分词工具切分语料，统计出ngram的出现次数                         这个最初看起来真的没啥说的，写一段程序，把语料扔进去切然后统计一下呗。如果你是做实验，没                   问题。如果是实际应用，那么你就悲剧了，动辄几个T的语料，搞不好要个把月的才能得到结果，也说不定                 中间停了次电，额~~~。我们在实际工作中主要有两种方案解决这个大语料处理问题：                         a、朴素的想法，尽可能写出高质量的程序、把语料切分很多块，然后把程序和语料块分发到多台机器                上跑，等所有机器跑完以后，把各台机器上的局部统计结果放到一台机器上做最后的全局统计即可。其实很                 多大公司也都是这么干的(真的^-^)，直到在下面这个牛逼的玩意出现。                         b、MapReduce，这个东西我只是用过，不是很懂，但是知道它确实好用。程序员只需要在map中                 添加将语料切分成词的代码，在reduce中对结果进行统计合并即可，其它诸如文件分发等工作都由                             MapReduce框架帮忙调度和处理，其实就是这套框架帮忙自动化处理了a中那些手工工作，在信息量越来越                大的今天，这套框架的应用越来越多。                        一般情况下，我们只统计一元、二元、三元语言模型。                     3、对统计出来的ngram数据进行裁剪得到最终的语言模型                        经过第二步以后，我们已经得到了原始的语言模型。但是一般来讲，因为其条目太多，因此占用空间                 过大无法被应用程序装入内存，也就不能直接应用到实际产品当中，因此我们需要裁剪掉一些条目，对我                   们的语言的模型进行瘦身。                       裁剪的方法是规则与统计相结合，使语言模型的最终大小达到应用可处理的标准。裁剪始终要遵循的                   思路是始终剪切掉那些承载信息量少的条目。                       在实际的工作中，我们一般先利用规则进行裁剪，然后再利用算法进行裁剪，最终获得有使用价值的语               言模型，下面详细描述处理过程。                       为了方便描述，我们引入几个变量，FinalSize(最终语言模型的条目数)、LoadSize(可加载到内存的最                   大条目数)、KeepSize(裁剪过程中当前的条目数)。                       基于规则进行裁剪。在读入第二步的原始语言模型的统计数据时，我们直接剪切掉低频词(阈值我们自                 己设定)，当我们读完统计数据时的到的条目数量就是KeepSize，如果KeepSize小于FinalSize，那么我们只                 需把内存中的条目都输出的磁盘就得到了语言模型，否则就要接着利用算法进行进一步裁剪。                       利用算法进行裁剪。初始时我们把KeepSize设置为LoadSize，然后利用算法不断计算所有条目承载的                 信息量并按其进行排序，裁剪掉排名在KeepSize之后的条目，然后把KeepSize折半，不断的重复上面的裁剪               动作，直到KeepSize小于FinalSize停止，把内存中的条目Flush出来就是最终的语言模型了(具体算法大家可               以参见一些开源的工具包)。                 ","title":"建立语言模型"},{"content":"test 2006 年百度之星程序设计大赛初赛题目 5 座位调整 题目描述： 百度办公区里到处摆放着各种各样的零食。百度人力资源部的调研发现，员工如果可以在自己喜欢的美食旁边工作，工作效率会大大提高。因此，百度决定进行一次员工座位的大调整。 调整的方法如下： 1 ． 首先将办公区按照各种零食的摆放分成 N 个不同的区域。（例如：可乐区，饼干区，牛奶区等等）。 2 ． 每个员工对不同的零食区域有不同的喜好程度（喜好程度度的范围为 1 — 100 的整数， 喜好程度越大表示该员工越希望被调整到相应的零食区域）。 3 ． 由于每个零食区域可以容纳的员工数量有限，人力资源部希望找到一个最优的调整方案令到总的喜好程度最大。 数据输入： 第一行包含两个整数 N ， M ，（ 1<=N ， M<=300 ）。分别表示 N 个区域和 M 个员工。 第二行是 N 个整数构成的数列 a ，其中 a[i] 表示第 i 个区域可以容纳的员工数， (1<=a[i]<=M ， a[1]+a[2]+..+a[N]=M) 。 紧接着是一个 M*N 的矩阵 P ， P （ i ， j ）表示第 i 个员工对第 j 个区域的喜好度。 答案输出： 对于每个测试数据，输出可以达到的最大的喜好程度。 输入样例 3 3 1 1 1 100 50 25 100 50 25 100 50 25 输出样例 175 数据解释：此数据只存在一种安排方法，三个员工分别安置在三个区域。最终的喜好程度为 100+50+25=175 //     int numArea = 0; // >=1     int numEmployee = 0; //<= 300          printf(\"input number of areas:\\n\");     scanf(\"%d\",&numArea);     while( numArea <1 )     {         printf(\"error,input again\\n\");         scanf(\"%d\",&numArea);     }     printf(\"input number of employee:\\n\");     scanf(\"%d\",&numEmployee);     while( numEmployee > 300 || numEmployee <= 0 )     {         printf(\"error,input again\\n\");         scanf(\"%d\",&numEmployee);     }     int i = 0,j = 0;     int * EmployeeOfEachArea = malloc(sizeof(int)*numArea);     memset(EmployeeOfEachArea,0,sizeof(int)*numArea);     printf(\"input each area max nunmer of employee : numEmployee = %d numArea = %d\\n\",numEmployee,numArea);     for( i = 0; i < numArea; i++ )     {         scanf(\"%d\",&EmployeeOfEachArea[i]);     }         int ** SatisfactionOfEmployee = NULL;     SatisfactionOfEmployee = malloc(sizeof(int)*numArea);          for( i = 0; i < numArea; i++ )     {         SatisfactionOfEmployee[i] = malloc(sizeof(int)*numEmployee);     }     for( i = 0; i < numArea; i++ )     {         for( j = 0; j < numEmployee; j++ )         {             SatisfactionOfEmployee[i][j] = i*j;             printf(\"%d \",SatisfactionOfEmployee[i][j]);         }         printf(\"\\n\");     }                    // 首先计算M个员工的全排列，     //然后把他们分别放入已经准备好的区域中；     //最后计算总的满意度的值     system(\"PAUSE\");     return 0; 母牛生小牛 Problem 设有一头小母牛，从出生第四年起每年生一头小母牛，按此规律，第N年时有几头母牛？ Input 本题有多组数据。每组数据只有一个整数N，独占一行。(1≤N≤50) Output 对每组数据，输出一个整数（独占一行）表示第N年时母牛的数量 Sample Input 1 4 5 20 Sample Output 1 2 3 872 int brow(int year) {     int num = 1;     int i;     if( year > 3 )     {         for( i = 1; i < year - 2; i++ )         {             num += brow(i);         }     }     return num; }     int a[51];     int i;     int N = 0;     a[1] = a[2] = a[3] = 1;     a[4] = 2;          for( i = 5; i <= 50; i++ )     {         a[i] = a[i-1] + a[i-3];         printf(\"%d\\n\",a[i]);     }          scanf(\"%d\",&N);     printf(\"%d\",a[N]); http://www.programfan.com/acm/ 2006 年百度之星程序设计大赛初赛题目 6 百度语言翻译机 时限 1s 百度的工程师们是非常注重效率的，在长期的开发与测试过程中，他们逐渐创造了一套他们独特的缩率语。他们在平时的交谈，会议，甚至在各中技术文档中都会大量运用。 为了让新员工可以更快地适应百度的文化，更好地阅读公司的技术文档，人力资源部决定开发一套专用的翻译系统，把相关文档中的缩率语和专有名词翻译成日常语言。 输入数据： 输入数据包含三部分 1. 第一行包含一个整数 N （ N<=10000 ），表示总共有多少个缩率语的词条。 2. 紧接着有 N 行的输入，每行包含两个字符串，以空格隔开。第一个字符串为缩率语（仅包含大写英文字符，长度不超过 10 ），第二个字符串为日常语言（不包含空格，长度不超过 255 ） . 3. 从第 N+2 开始到输入结束为包含缩略语的相关文档。（总长度不超过 1000000 个字符） 输出数据： 输出将缩率语转换成日常语言的文档。（将缩率语转换成日常语言，其他字符保留原样） 输入样例 6 PS 门户搜索部 NLP 自然语言处理 PM 产品市场部 HR 人力资源部 PMD 产品推广部 MD 市场发展部 百度的部门包括 PS ， PM ， HR ， PMD ， MD 等等，其中 PS 还包括 NLP 小组。 输出样例 百度的部门包括门户搜索部，产品市场部，人力资源部，产品推广部，市场发展部等等，其中门户搜索部还包括自然语言处理小组。 注意： 1 ． 输入数据中是中英文混合的，中文采用 GBK 编码。 2 ． 为保证答案的唯一性，缩率语的转换采用正向最大匹配（从左到右为正方向）的原则。请注意输入例子中 PMD 的翻译。     int numWords = 0;     printf(\"input number of words :\\n\"); //    scanf(\"%d\",&numWords);     numWords = 6;     Word * word = malloc(sizeof(Word)*numWords);     memset(word,0,sizeof(Word)*numWords);     int i;     printf(\"input words :\\n\");     for( i = 0; i < numWords; i++ )     {     //    scanf(\"%s %s\",&(word[i].a),&(word[i].b));     }     strcpy(word[0].a,\"PS\");     strcpy(word[0].b,\"门户搜索部\");     strcpy(word[1].a,\"NLP\");     strcpy(word[1].b,\"自然语言处理\");     strcpy(word[2].a,\"PM\");     strcpy(word[2].b,\"产品市场部\");     strcpy(word[3].a,\"HR\");     strcpy(word[3].b,\"人力资源部\");     strcpy(word[4].a,\"PMD\");     strcpy(word[4].b,\"产品推广部\");     strcpy(word[5].a,\"MD\");     strcpy(word[5].b,\"市场发展部\");     for( i = 0; i < numWords; i++ )     {         printf(\"%s %s\\n\",word[i].a,word[i].b);     }     printf(\"\\n\\n\\n\");     char * ddd = \"百度的部门包括PS，PM，HR，PMD，MD等等，其中PS还包括NLP小组。\";     int length = strlen(ddd) + 1;     char * str = malloc(length * sizeof(char));     strcpy(str,ddd);     int j = 0;     char * t = malloc(sizeof(char)*4);     memset(t,0,sizeof(char)*4);     for( i = 0; i < length - 1; i++ )     {         if( str[i] >= 'A' && str[i] <= 'Z' )         {         //    printf(\"%c \",str[i]);                 t[j] = str[i];                 j++;         }         else         {             if( j != 0 )             {                 int k = 0;                 for( k = 0; k < 6; k++ )                 {                     if(strcmp(t,word[k].a))                     {                         printf(\"%s\\n\",word[k].b);                     }                     }                              }             j = 0;         }     }","title":"test"},{"content":"  By 何明桂（http://blog.csdn.net/hmg25） 转载请注明出处 Iphone4S的Siri让人眼前一亮，网上出现了无数调戏Siri的视频。真是让android用户们心痒不已。好在随后android阵营中的高手迅速反击，推出了Iris。悲剧的是Iris仅支持英文，让我们这些英语烂的无比的人调戏Iris不成，反被它给调戏了。真是郁闷的不行啊~_~ 所以我打算使用android的资源自己打造一个中文版的Siri，让我们用中文随意的来调戏它。(我自己做了一个简单的，哈哈，放在优亿市场里，有兴趣的童鞋可以去体验下http://www.eoemarket.com/apps/61634)    首先，我们来分析Siri的构成，应该大致可以分为3个组成部份：语音识别、自然语言处理、语音输出。对于语音识别，我们可以使用google的语音识别API进行语音的识别，讲语音转成文字。语音输出，其实就是使用TTS，讲文字进行语音合成播放出来，这个android也是有接口可以利用的。真正核心的是自然语言识别处理这个部分，Siri功能的好坏判断很大一部分是取决于此的，这需要很大一个数据库来维持运转，在本地是无法实现的，即使iphone的Siri也是讲语音识别的指令语音上传到Apple的服务器上去解析后返回。由于apple的接口不开放，所以我们无法使用他们的接口，好在世界上拥有这样服务器的不止苹果一家，android上的Iris利用的就是http://start.csail.mit.edu/（自然语音问答系统）这个网站提供的接口以及一个叫做cleverbot的一款智能聊天平台http://www.cleverbot.com/这个聊天网站是支持汉语的，不过，只是支持拼音输入——汗啊。    所以我们的核心任务就是寻找一个支持中文汉字输入的问答系统。经过在网络上长时间的搜索，结果发现——很遗憾，没有找到(PS:如果有谁找到了比较好的网址，麻烦共享，告诉我一声)，不过对于我们调戏Siri的这个需求，我找到了一个较好的替代品——聊天机器人.http://www.xiaoi.com/widget/1007/小i智能聊天机器人。    经过短时间的摸索，我实现了一个类来，初始化连接小i机器人的接口，发送数据以及接受反馈。用到的接口地址如下： view plaincopy to clipboardprint?    private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\";   private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";   private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";   private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\"; private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\"; private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";       http连接上边的Webbot_Path，会反馈回来一些数据： view plaincopy to clipboardprint? var L_IDS_SEND_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";   var L_IDS_RECV_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";   var L_IDS_GET_RESOURCE_URL = \"http://122.226.240.164/engine/widget1007/getres.do\";   var __sessionId = \"86491993134658194\";   document.write(\"<script src='http://122.226.240.164/engine/widget1007/_core.js?encoding=utf-8&'><\\/script>\");   var L_IDS_SEND_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\"; var L_IDS_RECV_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\"; var L_IDS_GET_RESOURCE_URL = \"http://122.226.240.164/engine/widget1007/getres.do\"; var __sessionId = \"86491993134658194\"; document.write(\"<script src='http://122.226.240.164/engine/widget1007/_core.js?encoding=utf-8&'><\\/script>\");       反馈回来的数据包 括上边的发送和接收地址，以及一个sessionId，这个sessionId很重要，类似于一个key，用于后边的会话中。由于发送和接收地址是固定的，可以直接写死，但是sessionId是变动的，所以首先需要将它从反馈回来的茫茫数据中提取出来，我使用的是一个简单的正则表达式： view plaincopy to clipboardprint?        String strResult = EntityUtils.toString(httpResponse.getEntity());   Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId    Matcher m = p.matcher(strResult);   if (m.find())   mSessionId = m.group(1);   String strResult = EntityUtils.toString(httpResponse.getEntity()); Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId Matcher m = p.matcher(strResult); if (m.find()) mSessionId = m.group(1);     得到sessionId后，我们就可以进行初始化了，初始化的过程很简单，将sessionId将填入下边格式中，发送到服务器去就行了。 view plaincopy to clipboardprint? String   strSendJoin = Send_Path+ \"SID=\"+ mSessionId+\"&USR=\"+ mSessionId+ \"&CMD=JOIN&r=\";   String strSendJoin = Send_Path+ \"SID=\"+ mSessionId+\"&USR=\"+ mSessionId+ \"&CMD=JOIN&r=\"; 初始化完成后，就可以使用下边的格式网址发送问题以及接收答案： String strSend = Send_Path + \"SID=\" + mSessionId + \"&USR=\"+ mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg +\"&FTN=&FTS=&FTC=&r=\"; String strRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\"+mSessionId + \"&r=\";xiaoi.sendMsg(mQuestion); results = xiaoi.revMsg(); 接收到的内容也是需要提取的，使用的是正则表达式： view plaincopy to clipboardprint?  String  msgTmp = EntityUtils.toString(httpResponse.getEntity());   Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\");   Matcher m = p.matcher(msgTmp);    (m.find()) {    msg = m.group(1);   String msgTmp = EntityUtils.toString(httpResponse.getEntity()); Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\"); Matcher m = p.matcher(msgTmp); if (m.find()) { msg = m.group(1); }     通过上述的小i聊天机器人的接口，你便可以实现一个简单的，可以自由聊天对话的Siri。小I机器人还是很智能的，聊天的对话也很有意思，但是仅仅只能聊天，这个和iphone Siri的差距太大了，所以稍后我们将给它添加另外一个智能的大脑。 本文完整代码如下： view plaincopy to clipboardprint? public class XiaoI {          private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\";       private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";       private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";          private String mSessionId = null;       private HttpClient httpClient = null;          public boolean initialize() {           boolean success=false;             HttpParams httpParams = new BasicHttpParams();           HttpConnectionParams.setConnectionTimeout(httpParams, 30000);           HttpConnectionParams.setSoTimeout(httpParams, 30000);           httpClient = new DefaultHttpClient(httpParams);           try {               String strGetId = Webbot_Path;               HttpGet httpRequest = new HttpGet(strGetId);               HttpResponse httpResponse = httpClient.execute(httpRequest);               if (httpResponse.getStatusLine().getStatusCode() == HttpURLConnection.HTTP_OK) {                   String strResult = EntityUtils.toString(httpResponse                           .getEntity());                   Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId                    Matcher m = p.matcher(strResult);                   if (m.find()) {                       mSessionId = m.group(1);                       String strSendJoin = Send_Path + \"SID=\" + mSessionId                               + \"&USR=\" + mSessionId + \"&CMD=JOIN&r=\";                       HttpGet httpRequest1 = new HttpGet(strSendJoin);                       httpResponse = httpClient.execute(httpRequest1);                          String strRevAsk = Recv_Path + \"SID=\" + mSessionId                               + \"&USR=\" + mSessionId + \"&r=\";                       HttpGet httpRequest2 = new HttpGet(strRevAsk);                       httpResponse = httpClient.execute(httpRequest2);                       success=true;                   }               }           } catch (ClientProtocolException e) {               e.printStackTrace();           } catch (IOException e) {               e.printStackTrace();           } catch (Exception e) {               e.printStackTrace();           }finally{               return success;           }       }          public void sendMsg(String msg) {           String strTalksend = Send_Path + \"SID=\" + mSessionId + \"&USR=\"                   + mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg                   + \"&FTN=&FTS=&FTC=&r=\";           HttpGet httpRequest = new HttpGet(strTalksend);           try {               httpClient.execute(httpRequest);           } catch (ClientProtocolException e) {               // TODO Auto-generated catch block                e.printStackTrace();           } catch (IOException e) {               // TODO Auto-generated catch block                e.printStackTrace();           }       }          public String revMsg() {           String strTalkRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\"                   + mSessionId + \"&r=\";           HttpGet httpRequest = new HttpGet(strTalkRec);           String msg = null;           try {               HttpResponse httpResponse = httpClient.execute(httpRequest);               if (httpResponse.getStatusLine().getStatusCode() == 200) {                   String msgTmp = EntityUtils.toString(httpResponse.getEntity());                   Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\");                   Matcher m = p.matcher(msgTmp);                   if (m.find()) {                       msg = m.group(1);                   }               }           } catch (ClientProtocolException e) {               // TODO Auto-generated catch block                e.printStackTrace();           } catch (IOException e) {               // TODO Auto-generated catch block                e.printStackTrace();           }           return msg;          }   }   public class XiaoI { private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\"; private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\"; private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\"; private String mSessionId = null; private HttpClient httpClient = null; public boolean initialize() { boolean success=false; HttpParams httpParams = new BasicHttpParams(); HttpConnectionParams.setConnectionTimeout(httpParams, 30000); HttpConnectionParams.setSoTimeout(httpParams, 30000); httpClient = new DefaultHttpClient(httpParams); try { String strGetId = Webbot_Path; HttpGet httpRequest = new HttpGet(strGetId); HttpResponse httpResponse = httpClient.execute(httpRequest); if (httpResponse.getStatusLine().getStatusCode() == HttpURLConnection.HTTP_OK) { String strResult = EntityUtils.toString(httpResponse .getEntity()); Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId Matcher m = p.matcher(strResult); if (m.find()) { mSessionId = m.group(1); String strSendJoin = Send_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&CMD=JOIN&r=\"; HttpGet httpRequest1 = new HttpGet(strSendJoin); httpResponse = httpClient.execute(httpRequest1); String strRevAsk = Recv_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&r=\"; HttpGet httpRequest2 = new HttpGet(strRevAsk); httpResponse = httpClient.execute(httpRequest2); success=true; } } } catch (ClientProtocolException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } catch (Exception e) { e.printStackTrace(); }finally{ return success; } } public void sendMsg(String msg) { String strTalksend = Send_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg + \"&FTN=&FTS=&FTC=&r=\"; HttpGet httpRequest = new HttpGet(strTalksend); try { httpClient.execute(httpRequest); } catch (ClientProtocolException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } } public String revMsg() { String strTalkRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&r=\"; HttpGet httpRequest = new HttpGet(strTalkRec); String msg = null; try { HttpResponse httpResponse = httpClient.execute(httpRequest); if (httpResponse.getStatusLine().getStatusCode() == 200) { String msgTmp = EntityUtils.toString(httpResponse.getEntity()); Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\"); Matcher m = p.matcher(msgTmp); if (m.find()) { msg = m.group(1); } } } catch (ClientProtocolException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } return msg; } } 使用方法：XiaoI xiaoi = new XiaoI();                 xiaoi.initialize();                xiaoi.sendMsg(mQuestion);        results = xiaoi.revMsg(); 由于发送接收耗时较多，最好放后台处理。 By 何明桂（http://blog.csdn.net/hmg25） 转载请注明出处 Iphone4S的Siri让人眼前一亮，网上出现了无数调戏Siri的视频。真是让android用户们心痒不已。好在随后android阵营中的高手迅速反击，推出了Iris。悲剧的是Iris仅支持英文，让我们这些英语烂的无比的人调戏Iris不成，反被它给调戏了。真是郁闷的不行啊~_~ 所以我打算使用android的资源自己打造一个中文版的Siri，让我们用中文随意的来调戏它。(我自己做了一个简单的，哈哈，放在优亿市场里，有兴趣的童鞋可以去体验下http://www.eoemarket.com/apps/61634)    首先，我们来分析Siri的构成，应该大致可以分为3个组成部份：语音识别、自然语言处理、语音输出。对于语音识别，我们可以使用google的语音识别API进行语音的识别，讲语音转成文字。语音输出，其实就是使用TTS，讲文字进行语音合成播放出来，这个android也是有接口可以利用的。真正核心的是自然语言识别处理这个部分，Siri功能的好坏判断很大一部分是取决于此的，这需要很大一个数据库来维持运转，在本地是无法实现的，即使iphone的Siri也是讲语音识别的指令语音上传到Apple的服务器上去解析后返回。由于apple的接口不开放，所以我们无法使用他们的接口，好在世界上拥有这样服务器的不止苹果一家，android上的Iris利用的就是http://start.csail.mit.edu/（自然语音问答系统）这个网站提供的接口以及一个叫做cleverbot的一款智能聊天平台http://www.cleverbot.com/这个聊天网站是支持汉语的，不过，只是支持拼音输入——汗啊。    所以我们的核心任务就是寻找一个支持中文汉字输入的问答系统。经过在网络上长时间的搜索，结果发现——很遗憾，没有找到(PS:如果有谁找到了比较好的网址，麻烦共享，告诉我一声)，不过对于我们调戏Siri的这个需求，我找到了一个较好的替代品——聊天机器人.http://www.xiaoi.com/widget/1007/小i智能聊天机器人。    经过短时间的摸索，我实现了一个类来，初始化连接小i机器人的接口，发送数据以及接受反馈。用到的接口地址如下： view plaincopy to clipboardprint?    private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\";   private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";   private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";   private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\"; private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\"; private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";       http连接上边的Webbot_Path，会反馈回来一些数据： view plaincopy to clipboardprint? var L_IDS_SEND_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";   var L_IDS_RECV_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";   var L_IDS_GET_RESOURCE_URL = \"http://122.226.240.164/engine/widget1007/getres.do\";   var __sessionId = \"86491993134658194\";   document.write(\"<script src='http://122.226.240.164/engine/widget1007/_core.js?encoding=utf-8&'><\\/script>\");   var L_IDS_SEND_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\"; var L_IDS_RECV_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\"; var L_IDS_GET_RESOURCE_URL = \"http://122.226.240.164/engine/widget1007/getres.do\"; var __sessionId = \"86491993134658194\"; document.write(\"<script src='http://122.226.240.164/engine/widget1007/_core.js?encoding=utf-8&'><\\/script>\");       反馈回来的数据包 括上边的发送和接收地址，以及一个sessionId，这个sessionId很重要，类似于一个key，用于后边的会话中。由于发送和接收地址是固定的，可以直接写死，但是sessionId是变动的，所以首先需要将它从反馈回来的茫茫数据中提取出来，我使用的是一个简单的正则表达式： view plaincopy to clipboardprint?        String strResult = EntityUtils.toString(httpResponse.getEntity());   Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId    Matcher m = p.matcher(strResult);   if (m.find())   mSessionId = m.group(1);   String strResult = EntityUtils.toString(httpResponse.getEntity()); Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId Matcher m = p.matcher(strResult); if (m.find()) mSessionId = m.group(1);     得到sessionId后，我们就可以进行初始化了，初始化的过程很简单，将sessionId将填入下边格式中，发送到服务器去就行了。 view plaincopy to clipboardprint? String   strSendJoin = Send_Path+ \"SID=\"+ mSessionId+\"&USR=\"+ mSessionId+ \"&CMD=JOIN&r=\";   String strSendJoin = Send_Path+ \"SID=\"+ mSessionId+\"&USR=\"+ mSessionId+ \"&CMD=JOIN&r=\"; 初始化完成后，就可以使用下边的格式网址发送问题以及接收答案： String strSend = Send_Path + \"SID=\" + mSessionId + \"&USR=\"+ mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg +\"&FTN=&FTS=&FTC=&r=\"; String strRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\"+mSessionId + \"&r=\";xiaoi.sendMsg(mQuestion); results = xiaoi.revMsg(); 接收到的内容也是需要提取的，使用的是正则表达式： view plaincopy to clipboardprint?  String  msgTmp = EntityUtils.toString(httpResponse.getEntity());   Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\");   Matcher m = p.matcher(msgTmp);    (m.find()) {    msg = m.group(1);   String msgTmp = EntityUtils.toString(httpResponse.getEntity()); Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\"); Matcher m = p.matcher(msgTmp); if (m.find()) { msg = m.group(1); }     通过上述的小i聊天机器人的接口，你便可以实现一个简单的，可以自由聊天对话的Siri。小I机器人还是很智能的，聊天的对话也很有意思，但是仅仅只能聊天，这个和iphone Siri的差距太大了，所以稍后我们将给它添加另外一个智能的大脑。 本文完整代码如下： view plaincopy to clipboardprint? public class XiaoI {          private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\";       private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";       private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";          private String mSessionId = null;       private HttpClient httpClient = null;          public boolean initialize() {           boolean success=false;             HttpParams httpParams = new BasicHttpParams();           HttpConnectionParams.setConnectionTimeout(httpParams, 30000);           HttpConnectionParams.setSoTimeout(httpParams, 30000);           httpClient = new DefaultHttpClient(httpParams);           try {               String strGetId = Webbot_Path;               HttpGet httpRequest = new HttpGet(strGetId);               HttpResponse httpResponse = httpClient.execute(httpRequest);               if (httpResponse.getStatusLine().getStatusCode() == HttpURLConnection.HTTP_OK) {                   String strResult = EntityUtils.toString(httpResponse                           .getEntity());                   Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId                    Matcher m = p.matcher(strResult);                   if (m.find()) {                       mSessionId = m.group(1);                       String strSendJoin = Send_Path + \"SID=\" + mSessionId                               + \"&USR=\" + mSessionId + \"&CMD=JOIN&r=\";                       HttpGet httpRequest1 = new HttpGet(strSendJoin);                       httpResponse = httpClient.execute(httpRequest1);                          String strRevAsk = Recv_Path + \"SID=\" + mSessionId                               + \"&USR=\" + mSessionId + \"&r=\";                       HttpGet httpRequest2 = new HttpGet(strRevAsk);                       httpResponse = httpClient.execute(httpRequest2);                       success=true;                   }               }           } catch (ClientProtocolException e) {               e.printStackTrace();           } catch (IOException e) {               e.printStackTrace();           } catch (Exception e) {               e.printStackTrace();           }finally{               return success;           }       }          public void sendMsg(String msg) {           String strTalksend = Send_Path + \"SID=\" + mSessionId + \"&USR=\"                   + mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg                   + \"&FTN=&FTS=&FTC=&r=\";           HttpGet httpRequest = new HttpGet(strTalksend);           try {               httpClient.execute(httpRequest);           } catch (ClientProtocolException e) {               // TODO Auto-generated catch block                e.printStackTrace();           } catch (IOException e) {               // TODO Auto-generated catch block                e.printStackTrace();           }       }          public String revMsg() {           String strTalkRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\"                   + mSessionId + \"&r=\";           HttpGet httpRequest = new HttpGet(strTalkRec);           String msg = null;           try {               HttpResponse httpResponse = httpClient.execute(httpRequest);               if (httpResponse.getStatusLine().getStatusCode() == 200) {                   String msgTmp = EntityUtils.toString(httpResponse.getEntity());                   Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\");                   Matcher m = p.matcher(msgTmp);                   if (m.find()) {                       msg = m.group(1);                   }               }           } catch (ClientProtocolException e) {               // TODO Auto-generated catch block                e.printStackTrace();           } catch (IOException e) {               // TODO Auto-generated catch block                e.printStackTrace();           }           return msg;          }   }   public class XiaoI { private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\"; private String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\"; private String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\"; private String mSessionId = null; private HttpClient httpClient = null; public boolean initialize() { boolean success=false; HttpParams httpParams = new BasicHttpParams(); HttpConnectionParams.setConnectionTimeout(httpParams, 30000); HttpConnectionParams.setSoTimeout(httpParams, 30000); httpClient = new DefaultHttpClient(httpParams); try { String strGetId = Webbot_Path; HttpGet httpRequest = new HttpGet(strGetId); HttpResponse httpResponse = httpClient.execute(httpRequest); if (httpResponse.getStatusLine().getStatusCode() == HttpURLConnection.HTTP_OK) { String strResult = EntityUtils.toString(httpResponse .getEntity()); Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId Matcher m = p.matcher(strResult); if (m.find()) { mSessionId = m.group(1); String strSendJoin = Send_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&CMD=JOIN&r=\"; HttpGet httpRequest1 = new HttpGet(strSendJoin); httpResponse = httpClient.execute(httpRequest1); String strRevAsk = Recv_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&r=\"; HttpGet httpRequest2 = new HttpGet(strRevAsk); httpResponse = httpClient.execute(httpRequest2); success=true; } } } catch (ClientProtocolException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } catch (Exception e) { e.printStackTrace(); }finally{ return success; } } public void sendMsg(String msg) { String strTalksend = Send_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg + \"&FTN=&FTS=&FTC=&r=\"; HttpGet httpRequest = new HttpGet(strTalksend); try { httpClient.execute(httpRequest); } catch (ClientProtocolException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } } public String revMsg() { String strTalkRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\" + mSessionId + \"&r=\"; HttpGet httpRequest = new HttpGet(strTalkRec); String msg = null; try { HttpResponse httpResponse = httpClient.execute(httpRequest); if (httpResponse.getStatusLine().getStatusCode() == 200) { String msgTmp = EntityUtils.toString(httpResponse.getEntity()); Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\"); Matcher m = p.matcher(msgTmp); if (m.find()) { msg = m.group(1); } } } catch (ClientProtocolException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } return msg; } } 使用方法：XiaoI xiaoi = new XiaoI();                 xiaoi.initialize();                xiaoi.sendMsg(mQuestion);        results = xiaoi.revMsg(); 由于发送接收耗时较多，最好放后台处理。","title":"打造Android的中文Siri语音助手(一)——小I机器人的接口"},{"content":"基于向量空间模型的文本聚类算法 [日期：2009-07-27] 来源：  作者： [字体：大 中 小] 姚清耘，刘功申，李翔 ( 上海交通大学信息安全工程学院，上海 200240)   摘要： 文本聚类是聚类的一个重要研究分支，是聚类方法在文本处理领域的应用。该文探讨了基于向量空间模型的文本聚类方法，提出 了一种文本聚类的改进算法—— LP 算法。同时，基于语料库的实际聚类效果，就维度确定、特征选择等方面提出优化方案。实验证明， LP 算法有效地减少了聚类所消耗的时间，实用性和灵活性都较高。 关键词： 向量空间模型；文本聚类；语料库 VSM–based Text Clustering Algorithm YAO Qing-yun,LIU Gong-shen,LI Xiang (School of Information Security Engineering,Shanghai Jiaotong University,Shanghai 200240) 【 Abstract 】 Text clustering,one of the most important research braches of clustering,is the application of clustering algorithm in text processing. This paper discusses different Vector Space Model(VSM)-based clustering algorithms and presents an improved text clustering algorithm——Level-Panel(LP)algorithm.In addition,according to the effects of clustering for the corpus,it presents optimizations of clusteringalgorithm, including dimension determining,feature selection,etc.It is proved that LP algorithm can effectively reduce the time spending in clustering process.It is high in practicability and flexibility. 【 Key words 】 Vector Space Model(VSM);text clustering;corpus 1 文本聚类研究现状 Internet 已经发展为当今世界上最大的信息库和全球范围内传播信息最主要的渠道。随着 Internet 的大规模普及和企业信息化程度的提高，各种资源呈爆炸式增长。在中国互联网络信息中心 (CNNIC)2007 年 1 月最新公布的中国互联网络发展状况统计报告中显示， 70.2% 的网络信息均以文本形式体现。对于这种半结构或无结构化数据，如何从中获取特定内容的信息和知识成为摆在人们面前的一道难题。近年来，文本挖掘、信息过滤和信息检索等方面的研究出现了前所未有的高潮。 作为一种无监督的机器学习方法，聚类技术可以将大量文本信息组成少数有意义的簇，并提供导航或浏览机制。 文本聚类的主要应用点包括： (1) 文本聚类可以作为多文档自动文摘等自然语言处理应用的预处理步骤。其中比较典型的例子是哥伦比亚大学开发的多文档自动文摘系统 Newsblaster[1] 。该系统将新闻进行 聚类处理，并对同主题文档进行冗余消除、信息融合、文本生成等处理，从而生成一篇简明扼要的摘要文档。 (2) 对搜索引擎返回的结果进行聚类，使用户迅速定位到所需要的信息。比较典型的系统有 Infonetware Real Term Search 。 Infonetware 具有强大的对搜索结果进行主题分类的功能。另外，由 Carrot Search 开发的基于Java 的开源 Carrot2 搜索结果聚合聚类引擎 2.0 版也是这方面的利用， Carrot2 可以自动把自然的搜索结果归类( 聚合聚类 ) 到相应的语义类别中，提供基于层级的、同义的以及标签过滤的功能。 (3) 改善文本分类的结果，如俄亥俄州立大学的 Y.C.Fang 等人的工作 [2] 。 (4) 文档集合的自动整理。如 Scatter/Gather[3] ，它是一个基于聚类的文档浏览系统。 2 文本聚类过程 文 本聚类主要依据聚类假设：同类的文档相似度较大，非同类的文档相似度较小。作为一种无监督的机器学习方法，聚类由于不需要训练过程、以及不需要预先对文档 手工标注类别，因此具有较高的灵活性和自动化处理能力，成为对文本信息进行有效组织、摘要和导航的重要手段。文本聚类的具体过程如图 1 所示。   图 1 文本聚类过程 2.1 文本信息的预处理 文本聚类的首要问题是如何将文本内容表示成为数学上可分析处理的形式，即建立文本特征，以一定的特征项 ( 如词条或描述 ) 来代表目标文本信息。要建立文本信息的文本特征，常用的方法是：对文本信息进行预处理 ( 词性标注、语义标注 ) ，构建统计词典，对文本进行词条切分，完成文本信息的分词过程。 2.2 文本信息特征的建立 文本信息的特征表示模型有多种，常用的有布尔逻辑型、向量空间型、概率型以及混合型等。其中，向量空间模型 (Vector Space Model,VSM) 是近几年来应用较多且效果较好的方法之一 [4] 。 1969 年， Gerard Salton提出了向量空间模型 VSM ，它是文档表示的一个统计模型。该模型的主要思想是：将每一文档都映射为由一组规范化正交词条矢量张成的向量空间中的一个点。对于所有的文档类和未知文档，都可以用此空间中的词条向量（ T1 ,W 1 ,T 2 ,W2 ,…, Tn , Wn ）来表示 ( 其中， Ti 为特征向量词条； Wi 为 Ti 的权重 )[5] 。一般需要构造一个评价函数来表示词条权重，其计算的唯一准则就是要最大限度地区别不同文档。这种向量空间模型的表示方法最大的优点在于将非结构化和半结构化的文本表示为向量形式，使得各种数学处理成为可能。 2.3 文本信息特征集的缩减 VSM 将 文本内容表示成数学上可分析处理的形式，但是存在的一个问题是文档特征向量具有惊人的维数。因此，在对文本进行聚类处理之前，应对文本信息特征集进行缩 减。通常的方法是针对每个特征词条的权重排序，选取预定数目的最佳特征作为结果的特征子集。选取的数目以及采用的评价函数都要针对具体问题来分析决定。 降 低文本特征向量维数的另一个方法是采用向量的稀疏表示方法。虽然文本信息特征集的向量维数非常大，但是对于单个文档，绝大多数向量元素都为零，这一特征也 决定了单个文档的向量表示将是一个稀疏向量。为了节省内存占用空间，同时加快聚类处理速度，可以采用向量的稀疏表示方法。假设确定的特征向量词条的个数为 n ，传统的表示方法为而（ T1 ,W 1 ,T 2 ,W2 ,…, Tn , Wn ）稀疏表示方法为 (D 1 ,W1 ,D2 ,W2 ,Dp,…,Wp , n)(Wi ≠ 0) 。其中， Di 为权重不为零的特征向量词条； Wi 为其相应权重； n 为向量维度。这种表示方式大大减小了内存占用，提升了聚类效率，但是由于每个文本特征向量维数不一致，一定程度上增加了数学处理的难度。 2.4 文本聚类 在将文本内容表示成数学上可分析处理的形式后，接下来的工作就是在此数学形式的基础上，对文本进行聚类处理。文本聚类主要有 2 种方法：基于概率 [6] 和基于距离 [7] 。基于概率的方法以贝叶斯概率理论为基础，用概率的分布方式描述聚类结果。基于距离的方法，就是以特征向量表示文档，将文档看成向量空间中的一个点，通过计算点之间的距离进行聚类。 目前，基于距离的文本聚类比较成熟的方法大致可以分为 2 种类型：层次凝聚法和平面划分法。 对于给定的文件集合 D ={d1 , d 2 ,…,di ,…, dn } ，层次凝聚法的具体过程如下： (1) 将 D 中的每个文件 di 看成一个具有单个成员的簇 ci ={di } ，这些簇构成了 D 的一个聚类 C={c1 ,c2 ,…,ci,…,cn }; (2) 计算 C 中每对簇 (ci ,cj ) 之间的相似度 sim{ ci ,cj } ； (3) 选取具有最大相似度的簇对 (ci ,cj ) 将 ci 和 cj 合并为一个新的簇 ck =sim ci ∪ cj ，从而构成了 D 的一个新的聚类 C =(c1 , c 2 ,…,cn-1 ); (4) 重复上述步骤，直至 C 中剩下一个簇为止。该过程构造出一棵生成树，其中包含了簇的层次信息以及所有簇内和簇间的相似度。对于给定的文件集合 {} D ={d1 , d2 ,…,di ,…, dn } ，平面划分法的具体过程如下： (1) 确定要生成簇的数目 k ； (2) 按照某种原则生成 k 个聚类中心作为聚类的种子 S=(s1 ,s2 ,…,si ,…,sk ); (3) 对 D 中的每个文件 di ，依次计算它与各个种子 sj 的相似度 sim (di ,sj ); (4) 选取具有最大相似度的种子     ，将 di 归入以 sj 为聚类中心的簇 cj ，从而得到 D 的一个聚类 C ={ci ,cj } (5) 重复此步骤若干次，以得到较为稳定的聚类结果。这 2 种类型各有优缺点。层次凝聚法能够生成层次化的嵌套簇，准确度较高。但在每次合并时，需要全局地比较所有簇之间的相似度，并选出最佳的 2 个簇，因此执行速度较慢，不适合大量文件的集合。而平面划分法相对来说速度较快，但是必须事先确定 k 的取值，且种子选取的好坏对群集结果有较大影响。 综合考虑这 2 种聚类类型的优缺点，本文提出了一种基于向量空间模型的文本聚类的改进方法—— LP 算法。具体过程如下： 对于给定的文件集合 D ={d1 , d 2 ,…,di ,…, dn }: (1) 将 D 中的每个文件 di 看作是一个具有单个成员的簇 ci ={di } ； (2) 任选其中一单个成员簇 ci 作为聚类的起点； (3) 在其余未聚类的样本中，找到与 ci 距离满足条件的 dj ( 可以是与 ci 距离最近的点，即相似度 sim (c i ,dj )最大的 dj ，也可以是与 ci 距离不超过阈值 d 的点，即相似度 sim (ci ,dj ) ≥ d 的任意 dj ) 。将 dj 归入 ci 形成一个新的簇 ck =sim ci ∪ dj ; (4) 重复步骤 (3) ，直至与 ci 距离最近的 dk 与 ci 之间的距离超过阈值 d ，此时认为已经聚完了一类； (5) 选择一个未聚类的单个成员簇，重复步骤 (3) 和步骤 (4) ，开始新的一轮聚类，直至所有的单个成员簇 ci都参与了聚类。 LP 算法不需要比较所有簇之间的相似度，执行速度较快，适合大量文件的集合，实用性更高。同时，在聚类过程中不需要事先确定 k 的取值，降低了与领域知识的依赖性，提高了灵活性。 3 实验设计 本文采用搜狐研发中心搜狗实验室的互联网语料链接关系库 SOGOU-T 。该关系库提供了一个大规模互联网链接关系对应表，用于验证各种链接关系分析算法的有效性与可行性。语料关系库中的数据分为 10 大类(C000007 汽车， C000008 财经， C000010 IT ， C000013 健康， C000014 体育， C000016 旅游， C000020 教育， C000022 招聘， C000023 文化， C000024 军事 ) 。语料关系库可供下载的共有 3 个版本： Mini 版，精简版，完整版。本文使用前 2 个版本进行实验。语料库的组织方式如下：为 10 个大类各建立 1 个文件夹，在每个文件夹中，每 1 份语料自成 1 个 .txt 文件。 实验过程如下： (1) 将所有文件夹下的 .txt 文件随机连结成一个大的完整文件，同时保留 .txt 文件的所属类别 ( 本实验保留了类别的最后 2 位： 07,08, … ) 。 (2) 采用中国科学院计算技术研究所数字化室 & 软件室发布的中文自然语言处理开放平台汉语词法分析系统ICTCLAS 。利用 ICTCLAS_Win ，将 (1) 中的文件进行一级标注的词语切分。 (3) 统计标注好的切分词语的词频。 (4) 按照权重 ( 词频 ) 的大小整理切分词语，并保留权重超过一定限定值 ( 阈值 ) 的特征项。 ( 本实验保留了词频大于 100 的词语作为特征项 ) 同时，根据汉语的特点，在实验中设计了 2 种情况，以分析比较词性对于聚类效果的影响： 1) 所有类型的词语都参与聚类； 2) 只保留被标注为名词的词语。 (5) 根据 (4) 中确定的切分词语构造空间向量的基向量，同时确定空间向量的维数等参数。 (6) 将语料库中的每一份语料文件 (.txt 文件 ) 都表示为一个空间向量。在实验过程中，采用了如下 2 种表示方法： 1) 传统的空间向量表示方法： (T 1 ,W 1 ,T2 , W2 ,…, T n ,Wn ) ； 2) 稀疏的空间向量表示方法： (D 1 ,W 1 ,D2 , W2 ,…,D p ,Wp ,n) 。 (7) 聚类：聚类过程是实验的重点，也是目标所在。 1) 在开始聚类前，首先对 (6) 中已经表示好的文本空间向量做归一化处理。向量归一化在模式识别中是很重要的一环，其目的是把事件的统计分布概率统一归纳在 0-1 灰色聚类的隶属性上，这样，聚类过程对于每一个空间向量的敏感度都是一样的。 传统空间向量：     , 其中 :    ; 稀疏空间向量：      其中     2) 在实验中，采用欧几里德距离来表示任意 2 个文本向量之间的距离。     稀疏空间向量：计算方法与传统空间向量类似，计算相同词条之间距离平方和的算术平方根。 3)LP 算法要求预先确定阈值。实验中，采取的阈值策略是：制定初始阈值 ( 即针对单个成员簇的阈值，此阈值根据实验效果多次调整 ) ，当 2 个簇合并为 1 个簇时，新簇的阈值由合并算法根据被合并簇的聚类特征求出。 2 个簇进行合并，其特征向量分别为 X=(T1 ,x1 ,T 2 ,x2 ,…,Tn ,Xn ),Y=(T1 ,y1 , T2 , y 2 ,…,Tn , yn ) ，则组成的新簇的特征向量为   合并定理：假定对 2 个簇进行合并，合并后的簇的阈值表示为   其中， dist 指 2 个特征向量之间的距离。 4 数据分析 实验中对于本文提到的 3 种聚类方式都有涉及，对于它们的优劣在实验层面上做了研究比对。 A ：所有类型的词语都用于构建空间向量； B ：只采用名词构建空间向量； C ：采用传统的空间向量表示方法； D ：采用稀疏的空间向量表示方法。 Mini 版 (SogouC.mini.20061102) ：共 100 篇文档，每个类别 10 篇。 精简版 (SogouC.reduced.20061102) ：共 10 020 篇文档，每个类别 1 002 篇。 表 1 是实验结果。其中， t(time) 表示聚类消耗时间，单位为 ms ； a(accuracy) 表示聚类准确度。聚类消耗的时间依赖于执行的具体状况，因而有一定的差异。表中所取的数据是排除突变数据 ( 即坏数据 ) 之后，多次实验结果的平均值。 表 1 聚类实验效果   对实验结果进行分析，可以总结出以下 5 点： (1) 对于精简版的聚类， 3 种方法的效果都优于 Mini 版。这是因为，精简版的基础数据量较大，个别的突变数据对于聚类效果的影响就相对较小。 (2) 采用稀疏向量表示法之后，聚类的时间消耗减少了约 4/5 ，表明对于高维向量采用其稀疏表示可以有效地节省内存占用空间，加快聚类处理速度。 (3) 相较于层次聚类， LP 算法在时间消耗上下降了约 30% ，因此，对于数据量较大，实时性要求较高的场合，由于有效地减少了消耗时间， LP 算法还是显示出了它的优势。 (4) 相较于平面划分法， LP 算法在聚类的准确性上提高了 11%~13% ，达到了 77%~83% ，从而保证了聚类的准确度在可接受的范围之内。 (5) 本次实验中， LP 算法在聚类准确性上略逊于层次法，笔者认为这主要是因为：层次法的主要思想是全局最优，每次聚为一个簇的 2 个成员之间的相似度都是最大的，而在 LP 算法中，决定将 2 个成员归为一类的唯一衡量就是阈值 d 。阈值选取的好坏对于实验效果的影响非常大。因此，如何选取阈值的初始值以及在聚类过程中如何动态地调整阈值是下一步的主要工作。 5 结束语 文本聚类在文本模式识别中占有重要的地位，这也是本文研究的价值所在。本文分析了基于距离的文本聚类中比较成熟的 2 种方法：层次凝聚法和平面划分法，并提出了一种改进方法 LP 。从实验效果上看， LP 算法速度更快，灵活性也更高。在后续的工作中，还将进一步在实验的基础上对算法进行反复修正和拓展，以达到更好的聚类和实用效果。 参考文献 [1]Hatzivassiloglou V.Simfinder:A Flexible Clustering Tool for Summarization[C]//Proc of NAACL Workshop on Automatic Summarization,Association for Computational Linguistics. Pittsburgh,USA:[s.n.],2001:4-14. [2]Fang Y C,Parthasarathy S,Schwartz F.Using Clustering to Boost Text Classification[C]//Proc.of the IEEE ICDM Workshop on Text Mining.Maebashi City,Japan:[s.n.],2002:1-9. [3]Cutting D,Karger D.Scatter/Gather:A Cluster Based Approach to Browsing Large Document Collection[C]//Proc.of SIGIR’92.New York,USA:ACM Press,1992:318-329. [4] 王永成 . 中文信息处理技术及其基础 [M]. 上海 : 上海交通大学出版社 ,1990. [5]Salton G,Wong A,Yang C S.A Vector Space Model for Automatic Indexing[J].Communications of ACM,1995,18(11):613-620. [6]Ackerman M,Billsus D,Gaffney S.Learning Probabilistic User Profiles[J].AI Magazine,1997,18(2):47-56. [7]Cheeseman P,Stutz J.Bayesian Classification(AutoClass):Theory and Results[C]//Proc.of Advances in Knowledge Discovery and Data Mining.Menlo Park,CA,USA:American Association for Artificial Intelligence,1996:153-180.","title":"基于向量空间模型的文本分类算法"},{"content":"不敢说自己这一年过的很精彩，但也算是从立项升级到1.0和2.0又跳到3.0，经历了考研，经历了本科的最后半年，经历了找工作，去上了半年班，又走进了北大继续学习。虽说没有chrome版本号更新快吧，虽说之间出了很多bug吧，不过项目还是在向前成长中。 0.0-1.0时代——考研和北科最后的岁月 我是奔着北大的软件理论去的，传言今年只招个位数的人，事实上也确实如此，只是我这人脑子单线程只考虑到了考上会怎么样，没想到考不上会怎么样了，就那么报名去了。考研其实到了今年就是一个收尾的工作了，大部分的复习工作都是去年做的，只是在一月初做了个验收罢了。可能是我这个人比较特殊网上各种考研苦考验心理压力大的情况在我并没有太留意在我身上是怎样的。我只是觉得每天在重复同样的时间作息表，开始着新的不同的学习进度。有时候在写需求规划，有时候在研发新的知识点，有时候在测试知识点是否掌握，有时候还要回滚，就好像一个不用打卡凭自觉去上班的事情，只是为了等最后那一天能按时上线。反正上班也有可能被踢掉，考不上不也差不多么，其实如果这么想的话心理压力就会小很多。 考试那天天很冷，穿的又少就那么发烧了，好在只在数学一科上做糊涂了，别的都硬撑下来了，考完了感觉就散掉了，迷糊了一晚上过了两天才缓过来，现在想想真的是要靠最后那点精神力量支撑住才能坚持下来，要不然可能没考完就直接去医院了。 尽管数学没有考好，好在其他的成绩还不错总分还是过了复试线，在一个中游的水平，在被刷和录取之间的一个名次就去面试了。其实我感觉进复试的人水平都差不多的，成绩差不了几分，机试除了极个别人大家成绩都一样，但是不知道为什么大家都那么紧张。可能当时我已经在baidu实习了心里有了点着落反而心态很好。我觉得我最后面试是赢在心态好了，表现的很从容也碰到了不会的问题笑着承认自己不清楚，最后顺利被录取了，进入了自己梦想中的学校。在知识层面上当你和别人都差不多的时候，可能你的心态和举止才是打动别人的地方。 回到学校开始做毕业设计，做的是一个股票分析的软件，说实话除了编程，用到的基本都是概率分析，经济学方面的一些知识。不过我倒是一直也对股票比较感兴趣，正好乐得其中。可能大部分人的本科毕设都差不多就是在deadline前一天突击一下，平时除非开会啥的也就不理指导老师了。我那时候由于要去实习不能天天在学校呆着，基本都是在deadline前第三天开始突击，然后就直接邮件汇报了，每碰到个新想法就去发邮件。结果我这个半年都没怎么见过老师的人居然成为老师最喜欢的人，因为每次我的报告什么的都是最早交的，进度啥的也是我最先报告的，而且尽管老师见不到我却总能觉得我的项目在进展。其实我没有做的多好，整个毕设我也就编了100来行的代码不过复制粘贴了几遍，改了中间几个符号出来了四套策略，但是给老师的印象却是超级好，以至于在他的研究生面前夸我好，还让一个研究生接手我的程序⊙﹏⊙……可能是别人的表现实在是太不积极了吧，虽然我也没尽力就这样了。可能当你周围的人做的都不大好，你不要跟随他们，只要做的比他们都好一点点效果就会很不一样。 毕设答辩完后就顺利毕业了，我更愿意以毕业作为第一个版本的release而不是拿到北大的录取通知书为一个release，因为我知道北大现在对我来说还只是一个贴在身上的标签，对我的影响还很小很小。 0.0-2.0——找工作，百度实习 其实1.0和2.0这两个版本基本上是并行开发的，因为整个上半年都是在实习和学校之间奔波。 考试前单线程的以为自己能考上，考完了后开始单线程的以为自己考不上了囧……所以过完年很早就回来找工作了 年后工作的机会真的比年前少很多，在网上投了很多简历都石沉大海，投过的人都懂得，后来只好去一些招聘会看看，眼高手低的又觉得没什么好企业。被忽悠去面试后来发现是一家培训机构想让你过去接受他的培训……不过那次被忽悠的时候那个人指出了很多我简历格式上的硬伤，虽然没去他的培训不过我还是根据他说的改了一下简历，把一些亮点放在了更靠前的位置，补充了一些吸引人的东西。之后继续网投，几乎相同的简历内容，改了下格式我基本就每天都能有一个面试通知了。可能对我这样一直认为技术才是最重要的人来说往往忽视了格式对于表现一个人技术能力的重要性。没有一个合理的格式，你会把自己打造的没有任何特点。 主要的工作招聘信息都是从应届生网站上找的，那些天白天就在准备面试和面试，晚上就是打开应届生趁着当天的信息都发布差不多了，挑着企业投简历，广撒网。面试了好多公司有大有小，由于都是技术岗位感觉面试还都蛮轻松主要都是探讨技术问题，有一些公司也问到了薪水问题不过最后都没有结果，可能是觉得我的考研结果还没出来不确定性太大吧。 当时投百度纯粹是为了好玩，根本没想过我这样的能去百度，但是有时候去试一下不可思议的事情就发生了。 我当时应聘的是QA的职位，投完简历都不知道QA是干什么的……去百度面试的前一晚看了一晚上测试相关的书，毕竟不想面试的太难看。不过第二天去的时候想了一脑子调戏百度的问题想着他要敢给我难堪最后我也不让他痛快。到了百度大厦先是很花痴的绕着转了一圈，因为感觉没准这是这辈子最后一次来了，没想到最后在大厦里度过了半年的时光。 和我一块去的还有另一个人，再次证明了你有时候不需要表现的最好，只需要表现的比和你一块的人好点就行。不过我感觉那天我要比那人好好多O(∩_∩)O哈哈~。由于最初就没想过能留下也就没去找百度的面经，不过之前由于很向往google看了很多人面google的经历，结果发现百度是同一个套路，顿时有了很熟悉的感觉，可以说那天是我那么多次面试发挥的最好的一次。完全是在技术层次的冲击，代码方面的挑战，整个面试过程就像是头脑风暴，一个接一个的挑战一个接一个的问题让人酣畅淋漓，不知不觉就面了三个多小时，思维从来就没有停顿的时刻，大脑一直在高速运转。 可以看出百度还是很重视人才的，当知道我成绩还没下来的时候他们直接就给我提供了一个实习的offer告诉我如果我考上了就去上学，考不上到时候我们根据你实习期的表现来决定是不是转正。弄得我当初想的好几个调戏百度的问题都没好意思问。晕晕乎乎的走出百度大厦感觉脑子都裂掉了，面试的太爽了。 半年的时间里对QA，对百度和对工作都有了更深的了解，也暴露出了自身很多的不足。 百度确实是个很锻炼人的地方，哪怕是之前我认为没什么技术含量的测试。这里的QA做的是全程质量监控，从立项就开始进入项目，做需求的时候就要开始寻找项目的潜在问题，要对需求设计文档进行审核，进行测试用例的设计，rd编码前还要对设计文档进行审核寻找编码中可能的缺陷，还要维护一套线下的测试环境，对一组测试机进行管理，有时候还要和线上的用户交流，寻找线上还存在的问题。这里面有道是技术层面上的要求，有的是沟通能力的要求，有的是责任意识的要求，由于我已开始的工作就是这样的所以也就没觉得什么，直到有别的公司的人跳槽来百度…… 测试环境还要自己搭么？测试用例是要自己写么？需求沟通会也要去呀？等等等等。这才让我感觉到百度的QA和别的地方是不一样的，我们真的是“全程”质量监控。在这里我们和rd，pm的关系也是平等的，因为我们也是全程控制着项目，在任何一个环节都必须要我们通过才能进行下去。 这里也汇聚这一些极其优秀的工程师，可以对好几个大项目了如指掌，排查问题都是信手拈来，他们有的甚至在挑linux内核和C库函数里面的bug放到群里和人共享，能和这些聪明的人共事是幸运的。有时候我想这里汇聚着恐怕是国内最好的工程师，他们在技术层面上已经达到很高的高度了，可能在国内缺乏的是那种创新的方向，才无法使他们的才华能放在突破性的方向上。 相信大家可能都因为种种原因对百度的印象不大好，我来百度之前也是一样的。其实国内很多的互联网公司也都大体一样，真正进去之后可能才会发现一些问题的原因。虽然百度已经是国内市值第一的互联网公司，但是恐怕公司内部根本就没有树立起来自己是个大公司这个概念。在公司内部其实是一个个的项目小组各自为战，一些新的项目组人数少的可怜。我第一个mentor之前就是做饱受争议的百度文库的QA她告诉我第文库一版上线的时候其实整个项目组也就四个人。完全是一个创业小组的模式借助着百度的一个logo就把项目上线了。百度里这种情况很常见相信其他互联网企业也大体如此，由于每个人都认为自己是在一个小作坊里工作，那些所谓的法律意识，社会责任意识就很难被竖立起来。而公司又缺乏在整体层面上对项目的规范，这种情况下一个项目环节出现问题其实是对整个百度logo的损害。而百度又缺乏这种成为一家伟大企业的愿望，可以说直到文库风波之后百度内部才开始传播这种我们已经是一个大公司的这种概念，传播我们要有社会责任感这些想法。 当然也正是因为这种小作坊式的模式，使得每个人都能有很好的发展，因为在一个项目里你就是独当一面的那个人，各种能力和责任意识就会成长的很快。一个项目就那么几个人缺谁都不行，大家人少关系也好相处，大家又都是年轻人话题也很多，可以说在里面人际方面相当好处。在里面一切流程都是简化掉的，把你更多的精力都集中的主要工作上，而不是浪费在繁琐的走流程。可以说我有时感觉百度内部的管理制度真的是漏洞百出，之所以能走到今天完全是靠百度员工超强的个人能力撑起来的。 大厦里面的工作环境就不细说了，很赞就是了，不打卡我也很欣赏。 说一下对我个人成长的体验吧。技术方面确实有很多欠缺的，跟着大家学了很多技术方面的东西，但感觉都是东拼西凑不成体系的东西。工作的好处就是通过项目让你确实感受到你是可以做出东西来的，弊端就是你看到你做出来的东西给自己一个自己很强的假象，以为自己就可以做很多事情了，实际上还是要冷静下来思考自己的不足在哪里。我实际的感受就是工作学到的知识只是工作中需要的，如果真想给自己充电的话，学习知识最好的场所还是在学校，工作真的很少有心思再给自己充电了。 沟通在工作中真的起着至关重要的作用，我现在觉得沟通的地位是要放在技术之上的。项目最后的问题很多都不是因为技术上碰到了不可逾越的问题而是大家在一开始就对彼此的说法产生了误解。有时在工作的时候真的是不能半天就在那坐着，要像TCP请求那样来回不断的发送和确认，总之是不能坐那不动的，经常把问题当面沟通才能保证工作的质量。 还有就是在百度的第二个mentor对我产生了很重要的影响，mentor不仅在技术上令人敬仰，人格上更是无懈可击，她在潜移默化中教了我很多东西。当我拿到北大录取通知书后其实去百度就是为了混实习工资了（囧，说实话不要鄙视我）。工作很不用心，各种出错，报了不知道多少次线上bug，可是mentor每次都原谅了我帮我承受住来自上边的压力，弄得我十分不好意思。mentor也知道我不会在这久呆，但还是教了我很多有意义的事情，比如seven habits里的一些建议对我启发很大。可能有时候一味的责怪一个人会让人逆反，而这种用人格力量来感染人会有很好的效果，一阵子后我真的是在那拼命工作，再也不混了，倒不是为了工作怎么样，而只是真心的想为mentor做些什么。 而且我从mentor身上感受到了她对工作项目的那种热爱，不是为了一些杂七杂八的理由来工作，而是因为做的是她热爱的项目而工作，整个项目组的人都被她的这种精神感染，虽说她不是这个项目的老大，但各种大小会都要叫她，因为她也总是希望了解到项目里的任何一个细节。我也渐渐被这种精神感染，当大家都是在为自己所喜爱的事情工作的时候，你会觉得真的工作不是很累。 8月底的时候离开了百度，最后天绕着大厦又走了一圈，我不知道以后还会不会再来，但这里已经在我生命痕迹里留下重要的一笔，我愿意把这段实习的结束当做2.0的release。 2.0-3.0——研究生生活 进入北大我又开始想混了，毕竟百度的工作还是很辛苦的，更何况我是在去北大报道前几天才从百度离职，当时只想歇着。 好吧我还想说你只要表现的比别人好一点点就好了。我们这些统考来北大的人都是没有分具体专业的，大部分实验室都已经被报送的学生报满了，而我想去的操作系统实验室每年又都很火，统考来的好多都想去。我就在开学没事情做的那几天给老师发了个邮件，被老师叫过去谈谈。由于我各方面条件还可以，百度的这段经历也算给我加分了，再加上对面试神马的已经完全没有心理压力了，感觉谈的还不错。之后静静的等着发现我又是最早去找老师的，可能先入为主了最后空出一个名额给我。 当然以混为主的思维还是统领着我这半年，但我发现在这个学校真的是很难混……我现在还没什么老师安排的项目，但光课程上就让我感受到原来前所未有的压力。这里的课程难度和作业难度完全和本科不是一个量级，看我母校的研究生同学们用着比本科还轻松的方法就能顺利通过考试我是无比羡慕。 但是正是这种课程让我收获很多。之前不过是编一些很简单的程序，这里的课上要编反汇编器，编译优化器，要编自然语言处理的语法分析，要编基于hadoop的mapreduce程序，实验室的师兄还告诉我最好编一个操作系统……在学术方面我曾经为了做讨论版一个星期读了9篇英文论文，和师兄探讨一些新的方向和思路。昨天晚上还在那里研究NP问题…… 看到了实验室的本科生一个个强的不得了，我才体会到本科四年的差距不是考考研的一跃龙门就有任何缩小的，进入北大这个大门只是让我和一些更聪明能力更强的人在一起。By the way 这些比我更聪明，能力比我更强的人还比我更勤奋。 尽管这半年北大的生活我更多的是在适应和混，但我希望接下来我能有个新的开始，就以此作为3.0release的时间吧，向着更高的版本号前进。","title":"[置顶] 我的2011--0.0到3.0的版本变迁史"},{"content":"隐马尔可夫模型 (Hidden Markov Model，HMM) 最初由 L. E. Baum 和其它一些学者发表在一系列的统计学论文中，随后在语言识别，自然语言处理以及生物信息等领域体现了很大的价值。平时，经常能接触到涉及 HMM 的相关文章，一直没有仔细研究过，都是蜻蜓点水，因此，想花一点时间梳理下，加深理解，在此特别感谢 52nlp 对 HMM 的详细介绍。 　　考虑下面交通灯的例子，一个序列可能是红-红/橙-绿-橙-红。这个序列可以画成一个状态机，不同的状态按照这个状态机互相交替，每一个状态都只依赖于前一个状态，如果当前的是绿灯，那么接下来就是橙灯，这是一个确定性系统，因此更容易理解和分析，只要这些状态转移都是已知的。但是在实际当中还存在许多不确定性系统。 　　在日常生活当中，我们总是希望根据当前天气的情况来预测未来天气情况，和上面的交通灯的例子不同，我们不能依靠现有知识确定天气情况的转移，但是我们还是希望能得到一个天气的模式。一种办法就是假设这个模型的每个状态都只依赖于前一个的状态，这个假设被称为马尔科夫假设，这个假设可以极大简化这个问题。显然，这个假设也是一个非常糟糕的假设，导致很多重要的信息都丢失了。 　　当涉及到天气的时候，马尔科夫假设描述为，假设如果我们知道之前一些天的天气信息，那么我们就能预测今天的天气。当然，这个例子也是有些不合实际的。但是，这样一个简化的系统可以有利于我们的分析，所以我们通常接受这样的假设，因为我们知道这样的系统能让我们获得一些有用的信息，尽管不是十分准确的。 　　谈到 HMM，首先简单介绍一下马尔可夫过程 (Markov Process)，它因俄罗斯数学家安德烈·马尔可夫而得名，代表数学中具有马尔可夫性质的离散随机过程。该过程中，每个状态的转移只依赖于之前的 n 个状态，这个过程被称为1个 n 阶的模型，其中 n 是影响转移状态的数目。最简单的马尔科夫过程就是一阶过程，每一个状态的转移只依赖于其之前的那一个状态。注意这和确定性系统不一样，因为这种转移是有概率的，而不是确定性的。 　　马尔可夫链是随机变量 X1, … , Xn 的一个数列。这些变量的范围，即他们所有可能取值的集合，被称为“状态空间”，而 Xn  的值则是在时间 n 的状态。如果 Xn+1 对于过去状态的条件概率分布仅是 Xn 的一个函数，则 　　这里 x 为过程中的某个状态。上面这个恒等式可以被看作是马尔可夫性质。 　　马尔可夫链的在很多应用中发挥了重要作用，例如，谷歌所使用的网页排序算法（PageRank）就是由马尔可夫链定义的。 　　下图展示了天气这个例子中所有可能的一阶转移： 　　注意一个含有 N 个状态的一阶过程有 N2 个状态转移。每一个转移的概率叫做状态转移概率 (state transition probability)，就是从一个状态转移到另一个状态的概率。这所有的 N2 个概率可以用一个状态转移矩阵来表示，其表示形式如下： 　　对该矩阵有如下约束条件： 　　下面就是海藻例子的状态转移矩阵： 　　这个矩阵表示，如果昨天是晴天，那么今天有50%的可能是晴天，37.5%的概率是阴天，12.5%的概率会下雨，很明显，矩阵中每一行的和都是1。 　　为了初始化这样一个系统，我们需要一个初始的概率向量： 　　这个向量表示第一天是晴天。 　　到这里，我们就为上面的一阶马尔科夫过程定义了以下三个部分： 　　状态：晴天、阴天和下雨 　　初始向量：定义系统在时间为0的时候的状态的概率 　　状态转移矩阵：每种天气转换的概率 　　所有的能被这样描述的系统都是一个马尔科夫过程。 　　然而，当马尔科夫过程不够强大的时候，我们又该怎么办呢？在某些情况下，马尔科夫过程不足以描述我们希望发现的模式。 　　例如，一个隐居的人可能不能直观的观察到天气的情况，但是民间传说告诉我们海藻的状态在某种概率上是和天气的情况相关的。在这种情况下我们有两个状态集合，一个可以观察到的状态集合（海藻的状态）和一个隐藏的状态（天气状况）。我们希望能找到一个算法可以根据海藻的状况和马尔科夫假设来预测天气的状况。 　　一个更现实的例子是语音识别，我们听到的声音是声带、喉咙和一起其他的发音器官共同作用的结果。这些因素相互作用，共同决定了每一个单词的声音，而一个语音识别系统检测的声音（可以观察的状态）是人体内部各种物理变化（隐藏的状态、引申一个人真正想表达的意思）产生的。 　　某些语音识别设备把内部的发音机制作为一个隐藏的状态序列，把最后的声音看成是一个和隐藏的状态序列十分相似的可以观察到的状态的序列。在这两个例子中，一个非常重要的地方是隐藏状态的数目和可以观察到的状态的数目可能是不一样的。在一个有3种状态的天气系统（sunny、cloudy、rainy）中，也许可以观察到4种潮湿程度的海藻（dry、dryish、damp、soggy）。在语音识别中，一个简单的发言也许只需要80个语素来描述，但是一个内部的发音机制可以产生不到80或者超过80种不同的声音。 　　在上面的这些情况下，可以观察到的状态序列和隐藏的状态序列是概率相关的。于是我们可以将这种类型的过程建模为有一个隐藏的马尔科夫过程和一个与这个隐藏马尔科夫过程概率相关的并且可以观察到的状态集合。这就是本文重点介绍的隐马尔可夫模型。 　　隐马尔可夫模型 (Hidden Markov Model) 是一种统计模型，用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数，然后利用这些参数来作进一步的分析。下图是一个三个状态的隐马尔可夫模型状态转移图，其中x 表示隐含状态，y 表示可观察的输出，a 表示状态转换概率，b 表示输出概率。 　　下图显示了天气的例子中隐藏的状态和可以观察到的状态之间的关系。我们假设隐藏的状态是一个简单的一阶马尔科夫过程，并且他们两两之间都可以相互转换。 　　对 HMM 来说，有如下三个重要假设，尽管这些假设是不现实的。 　　 　　假设1：马尔可夫假设（状态构成一阶马尔可夫链） 　　假设2：不动性假设（状态与具体时间无关） 　　假设3：输出独立性假设（输出仅与当前状态有关） 　　隐藏的状态和可观察到的状态之间有一种概率上的关系，也就是说某种隐藏状态 H 被认为是某个可以观察的状态 O1 是有概率的，假设为 P(O1 | H)。如果可以观察的状态有3种，那么很显然 P(O1 | H)+P(O2 | H)+ P(O3 | H) = 1。   　　这样，我们也可以得到一个另一个矩阵，称为混淆矩阵 (confusion matrix)。这个矩阵的内容是某个隐藏的状态被分别观察成几种不同的可以观察的状态的概率，在天气的例子中，这个矩阵如下图： 　　上边的图示都强调了 HMM 的状态变迁。而下图则明确的表示出模型的演化，其中绿色的圆圈表示隐藏状态，紫色圆圈表示可观察到状态，箭头表示状态之间的依存概率，一个 HMM 可用一个5元组 { N, M, π，A，B } 表示，其中 N 表示隐藏状态的数量，我们要么知道确切的值，要么猜测该值，M 表示可观测状态的数量，可以通过训练集获得， π={πi} 为初始状态概率，A={aij} 为隐藏状态的转移矩阵 Pr(xt(i) | xt-1(j))，B={bik} 表示某个时刻因隐藏状态而可观察的状态的概率，即混淆矩阵，Pr(ot(i) | xt(j))。在状态转移矩阵和混淆矩阵中的每个概率都是时间无关的，即当系统演化时，这些矩阵并不随时间改变。对于一个 N 和 M 固定的 HMM 来说，用 λ={ π, A, B } 表示 HMM 参数。 　　在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中，状态并不是直接可见的，但受状态影响的某些变量则是可见的。每一个状态在可能输出的符号上都有一概率分布。因此输出符号的序列能够透露出状态序列的一些信息。 　　在 HMM 中有三个典型问题： 　　（一） 已知模型参数，计算某一给定可观察状态序列的概率 　　 假设我们已经有一个特定的隐马尔科夫模型 λ 和一个可观察状态序列集。我们也许想知道在所有可能的隐藏状态序列下，给定的可观察状态序列的概率。当给定如下一个隐藏状态序列： 　　那么在 HMM 和这个隐藏状态序列的条件下，可观察状态序列的概率为： 　　而隐藏状态序列在 HMM 条件下的概率为： 　　因此，隐藏状态序列和可观察状态序列的联合概率为： 　　那么所有可能的隐藏状态序列上，可观察状态序列的概率为： 　　例如，我们也许有一个海藻的“Summer”模型和一个“Winter”模型，因为海藻在夏天和冬天的状态应该是不同的，我们希望根据一个可观察状态（海藻的潮湿与否）序列来判断现在是夏天还是冬天。 　　我们可以使用前向算法来计算在某个特定的 HMM 下一个可观察状态序列的概率，然后据此找到最可能的模型。 　　这种类型的应用通常出现在语音设别中，通常我们会使用很多 HMM，每一个针对一个特别的单词。一个可观察状态的序列是从一个可以听到的单词向前得到的，然后这个单词就可以通过找到满足这个可观察状态序列的最大概率的 HMM 来识别。 　　下面介绍一下前向算法 (Forward Algorithm) 　　如何计算一个可观察序列的概率？ 　　1. 穷举搜索 　　给定一个 HMM，我们想计算出某个可观察序列的概率。考虑天气的例子，我们知道一个描述天气和海藻状态的 HMM，而且我们还有一个海藻状态的序列。假设这个状态中的某三天是（dry，damp，soggy），在这三天中的每一天，天气都可能是晴朗，多云或者下雨，我们可以用下图来描述观察序列和隐藏序列： 　　在这个图中的每一列表示天气的状态可能，并且每个状态都指向相邻的列的每个状态，每个状态转换在状态转移矩阵中都有一个概率。每一列的下面是当天的可观察的海藻的状态，在每种状态下出现这种可观察状态的概率是由混淆矩阵给出的。 　　一个可能的计算可观察概率的方法是找到每一个可能的隐藏状态的序列，这里有32 = 27种，这个时候的可观察序列的概率就是 Pr(dry, damp, soggy | HMM)=Pr(dry, damp, soggy | sunny, sunny, sunny) + . . . . + Pr(dry, damp, soggy | rainy, rainy, rainy)。 　　很显然，这种计算的效率非常低，尤其是当模型中的状态非常多或者序列很长的时候。事实上，我们可以利用概率不随时间变化这个假设来降低时间的开销。 　　2. 使用递归来降低复杂度 　　我们可以考虑给定 HMM 的情况下，递归的计算一个可观察序列的概率。我们可以首先定义一个部分概率，表示达到某个中间状态的概率。接下来我们将看到这些部分概率是如何 在time=1 和 time = n (n > 1) 的时候计算的。 　　假设一个T时间段的可观察序列是： 　　1) 部分概率 　　下面这张图表示了一个观察序列（dry，damp，soggy）的一阶转移 　　我们可以通过计算到达某个状态的所有路径的概率和来计算到达某个中间状态的概率。比如说，t=2时刻，cloudy的概率用三条路径的概率之和来表示： 　　我们用 αt(j) 来表示在 t 时刻是状态 j 的概率，αt(j)=Pr(观察状态 | 隐藏状态 j ) x Pr(t 时刻到达状态 j 的所有路径)。 　　最后一个观察状态的部分概率就表示了整个序列最后达到某个状态的所有可能的路径的概率和，比如说在这个例子中，最后一列的部分状态是通过下列路径计算得到的： 　　因为最后一列的部分概率是所有可能的路径的概率和，所以就是这个观察序列在给定 HMM 下的概率了。 　　2) 计算 t=1时候的部分概率 　　当 t=1 的时候，没有路径到某个状态，所以这里是初始概率，Pr(状态 j | t=0) = π(状态 j )，这样我们就可以计算 t=1 时候的部分概率为： 　　因为在初始的时候，状态 j 的概率不仅和这个状态本身相关，还和观察状态有关，所以这里用到了混淆矩阵的值，k1 表示第一个观察状态，bjk1 表示隐藏状态是 j，但是观察成 k1 的概率。 　　3) 计算 t>1 时候的部分概率 　　还是看计算部分概率的公式是：αt(j) = Pr(观察状态 | 隐藏状态 j) x Pr(t 时刻到达状态 j 的所有路径)。 这个公式的左边是从混淆矩阵中已知的，我只需要计算右边部分，很显然右边是所有路径的和： 　　需要计算的路径数是和观察序列的长度的平方相关的，但是 t 时刻的部分概率已经计算过了之前的所有路径，所以在 t+1 时刻只需要根据 t 时刻的概率来计算就可以了： 　　这里简单解释下，bjk(t+1) 就是在 t+1 时刻的第 j 个隐藏状态被认为是当前的观察状态的概率，后面一部分是所有t时刻的隐藏状态到 t+1 时候的隐藏状态j的转移的概率的和。这样我们每一步的计算都可以利用上一步的结果，节省了很多时间。 　　4) 公式推导   　　5) 降低计算复杂度 　　我们可以比较穷举和递归算法的复杂度。假设有一个 HMM，其中有 n 个隐藏状态，我们有一个长度为 T 的观察序列。 　　穷举算法的需要计算所有可能的隐藏序列： 　　需要计算： 　　很显然穷举算法的时间开销是和 T 指数相关的，即 NT，而如果采用递归算法，由于我们每一步都可以利用上一步的结果，所以是和 T 线性相关的，即复杂度是 N2T。 　　这里我们的目的是在某个给定的 HMM 下，计算出某个可观察序列的概率。我们通过先计算部分概率的方式递归的计算整个序列的所有路径的概率，大大节省了时间。在 t=1 的时候，使用了初始概率和混淆矩阵的概率，而在t时刻的概率则可以利用 t-1 时刻的结果。 　　这样我们就可以用递归的方式来计算所有可能的路径的概率和，最后，所有的部分概率的计算公式为 　　使用天气的例子，计算 t=2 时刻的 cloudy 状态的概率方法如图： 　　我们使用前向算法在给定的一个 HMM 下计算某个可观察序列的概率。前向算法主要采用的是递归的思想，利用之前的计算结果。有了这个算法，我们就可以在一堆 HMM 中，找到一个最满足当前的可观察序列的模型（前向算法计算出来的概率最大）。 　　（二） 根据可观察状态的序列找到一个最可能的隐藏状态序列 　　和上面一个问题相似的并且更有趣的是根据可观察序列找到隐藏序列。在很多情况下，我们对隐藏状态更有兴趣，因为其包含了一些不能被直接观察到的有价值的信息。比如说在海藻和天气的例子中，一个隐居的人只能看到海藻的状态，但是他想知道天气的状态。这时候我们就可以使用 Viterbi 算法来根据可观察序列得到最优可能的隐藏状态的序列，当然前提是已经有一个 HMM。 　　另一个广泛使用 Viterbi 算法的领域是自然语言处理中的词性标注。句子中的单词是可以观察到的，词性是隐藏的状态。通过根据语句的上下文找到一句话中的单词序列的最有可能的隐藏状态序列，我们就可以得到一个单词的词性（可能性最大）。这样我们就可以用这种信息来完成其他一些工作。 　　下面介绍一下维特比算法 (Viterbi Algorithm) 　　一．如何找到可能性最大的隐藏状态序列？ 　　通常我们都有一个特定的 HMM，然后根据一个可观察状态序列去找到最可能生成这个可观察状态序列的隐藏状态序列。 　　1. 穷举搜索 　　我们可以在下图中看到每个隐藏状态和可观察状态的关系。 　　通过计算所有可能的隐藏序列的概率，我们可以找到一个可能性最大的隐藏序列，这个可能性最大的隐藏序列最大化了 Pr(观察序列 | 隐藏状态集)。比如说，对于上图中的可观察序列 (dry damp soggy)，最可能的隐藏序列就是下面这些概率中最大的： 　　Pr(dry, damp, soggy | sunny, sunny, sunny), ……，Pr(dry, damp, soggy | rainy, rainy, rainy) 　　这个方法是可行的，但是计算代价很高。和前向算法一样，我们可以利用转移概率在时间上的不变性来降低计算的复杂度。 　　2. 使用递归降低复杂度 　　在给定了一个可观察序列和HMM的情况下，我们可以考虑递归的来寻找最可能的隐藏序列。我们可以先定义一个部分概率 δ，即到达某个中间状态的概率。接下来我们将讨论如何计算 t=1 和 t=n (n>1) 的部分概率。 　　注意这里的部分概率和前向算法中的部分概率是不一样的，这里的部分概率表示的是在t时刻最可能到达某个状态的一条路径的概率，而不是所有概率之和。 　　1) 部分概率和部分最优路径 　　考虑下面这个图以及可观察序列 (dry, damp, soggy) 的一阶转移 　　对于每一个中间状态和终止状态 (t=3) 都有一个最可能的路径。比如说，在 t=3 时刻的三个状态都有一个如下的最可能的路径： 　　我们可以称这些路径为部分最优路径。这些部分最优路径都有一个概率，也就是部分概率 δ。和前向算法中的部分概率不一样，这里的概率只是一个最可能路径的概率，而不是所有路径的概率和。 　　我们可以用 δ(i, t) 来表示在t时刻，到状态i的所有可能的序列（路径）中概率最大的序列的概率，部分最优路径就是达到这个最大概率的路径，对于每一个时刻的每一个状态都有这样一个概率和部分最优路径。 　　最后，我们通过计算 t=T 时刻的每一个状态的最大概率和部分最优路径，选择其中概率最大的状态和它的部分最优路径来得到全局的最优路径。 　　2) 计算 t=1 时刻的部分概率 　　当 t=1 时刻的时候，到达某个状态最大可能的路径还不存在，但是我们可以直接使用在 t=1 时刻某个状态的概率和这个状态到可观察序列 k1 的转移概率： 　　3) 计算 t>1 时刻的部分概率 　　接下来我们可以根据 t-1 时刻的部分概率来求 t 时刻的部分概率 　　我们可以计算所有到状态 X 的路径的概率，找到其中最可能的路径，也就是局部最优路径。注意到这里，到达X的路径必然会经过 t-1 时刻的 A、B 和 C，所以我们可以利用之前的结果。达到X的最可能的路径就是下面三个之一： 　　(状态序列)，. . .，A，X (状态序列)，. . .，B，X (状态序列)，. . .，C，X 　　我们需要做的就是找到以 AX、BX 和 CX 结尾的路径中概率最大的那个。 　　根据一阶马尔科夫的假设，一个状态的发生之和之前的一个状态有关系，所以X在某个序列的最后发生的概率只依赖于其之前的一个状态： Pr (到达A的最优路径) . Pr (X | A) . Pr (观察状态 | X) 　　有个了这个公式，我们就可以利用t-1时刻的结果和状态转移矩阵和混淆矩阵的数据： 　　将上面这个表达式推广一下，就可以得到 t 时刻可观察状态为 kt 的第 i 个状态的最大部分概率的计算公式： 　　其中 aji 表示从状态 j 转移到状态 i 的概率，bikt 表示状态i被观察成 kt 的概率。 　　4) 后向指针 　　考虑下图 　　在每一个中间状态和结束状态都有一个部分最优概率 δ(i, t)。但是我们的目的是找到最可能的隐藏状态序列，所以我们需要一个方法去记住部分最优路径的每一个节点。 　　考虑到要计算 t 时刻的部分概率，我们只需要知道 t-1 时刻的部分概率，所以我们只需要记录那个导致了 t 时刻最大部分概率的的状态，也就是说，在任意时刻，系统都必须处在一个能在下一时刻产生最大部分概率的状态。如下图所示： 　　我们可以利用一个后向指针 φ 来记录导致某个状态最大局部概率的前一个状态，即 　　这里 argmax 表示能最大化后面公式的j值，同样可以发现这个公式和 t-1 时刻的部分概率和转移概率有关，因为后向指针只是为了找到“我从哪里来”，这个问题和可观察状态没有关系，所以这里不需要再乘上混淆矩阵因子。全局的行为如下图所示： 　　5) 优点 　　使用 viterbi 算法对一个可观察状态进行解码有两个重要的优点： 　　a) 通过使用递归来减少复杂度，这点和之前的前向算法是一样的 　　b) 可以根据可观察序列找到最优的隐藏序列，这个的计算公式是： 其中  　　这里就是一个从左往右翻译的过程，通过前面的翻译结果得到后面的结果，起始点是初始向量 π。 　　3. 补充 　　但在序列某个地方有噪声干扰的时候，某些方法可能会和正确答案相差的较远。但是 Viterbi 算法会查看整个序列来决定最可能的终止状态，然后通过后向指针来找到之前的状态，这对忽略孤立的噪声非常有用。 　　Viterbi 算法提供了一个根据可观察序列计算隐藏序列的很高效的方法，它利用递归来降低计算复杂度，并且使用之前全部的序列来做判断，可以很好的容忍噪声。 　　在计算的过程中，这个算法计算每一个时刻每一个状态的部分概率，并且使用一个后向指针来记录达到当前状态的最大可能的上一个状态。最后，最可能的终止状态就是隐藏序列的最后一个状态，然后通过后向指针来查找整个序列的全部状态。 　　（三） 根据观察到的序列集来找到一个最有可能的 HMM。  　　在很多实际的情况下，HMM 不能被直接的判断，这就变成了一个学习问题，因为对于给定的可观察状态序列 O 来说，没有任何一种方法可以精确地找到一组最优的 HMM 参数 λ 使 P(O | λ) 最大，于是人们寻求使其局部最优的解决办法，而前向后向算法（也称为Baum-Welch算法）就成了 HMM 学习问题的一个近似的解决方法。 　　前向后向算法首先对于 HMM 的参数进行一个初始的估计，但这个很可能是一个错误的猜测，然后通过对于给定的数据评估这些参数的的有效性并减少它们所引起的错误来更新 HMM 参数，使得和给定的训练数据的误差变小，这其实是机器学习中的梯度下降的思想。 　　对于网格中的每一个状态，前向后向算法既计算到达此状态的“前向”概率，又计算生成此模型最终状态的“后向”概率，这些概率都可以通过前面的介绍利用递归进行高效计算。可以通过利用近似的 HMM 模型参数来提高这些中间概率从而进行调整，而这些调整又形成了前向后向算法迭代的基础。 　　另外，前向后向算法是 EM 算法的一个特例，它避免了 EM 算法的暴力计算，而采用动态规划思想来解决问题，Jelinek 在其书《Statistical Methods for Speech Recognition》中对前向后向算法与 EM 算法的关系进行了详细描述，有兴趣的读者可以参考这本书。 　　类似于上面讲到的前向算法，我们也可以定义后向变量 βt(i) 来计算给定当前隐藏状态 i 时，部分观察序列 ot+1，ot+2，…，oT的概率，即： 　　与前向算法类似，我们也可以通过迭代算法有效计算 βt(i)，计算公式如下： 　　其中 　　进一步我们可以发现 　　因此 　　下面开始介绍前向后向算法。 　　首先我们需要定义两个辅助变量，这两个变量可以用前文介绍过的前向变量和后向变量进行定义。 　　第一个变量定义为 t 时状态 i 和 t+1 时状态 j 的概率，即 　　该变量在网格中所代表的关系如下图所示： 　　 　　该等式等价于 　　利用前向变量和后向变量，上式可以表示为 　　第二个变量定义为后验概率，也就是在给定观察状态序列和 HMM 的情况下，t 时状态 i 的概率，即 　　利用前向变量和后向变量，上式可以表示为 　　因此，下式为在任意时刻状态 i 的期望，也就是从状态 i 转移到观察状态 o 的期望 　　同样，下式也就是从状态 i 转移到状态 j 的期望 　　我们可以发现定义的这两个变量之间的关系为 　　下面介绍前向后向算法的参数学习过程，在学习的过程中，不断更新 HMM 的参数，从而使得 P(O | λ) 最大。我们假设初始的 HMM 参数为  λ={ π, A, B }，首先计算前向变量 α 和后向变量 β，再根据刚刚介绍的公式计算期望 ξ 和 ζ，最后，根据下面的3个重估计公式更新 HMM 参数。 　　如果我们定义当前的 HMM 模型为 λ={ π，A，B }，那么可以利用该模型计算上面三个式子的右端；我们再定义重新估计的 HMM 模型为，那么上面三个式子的左端就是重估的 HMM 模型参数。Baum 及他的同事在70年代证明了，因此如果我们迭代地计算上面三个式子，由此不断地重新估计 HMM 的参数，那么在多次迭代后可以得到 HMM 模型的一个最大似然估计。不过需要注意的是，前向后向算法所得的这个最大似然估计是一个局部最优解。 　　参考资料： 　　1. http://blog.csdn.net/eaglex/article/details/6376826 　　2. http://en.wikipedia.org/wiki/Markov_chain 　　3. http://en.wikipedia.org/wiki/Hidden_Markov_model 　　4. Lawrence R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. Proceedings of the IEEE, 77 (2), p. 257–286, February 1989. 　　5. L. R. Rabiner and B. H. Juang, “An introduction to HMMs,” IEEE ASSP Mag., vol. 3, no. 1, pp. 4-16, 1986. 　　6. http://jedlik.phy.bme.hu/~gerjanos/HMM/node2.html 　　7. http://www.cs.brown.edu/research/ai/dynamics/tutorial/Documents/HiddenMarkovModels.html 　　8. 隐马尔可夫模型简介，刘群","title":"隐马尔可夫模型（HMM）攻略"},{"content":"最近在做机器翻译的作业，学习一个自然语言工具包NLTK（http://www.nltk.org/），用python做的一个项目，虽然是以教学为目的的项目，但看过去做得很大很全，据说有十万行代码，而且是python代码，我对大型项目的规模没有一个定量的概念，但至少这代码行数于我而言就已经算是巨大了。 文档更是非常清楚和详尽，从安装到代码和数据下载，到python的基本知识和调用语法到自然语言分析处理，或许是由于它原本就是为了教学的吧。我根据教程简单调用了它的几个功能（比如双语互翻、词频统计），也觉得很好用。 这让我想到另一个项目，是前不久老师让我们去了解的一个Google的项目DataWiki，是一个能够让用户自定义数据格式并上传格式化数据的平台，通过实验室同学做的那个报告，我感觉这个项目做得并不完善，许多很容易想到的功能都没有，据同学说那个项目本身就简单，就几个java文件实现的，没有什么东西。 这两个不同领域的项目相比较，从实现的全面性和功能的易用性来讲，显然前者有着明显的优势，但也并不是说后者就没有意义。我想，DataWiki的亮点，就在于它提出了用户自定义数据格式并提供了这样一个创建并上传格式化数据的平台，为格式化数据的增加提供了一种途径，而我们所能简单看到的一些功能上的缺陷只是一些用户体验和易用性上的问题，或许是几个页面几行代码就能解决的简单问题，在这一方面上的改进是永远没有尽头的也永远只是核心功能上的点缀，但是核心思想上的创新才是这一项目的存在价值。NLTK与之不同，自然语言处理这一理念已经被关注和研究多年，并且有了许多经典的理论成果和实现算法，NLTK所做的，是一个教科书式的工作，也就是将零散在各处的理念、算法整合并加以实现，它的存在价值，是全面、易用。两个项目的不同侧重点让我看到，一个项目，如果没有创新亮点，就应该尽量做到功能上的全面和易用；有了创新亮点，就应该更突出核心思想，当然一些用户体验上的功能性的东西也是要慢慢加上去的。","title":"NLTK与DataWiki"},{"content":"    目前正在学习自然语言处理相关的概率模型，在一篇名为《Classification Probability Models and Conditional Random Fields》论文中讲述了常用的几个经典的概率模型，并分析了他们之间的关系和区别，深入浅出，讲的非常的好。     在很多任务中，面临的问题都是对给定的输入X，对输入赋予一个恰当的分类标签Y。在自然语言处理中，如文本分类、词性标注、分词等等，均可表示为以上形式。在常用的概率模型中，主要分为两类，一类是生成模型（generative model），这类模型计算X与Y的联合概率（joint probability）P(X,Y)，常见的有朴树贝叶斯模型，隐马尔科夫模型等。另一类称为判别模型（discriminative model），这类模型计算X与Y的条件概率(conditional probability) P（Y|X），常见的有最大熵模型、条件随机场模型等。     这四个模型的关系可用图表示如下：     在这些概率模型中，最基本的就是朴树贝叶斯模型。贝叶斯模型是一个分类模型，对于输入赋予一个标签。在朴树贝叶斯模型中，对于给定的输入向量X，对其赋予一个类别标识，其概率可以表示为：    因为我们求的只是相对概率，所以可以将分母去掉，只计算分子即可。在上面的公式中，如果直接进行计算将会有较高的复杂度，所以使用一个链式规则对计算进行分解，得到新的计算公式：         在实际应用中，常常假设输入向量X的各维是条件独立的（朴树贝叶斯假设） 即p(x i |y, x j ) = p(x i |y) 。这样上面的计算公式可转换为： 这就是朴树贝叶斯分类规则。","title":"Classification Probability Models and Conditional Random Fields（1）--Naive Bayes"},{"content":"AI会议排名_周志华   http://blog.sina.com.cn/s/blog_631a4cc40100xl7d.html 南京大学周志华教授写的一个很经典的帖子。不过IJCAI能不能算成是no.1的会议有待商榷，不过总体还算客观！ 说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern   Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大,所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中,有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大,因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可   以给到1+,也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比   IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的   介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字   “Advances in Neural Information Processing Systems”, 所以, 与ICML\\ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML\\ECML\\COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示\\推理\\学习等很多方面, AUAI  (Association of UAI) 主办, 每年开. tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal   Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说).","title":"AI会议排名_周志华"},{"content":"以前说过，一个 real life 自然语言处理系统，其质量和可用度除了传统的 data quality 的衡量指标查准度（precision）和查全度（recall）外，还有更为重要的三大指标：海量处理能力（scalability）, 深度（depth）和鲁棒性（robustness）（参见：《“三好”立委要做“三有”系统》）。 本文就简单谈一下鲁棒性。 为了取得语言处理的鲁棒性（robustness），一个行之有效的方法是实现四个形容词的所指：词汇主义（lexicalist）; 自底而上（bottom-up）; 调适性（adaptive）；和数据制导（data-driven）。这四条是相互关联的，但各自重点和视角不同。系统设计和开发上贯彻这四项基本原则， 是取得鲁棒性的良好保证。有了鲁棒性，系统对于不同领域的语言，甚至对极不规范的社会媒体中的语言现象，都可以应对。这是很多实用系统的必要条件。 先说词汇主义策略。词汇主义的语言处理策略是学界和业界公认的一个有效的方法。具体说来就是在系统中增加词汇制导的个性规则的总量。自然语言的现象是如此复杂，几乎所有的规则都有例外，词汇制导是必由之路。从坚固性而言，更是如此。基本的事实是，语言现象中的所谓子语言（sublanguage），譬如专业用语，网络用语，青少年用语，他们之间的最大区别是在词汇以及词汇的用法上。一般来说，颗粒度大的普遍语法规则在各子语言中依然有效。因此，采用词汇主义策略，可以有效地解决子语言的分析问题，从而提高系统的鲁棒性。 自底而上的分析方法。这种方法对于自浅而深的管式系统 (pipeline system) 最自然。系统从单词出发，一步一步形成越来越大的句法单位，同时解析句法成分之间的关系。其结果是自动识别（构建）出来的句法结构树。很多人都知道社会媒体的混乱性，这些语言充满了错别字和行话，语法错误也随处可见。错别字和行话由词汇主义策略去对付，语法错误则可以借助自底而上的分析方法。其中的道理就是，即便是充满了语法错误的社会媒体语言，其实并不是说这些不规范的语言完全不受语法规则的束缚，无章可循。事实绝不是如此，否则人也不可理解，达不到语言交流的目的。完全没有语法的“语言”可以想象成一个随机发生器，随机抽取字典或词典的条目发射出来，这样的字串与我们见到的最糟糕的社会媒体用语也是截然不同的。事实上，社会媒体类的不规范语言（degraded text）就好比一个躁动不安的逆反期青年嬉皮士，他们在多数时候是守法的，不过情绪不够稳定，不时会”突破”一下规章法律。具体到语句，其对应的情形就是，每句话里面的多数短语或从句是合法的，可是短语（或从句）之间常常会断了链子。这种情形对于自底而上的系统，并不构成大的威胁。因为系统会尽其所能，一步一步组合可以预测（解构）的短语和从句，直到断链的所在。这样一来，一个句子可能形成几个小的句法子树（sub-trees），子树之内的关系是明确的。 朋友会问：既然有断链，既然那些子树没有形成一个完整的句法树来涵盖所分析的语句，就不能说系统真正鲁棒了，自然语言理解就有缺陷。抽象地说，这话不错。但是在实际使用中，问题远远不是想象的那样严重。其道理就是，语言分析并非目标，语言分析只是实现目标的一个手段和基础。对于多数应用型自然语言系统来说，目标是信息抽取（Information Extraction），是这些预先定义的抽取目标在支持应用（app）。抽取模块的屁股通常坐在分析的结构之上，典型的抽取规则 by nature 是基于子树匹配的，这是因为语句可以是繁复的，但是抽取的目标相对单纯，对于与目标不相关的结构，匹配规则无需cover。这样的子树匹配分两种情形，其一是抽取子树（subtree1）的规则完全匹配在语句分析的子树（subtree2）之内（i.e. subtree2 > subtree1），这种匹配不受断链的任何影响，因此最终抽取目标的质量不受损失。只有第二种情形，即抽取子树恰好坐落在分析语句的断链上，抽取不能完成，因而影响了抽取质量。值得强调的是，一般来说，情形2的出现概率远低于情形1，因此自底而上的分析基本保证了语言结构分析的鲁棒性，从而保障了最终目标信息抽取的达成。其实，对于 worst case scenario 的情形2，我们也不是没有办法补救。补救的办法就是在分析的后期把断链 patch 起来，虽然系统无法确知断链的句法关系的性质，但是patched过的断链形成了一个完整的句法树，为抽取模块的补救创造了条件。此话怎讲？具体说来就是，只要系统的设计和开发者坚持调适性开发抽取模块（adaptive extraction）的原则，部分抽取子树的规则完全可以建立在被patched的断链之上，从而在不规范的语句中达成抽取。其中的奥妙就是某样榜戏中所说的墙内损失墙外补，用到这里就是结构不足词汇补。展开来说就是，任何子树匹配不外乎check两种条件约束，一是节点之间句法关系的条件（主谓，动宾，等等），另外就是节点本身的词汇条件（产品，组织，人，动物，等等）。这些抽取条件可以相互补充，句法关系的条件限制紧了，节点词汇的条件就可以放宽；反之亦然。即便对于完全合法规范的语句，由于语言分析器不可避免的缺陷而可能导致的断链（世界上除了上帝以外不存在完美的系统），以及词汇语义的模糊性，开发者为了兼顾查准率和查全率，也会在抽取子树的规则上有意平衡节点词汇的条件和句法关系的条件。如果预知系统要用于不规范的语言现象上，那么我们完全可以特制一些规则，利用强化词汇节点的条件来放宽对于节点句法关系的条件约束。其结果就是适调了patched的断链，依然达成抽取。说了一箩筐，总而言之，言而总之，对于语法不规范的语言现象，自底而上的分析策略是非常有效的，加上调适性开发，可以保证最终的抽取目标基本不受影响。 调适性上面已经提到，作为一个管式系统的开发原则，这一条很重要，它是克服错误放大（error propagation）的反制。理想化的系统，模块之间的接口是单纯明确的，铁路警察，各管一段，步步推进，天衣无缝。但是实际的系统，特别是自然语言系统，情况很不一样，良莠不齐，正误夹杂，后面的模块必须设计到有足够的容错能力，针对可能的偏差做调适才不至于一错再错，步步惊心。如果错误是 consistent/predictable 的，后面的模块可以矫枉过正，以毒攻毒，错错为正。还有一点就是歧义的保存（keeping ambiguity untouched）策略。很多时候，前面的模块往往条件不成熟，这时候尽可能保持歧义，运用系统内部的调适性开发在后面的模块处理歧义，往往是有效的。 最后，数据制导的开发原则，怎样强调都不过分。语言海洋无边无涯，多数语言学家好像一个爱玩水的孩子，跳进大海就乐不思蜀。见水珠不见海洋，见树木不见森林，一条路走到黑，是太多语言学家的天生缺陷。如果由着他们的性子来，系统叠床架屋，其执行和维护的 overhead 会越来越大，而效果却可能越来越差（diminishing returns）。数据制导是迫使语言学家回到现实，开发真正有现实和统计意义的系统的一个保证。这样的保证应该制度化，这牵涉到开发语料库（dev corpus）的选取，baseline 的建立和维护，unit testing 和 regression testing 等开发操作规范的制定以及 data quality QA 的配合。理想的数据制导还应该包括引入机器学习的方法，来筛选制约具有统计意义的语言现象反馈给语言学家。从稍微长远一点看，自动分类用户的数据反馈，实现某种程度的粗颗粒度的自学习，建立半自动人际交互式开发环境，这是手工开发和机器学习以长补短的很有意义的思路。 适配性、数据制导的容错性开发肯定可以对付一些错字漏字的现象， 但是究竟能对付多少，那些是可以对付的，那些是难以对付的，需要具体分析。 可以看一下两极的情形：天花板和地板。 容错、纠错的系统本质上是对人的容错、纠错能力的 modeling，因此人的理解能力是系统可能逼近的天花板。理论上讲，如果人能理解错字漏字的语句，基本说明了这个不规范的句子形式内涵语义的冗余度，那么一个容错系统应该也可以做到。（当然，人的理解可能不自觉地调动常识、专业知识、联想和推理等非语言学手段，模拟起来很困难，在目前，简单地容错开发是远远达不到人的理解力的。）如果错字漏字造成真正的语句歧义，那么最好的系统最多做到分析的多路径。最后，如果错字漏字严重到人都搞不清什么意思的时候，机器自然是两眼一抹黑。 地板就是一个完全规范的“紧式”语言系统，错字漏字的地方就造成断链，局部语义无法合成为完整的理解（problem with the semantic compositionality, the key to language understanding）。所谓容错开发，就是尽可能把紧式系统松绑为“宽式”系统，把非排歧的无关紧要的条件放宽。譬如英语中的主谓一致关系的条件（第三人称单数的主语需要其谓语动词有词尾-s， 这个中小学语法课上的金科玉律就是紧式语法的条条框框，在宽式开发中一般不用）。再如汉语动词的被动语态，有些句子加了“被”字，万一这个关键的被动语态的助词漏掉了，容错系统应该仍然分析无误，如果逻辑动宾的语义搭配条件参与了分析的过程的话（如：“饭吃了” 就是 “饭被吃了”，而“我吃了” 则不一样）。 来源： http://blog.sciencenet.cn/home.php?mod=space&uid=362400&do=blog&id=516413","title":"坚持四项基本原则，开发鲁棒性NLP系统"},{"content":"你知道有多少网页被编入Bing的索引部分却永远不会显示在顶部结果之中吗？ 你知道Bing是如何通过深入了解用户的信息需求，帮助人们从网站获取知识吗？ 你知道Bing是如何在云基础平台上构建从而支持数以百万计的应用程序和服务吗？ 微软亚洲研究院常务副院长马维英博士，就这些重要内容和相关的初步结果，给从事网络与互联网挖掘、自然语言处理、机器学习等领域的实习生们带来了一场精彩的讲座。如果你也对这些课题深感兴趣，欢迎来一起聆听马维英博士的精彩演讲吧！   演讲人：马维英博士 整理：廖振、宣金学 从图书馆检索到互联网搜索：飞跃与困境   演讲一开始，幻灯片上就显示出过去图书馆查阅图书的场景，马维英博士解释到，“其实我们最早的索引是从图书馆的目录开始的”，借助于目录，我们可以更快地找到相应的图书。但，随着互联网的出现，大量的网页涌现在互联网上，知识已经呈爆炸性的增长，人们获取知识的途径，也从最初跑图书馆，发展到利用搜索引擎来进行搜索。而今，人们如果遇到问题，往往要先从各大的搜索引擎搜寻一下解决方案。   目前流行的搜索引擎主要分为网络爬虫、网页索引和结果排序等几个关键部分。这种搜索这种技术有点像图书馆的“倒排表检索”，把所有互联网上的信息组织起来。直到现在，其实也还没有超出这种思路。现在我们所看到的互联网上面，大概有上万亿个网页，其中约有250亿的页面可以被检索而有可能出现在用户搜索结果里，大概只是1%～5%，也就是说，目前互联网上的所有网页中，最多只有5%的页面，用户有机会看到。而在信息大爆炸的当今，用户可能只有精力关注前100个搜索结果。如果再过10年，5%这个比例可能会迅速降到百分之零点几了。   既然大部分的计算给用户带来的价值越来越少，那么，我们能不能直接关注用户的需求？能不能像完成用户交给搜索引擎的一个任务那样，帮助用户作出决策并采取行动，而不是给用户10个链接让他们逐个打开，自己判断其中是否包含有用的信息？另外，由于绝大部分的网页用户根本看不到，那能不能将这部分搜索资源应用到更多的像应用软件在线商店这样的细分领域？因此，如何让用户更满意，更喜欢用微软开发的搜索引擎Bing，则是我们研究院从事该方面研究工作的研究员和实习生们密切关注的问题。   下一代搜索引擎：“3”个改变与“5”个方向   就下一个阶段的互联网搜索，马维英博士提出了下面一些新颖的观点。   (一) 搜索引擎索引结构的改变：目前我们在做搜索引擎的时候，往往是希望索引的越多越好，这样用户进行查询的时候，才不会因为我们索引中没有该网页，而返回不了用户希望得到的结果。可我们就Bing上面的研究发现，索引中大约95%的页面，即便采用当前最好的排序策略，对于所有用户提交的查询，都无法被排到结果的首页，从而基本上就无法被用户看到，以至于白白地浪费了我们的服务器资源去存放这些网页。我们何不改变一下传统的索引模式，改为只去索引那些能被用户看到的页面呢？   (二) 用户的搜索行为的改变：当前的搜索引擎，仅仅接受用户输入的几个关键词语，就返回给用户结果。除了让用户输入关键词，我们还可以让用户提供一些其他信息，比如简单的语音，简单的构图，这些在搜索语音和图像时候，往往非常管用。马维英博士还当场展示了几个对应的demo。   (三) 结果呈现的改变：目前在网页搜索中，我们对于一个用户提交的查询，都是返回前十个网页，这似乎已经形成了习惯，而导致大家都不希望去改变它。而我们研究院最新的一些研究成果，已经可以在很多方面提升用户的体验，在用户搜索一个关键词的时候，把该词语作为一个实体，将其各个属性都输出给用户，例如输入“中国”，我们可以提供人口、国土面积等信息。   基于此，我们从五个方向对未来搜索引擎进行了的战略性思考。第一是从组织所有的网页信息，到直接关注用户的搜索目的。搜索引擎一开始的目标是希望组织所有的页面，这个概念非常强大，但有弱点，而且现在的效率已经越来越低。因此要通过做需求理解(Intent Understanding) 来关注用户的需求是什么，而不再是关键字搜索；第二是建立知识库，利用各式各样的挖掘技术，把结构性的Web中的对象 (entity) 关系抽出来之后，以知识的方法来表示；第三是语义的检索与任务完成，也就是帮助用户完成任务的搜索；第四是从搜索内容走向搜索应用和服务；第五是云平台和建立生态系统，在这样的生态系统中，会有更多的开发人员创建自己的微应用和微服务。   微软下一阶段的搜索策略：“云+端”平台的整合   在搜索之外，对于另外一个热门的话题“云计算”，马维英博士也说出了自己的看法。他说道，微软在云计算的道路上已经迈出了自己的步伐，将Windows操作系统都搬到云中去了，这意味着，以后我们可以利用一根网线，控制自己的操作系统，并且获得云中的服务。   其他的大型互联网公司都有自己独特的云。总体来讲，目前的云存在如下几类：第一种是信息云(Information Cloud)，目前的几家主要的搜索引擎供应商是该领域的领导者；第二种是社交媒体云，比如Facebook之类的网站中，包含了相当多的用户数据；第三种就是娱乐云，比如XBOX  Live的平台上也存在着大量数据和信息。这些云如果在将来结合在一起，其所产生的机会将不可胜数。   而从搜索应用的切入角度看，最有希望和机会的是社交媒体搜索。搜索会与社交网络发生更多的融合，比如通过Facebook找人与人的关系，微软亚洲研究院开发的“人立方”关系搜索引擎，纯粹是机器自动抽取数据和信息，而这些信息全是Facebook上的用户手工产生的，这无疑将会得到非常全面的关系图谱。   PC已发展了30年，直到今天还有很多数据是以“孤岛”的形态存在你的手机里、即时通信软件里、在图片库里等等。“云＋端”可以连接所有的数据、设备、应用和服务，最终连接所有的人和事件。数据是核心，“云＋端”平台的整合，就是要把人类社会历史上的最后一公里走完。这也是我们所正在面临的一个历史机遇。其下一步就是所谓的自然用户界面，比如“云+端”的界面，这会让任何人都更加便捷地像使用搜索引擎那样拥有超级的计算能力。   那么“云+端”平台的整合，意味着把一台超级电脑摆到每位用户面前，将来的潜力会是无比巨大的。其实，任何关于未来的预测都是很困难的，不过有一点非常明确，那就是，整个转变的进程已然开启，并且无法逆转。这意味着，无论是硬件还是软件，无论是互联网还是移动通信领域，都正在面临同样的历史机遇。 精彩问答：Bing的超越之道   整个演讲，原本预计是一个小时，由于在提问环节，大家的兴致很高，踊跃提问，马维英博士也进行了一一的回答，以致演讲持续了将近两个小时才结束，最后还有几位没有来得及发问的同学也在之后与马博士讨论起来。其中有一位实习生向马维英博士提问道：“在搜索领域，你认为微软Bing搜索引擎如何才能提高竞争力呢？”   马维英博士回答说，我记得就这个话题，鲍尔默曾经谈到三点，我觉得比较有意思，可以分享给你们。   第一. 做的比你的对手更好。就目前来看，各个大的主流搜索引擎，很难说谁比谁好，往往用户对于自己搜到的成功查询，不会记住，而假使搜索引擎A没有提供自己希望的答案而引擎B提供了，就会转向搜索引擎B。这种搜索引擎之间的切换，在用户当中是经常出现的，对于某一些查询，A可能更好，对另外的一些查询，可能B更好，总得来讲就不好评估。另外一方面，用户往往会由于习惯问题，导致使用某一种搜索引擎，除非该引擎结果很糟，他才会转变到另外一种上面。   第二. 卖的比你的对手更便宜。当今搜索引擎已经是免费的了，不可能再便宜了，当然，我们也可以采取一些措施，比如在微软的搜索引擎上搜索某些商品进行购买时，有可能返还用户小额现金(只是一个假设而已)。   第三. 做你的对手最不愿意做的事情。这个对不同的对手而言是不一样的，比如当前主流的搜索引擎是以十个搜索结果的列表作为返回页面，由于用户习惯了这种显示结果，往往搜索引擎是不会去改变的。而对于我们而言，是不是可以改变这种列表方式的呈现，比如我们得到一些更加精确的，经过数据挖掘得出的更立体化的结果，直接呈现给用户，可能会让用户更加满意。   在微软亚洲研究院，这种以实习生为主，推动和拉近实习生与研究员、研究院的管理者们的活动，是很常见的，实习生们甚至可以邀请自己感兴趣的研究员，来一次座谈会，例如实习生中颇受欢迎的“Tango有约”活动就是如此，一方面，研究员们可以就自己的研究，以及人生的经历，对实习生门进行传道授业解惑，另一方面，接触到实习生，也可以让研究员回想起自己以前的学习、工作和生活，让自己的心，年轻起来。   相关阅读： 研究之初的那些人生选择题——洪小文院长研究生涯演讲系列之一 让PC读懂你的图 无所不包的万物互联网--记David Culler在2010年“21世纪的计算”主题发言                                                                                  欢迎关注 微软亚洲研究院人人网主页：http://page.renren.com/600674137 微软亚洲研究院微博：http://t.sina.com.cn/msra ","title":"下一代互联网搜索的前沿：意图、知识与云"},{"content":"我想如今机器学习 (Machine Learning) 的重要性（不论是在学术界还是在工业界）已经不用再多强调了，比如说 2010 年的图灵奖得主 Leslie Valiant 就是学习理论 (Learning Theory) 的一位先驱大牛，正是他提出了“可能近似正确” (Probably Approximately Correct, PAC) 模型——每次念一念 PAC 的中文翻译就觉得好玩，不过 PAC 模型及其变种确实是如今学习理论里最为广泛使用的框架之一，而且正是 PAC 模型催生了现在应用超级广泛的 boosting 算法。学习理论中的两大巨头： PAC 模型和 Vapnik 的 Statistical Learning Theory (SLT) ，各自都拥有一个在实际中得到广泛认可和应用的算法（boosting 和 SVM），应该也不是偶然吧！ 不过这些八卦以后有机会再提吧。 另一个例子则是现在如火如荼的 Stanford 的在线机器学习课程 ml-class ，具体的注册人数是多少我现在找不到统计数字了，不过看起来 Stanford 的这次开创性的尝试确实是非常令人鼓舞的吧！如此的成功也许也是大多数人所始料未及的吧，开设的课程也从原来的机器学习、人工智能和数据库三门一下子增加了诸如自然语言处理、计算机安全、博弈论等课以及其他专业的各种课程。 回到机器学习，开设 ml-class 的 Andrew Ng 本身也是机器学习领域里的一位新星人物，和许多做纯科研的人不同的是，他的许多工作有“看得见摸得着”的很实际的东西，其中很著名的就是一个叫做 LittleDog 的机器狗，能走能跑能跳，可以应付各种复杂地形，平衡性超好，用脚踹都踹不翻；他的另一个很好玩的项目就是无人驾驶直升机（千万不要因为天上障碍物少、不会堵车，就觉得无人驾驶直升机比无人驾驶汽车要来得简单啊 ^_^），可以完成许多高难度的特技飞行动作。这些 robot/agent 相关的东西和机器学习的一大交集就是一个叫做增强学习 (Reinforcement Learning, RL) 的模型，实际上这已经是一个相当“古老”的 topic 了，理论和算法上都已经有相当的成果，不过传统的 RL 算法通常都有相当 aggressive 的“探索”环境的过程，而这在像机器人控制，特别是直升机控制方面有可能是不太现实的，极端的情况下某个高难度的“探索性”步骤有可能会使直升机 crash 掉。为了解决这个问题，Andrew Ng 提出了所谓的学徒学习 (Apprenticeship learning)，通过人类示范的方法来引导机器进行学习。当然这里牵涉到的不仅仅是“模仿”而已，从理论上可以证明，学徒可以学得和老师差不多的 performance ，当老师的示范带有“启发性”的信息的时候，学徒甚至可以超过老师。因为这次并不是专门要讲学徒学习或者增强学习，所以我不得不用非常模糊的词语（“差不多”、“启发性”之类的）糊弄过去，然而实际上模型和各种性能保证都是有严格的数学描述的，如果感兴趣的话，可以去看 Ng 的论文（至少需要先了解一些 Markov Decision Process 的基础知识） 。当然，除了理论保证之外，学徒学习还被成功地应用到了无人驾驶直升机的控制上。无论如何，像这种从实际问题出发，理论结合实践的研究方式，真的是非常令人振奋的啊！ 实际上对于机器学习这个话题来说，除了 Reinforcement Learning 和传统的 Artificial Intelligence 似乎看起来关系比较密切之外，其他的一些机器学习中研究的问题似乎相对于传统 AI ，反而更接近其他一些领域一点。比如我曾经被问及机器学习和统计分析有什么区别——实际上我对统计分析并不是很了解，我想它应该主要就是用概率统计的方法去分析数据的一门学科吧，看起来确实和机器学习很像（特别是现在“机器学习”已经快要等价于“统计学习”了的时候），然而后来我渐渐地发现其实还是有一些差别的，或者说，干脆就是不一样吧。统计分析的主要目的应该是去分析或者解释存在的数据，例如，用某个概率模型，从数据去估计分布的参数，并计算置信度之类的。 而机器学习，虽然看起来也比较类似，但是本质的区别在于，机器学习的目的不在于分析当前数据，而是在于对未来的预测。当然这种分类并不是很严格的，比如机器学习中的用于 density estimation 的最大似然方法，就是寻找最能“解释”当前数据的概率分布模型。但是，比如说，一个专攻油画的人也能画一些素描，总不至于因此就认为素描和油画是一样的吧。对于这个具体的例子来说，即便同样是在做 density estimation ，统计分析里可能通常都会假设数据确实是满足某个具体的带参数的分布，从而去研究如何更精确更健壮地估计对应的参数的问题（例如试验设计）；而机器学习则通常不会假设数据的真实分布是符合某个参数的概率模型的，或者甚至完全不做任何限制，而在这样的背景下，普通的最大似然是否真的可行呢？要达到给定的精读需要多少数据点、多少计算量？这些才是机器学习所关心的问题。 另一个很相关的领域是 Optimization ，因为优化和确实在机器学习中非常重要，特别是涉及到要解决具体问题的时候，最后通常都会需要解一个优化问题。所以看起来好像是机器学习就完全成了“提出目标函数”、“找到优化方法”两步曲了。诚然，一个有用的算法，能够高效地解出来当然是必要的，然而也并不能因此就把机器学习和优化等同起来或者甚至看成是优化的一个子问题，这就跟不能把所有用到数学的学科全都和数学等同起来一样嘛，何况机器学习中还有一些和优化关系不大的重要问题。 接下来我们不妨来看一下最经典的 Supervised Learning 问题的设定，来大致了解一下机器学习问题中所关心的问题是什么样的。用 和 分别表示输入和输出空间，给定一个训练集 ，目标是要学得一个函数 ，使得对于未来的 ，我们能够根据 来预测其对应的 。比如， 是在医院做的各种医疗检测的结果， 表示你是否患有癌症。当然，所谓“没有免费的午餐”，如果不假定观察到的数据 和未来的数据之间有一定的联系的话，这个任务是无法完成的。而在统计学习中，建立两者之间联系是通过一个共享的概率模型来实现的。 这里最基本的假设是 上存在一个（联合）分布 ，而 中的数据对 和未来的数据对 都是独立同分布 (IID) 地从 采样出来的。一种更为特殊的情况（包含在这种设定之中）是只假设 上有一个分布 ，而每个 对应的 是由一个确定的函数给出： ，或者加上有噪音的情况，比如很常见的 Gaussian 噪声假设 ，这里 是一个服从零均值的高斯分布的噪音随机变量，此时相当于条件分布 是一个期望等于 的高斯分布。这里 IID 的假设最重要的地方就在于同分布——也就是说训练数据和测试数据是符合同一个（未知的概率）模型的，这样一来我们才能通过训练数据来推测（未知的）测试数据相关的信息。比如，假设亚洲人患癌症和相应的医疗检测结果之间的关系模型与欧洲人的模型可能是不一样的，这样的话，就无法保证能直接通过从亚洲人这里采集的训练样本来得到合理的关于欧洲的癌症患病模型的预测了。 不过也有将这个要求加以放松的情况，对于刚才的例子，虽然欧洲人和亚洲人的模型并不完全一样，但是可能有一些相似之处（毕竟大家都属于同一个物种），所以，在训练数据的模型和测试数据的模型不相同但是“相差不大”的时候，是否仍然能进行学习呢？这就是 Domain Adaptation 所考虑的问题：如何来 formulate 两个模型之间的相似性，以及在满足什么样的相似性的情况下，该问题的 learnable 的，能达到什么样的 performance 等等，不过在这里暂时不展开讲了，对这个问题感兴趣的同学可以参考最近的一本书《Dataset Shift in Machine Learning》。这种涉及到多个不同数据源（模型）的学习问题还有 transfer learning 、multi-task learning 之类的。 除了放弃要求训练数据和测试数据来自同一个分布之外，还有更宽松的模型完全不要求数据是 IID 地采样自某个特定的概率分布。比如在 online learning 中就是如此，这使得模型更加宽松并且能应用到更加广泛的一类问题中，不过抛弃了概率分布之后也就无从谈及 expected loss 之类的概念了（因为“期望”的定义需要有概率分布的存在），所以关于 的衡量需要借助于其他的方式，比较常用的是 regret ，具体也不在这里展开讲了。接下来我们先重点讨论只有一个概率分布 的情况。 为了衡量所学得的 的好坏，我们还需要定义一个 loss function 。例如，对于经典的分类问题，我们令 取 0-1 loss: 不过 只是衡量在某一个（对）数据点上的损失，根据观察到的训练数据，我们可以定义经验风险 (Empirical Risk) 用于衡量 在整个训练数据上的 risk 。然而为了衡量“True Risk”，也就是谈论 在“将来的”或者“未知的”数据上的 risk ，我们就必须借助于刚才假设的联合概率分布 了，具体来说，我们把 Risk 定义为 的期望： 具体到分类问题和 0-1 loss ，我们有 这里 表示事件 的特征函数。此时 Risk 也就是 和 不相等的概率——这是一个非常自然的衡量标准。值得注意的是，这个 Risk 的 formulate 依赖于我们所提出的概率模型假设，虽然我们不知道真实的 ，但是我们可以根据从 中采样出来的 sample 来对 相关信息进行估计，这正是统计学习得意实现的基础。 具体来说，在刚才勾勒出来的世界观设定中，我们的目的已经很明确了：找一个使得 最小的 。麻烦在于 未知，所以 求不出来，但是我们可以通过 来求 ，然后根据大数定理，对于某个固定的 来说， 会随着 的增大趋向于 。这看起来似乎很诱人，直观上来看，它为我们提供了一个很直接的算法：寻找使得 最小的 作为解，由于 是（至少从理论上来说）可以求值的，因此这是一个合法的算法，通常叫做“经验风险最小化” (Empirical Risk Minimization, ERM) 算法。关于 ERM 的具体细节，以及它存在的问题和解决方法，我们将会在下次细说。 在结束之前，我们再对世界设定做一些细节上的补充：主要是定义一个 Bayes error 为我们能达到的最低的 risk: 或许应该叫做 Bayes Risk 更合适一点吧……不过名字什么的无所谓了，总之 这个记号以后会经常用到，这是学习的理论极限——再好的算法都无法达到更低的 risk ，这是由问题本身决定的，而和具体的算法无关。注意 并不一定等于零，例如下图例子中 上的二分类问题，图中显示出 0 和 1 两类的条件概率密度函数之间有重叠的部分： 不论你如何取 ，图中涂黄的部分总是会不可避免地被算入 risk 中，导致零 risk 不可达到。如果你觉得看图不太可信，也可以形式推导一下。不妨考虑二分类和 0-1 loss 的情况，此时引入一个 regression function ： 于是 注意 regression function 表示 时 的概率，如果 总是等于 0 或者 1 的话（对应地 总是确定地等于 0 或者 1）， 是恒等于 1 的，此时根据上式可以得到 。但是 在某个测度非零的集合上成立的话， 就总是要大于零了。 除了以上的分析之外，regression function 实际上还可以帮助我们确定出实现最优 Risk 的那个 classifier ，定义如下： 这个 classifier 被称作 Bayes classifier ，接下来我们将说明 是最优的：对任意其他的 ，我们要证明 。从刚才的推导中我们得到的一个额外公式并进一步化简得： 于是我们有 最后的非负是因为：如果 ，那么 并且根据定义知 ，此时不管 等于多少（0 或者 1），这个式子都是非负的； 的情况也类似。由 的最优性知道 即是 Bayes Risk ，亦即下确界在 取到。需要注意的是，这个最优的 Bayes classifier 虽然存在，并且可以把具体形式给出来，但是由于在实际中我们是不知道 的，因此也不知道 ，所以 也是未知的。 最后，需要说明的是，实际的机器学习过程中，我们通常是在某个函数空间 中进行学习的，极端情况下，我们令 包含所有从 到 的函数，这是一个及其巨大无比的函数空间，通常学习问题的难度随着函数空间的大小（复杂度）增大而增大，打一个形象的比喻，在如此巨大的一个函数空间中寻找一个最优的函数，就犹如在河滩上找一个最圆的鹅卵石一样困难：一方面鹅卵石太多了，另一方面我们肉眼很乱石头圆不圆的精度很低，只能凭感觉来，所以会拿到很多看起来其实都挺圆的但是并不是真正要找的那个石头。回到机器学习的情况，比如，如果我们用 ERM 方法来进行学习的话，对任意大多的 个训练数据，我们可以构造一个函数 ，使得对于训练数据中的点 ，而对于其他点令 ，这个函数的 empirical risk 是零，然而真实 risk 则完全没有任何保证。 因此，实际中我们都会使用更小的性质更好的函数空间来进行学习，例如所有的线性分类器，或者由某个核函数所诱导的 Reproducing Kernel Hilbert Space 之类的。此时 有可能在或者不在 中，记 这个是我们在 中所能达到的最小的 risk 。显然有 。 最后，总结一下：这次我们给出了最简单的 supervised learning 问题的基本框架，并定义了学习问题的目标（寻找 Risk 最小的函数）以及所能达到的理论最优值 ，特别地，在二分类问题和 0-1 loss 下，我们借助于 regression function 做了一些更细致的分析。supervised learning 按照 的情况可以大致分为 classification 和 regression 两种情况，而二分类正是 classification 问题中最典型和最简单的情况。在陈述学习问题的时候，我们还提到了 ERM 学习算法以及大数定理，但是没有做深入探讨，不出意外的话，这将成为下一次的主要话题。 最后，封面“人物”是：喜欢看书的那一只 Tachikoma 。Tachikoma 是动画片《攻壳机动队》里公安九课的机器人，他们不能严格称作“个体”，因为在么次执行任务之后，所有“人”之间会将记忆进行同步。然而他们仍然发展出了个性——这一点我没有觉得奇怪的，因为记忆原本就不是一个人的全部嘛。不过 Tachikoma 确实是迄今为止最令我感动的机器人。","title":"（转载自FreeMind）机器学习物语(1)：世界观设定"},{"content":"《哥德尔 埃舍尔 巴赫——集异璧之大成》 《天遇:混沌与稳定性的起源》 《深入理解计算机系统》 《哲人石丛书》 《矩阵分析》 《黑客与画家》 《Head FIrst 设计模式》 C++： 《Effective C++》 《More Effective C++》 Python： 《Python核心编程》 《Pyhon灰帽子》 《Python自然语言处理》 English: 《BEC词汇》 数学： 《什么是数学》 《数学之美》 Linux： 《UNIX环境高级编程》","title":"列个书单,静心读书,刻不容缓"},{"content":"中文分词 (Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂的多、困难的多。 之所以存在中文分词技术，是由于中文在基本文法上有其特殊性，具体表现在： 　　1．与英文为代表的拉丁语系语言相比，英文以空格作为天然的分隔符，而中文由于继承自古代汉语的传统，词语之间没有分隔。　古代汉语中除了连绵词和人名地名等，词通常就是单个汉字，所以当时没有分词书写的必要。而现代汉语中双字或多字词居多，一个字不再等同于一个词。 　　例如英语：'Knowledge is power“，可自然分割为 Knowledge/ is/ power 三个词。 　　而汉语里：“知识就是力量”,由于没有词语之间的分隔符，书写时无法切分成：知识/ 就是/ 力量 　　2．在中文里，“词”和“词组”边界模糊 　　现代汉语的基本表达单元虽然为“词”，且以双字或者多字词居多，但由于人们认识水平的不同，对词和短语的边界很难去区分。 　　例如：“对随地吐痰者给予处罚”，“随地吐痰者”本身是一个词还是一个短语，不同的人会有不同的标准，同样的“海上”“酒厂”等等，即使是同一个人也可能做出不同判断，如果汉语真的要分词书写，必然会出现混乱，难度很大。 中文分词是文本挖掘的基础，对于输入的一段中文，成功的进行中文分词，可以达到电脑自动识别语句含义的效果。 　　中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。        现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。 1、基于字符串匹配的分词方法 　　这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；常用的几种机械分词方法如下： 　　1）正向最大匹配法（由左到右的方向）； 　　2）逆向最大匹配法（由右到左的方向）； 　　3）最少切分（使每一句中切出的词数最小）； 　　4）双向最大匹配法（进行由左到右、由右到左两次扫描） 　　还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。 　　一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。 　　对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。 2、基于理解的分词方法 　　这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。 3、基于统计的分词方法 　　从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 　　另外一类是基于统计机器学习的方法。首先给出大量已经分词的文本，利用统计机器学习模型学习词语切分的规律（称为训练），从而实现对未知文本的切分。我们知道，汉语中各个字单独作词语的能力是不同的，此外有的字常常作为前缀出现，有的字缺常常作为后缀（“者”“性”），结合两个字相临时是否成词的信息，这样就得到了许多与分词有关的知识。这种方法就是充分利用汉语组词的规律来分词。这种方法的最大缺点是需要有大量预先分好词的语料作支撑，而且训练过程中时空开销极大。 　　到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。例如，海量科技的分词算法就采用“复方分词法”，所谓复方，就是像中西医结合般综合运用机械方法和知识方法。对于成熟的中文分词系统，需要多种算法综合处理问题。 有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。 1、歧义识别 　　歧义是指同样的一句话，可能有两种或者更多的切分方法。主要的歧义有两种：交集型歧义和组合型歧义，例如：表面的，因为“表面”和“面的”都是词，那么这个短语就可以分成“表面 的”和“表 面的”。这种称为交集型歧义（交叉歧义）。像这种交集型歧义十分常见，前面举的“和服”的例子，其实就是因为交集型歧义引起的错误。“化妆和服装”可以分成“化妆 和 服装”或者“化妆 和服 装”。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。 　　交集型歧义相对组合型歧义来说是还算比较容易处理，组合型歧义就必需根据整个句子来判断了。例如，在句子“这个门把手坏了”中，“把手”是个词，但在句子“请把手拿开”中，“把手”就不是一个词；在句子“将军任命了一名中将”中，“中将”是个词，但在句子“产量三年中将增长两倍”中，“中将”就不再是词。这些词计算机又如何去识别? 　　如果交集型歧义和组合型歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：“乒乓球拍卖完了”，可以切分成“乒乓 球拍 卖 完 了”、也可切分成“乒乓球 拍卖 完 了”，如果没有上下文其他的句子，恐怕谁也不知道“拍卖”在这里算不算一个词。 2、新词识别 　　命名实体（人名、地名）、新词，专业术语称为未登录词。也就是那些在分词词典中没有收录，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解。句子“王军虎去广州了”中，“王军虎”是个词，因为是一个人的名字，但要是让计算机去识别就困难了。如果把“王军虎”做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项既不划算又巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子“王军虎头虎脑的”中，“王军虎”还能不能算词？ 　　除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。","title":"关于中文分词"},{"content":"12月16日消息，语音虚拟助理软件Vlingo公司高层日前集体访华，该公司向中国媒体介绍了Vlingo在语音互动领域的优势，同时透露诺基亚、三星、黑莓等企业部分产品都已经预装了Vlingo产品，他们希望跟中国的运营商、手机制造商合作，把Vlingo的语音虚拟技术和产品带到中国市场，变成面向中国市场的Siri的杀手。 Vlingo首席科学家舒晗介绍，Vlingo公司成立于2006年，使命是要成为用户最佳的语音虚拟助理，现在员工超过105人，其中有10位拥有博士学位。公司总部位于美国剑桥市，同时在韩国首尔也设有亚洲分部。Vlingo的核心技术有两个方面：一方面在计算机云上面的语音识别技术；还有在计算机云上面的自然语言处理技术。这两个核心技术现在已经在手机、智能电视和车载设备上实行了一系列的产品。 Vlingo公司全球营销执行副总裁Christopher Barnett表示，Vlingo已经预装到全球几家著名的手机品牌上，其中包括诺基亚、三星、黑莓等等，这些预装的手机一些已经投入了中国市场。在Android、黑莓的市场，还有苹果的应用商店里，Vlingo的产品已经被下载超过1100万次。 语音市场年增长率翻番 舒晗介绍，第一代语音产品是用语音来进行网络搜索，这是从2008年4月开始的，Vlingo在世界范围内首推的用语音进行网络搜索产品。今年初的时候，百度也有了同样的产品。 第二代技术叫语音动作，语音动作可以做更多的应用，包括发短信、导航、打电话等等。 第三代产品叫语音虚拟助理，第三代产品跟第二代产品的不同之处在于，在用户没有把需要的内容告诉Vlingo的时候，Vlingo可以跟用户对话，用对话的办法问用户一些不同的问题，这样用户可以再补充一些内容。从用户满意度的角度，这三代产品是越来越好。 Christopher Barnett表示，现在语音识别市场是个蓝海，还有很多机会。从Vlingo的市场表现来看，年增长率都是翻番的，他预计这一市场明年跟今年比的话，很可能也是翻番的。 渴望与中国企业合作 Christopher Barnett表示，现在苹果还没有把Siri这个产品带到中国来，也不会把这个软件从苹果的操作系统放到Android或者其他的手机上，但是Vlingo可以做到这点。Vlingo希望跟中国的运营商、手机制造商合作，把Vlingo的语音虚拟技术和产品带到中国市场，在不同的手机操作系统上带到中国市场。 据悉，语音虚拟助理核心技术有两种：第一种是用户在用语音跟手机对话的时候，先要把语音变成文字，这个核心技术叫语音识别；要对文字进行更深刻的处理，这个技术叫做自然语言处理跟对话管理。Vlingo公司跟苹果公司在拥有的技术上是非常相似的，但是在语音识别这个技术上，Vlingo公司是具有自己的技术，苹果公司的技术是从另外一个公司来的，是通过合作伙伴来做的。 Christopher Barnett表示如果中国的这些公司希望自己来建这些技术、从零开始，这会非常困难，而且时间会很长。Vlingo公司愿意直接跟本地的一些公司进行合作，这样的话很快就可以在成本相对比较少的情况下把这套技术带到中国市场。 同时他透露，Vlingo公司的技术和产品不限于在智能手机上，它可以放在个人电脑上，也可以放在智能电视上，也可以放在车用设备上。因为Vlingo公司的技术是基于在计算机云上面的服务，这个技术通过SDK的办法，非常容易跟合作伙伴配合，所以跟Vlingo公司的合作有两个办法：一个办法是把Vlingo公司的软件用贴牌的办法，放在本地的手机制造商的手机上；或者可以用SDK的办法，把他们的软件变成语音化。 （2011-12-16，文章来源：http://tech.qq.com/a/20111216/000401.htm?pgv_ref=aio2012&ptlang=2052）","title":"Vlingo欲进中国语音服务市场 对抗苹果Siri"},{"content":"讲述信息产业历史或是硅谷传奇的图书，10年来不知看了有多少，有国外作者作品，也有国内“编著”作品，多数是道听途说、东拼西凑而成的大路货，几乎没有一本能像《浪潮之巅》这样让我感觉如此震撼，视野宏大，观点独到。读到精彩处真是热血沸腾，不忍释卷。作为一位信息技术产业从业者，如果想更透彻地了解整个产业，鉴信息技术之兴衰，考互联网之得失，《浪潮之巅》是必读之书。如果你是一位互联网创业者，尤其不要错过这本杰出之作。 ——冯大辉／丁香园CTO   《浪潮之巅》是我在2011年全年看到的最好的一本书，没有之一。好书大致分两种，一种讲“道”，一种讲“术”。“道”给人指明方向，“术”给人提供解决问题的工具。吴军老师这本书讲的就是IT产业的“道”。在读本书之前，我对于IT产业有了解，但都是碎片化的，如同盲人摸象。阅毕本书，我终于将IT这头大象的腿、耳朵、尾巴连了起来，形成了一张完整的IT产业地图。这种融会贯通的感觉，着实令人酣畅淋漓。力荐！ ——许维／《天下网商》执行主编   读《浪潮之巅》，品IT历史风云，每一个IT从业者都应该看看这本书。行业浪潮汹涌而来，你不改变自己，就要被改变。只有那些拥抱变化、顺势而为，甚至勇于否定自己以往成功经验的公司，才能保持基业长青。吴军博士在书中最后一章“下一个Google”对中国互联网的发展做出了大胆的预测，至于你信不信，由你，我反正是信了。 ——刘振飞／淘宝技术保障副总裁   《浪潮之巅》中的“浪潮”指的是互联网和IT行业的发展浪潮，Just-Pub的周筠老师称这本书是“所有立志进入IT行业的年轻人必读”。而我觉得，这本书还值得商科同学一读；再往小了说，这本书值得学习金融的同学一读，因为不论你将来从事卖方抑或买方的工作，不管你是要配置一个资产组合、为一家IT公司承销上市、为一家新创的互联网公司做风投，你都需要了解互联网和IT行业。从这个意义来讲，我觉得这本书是很好的入门书籍。 ——蒋科／浙江大学商学院李志文班 除了当年第一次读金庸的武侠以外，《浪潮之巅》是我到目前为止第一本看过第一遍之后马上就想重读一遍的书。因为第一次读时，我真的像饥饿的人看到面包一样，被心里的快感控制住，贪婪地读下去。因有太多的地方点燃了灵感，反而无法消化。而在第二次读的时候，这种“快感”才会消失，才能平静下来吸收营养。因此，精读的时候才是真正地把这本书的精华吸收进来的过程，为此，我也付出了好几个不眠之夜。 —— 吴楠／大连海事大学信息科学技术学院副教授 终于把吴军博士所著的《浪潮之巅》读完了，由于工作太忙，读这本书的时间基本上是在上下班坐地铁的时候。平日在地铁上的单程要45分钟，这时候会十分枯燥，但这几天在地铁上读这本书， 45分钟却过得异常的快，每到最后一站时，都意犹未尽。 —— 龚天乙／ StarryMedia(星点传媒 )CTO 《浪潮之巅》是近期科技、商业领域一部不可多得的好书，我想，它甚至在“史学”书架里也预订了属于自己的一个格子。个人电脑带来的技术革命浪潮奔涌了三十年，每个人都被它改变，连时代都以“信息”二字命名。那些弄潮者是如此神奇，它们的波澜壮阔与分崩离析，一定会被人们津津乐道。因为资本主义地球下的企业，就像冷兵器时代的乱战之国，每一个革命性的新技术或致敌死命的市场行为，就像英雄的宝剑神兵或暗器剧毒。再过二十年，它们可能都会成为电台评书的题材，当人们说起“世纪之交，经济衰退”，就如同袁阔成拍响惊堂木后的“东汉末年，群雄并起”一样。 ——刘阳子／《中国知识产权报》记者 我认为不懂历史的人是不懂现在，也看不到未来的。作为一个互联网行业、更大一点说是信息技术行业的从业者，不懂历史就是不知道自己的位置，不知道自己的位置就会迷失。《浪潮之巅》这部IT史记就是告诉我们自己在哪里，追逐的又是什么。 《浪潮之巅》的价值不在于陈述了事实，而在于总结出来的规律，所以写历史的人不是光知道史实就可以了，更重要的是他的理解和总结。 吴军作为一个计算机博士，不仅有对技术的理解，还有对行业的看法，以及非常好的文笔。我觉得作为一个技术人员来说，千万不能把自己局限于一个方面，广泛地获取各方面的知识，开拓自己的视野，给自己画个圈圈起来是一件可怕的事情。 —— 潘晓良／百姓网技术总监 对于一个 IT互联网行业的产品经理来说，没有什么比这样一本书更符合我的需求了。对于大势的理解和对科技潮流之趋向的把握，是吴军博士所擅长的。这本力作也不会让人失望，希望所有产品经理都能精读此书。 也许是受了“细节就是生产力”的洗脑，平时的工作总是太关注细节，却忘了抬起头来看一看剧烈变化的天空。相信大多数IT从业者也有这个迫切的需求，希望有人能给我们讲一讲 IT的历史、科技的历史。就像史蒂芬 •霍金的《时间简史》为许多人开启了现代物理学窥豹之路，相信这本《浪潮之巅》也能为IT和互联网从业者打开一扇全新的大门。 —— chrisyxj／豆瓣读者 看 Google黑板报连载的时候就很好奇，一个搞技术搞研究的人，怎么能对这些事情了如指掌。之后听吴军老师介绍他在整个IT领域的经历才明白，只有这样的经历才能对全局有足够的把握：清华大学毕业；参与过中国最早对Apple II的仿制；在约翰 •霍普金斯大学师从名师，研究自然语言处理；在 Google的早期加入其中，经历了互联网在美国的潮起潮落；加入 Google中国，体验过国内互联网的种种是非；再到现在的腾讯。很难想象，没有这样的从业经历，如何能把握这类企业的沉浮。 —— 陈钢／中南大学计算机专业博士 《浪潮之巅》很好看，吴军以不输吴晓波《激荡三十年》的文笔，沿着技术的脉络记录了近年的IT风云变幻。有的内容以前在 Google黑板报上看到过，但是网络阅读的感觉远远比不上纸本书在手中所能激起的豪迈。书要仔细地读，事要认真地做。浪潮之巅，碎浪如烟。 —— 李杨／展横智远总经理 看着吴军的《浪潮之巅》，既莫名激动，又绝望透顶，五味杂陈。激动是因为看到希望在升腾，绝望是因为看到硅谷的基因是创新和最先进的生产关系后，让人身处异地地感到机会就像烟雾般即刻消散。超级好的一本书，IT创业者、从业者必读，不推荐心里不安。 —— 李巍／工信部电信研究院高级工程师 这是一部讲述了近百年来工业界发展史的书，其中有科技巨人微软、苹果、惠普、IBM、AT&T等大公司的兴衰之路，也有对整个世界工业史的宏观叙述，还讲述了几个重要的商业模式、国际金融机构和世界经济操盘手。全书观点宏观而不空洞，跨时绵长而不累赘，故事传奇而不虚浮，评论精彩而不偏颇，是一部难得的史书。 对于非 IT从业人员来说，这本书读起来几乎没有什么阅读门槛，只要当作一本小说来读就可以了，就跟看故事会似的。对于IT从业人员来说，从这些传奇故事中吸取教训，开拓视野，无疑对于今后的事业会有很大的帮助。总而言之，这也算是一本“相见恨晚”的好书。 —— 严哲／华南理工大学软件学院08级学生、设计师、前端开发者 对于一个年轻人，一个学习语言出身，但是即将踏足互联网行业的人而言，吴军博士的《浪潮之巅》在一个恰如其分的时候出现在我的生命里。他用一种极朴实的语言和略带诙谐的口吻带我走进了充满厮杀和血雨腥风的互联网战国时代，让我一面对互联网行业的传奇和残酷叹为观止，一面又静静地开始思考互联网行业立足和发展的本质，不得不说，这是一次非常值得经历的阅读之旅！ —— Jennychou／译言网读者 这本书乍看书目好像是在讲IT发展史，但其实看完以后你会发现，从书里面补上的 IT发展史也许只占你收获的三分之一不到，更大的收获来自于其他方面。每个人的阅历和知识结构都不同，所以在接收完一块信息以后的体会也都不同。《浪潮之巅》给我最大的启发是，作为一个工程师，关心和学习一些经济知识真的非常有必要，这在通常情况下能让你更接近问题的本质。我们都说以史为鉴，看了吴军先生对AT&T，摩托罗拉等公司的兴衰分析，颇受启发。我们不仅看到了一家家公司沉浮的过程，还看到它们背后的原因（管理的、基因的、经济的），既有鱼吃，也有渔学，受益匪浅。 —— 刘志峰／思科系统（中国）研发有限公司杭州分公司","title":"《浪潮之巅》读者热评"},{"content":"最近想用信息论的方法做一个关于网络方面的研究，最大熵方法是一个很有意思的方法，无意中搜到下面这篇读书笔记，比自己总结的还要全面和详细，分享一下。 作者是北大软件学院的胡江堂，原文在这里：http://johnthu.spaces.live.com/blog/cns!2053CD511E6D5B1E!246.entry 1. 物理学的熵 2. 信息论的熵 3. 熵和主观概率（一个简单注释 4. 熵的性质 4.1. 当所有概率相等时，熵取得最大值 4.2. 小概率事件发生时携带的信息量比大概率事件发生时携带的信息量多 5. 最大熵原理：直觉讨论 6. 最大熵原理：一个手工例子 7. 最大熵原理：正式表述 8. 最大熵模型的训练：GIS算法 9. 最大熵模型：金融领域内的应用 参考文献 这 篇读书笔记主要写了对熵的理解、对最大熵原则的理解，还有一个手工计算的例子。在处理一般化的最大熵模型时，我采用了我偏爱的连续随机变量形式，而不是一 般有助于计算机理解的离散形式。连续而非离散的处理方式的一个好处就是，它能非常方便地推出最大熵模型的解是一个指数形式。如果使用离散形式，一样的结 论，那符号就看着复杂多了。 所有的东西都来自篇末的参考资料。 1. 物理学的熵 熵 是一个物理学概念，它是描述事物无序性的参数，熵越大则无序性越强。从宏观方面讲（根据热力学定律），一个体系的熵等于其可逆过程吸收或耗散的热量除以它 的绝对温度；从微观讲，熵是大量微观粒子的位置和速度的分布概率的函数。自然界的一个基本规律就是熵递增原理，即，一个孤立系统的熵，自发性地趋于极大， 随着熵的增加，有序状态逐步变为混沌状态，不可能自发地产生新的有序结构，这意味着自然界越变越无序。 2. 信息论的熵 在物理学中，熵是描述客观事物无序性的参数。信息论的开创者香农认为，信息（知识）是人们对事物了解的不确定性的消除或减少。他把不确定的程度称为信息熵。假设每种可能的状态都有概率，我们用关于被占据状态的未知信息来量化不确定性，这个信息熵即为： 其中是以2为底的对数，所以这个信息用位衡量。前面说过，在物理学的背景下，这个不确定性被称为熵（在通讯系统中，关于传输的实际信息的不确定性也被称为数据源的熵）。 扩展到连续情形。假设连续变量的概率密度函数是，与离散随机变量的熵的定义类似，信息熵的连续定义为： 上式就是我们定义的随机变量的微分熵。当被解释为一个随机连续向量时，就是的联合概率密度函数。 3. 熵和主观概率（一个简单注释） 因 为熵用概率表示，所以这涉及到主观概率。概率用于处理知识的缺乏（概率值为1表明对知识的完全掌握，这就不需要概率了），而一个人可能比另一个人有着更多 的知识，所以两个观察者可能会使用不同的概率分布，也就是说，概率（以及所有基于概率的物理量）都是主观的。在现代的主流概率论教材中，都采用这种主观概 率的处理方法。 4. 熵的性质 4.1. 当所有概率相等时，熵取得最大值 上面关于熵的公式有一个性质：假设可能状态的数量有限，当所有概率相等时，熵取得最大值。证明如下： 在只有两个状态的例子中，要使熵最大，每个状态发生的概率都是1/2，如下图所示： 4.2. 小概率事件发生时携带的信息量比大概率事件发生时携带的信息量多 证明略，可以简要说明一下，也挺直观的。如果事件发生的概率为1，在这种情况下，事件发生就没有什么“惊奇”了，并且不传达任何“信息”，因为我们已经知道这“信息”是什么，没有任何的“不确定”；反之，如果事件发生的概率很小，这就有更大的“惊奇”和有“信息”了。这里，“不确定”、“惊奇”和“信息”是相关的，信息量与事件发生的概率成反比。 5. 最大熵原理：直觉讨论 最 大熵原理是根据样本信息对某个未知分布做出推断的一种方法。日常生活中，很多事情的发生表现出一定的随机性，试验的结果往往是不确定的，而且也不知道这个 随机现象所服从的概率分布，所有的只有一些试验样本或样本特征，统计学常常关心的一个问题，在这种情况下如何对分布作出一个合理的推断？最大熵采取的原则 就是：保留全部的不确定性，将风险降到最小。在金融理论中，一个类似的教训是，为了降低风险，投资应该多样化，不要把所有的鸡蛋都放在一个篮子里。 吴 军（2006）举了一个例子。对一个均匀的骰子，问它每个面朝上的概率分别是多少。所有人都会说是1/6。这种“猜测”当然是对的，因为对这个“一无所知 ”的色子，假定它每一个朝上概率均等是最安全的做法，你不应该假设它被做了手脚。从信息论的角度讲，就是保留了最大的不确定性，让熵达到最大（从投资的角 度来看，这就是风险最小的做法）。但是，如果这个骰子被灌过铅，已知四点朝上的概率是1/3，在这种情况下，每个面朝上的概率是多少？当然，根据简单的条 件概率计算，除去四点的概率是 1/3外，其余的概率都是 2/15。也就是说，除已知的条件（四点概率为 1/3）必须满足外，对其它各点的概率，我们仍然无从知道，也只好认为它们相等。这种基于直觉的猜测之所以准确，是因为它恰好符合了最大熵原理。 回 到物理学例子中。在涉及物理系统的情形中，一般要确定该系统可以存在的多种状态，需要了解约束下的所有参数。比如能量、电荷和其他与每个状态相关的物理量 都假设为已知。为了完成这个任务常常需要量子力学。我们不假设在这个步骤系统处于特定状态；事实上我们假定我们不知道也不可能知道这一点，所以我们反而可 以处理被占据的每个状态的概率。这样把概率当作应对知识缺乏的一种方法。我们很自然地想避免假定了比我们实际有的更多的知识，最大熵原理就是完成这个的方 法。 这里可以总结出最大熵对待已知事物和未知事物的原则：承认已知事物（知识）；对未知事物不做任何假设，没有任何偏见。最大熵原理指 出，当我们需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设（不做主观假设，这点很重 要。）在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。我们常说，不要把所有的鸡蛋 放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。 6. 最大熵原理：一个手工例子 举 个例子，一个快餐店提供3种食品：汉堡(B)、鸡肉(C)、鱼(F)。价格分别是1元、2元、3元。已知人们在这家店的平均消费是1.75元，求顾客购买 这3种食品的概率。如果你假设一半人买鱼另一半人买鸡肉，那么根据熵公式，这不确定性就是1位（熵等于1）。但是这个假设很不合适，因为它超过了你所知道 的事情。我们已知的信息是： 以及关于对概率分布的不确定性度量，熵： 对前两个约束，两个未知概率可以由第三个量来表示，可以得到： 把上式代入熵的表达式中，熵就可以用单个概率来表示： 对这个单变量优化问题，很容易求出时熵最大，有，和。 总结一下。以上，我们根据未知的概率分布表示了约束条件，又用这些约束条件消去了两个变量，用剩下的变量表示熵，最后求出了熵最大时剩余变量的值，结果就求出了一个符合约束条件的概率分布，它有最大不确定性，我们在概率估计中没有引入任何偏差。 7. 最大熵原理：正式表述 假 设有一个随机系统，已知一组状态，但不知道其概率，而且我们知道这些状态的概率分布的一些限制条件。这些限制条件或者是已知一定的总体平均值，或者是它们 的一些界限。在给定关于模型的先验知识的条件下，问题是选择一个在某种意义下最佳的概率模型。Jaynes(1957)提出了一个最大熵原则：当根据不完 整的信息作为依据进行推断时，应该由满足分布限制条件的具有最大熵的概率分布推得。也就是说，熵的概念在概率分布空间定义一种度量，使得具有较高熵的分布 比其它的分布具有更大的值。显然，“最大熵问题”是一个带约束的最优化问题。 为方便叙述，考虑最大微分熵 对所有随机变量的概率密度函数，满足以下约束条件： 其中，是的一个函数。约束1和约束2描述的是概率密度函数的基本属性，约束3定义变量的矩，它随函数的表达式不同而发生变化，它综合了随机变量的所有可用的先验知识。为了解这个约束最优化问题，利用拉格朗日乘子法，目标函数为： 其中， 是拉格朗日乘子。对被积函数求的微分，并令其为0，有： 解得： 我 们看到这个概率密度函数具有指数形式。匈牙利数学家Csiszar曾经证明，对任何一组不自相矛盾的信息，最大熵模型不仅存在，而且是唯一的。而且它们都 有同一个非常简单的形式 -- 指数函数。我们还可以得到，在所有零均值随机向量可达到的微分熵中，多元正态分布具有最大的微分熵。最大熵的解，同时是最吻合样本数据分布的解。 8. 最大熵模型的训练：GIS算法和其他 上 节我们得到，一个最大熵模型可以有效地把各种信息综合在一起（无偏见地对待不确定性），而且具有指数函数的形式，下面模型的训练就要确定这个指数函数的各 个参数。最原始的最大熵模型的训练方法是一种称为通用迭代算法 GIS(generalized iterative scaling) 的迭代算法，由 Darroch 和 Ratcliff 在七十年代提出，大致可以概括为以下几个步骤： 1. 假定第零次迭代的初始模型为等概率的均匀分布。 2. 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，如果超过了实际 的，就把相应的模型参数变小；否则，将它们便大。 3. 重复步骤 2 直到收敛。 Darroch 和 Ratcliff没有能对这种算法的物理含义进行很好地解释，后来是由Csiszar解释清楚的，因此，人们在谈到这个算法 时，总是同时引用 Darroch 和Ratcliff 以及希萨的两篇论文。GIS 算法每次迭代的时间都很长，需要迭代很多次才能收敛，而且不太稳定，即使在 64 位计算机上都会出现溢出。因此，在实际应用中很少有人真正使用，大家只是通过它来了解最大熵模型的算法。 八十年代，Della Pietra在IBM对GIS算法进行了两方面的改进，提出了改进迭代算法IIS（improved iterative scaling）。这使得最大熵模型的训练时间缩短了一到两个数量级。这样最大熵模型才有可能变得实用。即使如此，在当时也只有 IBM 有条件是用最大熵模型。 由于最大熵模型在数学上十分完美，对科学家们有很大的诱惑力，因此不少研究者试图把自己的问题用一个类似最大熵 的近似模型去套。谁知这一近似，最大熵模型就变得不完美了，结果可想而知，比打补丁的凑合的方法也好不了多少。于是，不少热心人又放弃了这种方法。第一个 在实际信息处理应用中验证了最大熵模型的优势的，是原IBM现微软的研究员Adwait Ratnaparkhi。Ratnaparkhi的聪明之处在于他没有对最大熵模型进行近似，而是找到了几个最适合用最大熵模型、而计算量相对不太大的自 然语言处理问 题，比如词性标注和句法分析。拉纳帕提成功地将上下文信息、词性（名词、动词和形容词等）、句子成分（主谓宾）通过最大熵模型结合起来，做出了当时世界上 最好的词性标识系统和句法分析器。 9. 最大熵模型：金融领域内的应用 最大熵模型在自然语言处理领域 内得到了广泛的应用，在金融界，也能见到它的影子。当年最早改进最大熵模型算法的Della Pietra在九十年代初退出了学术界，而到在金融界大显身手。他和很多IBM语音识别的同事一同到了一家当时还不大，但现在是世界上最成功对冲基金公司 ----(Renaissance Technologies。我们知道，决定股票涨落的因素可能有几十甚至上百种，而最大熵方法恰恰能找到一个同时满足成千上万种不同条件的模型。 Della Pietra等科学家在那里，用于最大熵模型和其他一些先进的数学工具对股票预测，获得了巨大的成功。从该基金1988 年创立至今，它的净回报率高达平均每年34%。也就是说，如果1988年你在该基金投入一块钱，今天你能得到200块钱。这个业绩，远远超过股神巴菲特的 旗舰公司Berkshire Hathaway（同期，Berkshire Hathaway的总回报是16倍）。 参考文献 1. 吴军《数学之美系列十六（上）-不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型》，http://googlechinablog.com/2006/10/blog-post.html 2. 吴军《数学之美系列十六（下）-不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型》，http://googlechinablog.com/2006/11/blog-post.html 3. Jaynes, E.T., 1957. ”Information Theory and Statistical Mechanics”, Physical Review, vol.106, pp.620-630. http://bayes.wustl.edu/etj/articles/theory.1.pdf 4. Haykin, Simon《神经网络原理》（第10章 信息论模型，叶世伟等译，北京：机械工业出版社，2004） 5. 王厚峰. 机器学习课程讲义之六MEM (Maximum Entropy Model).北京大学软件与微电子学院，2007年春季学期 6. Penfield, Paul. Information and Entrop. MIT Open Course, Spring 2003. http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-050JInformation-and-EntropySpring2003/CourseHome/index.htm 7. Wei, Xiaoliang《最大熵模型与自然语言处理》www.cs.caltech.edu/~weixl/research/read/summary/MaxEnt2.ppt 8. 常宝宝《自然语言处理的最大熵模型》www.icl.pku.cn/WebData_http-dir-listable/ICLseminars/2003spring/最大熵模型.pdf 9. 廖先桃《最大熵理论及其应用》http://ir.hit.edu.cn/phpwebsite/index.php?module=documents&JAS_DocumentManager_op=downloadFile&JAS_File_id=196","title":"最大熵模型"},{"content":"  人工智能（Artificial Intelligence，AI）、机器学习(Machine Leaining，ML)、模式识别（Pattern Recognition，PR）、数据挖掘（Data Mining, DM），他们要解决的核心问题不同，但是运用的数学模型如出一撤，主要是统计学方法。   人工智能：就是计算机自动做决策，包含其他几个。 机器学习：研究重点是算法的学习过程，强调的是一个反馈的、自我完善的框架，是人工智能的一个分支。 模式识别：就是分类问题，是机器学习的一个方面，包括监督法和非监督法分类。 数据挖掘：就是在大型数据库上的机器学习应用，偏重于从大型数据库中找规律的应用方面。 自然语言处理NLP：   主要的统计学方法：  回归分析、决策树、贝叶斯学习、支持向量机SVM、PageRank 、K-Means、CRF条件随机场、隐马尔可夫模型。   开源软件包： Weka CRF++","title":"人工智能、机器学习、模式识别、数据挖掘、自然语言处理"},{"content":"  1. 背景介绍     搜索引擎的输入查询中，有相当一部分是带输入错误的查询。而提交有错误的查询给搜索引擎，通常得不到好的搜索结果，返回结果数很少并且和用户的期待相差很远，会严重影响了用户的使用体验。 如：                                 “火箭队对振勇士直拨” 直接用这样的查询，显然不可能给用户找到正确的网页。 但是，如果能自动地对查询进行纠错，修改成符合用户原本意图的查询“火箭队对阵勇士直播”，就可以返回适当的网页，增加用户对搜索引擎的良好体验。     据统计，在英文查询串中，有Spelling Error的查询串占到了总查询串的10%~15% (Cucerzan & Brill 2004)，而中文有输入错误的查询串保守估计也要占到所有查询串的5%~10%左右。所以，查询纠错（Query Correction）就成为网页搜索引擎研发过程中的一项非常重要的技术， 其主要任务是通过修正用户的有错误查询，以期望得到符合用户需求的查询结果，从而保证搜索引擎查询结果的正确率和召回率。     需要说明的是，由于中文汉字是笔画构字，不同与英文单词的字母构字，所以在输入上，不存在输入字库中没有的汉字，所以中文查询没有传统意义上的“错字”，而只有“别字”，这点和英文查询不同。从输入错误所占的比例来看，所有中文查询中，输入有误的查询所占比例也要小于英文的相应比例。 2. 相关工作     有关查询纠错的研究工作由来已久，但是中文查询纠错的的研究论文相对较少，大量的工作集中在英文的查询纠错上，但是基本原理是相通的。所以这里以英文的查询纠错为背景，介绍一下在英文查询纠错方面已有的一些进展。 查询纠错的任务始终集中在原有的任务设定上，即只纠正具有明显错误的查询。基于这样的任务设定，英文查询纠错有两条主要的研究路线，分别针对不同的纠错对象： (1) 拼写错误的词纠错 (Non-word error spelling correction)     有拼写错误的词是指那些不存在于词表中的“词”，查询中出现这样的词，其修正的目标就是用词表中正确的对应词替代它。拼写错误查询纠错的工作重点在于为查询中每一个不存在于词表的词生成一个候选词表，并为每一个候选词表按排序。传统上，侯选词的排序主要基于人工设定的分数：给不同的编辑操作(edit operation)赋予不同的得分；同时结合侯选词的频率，候选词本身在语料库中出现的频率较高的，排序可能靠前 (Damerau, 1964; Levenshtein, 1966)。这可以看成是一种基于规则的处理方法。      近些年来， 统计模型被广泛应用于自然语言处理的各个任务，包括查询串纠错。在 (Kernighan et al, 1990) 的工作基础上，(Brill & Moore, 2000) 提出一种改进的基于噪声信道模型的纠错方法，允许一般性的串到串的编辑操作，这种方法有助于对一些重要的认知错误，如\"le\"和\"al\"的混淆，进行建模。而(Toutanova & Moore 2002) 通过对英文单词“发音”信息的显式建模， 进一步改进了基于噪声信道模型的纠错。这两个基于噪声信道模型的方法都需要有一个错误拼写词/正确拼写词的对照表用于训练，而后者还需要一个词的发音词典。近年来，(Ahmad & Kondrak, 2005)发现还可以从用户查询日志来学习这样的模型，体现了用户查询日志的重要作用。与此类似的工作还有(Martin, 2004)，但是是从一个非常大的未预处理的生文本中学习的。 基本上，所有针对拼写错误的查询纠错，重点都在当前词本身，而很少考虑上下文信息。 (2) 真实词纠错 (Real-word spelling correction) 真实词纠错也可以称作上下文敏感的词纠错。它试图检测当前特定上下文环境下，一个词表中存在词的不恰当的使用。也就是说待纠错的词本身是存在的，但是在当前上下文中不适用。针对这个任务的常规做法是使用一个预先定义好的混淆集(Confusion Set)，如 (Golding & Roth, 1996) 和 (Mangu & Brill, 1997)的工作。和拼写错误纠错不同的是，对于真实词不适用的判断，需要更多的上下文信息。 近年来英文查询纠错已经不太区分词本身是有错误还是不适用，都可以在统一的机器学习的框架下来解决，(Li et al, 2006)就对比了噪声信道模型和最大熵模型。另外，把查询纠错和其他查询优化任务结合起来，统一处理，(Guo et al, 2008) 也做了一些尝试。 3. 中文查询纠错的解决方案 中文的查询纠错，本质上可以借鉴英文真实词的纠错方法（因为查询中不可能存在“错字”，只可能是“别字”），这里提供了一个中文纠错的解决方案：基本上可以分为三个重要的步骤来实现： (1) 首先，从带标记的错误查询/正确查询的对齐语料和特定的语言资源中挖掘查询中可以用于替换的<错误片段, 正确片段>，包括汉字字符形式的和拼音形式的， 这是基本的纠错资源。     (2) 其次，在查询中出现了疑似错误片段时，尝试把疑似错误片段替换为相应的正确片段，遍历所有可能性之后，会形成多个可能的查询候选（包括原来的查询）。     (3) 最后，假如包括原有查询在内有N个候选查询，这N个候选查询会统一按一定策略进行打分排序，得分最高的候选查询不是原有查询的话，那就是原有查询的修正。 3.1 纠错对获取 从以往对齐的“错误查询/正确查询”数据中自动挖掘曾经被用作替换的<错误片断, 正确片断>，比如之前的纠错片断<直拨, 直播>， 可能是在“视频直拨”/“视频直播”中出现过的，那么抛开对齐语料的特定上下文，<直拨,直播>就被抽取出来。在另外没有见过的上下文中，如“火箭队比赛直拨”，替换后，“火箭队比赛直播”就是原来查询“火箭队比赛直拨”的候选修改查询。这是所谓的“汉字字符纠错对”。 字符纠错对通常覆盖面比较窄，比如还有“指拨”到“直播”的纠错可能，但显然，这样的字符错误层出不穷，所以中文查询纠错还需要引入拼音纠错对，如<zhi'bo' , 直播>。这样只要查询中出现了和“zhi'bo' ”同音的片段， 就可以尝试替换成“直播”。基于拼音的替换大大增加了纠错的可能，纠错覆盖度提高不少。 拼音纠错对可以从中高频常用词词表生成，以满足一般性的词语替换。也可以从搜索引擎的查询日志中挖掘高频、较新的查询词，按照拼音转化成拼音纠错对，以满足未登陆词，OOV(Out of Vocabulary)，的词语替换。这样兼顾了一般性和特殊性的拼音纠错。 3.2 候选查询串生成      在经历了纠错对的替换后，每一次替换，会增加若干候选查询。 需要说明的是，存在一个错误片段，对应着多个可能正确片段的情况。 如“升至”，在以往的纠错历史上对应多个正确替换目标：“升值”、“圣旨”、“升职”等，就会形成多个查询候选。比如一个较复杂的例子： “QQ农场荒钻在那里领化肥” 其中 “荒钻”可以替换成“黄钻”， “那里”可以替换成“哪里”， “话费”可以替换成“花费”。其示意图如下图所示，其中每个节点是一个词切分单元。                                   图. 候选查询生成示意图 从“开始”到“结束”总共有八条路径，分别对应八个候选查询（包括原串） (1)  “QQ农场荒钻在那里领化肥” （原串） (2)  “QQ农场荒钻在那里领花费” (3)  “QQ农场荒钻在哪里领化肥” (4)  “QQ农场荒钻在哪里领花费” (5)  “QQ农场黄钻在那里领化肥” (6)  “QQ农场黄钻在那里领花费” (7)  “QQ农场黄钻在哪里领化肥” (8)  “QQ农场黄钻在哪里领花费” 从一般性的常识来看，(7)才是最优的查询。 后续的工作就是要从这些查询候选串中找到这个最优的查询串。 3.3  寻找最优的查询串     从多个候选中找到最优的结果，其本质上都是一个排序的过程。而排序的依据，则根据任务的不同，会有不同的算法。在中文查询纠错的任务中，可用于给候选查询打分的信息是多样性的，比如候选查询本身的质量如何，就可以用历史上这个查询被提交给搜索引擎后用户的行为来刻画。也可以用这个候选查询在多大程度上符合人的“语言习惯”来刻画它的质量。其他的判断依据还包括拼音是否一致（主要针对字符纠错而言），所用到替换在历史上是否是一个好的替换等。这些信息可以单独用于候选串的排序，也可以组合起来，相互平衡，共同决定所有候选查询的相对质量高低。 3.3.1 用户点击信息 (Click-Through Info.) 在用户的查询历史上，一个候选查询可能是用户多次提交过的，那么可以根据用户的历史点击记录来判断这个候选查询的质量。一个简单的思路就是考虑一个候选查询被不同用户提交的次数越多，那么这个候选查询的热门程度和无错误的可能性就越高，同时，如果这个候选查询被提交后，用户紧接着有点击搜索引擎返回结果的行为，且比例较高，则也可能说明这个候选查询的质量较高。 这两个因素都是和查询串质量成正比的，所以可以表示成： 其中Quality(Query) 表示候选查询Query的质量，T(Query)表示候选查询Query被提交的次数，δ(Query)表示Query提交给搜索引擎后，用户点击了返回结果的比例。按这个公式的计算， Quality(Query)实质上是Query提交后用户点击了搜索引擎返回结果的次数。从直观上来看，这个数值越大，说明这个候选查询的质量越高。当然还可以有其他的利用用户点击信息的方式和算法来评估查询的质量， 这里不一一赘述。 3.3.2 语言模型（Language Model）     在自然语言处理(Natural Language Processing, NLP)中，语言模型常用来刻画一个语言片段的合理性。就中文查询纠错，对一个候选查询()合理性的评估应用语言模型，实际上是评价这个候选查询在多大程度上符合汉语的语言事实。用这个串各个词之间的联合概率来表示： 这个联合概率的计算，理想的语言模型计算过程应该是：        但通常由于数据稀疏，高阶单元，比如这里的P(w4|w3,w2,w1)很难在训练数据中观察到足够的次数，所以往往引入Markov假设，按Markov链的特性进行简化： 其中k 是模型中设定的历史词个数，也是模型的阶数，也就是说wi只和它前面的k个词相关。由于目前能获得语言资源有限，所以k一般只取到2以内。对大部分自然语言处理任务而言，基本上也够用了。为了书写方便， 这里举一个简单的例子来说明语言模型的应用。如“大喊民国”这个查询，在局部修改之后，对应如下的候选查询： (1) 大瀚民国 (2) 打鼾民国 (3) 大韩民国 (4) 大汉民国 (5) 大喊民国 当取k=1时（二元语言模型），每个候选查询的合理性评估就是：  可以预见，一个估计合理的语言模型，可以计算得到 在实际应用中还需要知道各个概率单元的值。在数据量较为充足的情况下，通常使用最大似然估计(MLE)来获得每个概率单元的值。 其中#(wi)表示在观察语料中，wi出现的次数。“”表示任意词，#(·)就表示所有词出现的次数，基本上是语料库的大小了。     如果取k=1， 其中#(wi-1wi)表示在观察语料中，wi-1，wi邻接出现的次数。“”表示任意词，#(wi-1·)就表示wi-1在前，所有邻接词对的个数，基本等同于#(wi-1)。用到的观察语料需要“用语习惯”比较符合任务要求，在查询纠错中可以使用已有的正确的查询日志来计算这些概率单元。 尽管不符合语言学家的口味，语言模型在很多任务中证明是有效的。可能遇到的主要问题就是数据稀疏，也就是用的语料规模有限，最大似然估计不能准确的逼近实际概率。另外有些情况下概率会为0。比如wi在语料中没有出现过。常常得进一步修正概率估计的方式，可以有两个方案： (1) 平滑策略； (2) 上位概念策略； 对语言模型的平滑策略有很多种，具体可以参考相关论文(Chen and Goodman, 1998)。有一种比较通用的做法就是，在任务专用语料不足的情况下，可以用通用语料估计一个概率值，和专用语料进行线性平滑：      其他的概率单元计算类似。 另外，上位概念也可以解决部分数据稀疏问题，因为具体一个词出现的次数可能较少或者没有，但是它的上位概念出现的次数就可能足够多。在语义上可以用上位概念代替这个词。识别“超人归来”是个电影名，就可以计算相关的候选串概率： 显然语料中<电影>出现的次数会多很多。直观上看可以得到：   3.3.3 其他可用于排序的信息 除了用户点击信息和语言模型外，还可以采用其他可用于比较两个候选查询质量相对高低的信息。 (1) 词切分个数。一般情况下有错误的查询串，分词后词个数相对较多，因为错误的输入往往不能和前后字构成词，会单字成词，形成“碎片”的概率很高。 比如“QQ农场荒钻在哪里领化肥”，“荒钻”会被切分成两个单字词，“荒”和“钻”；而“QQ农场黄钻在哪里领化肥”中的“黄钻”是一个词。这就使得“QQ农场荒钻在哪里领化肥”整体的切词个数多于“QQ农场黄钻在哪里领化肥”的切词个数。分词个数能在一定程度上反映两个候选查询的相对质量高低，但从统计意义上来看，是一个比较弱的特征，应该和其他特征一起使用。 (2) 片段替换是否是一个好的替换。 一个替换，如<下栽,  下载> ，在纠错历史上经常出现这样的替换，则说明这是一个好的替换，反之则是一个不好的替换。 这里的“经常”需要统计替换次数后，用一个阈值来刻画。大于等于阈值的，是好的替换；否则是一个不好的替换。当然也可以用其他的策略来判断一个替换的好坏。     3.4 利用分类机制实现排序 评价两个候选查询之间的相对质量好坏，可以用一个训练好的分类器综合所有特征信息来排序，比如支持向量机(SVM)。使用支持向量机，需要定义一个两个候选查询对比的输入空间（Input Space） X。在这个输入空间上，对应的向量是两个候选查询的对比向量。就前面的讨论，可以定义一个5维的向量空间表示查询串q1和q2的差异，其中各个空间纬度代表的含义如下： F1： 表示q1的用户点击信息相对于q2的用户点击信息的倍数； F2： 表示 q1的一元语言模型得分相对于q2的一元语言模型得分的倍数； F3： 表示q1的二元语言模型得分相对于q2的二元语言模型得分的倍数； F4： 表示q1的词切分个数少于q2的词切分个数； F5： 表示q2到q1的片段替换是否是一个好的替换，1是好的替换，否则为0； 有了输入空间的定义， 人工标记一部分训练数据(q1, q2, y)。 其中y=1表示q1的质量好于q2。y =-1，表示q1的质量差于q2。 按照定义的输入空间，生成特征向量，然后可以用Traditional SVM训练分割对比向量为两个类别（一个类别代表质量相对较高，一个类类别代表质量相对较低），得到广义的最优分类超平面。在实际应用中，如果一个新的(Q, Q')对应的对比特征向量到这个超平面的距离是d，则： 有了候选查询的两两相对排序，就可以给所有的候选查询排序。排序最高的候选查询，如果不是原始查询的话，那么就是原始查询的一个最优修改。 4. 总结 本文概述了查询纠错的研究背景和用于中文查询纠错的一个解决方案。重点阐述了这个解决方案的重要组成部分。 由于只是概述，所以很多内容仅仅介绍了基本思想，没有做进一步的探讨。如生成候选查询串的时候，不是所有的候选查询串都需要保留下来，可以先粗筛选，用1-2个判别特征就可以筛除掉大部分质量肯定不高的候选查询，最优查询的挑选空间就会小很多。减轻后续步骤的压力。另外从常用词表生成拼音纠错对也需要一定的策略，不然生成的候选查询集合将过于庞大。本文中文查询纠错的解决方案主要是基于统计的策略，所以不可能做到百分之百准确， 还需要从更多的角度来深入挖掘、设计适合查询纠错的特征、模型和算法。 更多有关查询纠错的研究工作，可关注WWW, SIGIR, ACL, EMNLP等国际知名会议的相关论文。 参考文献 1. Cucerzan S. and Brill E. Spelling correction as an iterative process that exploits the collective knowledge of web users. Proceedings of EMNLP'04, pages 293-300, 2004. 2. Damerau F. A technique for computer detection and correction of spelling errors. Communication of the ACM 7(3):659-664, 1964. 3. Levenshtein V. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physice–Doklady 10: 707-710, 1966. 4. Kernighan M. D., Church K. W. and Gale W. A. A spelling correction program based on a noisy channel model. Proceedings of COLING 1990, pages 205-210, 1990. 5. Brill E. and Moore R. C. An improved error model for noisy channel spelling correction. Proceedings of 38th annual meeting of the ACL, pages 286-293, 2000. 6. Toutanova K. and Moore R. Pronunciation modeling for improved spelling correction. Proceedings of the 40th annual meeting of ACL, pages 144-151, 2002. 7. Ahmad F. and Grzegorz Kondrak G. Learning a spelling error model from search query logs. Proceedings of EMNLP 2005, pages 955-962, 2005. 8. Golding A. R. and Roth D. Applying winnow to context-sensitive spelling correction. Proceedings of ICML 1996, pages 182-190, 1996. 9. Mangu L. and Eric Brill E. Automatic rule acquisition for spelling correction. Proceedings of ICML 1997, pages 734-741, 1997. 10. Li M., Zhu M. H., Zhang Y. and Zhou M. Exploring distributional similarity based models for query spelling correction. Proceedings of COLING-ACL 2006, pages 1025-1032, 2006. 11. Guo J., Xu G., Li H., Cheng X., A Unified and Discriminative Model for Query Refinement, Proceeding of SIGIR 2008, 379-386. 2008. 12. Chen, S. F. and Goodman, J.: An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report: TR-10-98, Harvard University. 1998.","title":"网页搜索中查询纠错概述"},{"content":"本来是转载自阳志平《不上大学，可能缺少的是什么？》，当然原文以及其评论有正面的，当然有负面的。大体意思是不上学，可以通过这些自愿使得自己成功。 但是这里我理解的话，大学培养的更重要的是一个人的思维以及觉悟，资源和工具都是为我所用的，在有了觉悟的基础上，下面的资源似乎才变得更加有用。 如果不上大学，你会面临一些什么样的问题？ 人际：由于中国的畸形发展模式，资源日益集中到一二线城市，正在形成以超大城市集群为核心的发展模式，你将缺少在大型城市，一个正常的人际交往圈子； 技能：没有四年大学缓冲期，用于寻找自身的兴趣与培育技能，尤其是一些需要花费较多时间来培养的技能，可能导致基础不牢； 自信：对自身学习能力不再自信，厌倦学习，最终失去更新知识结构的动力； 敲门砖：多数场合，学历证书的确是迈入职场的敲门砖； 社会舆论：亲朋好友，包括部分爸妈会在不上大学与没有发展前途之间划等号； 未来方向：多数人的大学四年，给了他/她一个基本的职场锚，清楚毕业之后，大致朝哪个方向与领域发展，而你，更多将陷于一场又一场为生存而进行的斗争，最终没有任何职业方向可言，什么能糊口我就干什么，很容易陷入恶性循环，人生自由度日益低下。 如何突破困境？ 请首先使用批判性思维，认真思考上述问题。然后尝试写下你的答案，我如果不上大学，我如何拥有一个合适的人际交往圈子，具备谋生的手艺，以及拥有较高的人生自由度，自信地面对未来？ 以下常规性思维，真的有理吗？ 人际：考试成绩不好=我只配拥有社会上混的的朋友圈 技能：考试成绩不好=我离开学校之后，可以完全放弃学习 自信：考试成绩不好=我恐惧一切与书本相关的内容 敲门砖：考试成绩不好=我可以选择最大众化的发展路线，比如去广州富士康 社会舆论：考试成绩不好=我只能通过赚钱来向亲友证明自己 未来方向：考试成绩不好=我只能屈从于一些容易解决生存压力的工作 在大学诞生前后，无数例子都可以打败以上思维定势。除了成功学虚构的竭斯底里例子之外，世界上还存在一些用时间与耐心来突破这种困境的例子。比如，以下所列： 我是如何从煤矿工成为程序员的 珍妮的故事 Stay Hungry, Stay Foolish ！！ 不上大学，你可以学些什么？ 10年前，没有TED与斯坦福公开大学；没有hacker news与Y基金；没有github.com；没有app store与facebook平台；没有各类天使投资机构与创业者聚会… 今天，接受一流教育理念与卓越智慧如此容易。如果不上大学，可以学些什么？以下资源以图书为主，涉及部分网站。 元资源 通过它，可以找到更多资源。 TED：尖峰创想，启迪心智 斯坦福开放大学：免费读大学，最新的类似于IOS开发与Ruby开发系列课程，你可以免费找到 MIT开放课程：世界上最好的理工学院，关于人工智能等议题开启你的好奇心 YouTube:大量的学习视频都集中在这里 stackexchange：全世界的热心人士都集中在这里提问与回答问题 edge：牛人与牛人对话 PLOS：开源模式在学术界的应用，公共图书馆，你可以免费看到最新的论文；你也可以提交自己的论文 中文汉化： TEDtochina：TED中文社区，不仅仅是翻译。 网易公开课：有TED及各个国外大学的开放课程的翻译。 MyOOPS开放课程：来自全球顶尖大学的开放式课程，现在由世界各国的数千名义工志工为您翻译成中文。 东西网与译言：有部分edge翻译材料。 优酷:汉化了不少YouTube资源。 思维 对思维的深度思考与刻意练习，将拥有较高人生自由度。 图书 一般来说，以下推荐的图书组合，以中文图书为主，top10往往是必读，其他的可以泛读与选读。 心智黑客：如何基于心理学与脑科学为主的科学理论，来提升心智？ 网站 mindhacks：国外著名博客，内容以介绍心理学与脑科学研究为主。 BPS Research Digest：常有最新研究。 写作 中国的作文训练让我们丧失表达欲望。如果不上大学，你可以通过西式的创意写作课程补上这一课。当恢复对写作的热情之后，再从中国传统文化中汲取营养。 开始写作吧：以西方的创意写作课程图书为主。 编程 一种独特的手艺，轻松参与到国际化竞争的手段。 图书 全世界程序员都说好的图书：在stackoverflow书单基础上整理，多为经典图书。其中1-12本为stack overflow推荐，全世界程序员都说好的图书。 Ruby与Rails开发基础书单:涵盖了Ruby与Rails3新手所需要的主要技能。Ruby与Rails的组合，能够快速进入Web开发世界。 与小朋友一起学编程：一些入门的计算机编程读物，面向小朋友写的。所以不用担心自己读不懂。涵盖Python，Ruby与Arduino等。 经典开放课程 ios开发：涵盖ios5与ios4。 斯坦福iOS2011年开发教程 斯坦福iOS2010年开发教程 Web开发：Ruby与Rails 斯坦福关于用Ruby与Rails进行Web开发的开放课程 更多课程 计算机科学与创业 斯坦福课程。内容相当新，将于2012年开始。请立即注册。 精实创业 科技创业 计算机基础 自然语言处理 机器学习 SAAS 人机交互 概率 博弈论 以下为MIT的计算机科学课程： MIT的计算机科学课程 网站 github.com：世界上最优秀的程序员集中地带。 stackoverflow：世界上最热心的程序员集中地带，你的常见问题都可以在这里找到答案。 设计 设计逐步成为驱动世界发展的三巨头之一：教育、技术与设计。 图书 设计 网站 dribbble: 在这里，可以看到高质量的设计作品。 wherewedesign: 设计不仅仅是Web设计。 创新 图书 成为创新者：诞生伟大设计与产品的新模式：以triz及创新算法为主。 ReWork系列图书：一些有干货的创业图书。 Rework中文：小而美的创业之旅。 网站： paulgraham：一位对创业有独特思考的长者。 Hacker News: 技术创业者集中地带。 部分中文资源索引 除了学习之外，还有什么？ 建立信任！建立信任！建立信任！ 一个公司可以招募低学历的人才，但是，前提是对方信任你！除了学历之外，快速建立信任的方式就是用作品证明自己。 你可以，开一个个人博客。如果是2011年才开建，建议采用octopress，记录个人的学习成长。 如果你是侧重开发领域发展，那么，立即在github.com上提交个人的代码，即使它仅仅是完成某项作业，以及登陆stackoverflow回答别人的问题. 如果你是设计师，那么，请登陆dribbble分享个人做过的工作。 诸如此类，在你所感兴趣的领域，找到建立信任的方式。而参加开源项目、公益组织更是突破阶层板结，建立信任的良好方式。","title":"不上大学，可能缺少的是什么？"},{"content":"看了刘未鹏的推荐，我花了半天时间看了这条wiki：http://en.wikipedia.org/wiki/History_of_artificial_intelligence#cite_note-26 正如刘未鹏所说，确实大气磅礴，文中索引了大量牛人的研究成果，描述他们在摸索中挣扎前进的过程。 文中从希腊神话讲起，谈到了人工智能研究的起源和动机。然后讲到50年代的正式开展，接着两次兴衰，直到现在的发展过程。把AI相关的领域融合的非常之好，神经网络，自然语言处理，逻辑语言，agent等等以前认为关联不大的知识，产生的时候原来是和当时的AI发展息息相关的。科学家们再走着一条自我反省改进的道路，而到现在看来大部分的成就确是计算机硬件发展的结果，其实也多少让人觉得悲哀。文中写了天才Minky的对AI50年发展杰出的贡献，尽管他的一篇文章曾经让神经网络停止研究10年，但是在70年代，我觉得硬件条件还不足以支持神经网络的研究。神经网络的复兴正是随着分布式技术的发展而发展起来的。也许是我们太过执着，其实顺其自然，人工智能也许就会不期而至。我希望如此。 另一方面，除了硬件技术，资金支持也对AI的研究起着决定性的作用。让一群游手好闲的科学家去琢磨着也许现在根本无法实现的东西，确实需要胆量和勇气。更何况这帮人已经违背了多次先前许下的诺言。但是对AI的热爱是人性使然，这种激情，有时候可以创造奇迹。人类的力量已经一次次的超乎我们的想象。","title":"看wiki“History of artificial intelligence”有感"},{"content":"There are some links that I have collected from the Internet, maybe they are useful for some students or researchers in the Machine Translation field! Confrence ACL Anthology » D09 ACL2012 CICLing-2012 Conference Computational Linguistics and Intelligent Text Processing NAACL HOME PAGE Propor 2012 WikiCFP Call For Papers of Conferences, Workshops and Journals   NLP Group Department of Computer Science SCHOOL OF COMPUTER SCIENCE, Carnegie Mellon SMT Group Edinburgh - Main-HomePage The Stanford NLP (Natural Language Processing) Group 中文自然语言处理开放平台 计算所机器翻译机器翻译评测信息抽取语义分析句法分析词法分析语言学 Personal Webpage Dekai Wu Jörg Tiedemann Michael Collins Philipp Koehn Qin Gao's Box Research Programming Language CSDN.NET - 全球最大中文IT社区，为IT专业技术人员提供最全面的信息传播和服务平台 Homepage of DzSoft Ltd - The best Perl Editor and PHP Editor Perl, Python and Tcl - Dynamic Language Experts ActiveState python 语言特点_python 入门教程_老王python The Comprehensive Perl Archive Network - www.cpan.org The Perl Programming Language - www.perl.org Unix技术网 = 全球最大的Linux-Unix应用与开发者社区 = IT人的网上家园 VC操作文件之CFile操作详解 - 巴士飞扬-技术BLOG www.pudn.com - 程序员联合开发网 源代码数目148万个 中国 Perl 协会 用户推广组 Research Corpus LDC - Linguistic Data Consortium OPUS - an open source parallel corpus 专业词汇_英语角 中英双语阅读中英阅读双语新闻名著阅读双语故事轻松阅读 - 可可英语 人名大全姓氏大全 人文历史 - TXT小说下载TXT电子书下载TXT格式手机电子书下载 信息分类与情感发现 句酷_例句搜索_免费在线翻译 掌上百科 ◆Mdict词库，维基百科，Wiki资源 - Powered by Discuz! 科学网—[转贴]所有专业词汇汇总 - 有机 绍兴文理学院--中国汉英平行语料大世界 至善句库 语料库语言学在线 Research Evaluation Alignment Set Toolkit asiya An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation NIST Open Machine Translation (OpenMT) Evaluation The METEOR Automatic MT Evaluation Metric Research Tools BitPar cdec Decoder GenPar Toolkit giza-pp - GIZA++ statistical translation models toolkit - Google Project Hosting Joshua LF Aligner - FOSS4Trans LoPar Moses - Main-HomePage Natural Language Processing Laboratory at Northeastern University东北大学自然语言处理实验室机器翻译语言分析文本挖掘 Statistical Machine Translation Tagging guidelines for word alignment The EGYPT Toolkit Distribution Web Page TreeTagger Study Vedio AutoCAD 2007中文版标准教程 - 视频中心·网易学院 Phrase-based and factored statistical machine translation - videolectures.net VeryCD电驴大全 - 人生的价值，并不是用时间，而是用深度去衡量的 YouTube - Broadcast Yourself 优酷-中国第一视频网站,提供视频播放,视频发布,视频搜索 - 优酷视频 播布客 - 播布客 Oracle培训 培训咨询 企业护航服务 视频教程大全 免费在线视频教程教学网站  ","title":"Some useful links of Statistical Machine Translation (SMT) 统计机器翻译的有用链接"},{"content":"今年10月，Apple发布了iphone 4S with IOS 5,其中最大的亮点就是一个语音搜索软件-Siri。一时间，各种geek，伪geek，码农，非码农都流行起调戏siri，各种调戏视频，音频大量出现。不过，常言道“外行看热闹，内行看门道”，作为一个“伪内行”，或者“欲做内行而不得”的人，根据自己的知识，以及一些搜索工具，尝试了解了一下Siri的“门道”，在这里做个总结，列出siri所可能用到的技术（所谓可能，是因为很多是我猜测，或者没有准确的来源的资料）。 Siri是IOS上的个人助理应用：此软件使用到自然语言处理技术，使用者可以使用自然的对话与手机进行互动，完成搜寻资料、查询天气、设定手机日历、设定闹铃等服务。（来自维基百科） Siri所用到的技术，很多人会回答，人工智能以及云计算，的确，总体来说，是这两样技术，不过，这种概述感觉几乎没有任何意义，和不直接说“计算技术”（注意，不是计算机技术）呢。因此，在本文，我将介绍下我了解Siri可能采用的技术（由于有个人猜测，不一定准确）。 首先，在前端方面，即面向用户，和用户交互（User Interface，UI）的技术，主要是语音识别以及语音合成技术。语音识别技术是把用户的口语转化成文字，其中需要强大的语音知识库，因此需要用到所谓的“云计算”技术。而语音合成则是把返回的文字结果转化成语音输出，这个技术理论上本地就能完成（以前用过科大讯飞的在windows mobile上的本地语音阅读软件，软件很小，但能读的很好，还支持方言），但不知道Siri是否如此，当然，在云端完成也并无不可，在当前无线带宽下，那点语音流量根本不算什么。 其次，后台技术，这些其实才是真正的大角色（当然，普通用户是不会在意的，他们只会觉得前端很炫，哎，这就是做后端的悲哀，小小感叹一下）。这些技术的目的就是处理用户的请求，并返回最匹配的结果，这些请求类型很多，千奇百怪，要处理好并不简单。基本的结构猜测可能是分析用户的输入（已经通过语音转化），根据输入类型，分别采用合适的技术（合适的技术后面）进行处理。这些合适的后台技术包括，①以Google为代表的网页搜索技术；②以Wolfram Alpha为代表的知识搜索技术（或者知识计算技术）；③以Wekipedia为代表的知识库（和Wolfram Alpha不同的是，这些知识来自人类的手工编辑）技术（包括其他百科，如电影百科等）；④以Yelp为代表的问答以及推荐技术。 下面，对上面提到的各种技术进行简要介绍（如有空，后面的博文可能会对某些技术详细的介绍，大家耳熟能详的就免了），强调下，介绍的有些参考来源是维基百科相关词条，下面不一一列出：   语音识别以及语音合成技术 语音识别技术，也被称为自动语音识别（英语：Automatic Speech Recognition, ASR），其目标是将人类的语音中的词汇内容转换为计算机可读的输入，例如按键、二进制编码或者字符序列。与说话人识别及说话人确认不同，后者尝试识别或确认发出语音的说话人而非其中所包含的词汇内容。语音识别技术所涉及的领域包括：信号处理、模式识别、概率论和信息论、发声机理和听觉机理、人工智能等等。 语音合成是将人类语音用人工的方式所产生。若是将电脑系统用在语音合成上，则称为语音合成器，而语音合成器可以用软/硬件所实现。文字转语音(text-to-speech, TTS)系统则是将一般语言的文字转换为语音，其他的系统可以描绘语言符号的表示方式，就像音标转换至语音一样。 相比于语音识别，语音合成感觉难度要低，并且基本能够在本地完成（很多OS都有内置的语音合成引擎）。而语音识别，由于涉及到语义理解，难度大大加大，需要大量的知识库，一般需要在云端完成。 另外，在国内，这方面做的很好的公司也有，比如鄙人本科所在学校的科大讯飞，就是中文语音合成以及识别的领军企业,在英文语音合成领域其实也很牛，拿过很多奖。国外的话，比较早的应该是IBM ViaVoice。 网页搜索技术 这个我想没必要介绍，大家天天用的Google就是这个技术的代表。而国内的话，则以百度为代表。 知识计算（搜索）技术（Computational Knowledge） 这个技术的代表是Wolfram|Alpha。 不同于搜索互联网信息，Wolfram|Alpha将从公众的(包括公开的网页等)和获得授权的资源中，发掘、建立起一个异常庞大的经过组织的数据库，再利用高级的自然语言算法进行处理，最终构造出一个类似于谷歌搜索的工具。 和网页搜索技术不同的是，在这个系统中，得到的答案结构化程度很高，比如搜索China，能得到和中国相关的各种参数以及资料，并以接近表格的方式呈现。Wolfram|Alpha也能理解部分自然语言，比如输出How old are you，其会回答Wolfram|Alpha的年龄。想测试这项技术的请移步Wolfram|Alpha。 这个技术国内做的应该有，但还没有产品，也许百度的框计算算是半个。所以机会大大的有。 知识库技术 这名字是我自己起的，不知道有没有贻笑大方。。。这个技术的代表是维基百科，以及各种专门的百科网站。相比于网页搜索技术，基本以一个词条或者主题为单位，因此得到的数据价值高，知识量大，并且结构化程度好。相比于知识计算技术，这些技术需要人的参与，这有利也有弊，利就是，毕竟暂时人比机器聪明，编辑出来的知识更丰富，准确；弊就是，人力有限，即使像维基那样，发动社区的力量，也不能产生足够的知识，而知识计算，理论上，只需要算法够牛叉，是可以产生“无限”的知识的。 另外，写到这里，我想起Yahoo和Google的故事，当年，Yahoo是搜索老大，就像现在的Wikipedia在知识搜索领域一样，而Google是小弟，就像Wolfram|Alpha在知识搜索领域一样。但后来，却反过来了，Googe成了网页搜索老大，Yahoo成了小弟，原因就是Google相信算法的结果，把所有事都交给算法做，而Yahoo，很多索引都是人工编辑的。我想，也许有一天，Wikipedia和Wolfram|Alpha也会出现这样的情况。 问答推荐技术 其实这不能称为一个技术，应该属于知识库的技术。不同的是，这个技术针对的是一些生活信息，这些信息的地域化程度很高，典型代表为Yelp。由于这东西比较简单，就不仔细介绍了。 其实在国内，这方面的网站也有，那就是大众点评网这些。   读完此文，你也许会发现，其实Siri并没有什么革 命性的技术，其本质是将各种已经比较成熟的技术融合成一个产品，最终呈现给用户，因此，完全没必要对Siri神圣化。 另外，Siri其实是苹果买来的，在苹果买来之前，Siri本来是会推出Android版本的，可是，被苹果购买后，Android版本的Siri遥遥无期了（不过，这为广大的Android开发者以及公司提供了机会）。而Siri之前默默无闻，但是一从苹果推出，却声名鹊起，不得不佩服Apple以及Jobs的眼光以及执行力。 转自: http://www.sigma.me/2011/10/23/the-technology-behind-siri.html ","title":"Siri背后的技术"},{"content":"HNC（概念层次网络）理论 参考： http://www.hackchi.com/hnc/books/hnc/bookml.html    HNC是“Hierarchical Network Concepts（概念层次网络）”的简称，它是面向整个自然语言理解的理论框架。这个理论框架是以语义表达为基础的，它对语义的表达是概念化、层次化、网络化的，所以称它为概念层次网络理论。 1 HNC理论的形成     自然语言处理作为人工智能的一个分支，已有40年的发展历程，形成了计算语言学这一跨接语言、信息、认知科学和计算机技术的边缘学科。它的发展主要围绕以下三个方面： 1、自然语言的表述和处理模式； 2、自然语言知识的表示、获取和学习； 3、研制开发自然语言的应用系统。     在自然语言的表述和处理模式方面，源于印欧语系的语法学和句法分析一直居于主导地位。八大词类、六种句子成分、短语结构和句法树成为语言分析的基本概念和依托。对于这一传统分析模式，仅在20世纪70年代，曾一度受到菲尔墨（Fillmore）和山克（Schank）的质疑和挑战。80年代以来，语料库语言学的兴起使人们对统计模式产生了过高的期望，以致忽视了菲-山挑战的实质意义。参见人工智能点评     自然语言传统分析模式（含统计模式）的根本弱点何在？一言以蔽之，它不是描述语言感知过程的适当模式。     面对语音流的五重模糊（发音模糊、音词转换模糊、词的多义模糊、语义块构成的分合模糊、指代冗缺模糊），面对文字流后三重模糊，大脑的语言感知应付裕如，表现了强大的解模糊能力，自然语言处理技术当前无从望其项背。     近20年来，自然语言处理囿于传统模式，不图突破。参见批判提示但是，它所面临的所有重大课题，从音词转换到机器翻译，从全文检索、信息抽取到智能阅读助手，都在呼唤语言表述及处理新模式的诞生；呼唤上下文联想处理向“知其所以然”的语义理解前进；呼唤向语言感知方向靠拢。随着网络时代的来临，这一呼唤的迫切性和严峻性在与日俱增。     响应这一呼唤才意味着真正的突破，但突破的契机何在？悲观论者认为：语言感知过程p3密切依附于大脑中万亿神经元的神经网络，依附于浩瀚无垠的世界知识海洋，在对这个“网络”和“海洋”的奥秘未作充分揭示之前，模拟语言感知过程是不现实的。背景知识：1964年，美国科学院成立语言自动处理咨询委员会(简称ALPAC委员会)，调查机器翻译的研究情况，并于1966年11月公布了一个题为《语言与机器》的报告，简称ALPAC报告，对机器翻译采取否定的态度，报告宣称：“在目前给机器翻译以大力支持还没有多少理由。”报告还指出，机器翻译研究遇到了难以克服的“语义障碍”。在ALPAC报告的影响下，许多国家的机器翻译研究进入低潮，许多已经建立起来的机器翻译研究单位遇到了行政上和经费上的困难，在世界范围内，机器翻译的热潮突然消失了，出现了空前萧条的局面。摘自《自然语言的计算机处理》p408 ，冯志伟，1996年10月     事情果真是如此悲观的么？HNC理论对此进行了8年的探索，结论是，突破的契机是存在的，其要点是：     1、要把自然语言所表达的知识划分为概念、语言和常识三个独立的层面，对不同层面采取不同的知识表示策略和学习方式，形成各自的知识库系统。     2、建立网络式概念基元符号体系，即概念表述的数学表示式。这个符号体系或表示式应具有语义完备性，能够与自然语言的词语建立起语义映射关系，同时，它必须是高度数字化的，每一个符号基元（每个字母或数字）都具有确定的意义，可充当概念联想的激活因子。这个符号体系就是下文将要详细介绍的三大语义网络及五元组等，它是计算机把握并理解语言概念的基本前提。     3、建立语句的语义表述模式，即语句表述的数学表示式。这一模式的完备性应表现为可表述自然语言任何语句的语义结构，即乔姆斯基所提出的语言深层结构。这个深层结构就是下文将要简要介绍的句类格式。以句类格式为基点的语句分析叫做句类分析，是对大脑语言感知过程的初步模拟，在上述五重模糊或三重模糊的消解方面，理论上，句类分析应能接近甚至超过常人的水准。     上述三点是形成HNC理论的基本背景。     但是，解模糊处理仅仅是自然语言理解的万里长征的第一步，仅涉及HNC理解处理系统（本文第三部分有简略介绍）的部分模块。作为自然语言的一种表述和处理模式，HNC是开放的，并处于不断完善和深化的过程，在这一过程中，更需要不同学科的合作，特别是信息处理与语言学的合作，在8年的艰苦探索过程中作者深深感到这一合作的迫切性。现在这一合作的势态已初步形成，正是在合作者的鼓励和具体推动下（林杏光1997），HNC理论首次公开发表论文，主要目的在于扩大这一合作的势态。 2 HNC理论的基本内容     人对语言的理解本质上是一种认知行为，如果能描述大脑认知结构的具体模式，计算机就可以运用这些模式对自然语言进行理解处理。我们把认知结构分为局部和全局两类联想脉络，认为对联想脉络的表述是语言深层（即语言的语义层面）的根本问题。什么是局部联想和全局联想呢？简单地说，局部联想是指词汇层面的联想，全局联想是指语句及篇章层面的联想。更简单地说，理解句子有两种思路：一是从组成句子的词语入手，一是从句子的整体结构和上下文语境入手，前者就是局部联想，后者就是全局联想。当然，人在理解句子的时候，这两种联想不是截然分开的，而是并存的、相互作用的，计算机理解语言也应该综合运用这两类联想脉络。HNC的出发点就是通过建立两类联想脉络来“帮助”计算机理解自然语言。下面就分别介绍HNC建立的两类联想脉络。p4     2.1 局部联想脉络——五元组和语义网络     局部联想是词汇层面的联想，自然语言的词汇是用来表达概念的，因此，HNC建立的局部联想脉络体现为一个概念表述体系，这个概念表述体系可以简单概括如下：把概念分为抽象概念和具体概念，对抽象概念用五元组和语义网络来表达，对具体概念采取挂靠展开近似表达方法。        概念有抽象与具体之分。在一般人看来，抽象概念总是比具体概念难于把握，中文信息处理界已有的汉语语义分类系统，其内容主要是对比较容易把握的具体概念的分类，这样的语义分类系统没有摆脱对客观事物进行科学分类的束缚，对抽象概念则几乎束手无策。参见批判提示实际上，从深层来讲，抽象概念比具体概念更具有基元性、系统性，更容易表达；具体概念是客观存在物在人的思维中的一种直接反映，它里面包含了许多世界知识，而对世界知识是很难进行详尽表达的。所幸的是，人对具体概念理解和认识的深度可以比抽象概念浅，所以可以采取实用原则，“不求甚解”。HNC理论侧重于抽象概念的表达。     HNC理论通过五元组和语义网络层次符号来完整地表达抽象概念，前者表达抽象概念的外在表现，后者表达抽象概念的内涵。     任何一个概念都有需要从不同侧面予以表达，这种现象叫做概念的多元性表现。具体概念的多元性表现十分复杂，难以给出规范化的表达，抽象概念则有所不同，它的多元性表现在自然语言中有明显的迹象，这就是词性现象。印欧语系的词根或具有词根特色的词，可以加上不同的后缀分别构成动词、名词、形容词和副词，这种词性的转换就是抽象概念多元性的生动表现，也就是说，词根相同词性不同的词是对同一概念不同侧面的表达。汉语对抽象概念的多元性表现则没有相应的形式标志，而往往是同一个词兼有名词、动词、形容词、副词中的几个属性。汉语的词性模糊现象（即无形态变化）和西语以形态变化表现不同词性的现象都是抽象概念多元性的生动表现，形态变化的有无只是一种形式，本质在于抽象概念本身具有这种多元性表现的固有特征。     那么，抽象概念多元性表现的“多”是一个模糊的“多”，这是一确定的“多”呢？或者说，能否给以规范化的表达？或者再换一个说法，这个多元性表现的“多”是否存在某些基元（primitive）呢？答案是肯定的。抽象概念需要从动态、静态、属性、值和效应五个侧面加以表达，这就是抽象概念的五元组特性，简记为：（v，g，u，z，r ）特性，它们是抽象概念多元性表现的基元。任何抽象概念都具有的五元组特性，即都需要从五个侧面加以表达，不过，对某个抽象概念各个侧面的表达，自然语言中未必都有相应的词语，而且不同语种间存在着差别。反过来，自然语言中的一个表达抽象概念的词语必定是从五元组中的某个或某几个侧面来表达某个抽象概念。例如，“思考、思维、想法”就是分别从五元组的vg,g,r侧面对同一概念内涵的表达。五元组是词性的本质内容，是词性的基元。所以，不必为汉语词汇的大量兼类现象感到困惑。     为表达抽象概念的内涵，HNC设计了三大语义网络：基元概念语义网络、基本概念语义p5网络和逻辑概念语义网络。语义网络是树状的分层结构，每一层的若干节点分别用数字来表示，网络中的任一个节点都可以通过从最高层开始、到该节点结束的一串数字唯一地确定，这个数字串叫做层次符号。三大语义网络是抽象概念的三大聚类。     基元概念语义网络的一级节点分为两大类：一类是主体基元概念，另一类是复合基元概念。   主体基元概念有6个一级节点，分别是作用、过程、转移、较应、关系、状态，它们构成作用效应链。什么是作用效应链？作用效应链反映一切事物的最大共性。作用存在于一切事物的内部和相互之间，作用必然产生某种效应，在达到最终效应之前，必然伴随着某种过程或转移，在达到最终效应之后，必然出现新的关系或状态。过程、转移、关系和状态也是效应的一种表现形式。新的效应又会引发新的作用，如此循环往复，以至无穷，这就是宇宙间一切事物存在和发展的基本法则，也是语言表达和概念推理的基本法则。     这6个环节的源头是作用，结果是效应。自然语言的主要内容就是对这六个环节进行局部和总体的具体表述，我们对句类（见下文）的划分就是以此为标准的（这里顺便说明一下，山克的“概念从属理论”主要考虑了“转移”这一个环节，我们对“转移”二级节点的设计就部分吸收了“概念从属理论”的主要结果）。作用效应链既是用于表达概念的语义网络的核心，又是划分句类的标准，换句话说，它既是局部联想脉络的基础，又是全局联想脉络的基础，两个联想脉络通过它联系起来，所以，在一定意义上可以说作用效应链是HNC的理论基础。     复合基元概念主要涉及人类活动，这是因为，自然语言是人类的交际工具，其主要表述对象是人类活动而不是自然现象。复合基元概念总共设置了8个一级概念节点，根据人类活动的语境特征划分为三个层次，即生理本能活动、一般理智活动和社会性活动。     逻辑概念语义网络分为两类：一类是语言逻辑概念，大体上相应于汉语的虚词，有11个一级概念节点，分为语义块区分标志符、语义块组合标志符、语义块及句间关系说明等三类。这11个一级节点的划分主要基于它们对语义块感知及句类辨识的作用，面不是它们的语法特性。另一类是基本逻辑概念，有两个一级概念节点：比较和基本判断。     HNC语义网络的设计思想有两个来源：一是奎廉（Quillian）的语义网络理论、菲尔墨 的格语法和山克的概念从属理论；二是汉语的“字义基元化，词义组合化”现象。第一个来源提出了“语义基元”的杰出思想并暗含着“总体表述”的宏伟目标，第二个来源提供了语义基元的宝贵原料。汉语字少词多，仅用几千个汉字加以组合就构成许多的词。几千年来，汉语随着社会的发展而发展，新词不断增加，但组成词语的汉字却几千年很少变化。汉字字义的基元化和汉语词义的组合化是一个伟大的宝藏，HNC语义网络的形成深深受益于这一宝藏的启发。     三大语义网络为表达抽象概念的内涵而设计，最终将用它来描写自然语言词汇的语义，p6但网络本身却不是直接面向语言词汇的，而是面向构成词汇语义的概念基元的，适用于任何语种。网络上的任何节点本身都是概念，但这些概念只是庞大的概念海洋里的“元素”，即它们是概念基元，它们通过不同方式的组合而构成各种各样的、无数的概念，HNC定义了８种组合结构，用以表达复合概念。     三大语义网络的设计，可以解决现代语义学中的两个难题。一是义素分析法的难题。义素分析法试图用分解的方法、用义素（语义原子）来描述词汇语义，它对一些词的意义进行了成功的描写，但是，语言的义素到底有多少，义素分析法没找到答案，因而不能落实到对全部语言词汇的描写中。三大语义网络的各个节点，即概念基元，大体上相当于义素，可以用来描写任何语言的所有词汇语义。语义网络采用了分层的灵活结构，可以从高层到底层根据需要不断往下设置节点，而由于有上层的控制又不会零乱，从而解决了义素分析法的难题。二是语义场的难题。语义场理论看到了语汇语义的关联性和系统性，但是，语言中到底有多少个义场，义场该怎样划分，义场之间、义场内部都是怎样的关系，对这些问题理论都没能解答。三大语义网络建立了语言深层概念的网络，它是一个整体的设计，是一个完整的系统，它各个节点下的网络都形成相关联的概念的聚类，这些聚类就相当于语义场。更重要的是，通过语义网络，义场内部、义场之间都建立了联系，而且这各种各样的联系都可以通过层次符号显式地表达出来，从而使计算机能够掌握和操作。     五元组符号和语义网络的层次符号的适当组合可以实现对抽象概念的完整表达。这种表达方式能够显式地表达出自然语言概念之间的关联性，从而有助于计算机把握和理解。例如，“精神－振奋、无私－奉献、慷慨－就义、锦绣－山河、远大－前程、 承担－责任、召开－会议”精神g714、振奋v714em1、无私uvc3a2+u011、奉献vc3a2+u011、慷慨 gud02c33;gu9431c33;gu714y、就义vb02+v146、锦绣xj2-0、山河 wj2-0、远大ju221、前程 (gr10a8;gr910a8)、承担vc139、责任u139、召开vc3959、会议 gc3959参见HNC符号实例集1这些词语间的优先搭配在自然语言中是“理所当然”的，把这些搭配中的词用五元组和层次符号表示，各个搭配中的前后词语就会具有相同或相近的层次符号，而只是五元组符号不同，从而使它们之间搭配的“理所当然”得到显式的体现。可见，用五元组和语义网络层次符号表达语言概念的方法可以解决语义搭配（或称语义约束）的难题。传统的词性搭配不能解决语义问题，动词后可与名词搭配，但“动＋名”结构根本无法保证语义的正确，这种语法正确、语义荒谬的困难必须借助语义约束来解决，但语义约束一直找不到表达和把握的手段。三大语义网络完成了概念之间关联性的设计，找到了解决语义约束问题的根本途径。     对概念关联性的表达是语义网络的首要目标。概念基元的首要价值与其说是给出复合概念的精确表示，不如说是给出概念关联性知识和联想脉络的线索。自然语言理解的中心任务是解模糊，如同音模糊消解、一词多义模糊消解等，这些模糊的消解统称为多义选一处理。对自然语言词汇的多义选一处理是人类理解自然语言中最频繁、最基本的操作。对这一操作过程的形式模拟不在于并行处理或快速计算，而在于以什么巧妙的方式完成大量语义距离（语义关联性）的计算参见存疑录。层次符号的构造方式把最频繁、最基本的语义距离计算变成了对层次符号的简单逐层比较。这是HNC用语义网络层次符号表达概念的基本出发点。层次符号是一种灵活的分层结构，它到任一层都代表一个概念，至于这个（些）概念与相应的语言概念之间，究竟谁是谁的近似，已无关紧要。重要的是，层次网络符号对概念的局p7部联想脉络给出了明确的表示，便于计算机把握概念之间的关联性。     语义网络层次符号的设计为计算机理解自然语言的语义提供了有力的手段。当然，在工程实现上首先要用语义网络层次符号完成对自然语言词汇语义的描写，这是一项浩大而艰巨的工程，但这个瓶颈问题跟过去相比已有了本质的不同，过去缺乏语义描写的完备手段，现在手段已备，剩下的只是工作量的问题。     下面简单说明对具体概念的表达。一般来说，具体概念的精确表达要比抽象概念困难得多，因为它涉及到许多世界知识，这些世界知识是人类认识积累的结果。但另一方面，人在理解自然语言过程中对具体概念的认识深度可以比抽象概念浅得多，天生的盲人仍能同常人一样掌握自然语言，道理就在这里。所以，对具体概念的表达，应采取大胆近似的方案，这是对具体概念进行层次符号设计的基本出发点。HNC用“类别符号＋挂靠”的方式近似地表达具体概念。     具体概念的类别，从语言表达的角度来看，先分为物、人、物性三类（分另用符号 w，p，x 表示）比较合理。物有自然物与人工物之分，人工物又有现代与传统、物质与精神产品之分，当然还可以有各种各样的分类标准。人和物性也同样存在子类划分问题。在处理具体概念的分类问题时，不宜照搬自然科学的分类方法，HNC的着眼点主要是引起概念的联想，而不是分类的科学性。参见批判提示     对具体概念的内涵,HNC采用向抽象概念的基元概念和基本概念挂靠的方法表达。例如，人、一般人工物、现代产品这几类具体概念分别用符号 p，pw，w9 表示，基元概念是的概念节22b表示自身转移，那么，向它挂靠的 pw22b 就表示交通工具；219 表示针对性接收，w9219 就表示现代探测设备；411表示结合，p411就表示夫妻；382 表示废弃，pw382 就表示垃圾；基本概念里的概念节点711和712分别表示正和负，p711和p712就分别表示男人和女人。参见HNC符号实例集2显然，这种挂靠的表示方式都是很粗糙的近拟，但其重要意义在于：通过这一近似表示，计算机就能对有关概念之间的关联性有所“领会”。挂靠式表示方式的目的，就是在具体概念与抽象概念之间建立一种关联，并把这种关联用符号显式地表示出来，以利于计算机计算语义距离。     挂靠的表示方式只适用于一部分具体概念，一些基本的物质概念仍然需要进行独立的层次符号设计。为此，我们设计了一个基本物的语义网络，这个网络有７个一级节点：热、光、声、电磁、微观基本物、宏观基本物和生命体。这些节点的设置仍是服务于联想脉络的建立，并不完全遵循自然科学的标准。     按照上述设计，对概念基元就可以写出下面的语义表示式： F=Σ(字母串)(数字串)     F代表概念基元的HNC符号。字母串由概念类别符号(φ,j,l,jl,p,w,x)构成，数字串由１６进制数字的0～d构成。其中φ表示基元概念，j表示基本概念，l表示语言逻辑概念，jl表示基本逻辑概念。     复合概念的语义表示式为：p8 F=ΣFk     Fk之间的连接通过８种概念组合结构符号来表示。     2.2 全局联想脉络——语义块和句类     全局联想脉络是语句及篇章层面的联想。语义块和句类理论是在语句层面设计的全局联想脉络，篇章层面的联想脉络本文暂不介绍。     简单地说，语义块是句子的语义构成单位，形式上可以是一个词、一个短语或一个句子。语义块类似于传统语言学中的短语，但是，两者具有本质的区别，表现在：第一，从内涵上来看，语义块是语义，即语言深层的定义，短语则是语法，即语言表层的定义；第二，从形式上来看，语义块可包含或嵌套另外的一个甚至多个语句，而短语不能。另外，传统的短语更多的是被看作词的组合结构，而不是句子的直接构成单位。参照概念比较集粹     语义块这一概念的提出是为了便于从语言深层（即语义层面）描述一个句子。用词或短语描述句子，无法清楚地界定一个句子是否完备，如果问一个句子应该或者可能有多少个词或短语，便难以回答。但有了语义块的概念，就可以明确回答一个句子有多少语义块以及每个语义块的类型等问题。     在通常情况下，一个语义块包含核心部分和说明部分。语义块按其语义功能分类，语义块的语义功能主要取决于其核心部分。     语义块分为主语义块和辅语义块两大类。主和辅是从句意表达的角度来分的，主语义块是句义的“必不可少”的成分，辅语义块是句义的“可有可无”的成分。主语义块有４种：特征E、作者用A、对象B 和内容C。辅语义块有７种：条件、手段、工具、途径、参照、因、果。     E，A，B，C四大主语义块划分的理论依据是：一个语句表达的内容无非是两个方面，一是表达对象，二是对象的表现，前者是“什么”，后者是“怎么样”。作用者A、对象B语义块是表达对象，内容C、特征E语义块是表现。在表达对象中，B是一般表达对象，A是表达对象中的特殊对象；在表现中，E是一般表现，C是特殊表现。一个句子至少由一个对象语义块和一个表现语义块构成，但更为常见的结构是：两个对象语义块加一个表现语义块，一个对象语义块加两个表现语义块，两个对象语义块加两个表现语义块，还可以是多个对象语义块加多个表现语义块。所以，所谓“一个句子只有一个中心动词”的语法规范与语言表达的需要并不协调。     为什么E语义块叫做特征语义块呢？因为一个句子的基本语义信息就蕴涵在E语义块中。那么，什么是基本语义信息呢？它来源于作用效应链思想。一个句子总是对作用效应链的某一或某些环节的表达，所谓一个句子的基本语义信息就是指它所表达的关于作用效应链的某一或某些环节的信息。这样，作用效应链的６个环节自然就是基本语义信息的分类标准，因而也是E语义块的分类标准。不同类别的E语义块构成不同类别的句子，从而引入了句类的概念。HNC的句类是句子的语义类别，与传统的句类是完全不同的概念，后者指陈述句、祈使句、疑问句和感叹句，基本上是句子的语用分类。p9参照概念比较集粹     只表达作用效应链的一个环节的句类称为基本句类，表达两个或多个环节的句类称为混合句类。 E语义块的命名与作用效应链６个环节的名称相一致，即作用、过程、转移、效应、关系、状态。由这些E语义块构成的句子，分别命名为作用句、过程句、转移句、效应句、关系句和状态句。     E语义块的核心部分一定是动词，而且，不同类别E语义块的动词来源于不同的基元概念。E语义块的分类标准，也就是句类的分类标准。这个标准是与三大语义网络密切关联的，它实际上也就是HNC理论层次网络符号体系设计的基本标准之一。这样，E语义块的辨识信息，或者说句类的辨识，就是明确无误地蕴涵在概念层次网络符号体系之中。     由于判断是人类思维活动的基本内容，也是语言表达的基本内容之一，我们据此以定义了一个句类：判断句。根据作用效应链定义的６个句类加上判断句，构成HNC的７个基本句类。每一个基本句类又分为若干个子类，子类的定义与相应基元概念网络的二级节点相对应。子类之下还可以再分子类。     基本句类可以构成混合句类。所谓混合句类，是指两个以上的基本句类在一个句子中共现，诸如作用效应句、过程转移句、状态判断句等。自然语言的句子是丰富的、复杂的，但它们表达的信息总是由７个基本句类组成的，这正是基本句类之所以称为“基本”的原因。在自然语言中，基本句类的混合往往（或者说主要）是两两混合，因此，混合句类理论上应有５＊６＋６＝３６个。“５＊６”是与作用效应链相对应的６个基本句类的两两混合，“＋６”是它们与判断句的混合。     上面说明了语义块和句类的概念，它们之间是什么关系呢？一句话：语义块是句类的函数。这就是HNC语义块和句类理论的基本论点。解说：对于集合X中的任一元素x，集合Y中存在唯一的元素y与x对应，即由X映射到Y有唯一的值，则称Y为X的函数，记作y=f(x)；若y的反函数x=f'(y)同时成立，即集合X、Y之间存在一一对应关系，则称X为Y的函数或Y为X的函数均可。HNC中，语义块在一个句子中的有无、个数和具体内涵随句类的不同而不同，即句类是自变量，句类决定语义块的类型，故称语义块构成类型（句类数学表示式）是句类的函数；而直观地，E语义块决定着句类，即E语义块的分类标准也就是句类的分类标准，从句类辨识角度看，应该称句类是E语义块的函数。由此，语义块与句类基本构成一一对应关系，谁是谁的函数都无所谓。     E，A，B，C四种语义块是抽象概括的结果，它们在一个句子中的有无、个数和具体内涵随句类的不同而不同。这就是“语义块是句类的函数”所概括的内容。例如，拿作用者语义块A来说，作用句中的A语义块是“产生影响者”，类似于一般所说的施事者，而转移句中的A语义块是转移的发出者，过程句、关系句和状态句中则不涉及A语义块。再如对象语义块B，作用句和效应句中的B语义块是“被影响者”或“接受者”，类似于一般所说的受事，过程句、关系句和状态句中的B语义块是过程、关系、状态的体现者或承受者，而关系的体现者显然有两个，即关系的双方，它们都是B语义块，彼此之间不存在施事和受事的关系。在转移句中，B语义块是转移的接收者，而转移“物”则是转移的内容，即C语义块。     我们把“语义块是句类的函数”具体体现为句类格式。句类格式是指一个句子的主语义块的排列顺序，例如作用句必须有三个主语义块：作用者A、作用X（即E语义块）和作用的对象B，三者的排列顺序不外乎６种：A＋X＋B，B＋X＋A，B＋A＋X，A＋B＋X，X＋A＋B，X＋B＋A。选择这６种格式的哪一种作为标准格式，不同语种间存在着差别，比如汉语和多数印欧语都采用第一种格式。标准格式中蕴涵着主语义块类别的辨识信息。     ７种基本句类和３６种混合句类的提出为语句深层结构的表达提供了简明而完备的手段，所谓深层结构就有了计算机可操作的数学表示式。例如：p10参见HNC句类示例1 句类   句类物理表示式 例句 作用句：   XJ＝A＋X＋B 张三打断了李四的腿。 过程句：   PJ＝PB＋P 李四的腿伤大有好转。 转移句：   TJ=TA+T+TB+TC 李四的朋友电告李四父母这个好消息。 效应句：   YJ=YB+Y+YC； 李四养好了腿伤。      YBC+Y 李四的腿伤养好了。 关系句：   RJ=RB1+R+RB2； 张三失去了他多年的女友。      RB+R 张三跟他多年的女友吹了。 状态句：   SJ=SB+S+SC； 张三穿着皮大衣。        SB+S； 张三升官了。        SB+SC 张小姐很漂亮。 判断句：   DJ=DA+D+DBC 张三认为李四不该那样做。 反应句 (作用句的子类)： X2J=X2B+X2+XBC+(X2C) 张先生怕李小姐发脾气。 基本状态句 (状态句的子类)： S00J=SB+S00+SC； 主席团坐在台上。      SC+S00+SB 台上坐着主席团。 作用关系句 (混合句类)： XRJ=A+XR+RB 张三挑拨李四和我的关系。 关系作用句 (混合句类)： RXJ=RB1+RX+B 张三多次帮助过李四。     这些表示式就是计算机赖以进行语句联想操作的基础。表示式中的每一项代表一个主语义块，这些主语义块的语义角色由该项的命名符号所唯一确定，它们是引发全局联想脉络的激活因子。     EABC语义块在形式上似乎与传统语言学的主谓宾补相对应，其实它们是完全不同的概念，有着本质的区别：EABC是语义层面的概念，是语言深层的描述量，它们是句类的函数，但与句子的格式无关；主谓宾补是语法层面的概念，是语言表层的描述量，它们与句类无关，但与句子的格式息息相关。EABC语义块和主谓宾补是从不同层面或角度对句子的结构提出分析的模式，不能相互代替。参照概念比较集粹     最后，简单叙述一下EABC概念的形成过程，这对于加深对这一概念的理解或许有所裨益。与主谓宾补相联系，语法学还有动词的及物和不及物以及双宾语等概念。但及物性的具体表现，仅在语法层面进行研究十分困难，它涉及宾语的分类问题，有的及物动词要求双宾语，有的不仅要求宾语，还要求补语。这些问题都必须进入语义层面，才能给出明确的答案。从理解来说，仅有及物的概念是远远不够的，重要的是：它“及”什么样的“物”？开始的时候，曾以为这只是语汇层面的特征，后来才发现不是这样，它也是概念层面的重要特征，这一发现导致“语义块是句类函数”概念的形成。但应该说，是格语法理论的创立者菲尔墨最先想到了这一点，他是对宾语和主语进行语义分类的第一位先行者。可惜他的理论匆忙出台，在理论总体性和层次性方面都比较欠缺。现在看来，主语和宾语的语义分类必须用ABC函数的概念，即将语义块作为句类的函数来处理才能给出完善的表述。至于双宾语，它p11一定是转移型概念，而同时要求宾语和补语的一定是作用效应型概念。 3  HNC理论的实现     上文介绍的两个联想脉络是HNC理论的基础部分，它的另一部分内容是自然语言理解的框架和具体实践。     HNC理论走向应用的第一步是语义块感知和句类辨识。语义块感知就是找出一个句子中的各个语义块，句类辨识就是通过感知得到一个句子的E语义块，进而确定这个句子所属的句类。计算机能否感知到语义块关系到HNC能否指导实践、是否有应用价值的问题，张全的博士论文（张全1996）对此做了肯定的回答。感知到语义块、辨识出句类以后，就可以运用句类知识对句子进行理解处理，这称为句类分析。在句类分析过程中，句类知识起着全局性的指导作用，主要有四方面的知识：一是句类格式知识，二是语义块构成知识，三是语义块之间的概念关联知识，四是语义块和句类的转换知识。语义块感知和句类辨识主要运用局部联想脉络，句类分析主要运用全局联想脉络，当然，处理过程中这两个联想脉络的运用不是截然分开的。     以句类分析为基础，HNC设计了自然语言处理系统的基本框架，这个框架由9个模块组成：1、单音词感知模块；2、语义块感知模块；3、句类分析模块；4、合理性分析模块；5、短时记忆知识模块；6、语境生成模块；7、隐藏知识提示模块；8、要点主题分析模块；9、短时记忆向长时间记忆扩展的模块。目前，部分模块已在计算机上得到实现。     自然语言处理离不开知识库，对知识库的设计和建立也是HNC理论的重要组成部分。人工智能早期一系列挫折，使人们认识到知识的重要性。要使计算机表现出智能，唯一的办法就是使它拥有并运用知识。正是这一认识促成了20世纪70年代到80年代的“专家系统热”，并取得了引人注目的成就。但这些专家系统的知识，都是局限于特定的领域，而一般自然语言理解（不包括特定领域的简单语言应用系统）所需要的知识则完全不同于通常的专家系统。它需要各种各样的知识，但可以分为三大类：概念知识、语言知识、常识及专业知识。前两类知识的本质区别在于：语言知识与具体语种有关，而概念知识与语种无关。把概念知识从语言知识中独立出来是势在必然的发展。把常识及专业知识独立出来对知识库的建立是非常方便和有力的，这一点不言而喻参见人工智能点评。我们已经建立了比较完备的概念知识库，目前正在紧张地进行汉语语言知识库的建立。我们曾建立过地理知识库，使用效果很好，所以具有建立常识及专业知识库的成功经验，但常识及专业知识库的建立目前还不是自然语言理解处理的迫切任务。     自然语言理解处理的进展必须由信息处理工作者和语言研究者共同推动。令人高兴的是，在我国计算语言学前辈的推动下，这两方面力量开展联合研究的局面已开始形成，并初步组成了联合攻关的队伍。p12 　 主要参考文献 黄曾阳.1996.HNC理解处理论文选录.中国科学院声学研究所声场声信息国家重点实验室自然语言理解课题组 林杏光.1997.正确引导汉语理解与研究——事关人工智能开发的一个重要前提.科技导报，1997（4） 苗传江.1997.HNC理论的基本内容.中科院声学所“HNC知识库培训班”教材 张全.1996.基于HNC理论的语义块感知处理.中国科学院声学所博士学位论文 Chomsky N.1957. Syntactic Structures. Hague:Mouton  Chomsky N.1965. Aspects of  the  Theory of  Syntax. Cambridge, MA:MIT Press  Fillmore  C  J.1968.  The  case  for  case.  In: Bach  E, Harms  R  eds.  Universals in  Linguistic  Theory. New York:Holt,Rinehart  and Winston Qullian M R . 1986.Semantic memory.In: Minsky  M Ed.Semantic Information Processing. Cambridge, MA: MIT Press Schank  R. 1973.Identification  of  conceptualizations  underlying natural language. In: Schank R, Colby  K  Eds. Computer Models  of Thought   and Language. San Francisco, CA: W  H  Freeman  and  Company Schank  R. 1957a. Conceptual  Information  Processing,  Amsterdam: North Holland Schank  R.  1957b. The structure  of  episodes  in    memory .In:Bobrow D, Collins A .eds.Representation and Understanding New York: Academic Press Schank  R. 1982. Dynamic Memory.New York:Cambridge University Press Schank  R.  Abelson  R.1997.  Scripts,Plans,Goals and Understanding. Hillsdale,NJ: Erlbaum p13 *  本文发表于《中文信息学报》,Vol.11,No.4,1997。发表时该刊加有主编按语：《HNC理论概要》的作者黄曾阳先生创立的面向整个自然语言理解的理论框架，在语义表达上有自己的特色，在语义处理上走了一条新路。鉴于汉语语法研究尚有诸多困惑，HNC理论所走的以语义表达为基础的新路子对突破汉语理解问题尤其有实际意义。参见陈力为的题词","title":"HNC理论概要*"},{"content":"( 几年前，我写了一套胶片，题目是《怎样做研究》，多次在实验室内部给学生们做报告，也曾对外讲过一次，听众反应良好。也有网友读过这套胶片，给我来信称有所收获。然而，胶片中的文字毕竟只是提纲携领，无法充分阐述我的想法，为此，借周末一点闲暇，把《怎样做研究》写成一篇文章，与师友切磋。     什么是科学      科学是分科的学问，客观地说，是起源于西方的。中国只有经验科学，典型的如中医。我的母亲是学中医的，我从小就对中医耳濡目染，生了病，妈妈就会请他的老师来，一贴小药下去，我的病就好了。因此，我对中医一直是很信服的。然而，近些年来，中医多受批评，发展也越来越缓慢，究其原因，中医不是科学，或者说只是经验科学，而非实证科学。中药的成分以及生化功效不曾用实验进行深入的分析，望闻问切的诊断方法完全凭经验而无法量化，阴阳五行的理论似是而非，祖传秘方的传承方式与知识共享的现代思维背道而驰。因此，尽管中医有诊治的整体观和方剂的个性化两大优点，但其停留于经验层面，而迟迟不能进入科学的殿堂，因此在现代社会中的发展必然步履维艰。中医不是科学，那到底什么是科学呢？科学（自然科学）是人们用来认识和改造自然世界的思维武器，科学研究可以分为基础研究（理论研究）和应用研究（技术研发）。      基础研究     万事万物皆有其规律，掌握并且利用这些规律就能够为人类造福，这些规律是隐蔽在纷繁复杂的现象背后的，要识破大自然的奥秘，读懂上帝的天书，非要下一番深入观察和探究的功夫不可。以揭示规律为目的的研究活动属于基础研究，从事这些活动的学者是科学家。规律不是被创造出来的，而是早已存在的，人们只有认识规律的权利，而没有创造规律的可能。从根本上讲，推动基础研究的也是人们在生产生活中的一些实际需要，但是随着基础研究的深入，理论已经成为一个庞大的体系，理论研究早已开始按照它自有的逻辑独立发展，而不必时时刻刻联系实际需要，比如著名的歌德巴赫猜想，可能在百年之后，发现其有重大的应用价值，但是目前到底有什么用，谁也说不清楚。理论的价值在今天这个非常讲求短期功利的社会中常常被忽视，现在有一种倾向认为只有产生实际经济效益的科研工作才有价值，这种极端化的观点显然是错误的，我们必须承认并高度尊重理论研究者的成就。理论研究的直接动力是科学家的好奇心，以及他们对科学荣誉的渴望。越是单纯的科学家越有希望发现真理，他们的科学探索有点像迷宫探宝或者海边拾贝，伟大的科学家都是没有丧失童趣的人，他们在实验室里是宁静而愉快的，他们是乐此不疲的，很多在常人看来难以忍受的寂寞在他们看来却是一种幸福。越是找不到答案，越是激发探索的热情，在一次次的失败中积累着烦闷与紧张，在终于取得突破后兴奋异常。与此同时，也必须承认科学荣誉也是激励科学家们前进的重要动力，只要别把荣誉看得高于真理，货真价实的荣誉仍然是值得追求的。理论上的突破对应用研究产生持续不断的推动力，在模式识别领域，神经网络、支持向量机、条件随机域等等机器学习技术不断出现，每当一项理论出现，应用研究者们争相将其应用于自己的研究课题中，于是基于神经网络、基于支持向量机、基于条件随机域的某某研究就成为一个标准的论文题目。首先把某项理论应用于某个实际课题的研究工作应该说还是具有一定的创新性的，毕竟用一个新的思路、新的模型去观察了一个旧的课题，HMM在语音识别上的成功应用就是一例。有人比喻说，理论工具仿佛是锤子，实际课题好比是钉子，一个新的锤子被打造出来，大家都借用过来砸一砸自己手头的钉子，确属常理。不过，需要注意的事，如果拿一个硕大无比的汽锤去砸一个纤细的大头针就荒诞可笑了，不注意思考问题与理论的适配关系而盲目跟风的事情在学术界也是司空见惯，比如我们就曾用HMM试图解决词义消歧的问题，而每个多义词的词义跟它前后一两个词并没有紧密的关系，因此词义消歧貌似和词性标注一样属于线性序列标注问题，其实是有根本差别的。     应用研究     我们是搞计算机的，计算机是一门应用科学，应用科学是由应用驱动的。时至今日，数学定理和物理学定律似乎已经被先哲们发现的差不多了，因此整个科学界中纯粹搞理论研究的人越来越少，很多大学教授都和工业界有着密切的联系，很多大企业也开办企业研究院，这些导致应用科学的研究如火如荼。最近，国家863设立了一个“中文为核心的多语言信息处理”重点项目，总经费7000万，这在多年前的大陆语言处理界完全是不可想象的。应用驱动，也可以说是市场驱动。市场是一个精灵古怪的家伙，搞应用研究的人如果对市场的未来没有一个基本准确地判断，往往会导致选题上的偏差。二十年前，国内一些研究者开始研究汉字手写输入技术，开始人们觉得从键盘输入汉字很困难，手写输入一定有前途，但是很快，拼音输入法大面积普及，而且拼音输入的速度远比在手写板上输入汉字快得多，于是汉字手写输入套件根本卖不动，前景黯淡。有人开始犹豫，有人开始转向搞印刷体汉字识别等，但忽然有一天，集成了手写功能的商务通大量热销，人们忽然发现原来在手持设备上由于键盘太小，输入不便，给手写功能留下了很大的应用空间。一直专注于手写识别的汉王公司也借着商务通的热销而把多年的科研成果成功地产业化了。再举一个例子：5年前，我认为以图像为输入的图像检索没有什么应用价值，问这些技术的倡导者，他们也只说能够在数码相册中可以找到一些应用，但近来听了微软一些学者们的演讲，他们提到可以用手机拍下一个植物的图片，传回服务器，在大量植物图片库中检索，找到最相似的植物，并给出植物的名称，特点等。哈哈，这对于我这个五谷不分的人来说实在是太有帮助了，可见对于一项技术是否有用实在要仔细思考，不要早下断言。技术和市场是一个互动的关系，有人认为技术严格地从用户的现实需求出发，这个观点总的来说没有错，但是忽视了技术创造需求的一面。大多数用户往往并不了解技术发展到了什么程度，他们提不出需求来，这时技术专家们需要把技术和产品做出来给人们看，刺激、引领用户的需求，比如数码相机，5年前我想大多数用户和我一样并没有淘汰胶卷相机的强烈要求，但当数码相机进入市场后，人人都意识到：原来我需要这个东东。在市场与技术的互动中，总的来说，还是市场在引导和拉动技术的发展。市场需要的是产品，产品往往集成了多项技术，因此一项被市场接受的产品能够推动多项技术的进步。比如搜索引擎，它拉动了自然语言处理、并行计算、海量存储设备、数据挖掘等等多项技术的发展。最近中国计算机学会设立了王选奖，在中国真正有市场眼光，能够发明一项技术，拉动一个行业的计算机专家，王选是第一人。怎样根据市场选择研究方向，设计产品，调整技术形态，我在后面还有详细阐述。     科学技术的力量     科学技术的力量是巨大的，爱因斯坦给出的公式E=M*C2，C是光速啊，质量乘以光速的平方，这是多么巨大的能量啊，爱因斯坦的理论直接导致了原子能的利用与开发。基因图谱的发现以及后基因组时代对基因图谱的深入分析必将为人类征服疾病提供一条崭新的解决道路，通过对损坏的基因进行修复，将使无数患者得以康复，无数家庭重拾幸福。互联网的发明，把全世界连为一体，过不了多久，石头里也会嵌入芯片，在这个世界上有生命的、无生命的各种物质之间都可能进行通讯，人们的生活面貌已经彻底改变了。当然，科学也是双刃剑：原子弹爆炸了，核战争始终威胁着人类；在对基因组这套上帝给出生命密码没有全面理解以前，任何盲动都可能导致基因污染，以至于玩火自焚；互联网上的虚拟生存让人们感到更加孤独。     研究的层次     研究是分层次的，很多大科学家在晚年登上了最高层，比如钱学森在80年代倡导思维科学，他对整个科学技术体系进行了重新分类。在中国的大学里，分为一级学科，二级学科等，我就处在计算机科学技术一级学科下面的计算机应用技术二级学科下。二级学科的带头人称为学科带头人，二级学科下面一个研究方向的带头人称为学术带头人，我就被指定为学术带头人。我的研究方向是信息检索，信息检索下面又有子方向，比如文本检索、文本挖掘、跨语言检索、跨媒体检索等，子方向下面设立具体的科研课题，比如文本挖掘中的多文档自动文摘课题，针对一项课题又有不同的解决办法，基于事件抽取与集成的多文档文摘就是利用一种具体的解决问题的方法。总结来说，就是6个层级： A. 一级学科 B. 二级学科 C. 研究方向 D. 子方向 E. 课题 F. 基于某种方法对课题进行的具体研究君子思不出其位，我是学术带头人，因此主要在思考C类的问题，也就是和信息检索相关的问题。一个学院的院长通常会思考A类的课题，学科带头人或者说是一个博士点的点长是要考虑B类问题的。一个人对相关的方向或学科有所了解，对自己的研究工作是很有好处的，只有看清了整体的学科面貌，才能知道自己处在那个位置上，自己未来的方向在哪里。我在读博士以及在微软做副研究员的时候，只看到E类问题，想到最多的是F类问题，因此你让我提一个新方向，让我对一项技术进行预测，我茫然无知。后来担任院长助理，负责学院的成果转化，需要了解学院里各个方向的发展状态，使我的视野开阔了一些。尽管我凡事不求甚解，但是喜欢总结归纳，因此对信息检索与其它学科的关系有了更多地认识，这对后来的选题很有帮助，特别是在应用研究方面，心里比较有底。学科好比一棵大树的树根，研究方向如同树干，具体的课题就是枝叶了。和学科中各个方向都相关的研究课题是最基础的研究课题，比如在人工智能中，各类机器学习算法是图像识别、语音识别和语言理解等各个方向都离不开的，机器学习技术提高一步，好比树根抬高了一寸，各项应用技术也都跟着进步，因此越是基础的研究，越会对业界产生较大较深远的影响力。不过，基础研究的突破比较难，而在某个应用课题上不考虑一般情况，只考虑具体需要，成功的可能性大。枝叶上的课题做多了，经过合并同类项，就会发现比较共性的基础课题，比如我们在做问答系统、多文档文摘、例句检索等课题时发现复述(paraphrasing)是一个共性的问题，于是把复述单拿出来展开专门的研究，如此，可以越做越深。      学者的层次      研究有层次，学者也有层次，大致可以分为： A. 大家（剑客）：提出问题 B. 专家（侠客）：解决问题 C. 学徒：修修补补 D. 抄袭者：抄来抄去 E. 搞伪科学的人：弄虚作假 A类是大家，站得高，看得远，他们往往能够前瞻性地提出某个学科领域中的若干重大问题，最著名的是希尔伯特的23个问题，对数学界影响深远。提出问题其实也是解决问题的一种方式，只不过他们是在很高的层面解决问题，类似一个软件系统分析员，他把一个复杂的工程问题分解为若干个有机联系的子问题，然后宣布只要这几个子问题解决了，整个大问题也就解决了。至于这几个子问题到底怎样解决，或者说相应的子系统到底怎样开发，他就不管了。胡乱地提问题并不难，小孩子也会向大人提出各种各样有趣的问题，有的大人也答不出来，问题的关键在于在适当的时候提出适合当前学术发展阶段的关键性课题，这绝对不是一般人能够做到的，这是需要具有对整个领域全面深入的理解才行的。 B类是专家，是在某个研究方向上有专长的人，他们沿着大家指出的方向探索前进，提出全新的方法体系来解决问题。比如在机器翻译领域中，日本长尾真教授提出了基于实例的机器翻译方法，从一个全新的视角看待机器翻译问题。专家经验丰富，能够自由地驾驭课题，稳步地推动课题的进展。 C类是学徒，就是我们这些普通的研究人员了，这部分人的注意力在具体的课题上。学徒们还没有宏大的视野，没有捕捉全局战略要点的本事，也还没有在一个研究方向上提出原创性的解决之道，他们跟在拓荒者后面捡拾麦穗，他们负责对科学大厦修修补补。他们一会儿听说了一个新的机器学习方法，赶紧在自己的课题上试一下；一会儿发现了一个以前忽略了的新的特征，立即想方设法把这个特征提取出来；一会儿为了参加一个技术评测，耐心地调一调系统参数；一会儿为了发表一篇论文构造出一个试验来。我们每天的研究活动差不多都是在这样进行的，很多时候在原地打转转。我这样描述学徒们的工作情景丝毫没有贬低的意味，在达到专家的水平，证悟研究真谛以前，跌跌撞撞、浑浑沌沌是在所难免的。只要遵守诚信之道，不抄袭，不造假，点点滴滴的贡献对科学界也是有帮助的。从更高的要求看，学徒的目标应该是成为专家，应该时常静下心来想一想，自己的工作是否有价值，是否有新意，揣摩一下大家们、专家们到底是怎样思考问题的，在不断地反思与实践中向上迈进。 D类学者根本算不上学者，他们为了评职称等目的，对别人的论文进行抄袭拼凑，他们是思想的窃贼，对学术界毫无贡献可言。 E类学者不仅仅是做贼了，他编造伪科学，毁坏科学界在公众中的形象，他们是科学界的公敌。以上的分类也只是为了讨论的方便，在各类之间并没有明确的界限，我只是依次谈出我心中做学问的境界而已。在人类已知的世界和未知的世界之间有一条动态边界，科学家就站在这条边界上，他们是挑战未知世界的勇士，他们每向前迈出一步，就意味着整个人类的已知世界向前拓展了一步，由此足见科学工作的艰难和科学家的伟大。研究又好比爬山，一座座山峰如同一个个研究领域，大家已登峰造极，一览众山小，把东南西北各条山路上的沟沟坎坎，把此山与他山之间的距离关系看得清清楚楚。隔行如隔山，隔行不隔道，在一个领域做到顶尖的学者已入化境，一通百通，你把另一个领域的问题讲给他听，他往往也能够很快地抓到要害。专家已到半山腰，看不到山的全貌，但是他找到了一条通往山顶的道路，并一步一步地向上攀登着。学徒还没有进入山门，他们一会儿仰望山顶，一会儿看看山腰，在山脚下绕来绕去找不到门径，费力不少，却并没有缩短与山顶的距离。。。。     怎样选题     前文曾提到科学研究的层次，并分了6个层级。此处所说的选题指的是从C到E三个层次上的选择问题，即：C. 研究方向、D. 子方向、E. 课题。选择研究方向是实验室(Lab)主任们需要重点思考的事情，选择子方向是研究小组(Group)的组长们需要重点思考的事情，选择课题是研究生们需要重点思考的事情。选择太多，很容易让人困惑，要想理出一个头绪来，需要一些基本的原则。微软的许峰雄来访时谈到了他选择课题的三个标准：有足够的兴趣，能成为世界第一，能赚钱。（1）兴趣，这个原则是非常重要的，我赞同，获得国家最高科技奖的“黄土之父”刘东生院士是搞地球环境科学的，经常在野外作业，按常人推断，这该是多么枯燥艰苦的工作啊，但他说：“枯燥？不！因为经常有新发现，其中的乐趣难以形容”。我坚信任何一个成功的科学家的直接工作动源都是兴趣，而不是意志。（2）成为世界第一，不容易，但是应该作为一种判断标准，如果某个领域已经非常成熟，很难有什么创新了，或者大牛云集，已经打破头了，则应该有所回避。（3）赚钱，许峰雄是在工业研究院中工作，比较注重实用，因此他强调了“赚钱”，我是在工科大学里工作，也比较偏重应用，因此是赞同“能赚钱”这个标准的。不过，“能赚钱”不等于立即赚钱，5年、10年，20年后能够赚钱的研究课题都是值得关注的。     谈谈我选择课题的一些体会：     1、 要有实际需求一个课题必须有实际需求，可能是现实的需求，也可能是潜在的需求；可能是直接的需求，也可能是间接的需求，总之是的的确确被人们所需要的。据个反例，比如自动文摘，自动文摘是我的博士论文课题，但是实际应用需求始终不清楚，自动文摘的结果用于编辑出版，质量肯定无法保证，用于帮助人们快速浏览资料吧，Google提供的包含查询词的简单的Snippet就起到了这个作用，因此，至今基于全文分析的单文档自动文摘到底用到哪里，仍然不清楚，这方面的研究已经有50多年的历史了，仍然是不死不活，总是找不到应用就无法得到政府和企业界的持续性支持，以往的付出成为鸡肋。我觉得单自动文摘不是一个好课题，目前阶段多文档文摘，或者说对某个题目的自动综述分析是非常好的题目。     2、有较大的未知空间以手写体汉字识别为例，市场上已经大面积应用了，在研究上就不宜再展开。         3、与自己以往的工作有关联如果你觉得自己的研究领域太窄，或者竞争对手太多，或者自己缺乏兴趣，则可以适当扩展研究方向，但最好是相关性地扩展，比如从自然语言处理(NLP)扩展到信息检索(IR)，IR要用到NLP的技术，这种扩展是从底层技术到应用系统的扩展，很自然。再比如从图片检索扩展到视频检索，只是处理对象有变化，很多原有的技术优势仍然能够发挥。如果跳跃性太大，比如搞NLP，忽然发现做数据挖掘有前途，于是单纯地转向数据库中数据挖掘，和文本处理完全脱节，这种做法一方面无法发挥既有的技术积累，另一方面也让同行感觉你不够专注，不容易得到认可。最要命的是有的人根本就没有自己的方向，什么课题都敢接，这样的人可以一时间让人觉得风风火火，经费也很充足，但过不了多久就会摔落下去，因为缺乏积累，学术形象不清，公鸡下蛋，干了自己不擅长的事情，在学术圈还怎么混？     4、有可能得到国家的支持对于资深学者，他选定一个课题后，可以写出立项建议，去说服政府或军方支持他的工作，从而填补国家空白，成为国内这个方向的先驱。哈工大的杨孝宗老师借鉴CMU在wearable computing方面的研究成果，在国内率先提出穿戴计算机的概念，坚持多年，就获得了军方的认可。对于刚出道的年轻人，无力直接影响政府，那只有自己预先判定一个几年后可能成为热点的方向，先走一步，做出一些成绩来，等到大气候适宜的时候，由于他已经取得了一定的成果，也有可能被认可为这个领域的先行者，得到国家的支持。      课题的类型     对一个课题的类型要有一个判断，是研究型的还是开发型的，如果是研究型的，要组织博士生们来攻关，鼓励大家大胆尝试，提出创见；如果是开发型的，要更多地召集硕士生们来做，强调利用一切现有的技术手段把技术或系统做到实用可靠。这两者要分的比较清楚，既不能通过各种打补丁的方法，或者说一大堆小技巧来对付研究型的课题，因为那样是做不出突破性进展的，也不能在开发类课题上总是异想天开，尝试还很不成熟的技术。如果是研究型课题，还要区别是基础研究还是应用研究，基础研究的结果不能直接被用户使用，类似重工业，应用研究的结果最终用户直接就能够用上，类似轻工业。对于基础研究，可以抛开具体应用的约束，专注于一些科学原理技术原理的突破。对于应用研究，则需要考虑用户的需求。课题还有长期(long term)和短期(short term)之分，长期研究的课题往往难度大，研究结果难以预料，短期项目则比较好预测，可以速战速决。 在一个具体的题目上作研究，应该遵从怎样的程序呢？我觉得可以概括为“螺旋式深入”，也就是在“阅读”，“思考”，“实验”，“写作”，再阅读。。。这四个阶段的时间分配可以根据实际情况灵活调整，刚进入课题的研究生阅读调研花费的时间要多一些，而在一个课题上已经开展了一两年工作的人则可能增量式地阅读资料，阅读时间自然比起步时少一些。专门用于思考、设计、推演的时间可能并不多，但思考是渗透在其它三个阶段中不断进行的，因此总的思考时间并不少。实验中编程的时间应该尽可能短，用更多的时间进行实验数据的分析。写作是常常被中国的研究生忽略的环节，写作的时间要足够长。收集资料，了解别人的工作，找出问题所在，针对性地提出自己的创意，用实验验证自己创意的正确性，总结归纳，撰写论文，发现新的问题，再收集资料，如此反复，这是研究活动的大致流程。     怎样阅读资料收集资料、阅读资料是从事研究工作的第一步，但是如何收集、阅读资料却很有学问，初学者如果没有得到足够的指导，常常走很多弯路。      1、 阅读重要的论文目前互联网上的信息量太大了，对每一条信息的重要性、可靠性的判断是一个人采集信息的关键环节。如果判断一篇论文是否重要呢？Google Scholar给出的引用数是一个有效指标，很多学者都引用的文章往往就是有价值的论文。有的同学觉得看中文论文容易，于是把自己能够查到的中文论文一网打尽，反复阅读，但是很多发表在三流刊物上为了评职晋级而炮制的论文完全没有阅读的价值，白白耽误了时间。即使是英文论文，国外一样有滥竽充数的文章，这样的论文引用数肯定低，用引用数可能很容易地把这样的论文淘汰掉。计算机领域的顶级会议论文非常重要，在NLP领域有ACL，在IR领域有SIGIR，在机器翻译领域有MT Summit，这些顶级会议的论文质量很高，内容很新，应该高度关注。期刊上的论文是一个作者或机构一个阶段的研究成果的总结，通常质量较高，但由于审稿及编辑出版的周期很长，因此内容不够新，适当关注即可。NLP领域的CL，机器翻译中的MT，信息检索领域的IP&M和JASIST等都是很好的期刊。进入一个领域，必须立即了解该领域有哪些顶级的国际会议和国际期刊。     2、以作者为线索理清脉络阅读论文一定要注意论文的作者是谁，研究机构是哪里，以作者为线索理一理就会发现全世界搞你这个方向的也就那么几个、十几个研究机构、研究者，以后就跟踪这些人的研究工作即可，还能够发现该作者的研究工作的演进脉络。如果拿到一篇文章就读，读完了也不知道作者是谁，时间长了，就会感到晕头晕脑，不知道从哪个期刊或会议上就会冒出一篇相关文章来，让你防不胜防。     3、阅读最新的论文学术发展很快，要集中尽力阅读近5年，特别是近3年的论文，对于5年前的论文，只看引用率最高的经典文章即可。      4、 抓住论文的要害读完一篇论文必须了解哪些关键内容呢？我觉得应该包括以下方面：作者为什么要做这项工作？要解决的是一个什么问题？作者在解决问题时遇到了怎样的困难？为了解决他的困难他提出了什么样的解决办法？试验结果是否可能真的证明他的方法好，数据是否充分，有没有和别人的工作，别的方法进行对比？你认为他的方法是否新颖，你从中学到了什么？该方法有哪些不足，你是否立即有了新的改进方案？如果有立即记录下来。带着上述问题，抓住要点，做好记录，一篇长文就会像庖丁解牛一样轰然倒下。     5、 批判式阅读真理越辩越明，我们读的是一篇学术论文，不是《圣经》，不能带着崇敬的心理去阅读，要像一个审稿人那样带着批判挑剔的心理阅读论文，在阅读中不断地找出论文中的问题，选题上的，方法上的，实验上的，表述上的，并不断地通过积极独立的思考给出自己认为见解。只有这样，资料才能够为你所用，而不会成为你的包袱。有的同学读资料，越读越丧失信心，发现别人做得太好了，自己的想法都被别人做完了，资料全读完了，自己也准备换课题了，这是失败的读法。中国的研究生要有信心，不要被国外所谓的名家吓住。中国的科研水平在快速提高，科研人员的素质也在快速提高。一位美籍华裔企业家在一篇文章中写道：“可不幸的是，除了很少顶尖学校的博士外，大部分博士所做的研究课题都是陈旧或者没有意义的。”不知道顶尖高校的含义是什么，但是我觉得我们的研究生要对自己的国家有信心，对自己的学校有信心，对自己的倒是有信心，对自己有信心。只要我们掌握正确的研究方法，广泛阅读国外最新的研究成果，大胆尝试自己人为正确的方法，充分释放我们的聪明才智，我们就丝毫不用对国外的研究工作顶礼膜拜。在科学研究上，欧美人从内心里是瞧不起我们亚洲人，我们中国人的，以至于欧美归来的学者们也以欧美为样板来评估我们教育科研体制，只要和美国不一样就是大错特错了，中国高校的教师们都是在误人子弟。我奉劝每一位研究生建立不崇拜权威，不崇拜欧美，只服从真理的独立思维模式，大胆质疑大胆批判，只有这样才能不死于他人之言下，才能有活脱脱的自己。     怎样思考     1、把问题定义清楚  有的同学做了很长时间的课题，还没有把问题定义清楚。以自动文摘问题，好像就是把一篇文章中的核心内容提取出来吗，还怎么定义，其实不然。文章是什么样的文章？议论文、记叙文、还是说明文，还是包括小说之类的文学作品？文章的来源是哪里？是《人民日报》等用语非常规范的文章，还是网络文体，比如blog或BBS上的文章？文章有没有长度上限，10万多字的博士论文是否需要提取摘要？能否对多篇话题相同的文章一起做摘要？再说摘要，适用于阅读的报道性文摘，还是用于判定主题相关性的指示性文摘？有字数限制还是比例限制？是否根据用户的需求有所偏向？对文摘的连贯性有没有要求？怎样评价一篇文摘的质量。只有把问题的初始状态（此处为原文）和问题的终止状态（此处为文摘）搞得一清二楚才敢说问题定义清楚了。“知止而后定”，问题定义清楚了，把子立好了看清了，心也容易静下来，再怎么做工作始终围绕一个目标，这样的工作才不会左右摇摆，才有意义。     2、思维逻辑要干净  思维逻辑啊，思维逻辑！我越来越觉得这是一个人从事科研工作的最重要的素质。我的学生有两种，大多数人的思维是非常干净的，而确有少数同学的思维是混乱的、粘滞的。思维逻辑混乱的同学需要通过不断的自我认识，找出自己的误区，否则在前进的过程中会步履维艰。打一个比方，人脑好比一台计算机，阅读资料是数据输入，加工处理则要靠思维逻辑，思维逻辑如果混乱就仿佛是程序有Bug，输入的数据再充分再正确也无济于事，甚至由于处理能力有限，输入的阅读，处理起来越乱。有的同学还钻牛角尖，抓住一个很细节的无关紧要的问题不放；有的同学思维的跳跃性非常大，一会在问题的高层思考，还没有讨论清楚，忽然又跳到一个底层的细节上；有的同学听不进别人的意见，满脑子只有自己的声音；有的同学不懂得矛盾对立统一的道理，总想把问题绝对化，造成自己很大的困扰；有的同学做惯了开发，总是想着怎么把一个系统实现，而不是把注意力集中在创新上；有的同学不善于剖析问题，分析数据，不懂得先分析再综合的思维方法，总是在问题的表层打转转，始终无法深入。。。。。。计算机大学本科专业科中的不少内容在今后的工作中都不能够直接地排上用场，但是有一位在医科大学教计算机课程的老师向我反映，她叫医科大学的学生编写程序非常困难，因为思维逻辑完全不对，医学需要大量的死记硬背，而很多病症都是模模糊糊，需要大量经验，而计算机的思维是非常严谨周密的。教管理学院学生的，教人文学院学生学习计算机的老师们也都有自己不同的感受。这是什么原因呢？原因就是计算机本科专业科，比如离散数学（包括数理逻辑）、算法与数据结构、高级程序设计等课程给计算机专业的学生奠定了一些思维逻辑上的基础，思维方式在无形地对一个人未来的工作发生着重要的影响。我有一个同事是从理科转过来学计算机的，我和他就明显不同，他很少编程，但每次做了一个试验后，他习惯于花大量的时间对数据进行分析。而我对数据分析不够重视，思维倾向于怎样巧妙地把搞出一套方法来，实现一个系统，解决一个问题。理科偏重于解释世界，工科偏重于实现系统。我们只有通过不断的内省，发现自己的思维方式，善加利用，有所修正，才能够顺畅地开展科研工作。     3、分析与综合 分析与综合是两把思维的利器，一定要好好运用。通常一个问题来了，我们感到无所适从，不要着急，请先使用“分析”这把“刀”，把问题划分为若干子问题，子问题之间的关联越少说明这一刀砍的越是合理，如果实在非要“连着骨头带着肉”，也没有关系，但是要记录下子问题之间的照应关系。子问题如果很容易解决了，就是本原问题，不用再分，如果还是比较复杂，可以进一步分析，得到一些“孙子”问题。经过深入分析，一个貌似强大的问题已经被我们看得清清楚楚，每个本原问题都比较容易找到解决方法了，研究者的精神也可以放松一下。但是，搞研究不是做工程，不能满足于用打补丁的办法解决一个具体问题。在分析之后，还有在拿起另一把武器“综合”，“综合”的作用是合并同类项，比如通过对子问题1、4、5的考察觉得这个问题象是一个球体，通过对子问题2、3的考察觉得这个问题象是一个圆形平面，经过归纳则可以给这个问题建立起“半球体”模型，很可能就接近真实情况。归纳后可以演绎一下，看看灵也不灵，那就是用测试数据进行测试了。概括地说，分析是分类并考察每一类的特征，分析是显微镜，帮我们看清了问题的每一个细节。综合是尽可能地找出统一的模型概括各类现象，统一的模型可能是多个模型的融合，但最好不是简单拼接，而是激光焊，在分子层面把多个模型融为一体。分析决定深度，综合决定高度，缺一不可。顺便给出一个观点：普遍认为国内存在着低水平充分研究的现象，比如搞搜索引擎，从很少有人涉猎到一下子冒出很多家来，都在搞搜索引擎，而且大多数都处于起步阶段，难分伯仲。大家都觉得研究空间狭窄，竞争激烈，但又不知道怎样解决这个问题。其实对一个课题进行深入分析，把大课题分解为小课题，各家不搞大而全，或者即使全，但在“全”中都有自己明确的重点，比如专注于搜索引擎中的分布式计算问题，或专注于跨语言检索问题，或专注于问答系统等精准搜索，等等。对于规模小一点的组，甚至可以专注于更细的问题，比如问答系统中的问题分类，跨语言搜索中的查询翻译及扩展等。我们知道大树的根，越深的地方分差越多，覆盖的泥土空间也越大，研究也是这个道理，都浮在表层，就会感到空间狭小，如果深入下去空间就大了，不容易撞车。而且在一个细分的问题上，可以集中优势兵力作出突破性的成果来，又因为研究同一个细分问题的学者相对较少，研究工作的积累也相对不足，你动一动就可能做出新的成果来。因此，我的建议是：深入、深入，再深入。     4、 创新思维 呜呼哀哉，我的弟子中真正有创新能力的寥寥无几啊。我一直在思考如何挖掘和培养学生们的创新能力，在此谈谈自己的想法。创新是科研工作的灵魂！“不创新，无宁死”，每个科研人员都应该有这种决心。科学界没有“省级运动会”，“全国运动会”，只有“奥运会”。你说你是中国首次提出“狭义相对论”的人，毫无意义。那么，创新就应该是在世界上内第一个提出某个想法的人，如果你的想法在地球的某个角落里已经有人提出了，那就不是创新，那就是重复，是浪费科研资源，浪费人力物力。人生短暂，大多数芸芸众生都在干着日复一日的重复劳动，有幸成为科研工作者，可以表现一下自己独特的想法，何其快哉，如果拾人牙慧，又何其痛哉。创新这件事没有固定的套路，如果有，就好像炒股票有了确定的获利方式一样，大家就都能赚钱，都能创新了。我很喜欢岳飞的一句话：“运用之妙，存乎一心”。史书记载：岳飞英勇善战，受到宗泽的赏识和器重。一次宗泽召见岳飞，说：“尔智勇才艺，世良将不能过，然好野战，非万全计。”因向飞传授作战阵图。飞说：“阵而后战，兵法之常，运用之妙，存乎一心。”宗泽听了以后，深为赞赏。搞研究象打战一样，固定的阵法战法也有，但真的想取胜需要“奇兵”，所谓“以正合以奇胜”。怎么出奇，完全靠指挥员的心思一转。什么样的人擅长创新的，我觉得首先是那些平素喜欢天马行空胡思乱想的人，孔子有一个非常有才具的学生叫子贡，但孔子说他只能问一知三，说他不如颜回，颜回能够问一知十。触类旁通，在自己的识体系内利用各种相似性建立关联，就为创新创造了条件。“草圣”张旭的草书是从公孙大娘剑舞中悟出的，因为张旭通过多年苦练对书法的基本技巧已经炉火纯青，再想发展在书法本身上已经很难汲取新的营养，最后他从剑舞中悟到了获得了新的草书结构，艺术上峰回路转，又上层楼。有不少同学，做研究只看和自己的课题最相关的文章，如果是一个新的领域，文章很少，他就会感到很苦闷，他更不知道从领域之外后的灵感了。搞文本检索的人，要了解一下图像检索的知识，从中可以获得启发，反之亦然。语言模型原来就是在语音处理中使用，现在却成了文本处理领域最成熟的理论方法。类比、嫁接从来都是创新的重要手法，视野不开阔，只知道自己眼皮底下的一点点东西，始终跳不出自己给自己设下的思维陷阱，是无法提出新想法的。创新还必须从自己独特的体验中来。以诗歌为例，诗反映的就是一个人独特的生命体验，因此最忌讳用一些大众常用的泛泛的词汇，比如形容长江大河，不能说“浪涛汹涌”，要说“惊涛拍岸，卷起千堆雪”。搞研究也是如此，要用你自己的眼睛去观察数据，发现别人没有发现的特征，发现别人没有发现的故障点，找到别人不曾用过的观察视角重新观察你的研究对象，人与人的阅历不同，观察和思考问题的方式各异，如果你能够再重复调研的基础上，以“我”为主，把自己的原始体验经过归纳总结表达出来，那么即使是很小的一点进步，因为与众不同，那也是一个有价值的创新。何况，在你这一个课题上，全世界能够有个人坚持不懈地干上两年三年，并不多，就怕你没有自我，只要坚持你自己，表达你自己，即便资质差一点，也一定能够有所创新。创新有种种，开辟一个新的领域，提出一个新的问题，是大的创新。在研究生阶段不容易做到，可以先扎扎实实地做一些方法层面的创新。方法上的创新也有大小之分，“模型创新”就属于比较大的创新，“模型”永远不等于“实际”，模型是对实际的最大限度的逼近，对于相同的输入，好的模型能够给出与真实情况更接近的输出。比如信息检索中有向量空间模型，这个模型因简单而常用，但是他没有考虑词项之间的关联，如果能够提出一种新的模型，把词项之间的项目制约关系也考虑进去，就有可能获得和更好的效果。这种创新还比较直接，如果能够彻底推翻向量空间模型，提出类似LSI（隐性语义索引）之类的模型，则是更大的创新。再比如长尾真提出基于实例的机器翻译，就是思维一转，对翻译过程看成实例匹配，而不是查词典和调序，这属于比较大的创新，也是我认为很有意义的创新。如果提出新的特征，或新的特征抽取方法，或采用别人在该问题上没有用过的机器学习方法等，只要能够说出道理来，也都是创新，只是不那么激动人心而已。创新思维是求异思维，不是求同思维，高人出手，变化多端，无所不用其极。要从东西南北，上下左右去观察事物，如果面对的事物太庞大，压扁了再处理行不行？切成碎片再处理行不行？烧化了变成水再处理行不行？在地面上实在处理不了，运到海底行不行，送到空间站上行不行？图像处理中有从空域到频域的变换，在空域里剥离不了的噪声到了频域里很容易分辨出来；在词义消歧中，bank不知道是“河岸”还是“银行”，放到宏观上下文一看，比如知道这边文章是金融类的，那它十有八九是“银行”。创新是要“悟道”，很多时候要靠直觉，直觉就是大脑的并行计算，它把各种信息综合在一起，给出答案来。因此，你的灵感来了，要赶紧抓住，我有时有了好点子，身边没有纸笔，就感谢写到手机里，生怕忘记。 在事业上每10年就是一代人，我们这一代（70年前后出生）是承前启后的一代。我们读书的年代没有互联网，与国际的交往也非常少，听说过ACL/Coling就不错了，都没想过去参加。在这种相对封闭的条件下成长起来的人，要想在一个点上做出国际领先的成果，是非常困难的。博士毕业后，多数人都承接前辈，成为课题组的青年掌门，作为掌门，要管理团队，要跑项目，事务性的工作太多，在点上的钻研不够。台湾苏克毅对于大陆青年学者过多地陷入管理事务深表惋惜，但是国情如此，一时不容易改变。我想我们这一代的使命应该是在老一辈的基础上把国家的科研工作体系进一步完善起来，在“面”上把握住研究方向，带领80年前后出生的一代人走上国际舞台，在他们中间发现优秀的学术人才，为他们创造一切可能的条件，支持他们在“点”上真正做出世界水平的发明创造，成为世界级的优秀学者。      怎样成为优秀的学者      1、 基本功 无论干哪个行当，基本功都是至关重要的，任何令人眼花缭乱的高级技巧都无非是一些基本招式的组合及变型。基本功不扎实，就会常常感到捉襟见肘，按下葫芦起了瓢；相反，如果基本功扎实，则可以左右逢源，不受牵绊，日有所进。做研究的基本功包括数学基础、编程能力、专业基本知识、英文阅读、创新能力、写作能力、英文口头交流能力、组织能力和社会活动能力。我一口气列出了9项能力，而且基本上是按照一个学者成长过程在不同阶段能力需求的先后次序开列的。刚入门时，有的同学由于是从外专业转到计算机专业来的，因此可以接着课程学习的时间补一补离散数学、概率论、线性代数等方面的数学基础。否则一旦进入了课题，发现有的公式看不懂，再回头看数学书，由于心情急迫，就直接翻到相关章节阅读，又看不懂，发现需要前导知识，于是下决心啃整本数学书，啃了头两张，又觉得进度太慢，心里发慌，如此反复，数学基础就越补越夹生了。因此，一定要在课程学习阶段把相关的数学基础打牢，并不需要记住每个公式，但是基本的概念和原理要非常清楚，进入课题后，如果遇到看不懂的公式，能够很快地查书解决即可。编程能力必须尽快过关，否则影响科研速度。编程不能抱着一本编程的书，死学其中的样例，而应该积极地从老师或师兄那里领到很小的编程任务，比如文本的预处理，或者一个演示的界面等，有了实际任务，也就有了压力和动力，同时也便于融入研究小组，得到老师和师兄的指点，这样进步会很快。导师会帮助你选一些对课题有帮助的研究生专业课，这些课程务必要认真学习。由于很多选修课没有考试的压力，我发现学生们学得不够认真，基础没有打牢，进入课题后很多概念不清楚，还需要再学习，耽误了时间。什么是功底，那就是对专业知识的一点一滴的积累，将来进入学术界，别的学者提到一些基本概念如果你都不知道，或者模模糊糊，就贻笑大方了。人生每个阶段应该完成的事尽量要在那个阶段完成，如果欠了账，将来想补，则要偿还三倍、五倍的利息，也未必能够补回来。人到中年，猛回头，发现自己的基础如同蜂窝，到处都是漏洞，这样的材料就注定无法做栋梁了，那时悔之晚矣。每个人都有一定的英语阅读能力，但是阅读能力的强弱差别很大。阅读能力弱的人读英文好似雾里看花，总觉得隔了一层，似懂非懂，读得很慢，却仍然抓不住重点，读完了，一周不看就忘了，仿佛是没有读过的新文章。阅读能力弱的原因有两个，一个是语言问题，另一个是专业基础知识不足的问题，因此除了多读之外，专业知识的积累对阅读的深入和速度也会有帮助。创新能力、写作能力，我在前面说的较多，不再赘述。在此谈谈英语口头交际能力。这个能力是在参加国际会议时才会深切地感到它的重要性的。在国际会议上，各个国家的学者操着不同的英语相互交流，你的听说能力更不上就成了一个聋哑人，异常尴尬。和“说”相比，更困难的是“听”，常常出现一个亚裔学生作完报告后听不懂听众提问的情况，我自己也是如此，印度英语、日本英语、爱尔兰英语，我的天，你必须竖起耳朵，运用你全部的语音信号处理能力和背景知识加推理能力才能够懂个大概。开国际会议，中国人常常聚堆，这更障碍了你提供英语能力的机会，应该主动找外国人攀谈，不要估计自己的面子，要不断地挑战自己，走向国际舞台，而不要退缩在一个角落里。现在的研究生比我们那时候得到了更好的英语训练，只要拿出自信来，即使第一次出国，不少学生就已经具备了和外国人直接交流的能力。实际上，外国人和你说上一两句，就能够判断你的英语水平，他们也会根据你的水平调整他的用词和语速，再加上你们的谈话一般限定在特定的技术话题，因此交流的难度已经大大降低了。更深层次的交流是能够了解外国人的文化，进而使谈话范围突破学术，和外国人交上朋友。有了上面的能力，你可以成为一个合格的“学术单兵”。个人能力强，先做好一个单兵很重要，但随着经验的增长，你开始指导一些本科生或低年级的研究生开展工作了，两三个人就是一个团队，组织能力强的人可以把自己的团队带的生龙活虎，“嗷嗷叫”，反之则团队内产生摩擦，反而降低效率。高年级的研究生毕竟不是老师，没有老师的权威，也不能总通过“打小报告”对自己的“小弟”构成威慑，他应该通过自己的为人、才华吸引低年级的同学，首先得到他们发自内心的尊重，让他们意识到跟随自己能够学到知识，然后再用憧憬、鼓励、恳谈、告诫等管理方法激发他们的热情，带动他们跟随自己一道工作。根据他们的能力和时间合理地安排任务，随时给予指导，并从生活上对他们进行关心，和他们成为朋友。要成为大学者，必须在学术界，以及相关的企业界、政界建立自己的人脉，从而有机会整合更大的资源，做出更大的事业，因此社会活动能力对于大学者是必不可少的。综合型的人才跨国个人学术原始积累阶段后，会成为学术组织者，此时他们的社交才能会得到充分的发挥。      2、 学术诚信 我个人是很赞成学术打假的，今天的学术界越来越受到商业的影响，虚的东西太多，需要有“方舟子”这样的人，震慑那些造假蒙事的所谓学者。作为研究生，要严格避免做犯以下错误：抄袭（包括在文章中混淆自己和他人的工作）；一稿多投（包括一篇文章略加修改就另投他刊）；编数据，刻意裁减数据，或改数据。我的邻居原是一个学院的副院长，他在编教材时被指整页抄袭了别人的书，受到原著者的追究和学校领导的批评，书生心窄，很快得了肺癌去世了。还有一位同学，把一篇英文论文的主要内容翻译过来，作为其论文的部分内容投往了国内一家重要期刊，结果被该刊物认为是剽窃，申辩无效，她受到学校的通报批评。在治学问题上，不能存在丝毫的侥幸心理，否则后果非常严重，不但个人学术前程被阻断，也使导师和你所在的研究机构蒙羞。也不要做自吹自擂，比如：在文章中说自己“第一个提出了某某方法”；不要打击同行，对同行的工作要客观而委婉地评价，绝不能不负责任地随意针砭；不要没参与工作，随便挂名。      3、学术勇气与耐力 年轻人一定要有学术勇气，敢想敢干，绝不能墨守成规，绝不能迷信权威。权威往往受到已有成见的束缚，他们忙于社会活动，不在研究第一线，他们的观点未必正确。因此，如果你得到了与权威不同的结论，并确信是正确的，要敢于坚持，当仁不让于师。学术界有很强的马太效应，喜欢锦上添花，不喜欢雪中送炭。在你没有被业内认可以前，你的能力和成绩很可能会超过你获得的回报，在怀才不遇之际，个别学者会踏上脱离学术圈开展独立研究的道路，窃以为不可，要做大事，需要耐住寂寞，脚踏实地地工作，学会与人合作和交往，迟早破壳而出。     4、进入科学共同体 在你具备了一定的基础后，可以通过发表论文，参加会议的方式进入学术界（科学共同体），这叫做入行了，出道了。在会议上宣读论文，听完别人的报告后当众提问或提出建议，在会间休息时主动结识知名学者，多多结交同龄的研究生和青年学者等等，都是进入“圈子”的途径。在公众场合，既要虚心地向成了名的“剑客”“侠客”们请教，又不必卑躬屈膝，令人生厌。“弱国无外交”，提高自身的研究水平是本，学术交往是末，你自己的工作做得出色，别人也乐于和你交往，反之，自己的工作稀松平常，得不到专家们的认可，只知交往，适得其反。     5、满怀研究乐趣世界上有一种职业，别人花钱让你做你喜欢的事情，那就是“研究”。经常有激动人心的发现，充分表现自我，其中的乐趣“如人饮水，冷暖自知”。科学家静心从事研究工作时发出阿尔法脑电波，据说和僧人入静时发出的脑电波是一样的，对人的身心健康很有好处，因此很多学者非常长寿，中文信息学会的创始人钱伟长先生今年97岁，用他的名字设立的首届“钱伟长奖”颁给了92岁的“亚伟速录机”的发明人唐亚伟先生。如果你喜欢旅游，那么更要从事科学研究了，学术会议常常在风景如画的地方举行，阳光沙滩海浪，学者们在阳伞下轻松交谈，没有政界的险恶，没有商场的喧嚣，何等惬意。学者自然不如企业家富有，失去一些财富，换来的是自由。今天是初九了，我的春节假日彻底结束，啰里啰唆，匆匆忙忙地了这许多文字，能够读到此处的读者一定很辛苦了，希望其中的一两个观点能够对您有所帮助。（完）","title":"如何做科研"},{"content":" 日常工作中经常要查找这方面的相关文章，做个索引，并不断更新，以便日后查找。   盘古分词：http://pangusegment.codeplex.com/   LingPipe 自然语言处理工具包的博客  Google （谷歌）中国的博客网志  博客园搜索引擎团队博客  Blogs about: Text Analysis  Lucene 源码剖析  The Porter Stemming Algorithm  SharpNLP WordNet  Artificial Intelligence articles  Sphinx-free open-source SQL full-text search engine      专题 布隆过滤器 布隆过滤器  Bloom Filters in C#  序列化 ProtoBuf.net  文本分类 矩阵运算和文本处理中的分类问题 ","title":"自然语言分析、文本分析、全文索引，搜索引擎相关资源汇总"},{"content":"企业对Hadoop以及大数据相关技术的兴趣日益高涨，这同时也让大数据技术方面的专家成为炙手可热的人才。 在本周于纽约举行的Hadoop全球大会上，众多分析师与IT管理者一再强调目前企业所面临的主要挑战之一就是在部署Hadoop方面人才匮乏。他们甚至表示，只要技术娴熟、堪当重任，企业愿意为这类员工缴纳健康保险。 目前相关人才之短缺从以下事例中可见一斑：来自JP摩根大通公司以及eBay的IT高管们在会上发展主题演讲，以借机汇集观众。 eBay公司体验、搜索与平台部门副总裁Hugh Williams向观众们表示，目前他们正在诚意招聘Hadoop专业人员，并热情邀请感兴趣的朋友与他面对面进行交流。 而JP摩根大通公司总经理Larry Feinsmith则半开玩笑地声称，他们不仅乐意雇用合格的专业人士，更会提供比eBay高出10%的优厚待遇。 在企业内部，“Hadoop可谓次世代的数据仓库，应该被看作新型数据源，”Forrester研究公司分析师James Kobielus如是说。“能够熟练使用Hadoop的人才理应获得更理想的薪酬，”因为他们对企业的帮助极为巨大。 比起当下常用的传统数据库管理系统，Hadoop使得企业可以存储并管理更为庞大的结构化与非结构化数据卷。 越来越多的企业开始试探性地摸索这一技术，旨在为像博客、点击流数据以及社交媒体内容这样数据量甚巨的服务提供存储及分析功能，以期获得更进一步的客户及业务洞察能力。 Kobielus认为，随着企业部署量的增加，此类分析技术人才的身份也必然要水涨船高。而且很多领域的专业人士都有潜力在Hadoop上大显身手，例如具备多元统计分析、数据挖掘、预测建模、自然语言处理、内容分析、文本分析以及社交网络分析等职业背景的技术人员。 “更加广义的大数据——特别是在Hadoop当中——需要管理者具备高级分析领域的工作经验，例如使用像MapReduce及R这样的新生代方案处理预测及统计建模，”他说道。他同时补充说，以上这些正是数据分析师或数据科学家在Hadoop环境中经常要应对的结构化及非结构化数据，在它们的帮助下企业能够显著提升洞察力及商务智能性。 Hadoop赢得的广泛关注也给企业带来了对Hadoop平台管理专家的刚性需求，Kobielus表示。他们工作是负责Hadoop集群、安全及管理，并对其进行优化以确保集群对企业的可用性。Hadoop应用程序之所以能够顺畅工作，靠的是“这些专家为其搭建及优先的运行平台”，他说道。 “原先负责管理Teradata及甲骨文Exadata的数据库管理员往往正是意图向Hadoop集群管理进发的主要群体，”他说。“他们会意识到这是一片全然不同的天地。”此外，存储管理专家也不可或缺，他们所做的工作能够帮助Hadoop环境与现有传统数据库技术相结合。 目前对Hadoop专业人员的需求主要分三大类：数据分析师（又称数据科学家）、数据工程师以及IT数据管理专家，Martin Hall如是说。他是Karmasphere公司总裁，该公司的主要经营项目正是为Hadoop环境开发软件产品。 Hall认为，数据管理专家的职责在于选择、安装、管理、规范以及扩展Hadoop集群。正是这些专家决定了Hadoop是应该立足于云还是采取预置模式，包括供应商该如何选择、使用哪一款Hadoop分布方案、集群规模以及被用于运行生产应用程序还是用于进行质量测试等。这一职位应该具备的技能，与负责传统关系数据库与数据库环境类的任务颇为相似，他指出。 同时，Hadoop数据工程师还要负责创建数据处理工作以及建立分布式MapReduce算法，以供数据分析师使用。那些在Java和C++等领域技能水平较为突出的专业人士能够在企业大规模部署Hadoop的浪潮中寻得更多机会，他说道。 第三类专业需求则是在SAS、SPSS以及以R为代表的编程语言等方面具备丰富经验的数据科学家，Hall指出。此类专业人士能够将建立、分析、共享以及智能整合加以集中，并存储于Hadoop环境当中。 就目前来看，Hadoop领域的人才短缺意味着企业会更加依赖于服务供应商提供的部署技术。支撑这一论点的一大迹象是，在专业的咨询及系统集成行业内，专攻Hadoop实际应用带来的收入要远远大于Hadoop产品销售所带来的收入，Kobielus如是说。 像Cloudera、MapR、Hortonworks以及IBM这样的企业如今已经在提供Hadoop的相关培训课程，我们应该充分利用这些资源，通过建立Hadoop卓越中心使自己的企业获得最大收益，他提醒道。 http://os.51cto.com/art/201111/301884.htm Ubuntu 11.10正式发布 http://os.51cto.com/art/201105/264870.htm Ubuntu 11.10加速企业安装Hadoop http://os.51cto.com/art/201111/300178.htm","title":"Hadoop人才需求高涨 你准备好了吗"},{"content":"                              By 何明桂（http://blog.csdn.net/hmg25） 转载请注明出处 Iphone4S的Siri让人眼前一亮，网上出现了无数调戏Siri的视频。真是让android用户们心痒不已。好在随后android阵营中的高手迅速反击，推出了Iris。悲剧的是Iris仅支持英文，让我们这些英语烂的无比的人调戏Iris不成，反被它给调戏了。真是郁闷的不行啊~_~ 所以我打算使用android的资源自己打造一个中文版的Siri，让我们用中文随意的来调戏它。(我自己做了一个简单的，哈哈，放在优亿市场里，有兴趣的童鞋可以去体验下http://www.eoemarket.com/apps/61634)    首先，我们来分析Siri的构成，应该大致可以分为3个组成部份：语音识别、自然语言处理、语音输出。对于语音识别，我们可以使用google的语音识别API进行语音的识别，讲语音转成文字。语音输出，其实就是使用TTS，讲文字进行语音合成播放出来，这个android也是有接口可以利用的。真正核心的是自然语言识别处理这个部分，Siri功能的好坏判断很大一部分是取决于此的，这需要很大一个数据库来维持运转，在本地是无法实现的，即使iphone的Siri也是讲语音识别的指令语音上传到Apple的服务器上去解析后返回。由于apple的接口不开放，所以我们无法使用他们的接口，好在世界上拥有这样服务器的不止苹果一家，android上的Iris利用的就是http://start.csail.mit.edu/（自然语音问答系统）这个网站提供的接口以及一个叫做cleverbot的一款智能聊天平台http://www.cleverbot.com/这个聊天网站是支持汉语的，不过，只是支持拼音输入——汗啊。    所以我们的核心任务就是寻找一个支持中文汉字输入的问答系统。经过在网络上长时间的搜索，结果发现——很遗憾，没有找到(PS:如果有谁找到了比较好的网址，麻烦共享，告诉我一声)，不过对于我们调戏Siri的这个需求，我找到了一个较好的替代品——聊天机器人.http://www.xiaoi.com/widget/1007/小i智能聊天机器人。    经过短时间的摸索，我实现了一个类来，初始化连接小i机器人的接口，发送数据以及接受反馈。用到的接口地址如下：     private String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\";\tprivate String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";\tprivate String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";       http连接上边的Webbot_Path，会反馈回来一些数据： var L_IDS_SEND_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";var L_IDS_RECV_MESSAGE_URL = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";var L_IDS_GET_RESOURCE_URL = \"http://122.226.240.164/engine/widget1007/getres.do\";var __sessionId = \"86491993134658194\";document.write(\"<script src='http://122.226.240.164/engine/widget1007/_core.js?encoding=utf-8&'><\\/script>\");       反馈回来的数据包 括上边的发送和接收地址，以及一个sessionId，这个sessionId很重要，类似于一个key，用于后边的会话中。由于发送和接收地址是固定的，可以直接写死，但是sessionId是变动的，所以首先需要将它从反馈回来的茫茫数据中提取出来，我使用的是一个简单的正则表达式：          String strResult = EntityUtils.toString(httpResponse.getEntity());\t Pattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId\t Matcher m = p.matcher(strResult);\t if (m.find())   mSessionId = m.group(1);     得到sessionId后，我们就可以进行初始化了，初始化的过程很简单，将sessionId将填入下边格式中，发送到服务器去就行了。 String   strSendJoin = Send_Path+ \"SID=\"+ mSessionId+\"&USR=\"+ mSessionId+ \"&CMD=JOIN&r=\"; 初始化完成后，就可以使用下边的格式网址发送问题以及接收答案： String strSend = Send_Path + \"SID=\" + mSessionId + \"&USR=\"+ mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg +\"&FTN=&FTS=&FTC=&r=\"; String strRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\"+mSessionId + \"&r=\";xiaoi.sendMsg(mQuestion); results = xiaoi.revMsg(); 接收到的内容也是需要提取的，使用的是正则表达式：     String  msgTmp = EntityUtils.toString(httpResponse.getEntity());   Pattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\");   Matcher m = p.matcher(msgTmp);\tif (m.find()) {\t   msg = m.group(1);\t}     通过上述的小i聊天机器人的接口，你便可以实现一个简单的，可以自由聊天对话的Siri。小I机器人还是很智能的，聊天的对话也很有意思，但是仅仅只能聊天，这个和iphone Siri的差距太大了，所以稍后我们将给它添加另外一个智能的大脑。 本文完整代码如下： public class XiaoI {\tprivate String Webbot_Path = \"http://webbot.xiaoi.com/engine/widget1007/webbot.js?encoding=utf-8\";\tprivate String Send_Path = \"http://122.226.240.164/engine/widget1007/send.js?encoding=utf-8&\";\tprivate String Recv_Path = \"http://122.226.240.164/engine/widget1007/recv.js?encoding=utf-8&\";\tprivate String mSessionId = null;\tprivate HttpClient httpClient = null;\tpublic boolean initialize() {\t\tboolean success=false;\t\t\tHttpParams httpParams = new BasicHttpParams();\t\tHttpConnectionParams.setConnectionTimeout(httpParams, 30000);\t\tHttpConnectionParams.setSoTimeout(httpParams, 30000);\t\thttpClient = new DefaultHttpClient(httpParams);\t\ttry {\t\t\tString strGetId = Webbot_Path;\t\t\tHttpGet httpRequest = new HttpGet(strGetId);\t\t\tHttpResponse httpResponse = httpClient.execute(httpRequest);\t\t\tif (httpResponse.getStatusLine().getStatusCode() == HttpURLConnection.HTTP_OK) {\t\t\t\tString strResult = EntityUtils.toString(httpResponse\t\t\t\t\t\t.getEntity());\t\t\t\tPattern p = Pattern.compile(\"sessionId = .(\\\\d+)\"); //get sessionId\t\t\t\tMatcher m = p.matcher(strResult);\t\t\t\tif (m.find()) {\t\t\t\t\tmSessionId = m.group(1);\t\t\t\t\tString strSendJoin = Send_Path + \"SID=\" + mSessionId\t\t\t\t\t\t\t+ \"&USR=\" + mSessionId + \"&CMD=JOIN&r=\";\t\t\t\t\tHttpGet httpRequest1 = new HttpGet(strSendJoin);\t\t\t\t\thttpResponse = httpClient.execute(httpRequest1);\t\t\t\t\tString strRevAsk = Recv_Path + \"SID=\" + mSessionId\t\t\t\t\t\t\t+ \"&USR=\" + mSessionId + \"&r=\";\t\t\t\t\tHttpGet httpRequest2 = new HttpGet(strRevAsk);\t\t\t\t\thttpResponse = httpClient.execute(httpRequest2);\t\t\t\t\tsuccess=true;\t\t\t\t}\t\t\t}\t\t} catch (ClientProtocolException e) {\t\t\te.printStackTrace();\t\t} catch (IOException e) {\t\t\te.printStackTrace();\t\t} catch (Exception e) {\t\t\te.printStackTrace();\t\t}finally{\t\t\treturn success;\t\t}\t}\tpublic void sendMsg(String msg) {\t\tString strTalksend = Send_Path + \"SID=\" + mSessionId + \"&USR=\"\t\t\t\t+ mSessionId + \"&CMD=CHAT&SIG=You&MSG=\" + msg\t\t\t\t+ \"&FTN=&FTS=&FTC=&r=\";\t\tHttpGet httpRequest = new HttpGet(strTalksend);\t\ttry {\t\t\thttpClient.execute(httpRequest);\t\t} catch (ClientProtocolException e) {\t\t\t// TODO Auto-generated catch block\t\t\te.printStackTrace();\t\t} catch (IOException e) {\t\t\t// TODO Auto-generated catch block\t\t\te.printStackTrace();\t\t}\t}\tpublic String revMsg() {\t\tString strTalkRec = Recv_Path + \"SID=\" + mSessionId + \"&USR=\"\t\t\t\t+ mSessionId + \"&r=\";\t\tHttpGet httpRequest = new HttpGet(strTalkRec);\t\tString msg = null;\t\ttry {\t\t\tHttpResponse httpResponse = httpClient.execute(httpRequest);\t\t\tif (httpResponse.getStatusLine().getStatusCode() == 200) {\t\t\t\tString msgTmp = EntityUtils.toString(httpResponse.getEntity());\t\t\t\tPattern p = Pattern.compile(\"\\\"MSG\\\":\\\"(.*?)\\\"\");\t\t\t\tMatcher m = p.matcher(msgTmp);\t\t\t\tif (m.find()) {\t\t\t\t\tmsg = m.group(1);\t\t\t\t}\t\t\t}\t\t} catch (ClientProtocolException e) {\t\t\t// TODO Auto-generated catch block\t\t\te.printStackTrace();\t\t} catch (IOException e) {\t\t\t// TODO Auto-generated catch block\t\t\te.printStackTrace();\t\t}\t\treturn msg;\t}} 使用方法：XiaoI xiaoi = new XiaoI();                 xiaoi.initialize();                xiaoi.sendMsg(mQuestion);        results = xiaoi.revMsg(); 由于发送接收耗时较多，最好放后台处理。","title":"打造Android的中文Siri语音助手(一)——小I机器人的接口"},{"content":" AI哲学：(符号主义、连接主义）， （行为主义，进化主义，群体主义）， 以及以上几个途径都不可回避的( 学习主义)。 符号主义：三个核心问题是知识表示、搜索和推理。[启发式搜索，极大极小搜索，Alpah-Beta剪枝], [归结原理，不确定性推理]。  [缺点：仅依靠逻辑推理，没有形象思维，使用的是搜索求解的机制]。 连接主义：以及结构模拟为核心，自上而下，用学习来调整网络连接中的权值，基本核心问题是网络结构和学习算法。( 感知器，霍普菲尔德网络，自组织特征映射网络三大类型网络）。适合在模式识别这些不太适合采用人工符号主义的领域。 行为主义：对生物行为能力的模拟，强调与环境的交互中逐步提高能力，基本特点是“感知--行为”模式、现场式智能、渐进式智能。强化学习是环境与系统之间建立最优映射关系的学习算法。 进化主义：对群体的进化来解决人工智能中核心的搜索（优化）问题。 群体主义：对过个体之间的协作来解决单个个体不能解决的问题，或提高单个个体解决问题的效率。（多智能体，蚁群优化算法，粒子群优化算法）。 学习主义：不单独存在学习主义，其它5条途径都离不开学习主义，主要有：机械式学习，指导式、类比、归纳和解释学习，重点的归纳学习有（决策树、朴素贝叶斯分类器，贝叶斯信念网，划分聚类方法，层次聚类方法)。   人工智能应用领域： 自然语言处理，计算机博弈（用来做实验），模式识别（即自动识别和分类， 主要用于文字、声音、图像), 计算机视觉( 三维重建）,专家系统，知识发现，智能控制（自动驾驶）。    符号主义： 搜索与问题求解:        问题的表示方法： 状态空间表示法 与 与或图。        状态空间表示法：将问题求解的每个可能的步骤表示成一个状态，全部状态与状态之间的转换构成一个以图的形式来表示的状态空间。        与或图：与或图的基础是问题的归约。 问题归约是指将问题经过一系列的分解(所有子问题都有解时，原问题有解)或变换(子问题中其中之一有解时，原问题有解)，不断降低问题难度。             盲目搜索算法：广度优先，深度优先，有界深度优先（比较好）。   => 八数码问题。        启发式搜索：可指导搜索朝着可能的“最快”方向前进的信息被称为启发信息，通常利用估计函数将顶点对应的启发式信息转化为其代价值。根据选择扩展顶点的范围，分别全局择优启发式搜索（即在Open表中所有点中选择一个估计函数值最小的点进行扩展）和 局部择优启发式搜索（在刚生成的子顶点集合中选择一个估计值最小的顶点进行扩展）。                A* 算法： 针对状态空间的启发式搜索。 估价函数f(Sn) = g(Sn) + h(Sn);   g(Sn) 表示从开始顶点S0 到顶点Sn的实际代价，h(Sn)是从顶点Sn到目标顶点Sg的最优路径的估计代价。A*算法的约束为, g*(Sn)是S0->Sn的实际最小代价, h*(Sn)是从Sn->Sg最小代价， A* 要求 h(Sn)<=h*(Sn), 即估计代价一定要小于实际最小代价。可以理解为，当h(Sn)> h*(Sn)时， 它所携带的信息不准确，可能在搜索中被排除。并且，当h(Sn) << h*(Sn)时，估计代价在搜索中起的作用被忽略，不能明确的指明搜索的方向故效率较低。从而， h(Sn) 要求在一定小于h*(Sn)的情况下， 尽量接近于h*(Sn)。         AO*算法：用于与或图的启发式搜索。??         博弈搜索：博弈树从选手的角度是一种与或树，可以使用极大极小搜索，在搜索过程中使用Alpha-Beta剪枝。 知识与推理：       连接主义： 人工神经网络： 人式神经网络的构造问题(如何模拟人脑结构) 与 人工神经网络的学习问题(如何自动确定连接的权值与网络的结构问题)。  生物神经元：树突、轴突 与 细胞体 分别对应信号的　输入、输出与处理。 细胞体内外有不同的电位，利用电位差来传递信号。 生物神经元的功能：时空整合（对同一突触的神经冲动有时间整合功能，对不同突触神经冲动有空间整合功能), 兴奋与抑制状态，脉冲与电位转换，传导，延时。 人工神经元：x1,x2,..,xn表示神经元的n个输入, w1,w2,...,wn表示对应的n个连接强度，s表示输出阈值即电位阈值, y表示输出。         整合函数：加权求和-s 或 径向距离         激活函数：。。。 人工神经网络的拓扑结构：                                                ","title":"AI"},{"content":"http://crfdecoder.sourceforge.net/ http://crfpp.sourceforge.net/ CRF中文分词开源版发布啦 langiner@gmail.com 中文分词是互联网应用不可缺少的基础技术之一，也是语音和语言产品必不可少的技术组件。   自2003年第一届国际中文分词评测以来，由字构词的分词方法获得了压倒性优势，国内主要通过CRF++开源软件包来学习该分词方法，但是CRF++过于复杂的代码结构，导致了该算法的普及率。本次首先发布了CRF中文分词开源版仅仅包含CRF++软件包中分词解码器部分，简化了CRF++复杂代码结构，清除了分词解码器不需要的代码，大大提高了分词解码器的可读性和可懂度。同时为了方便学习者可视化跟踪和调试代码，在Windows平台下分别建立了VC6.0和VS2008两个工程文件，使得VC6.0用户和VS2008用户都能轻玩转中文分词。 条件随机场 (CRF) 分词序列谈之一 Langiner@gmail.com   判别式机器学习技术来解决分词问题，其中判别式机器学习技术主要代表有条件随机场，最大熵/隐马尔科夫最大熵、感知机，支撑向量机等，有关它们的相同点与不同点以后有机会在谈，今天主要谈利用随机场解决分词问题 条件随机场(Conditional Random Fields)由John Lafferty提出并应用于自然语言处理领域，主要用于序列标注问题，如分词、实体识别、词性标注（当然词性数目相对比较小的情况下）、浅层句法分析等问题。 判别式机器学习技术解决分词问题基于由字构词理念，将分词问题转化为分类问题，通过定义每个字的词位信息（每个字在词中的位置）来确定字类别的序列预测，而词位信息可以定义为下面其中任何一种 两类（词首和词中）、三类（词首、词中和词尾）、四类（词首、词中、词尾，单字词）等等，一般而言，类别越多，字的区别能力越强，分类越精确，但是分类空间越大，模型也越大，解码空间越大，导致解码速度也越慢，实际系统中，三类别（词首、词尾和词中）是一个很好的平衡。 互联网上开源的随机场项目很多，最典型和使用最多的是CRF++，里面有完整的源代码和应用实例，通过该软件，我们可以方便学习和使用。CRF++开源代码最大问题是只有Linux版，考虑到Linux环境下，跟踪调试不方便，如果在Windows下通过建立Visual C++或者Visual Studio工程，通过跟踪调试，更加有效地学习该算法，本人将自己的学习实践中，将Linux版本移植到Windows平台上，并将其开源在SourceForge平台上（CRF中文分词开源版）。 利用哪些特征业绩如何利用这些特征是机器学习算法性能关键，中文分词主要使用字的上下文知识，上下文范围可以是3字、5字和7字，同时考虑到由字构词处理长词比较弱，可以考虑引入仿词模式、成语/习语等特征，有研究表示，加入核心词典会提高词典词的分类效果，这需要权衡，如果训练语料覆盖核心词典比较全面，核心词典的构词知识往往在语料中已经包含，但是如果训练语料对于核心词覆盖不够，可以考虑加入核心词的构词知识，但是这对核心词典有比较高的要求，我们认为北大计算语言研究所发布的GKB词典可以作为核心词典使用，如果没有比较好的核心词典，这个核心词的构词知识还是不要加入为好。  ","title":"CRF中文分词开源版发布啦"},{"content":"如果不上大学，你会面临一些什么样的问题？ 人际：由于中国的畸形发展模式，资源日益集中到一二线城市，正在形成以超大城市集群为核心的发展模式，你将缺少在大型城市，一个正常的人际交往圈子； 技能：没有四年大学缓冲期，用于寻找自身的兴趣与培育技能，尤其是一些需要花费较多时间来培养的技能，可能导致基础不牢； 自信：对自身学习能力不再自信，厌倦学习，最终失去更新知识结构的动力； 敲门砖：多数场合，学历证书的确是迈入职场的敲门砖； 社会舆论：亲朋好友，包括部分爸妈会在不上大学与没有发展前途之间划等号； 未来方向：多数人的大学四年，给了他/她一个基本的职场锚，清楚毕业之后，大致朝哪个方向与领域发展，而你，更多将陷于一场又一场为生存而进行的斗争，最终没有任何职业方向可言，什么能糊口我就干什么，很容易陷入恶性循环，人生自由度日益低下。 如何突破困境？ 请首先使用批判性思维，认真思考上述问题。然后尝试写下你的答案，我如果不上大学，我如何拥有一个合适的人际交往圈子，具备谋生的手艺，以及拥有较高的人生自由度，自信地面对未来？ 以下常规性思维，真的有理吗？ 人际：考试成绩不好=我只配拥有社会上混的的朋友圈 技能：考试成绩不好=我离开学校之后，可以完全放弃学习 自信：考试成绩不好=我恐惧一切与书本相关的内容 敲门砖：考试成绩不好=我可以选择最大众化的发展路线，比如去广州富士康 社会舆论：考试成绩不好=我只能通过赚钱来向亲友证明自己 未来方向：考试成绩不好=我只能屈从于一些容易解决生存压力的工作 在大学诞生前后，无数例子都可以打败以上思维定势。除了成功学虚构的竭斯底里例子之外，世界上还存在一些用时间与耐心来突破这种困境的例子。比如，以下所列： 我是如何从煤矿工成为程序员的 珍妮的故事 Stay Hungry， Stay Foolish ！！ 不上大学，你可以学些什么？ 10年前，没有 TED 与斯坦福公开大学；没有 hacker news 与Y基金；没有 github.com；没有 app store 与facebook 平台；没有各类天使投资机构与创业者聚会… 今天，接受一流教育理念与卓越智慧如此容易。如果不上大学，可以学些什么？以下资源以图书为主，涉及部分网站。 元资源 通过它，可以找到更多资源。 TED：尖峰创想，启迪心智 斯坦福开放大学：免费读大学，最新的类似于 IOS 开发与 Ruby 开发系列课程，你可以免费找到 MIT 开放课程：世界上最好的理工学院，关于人工智能等议题开启你的好奇心 YouTube：大量的学习视频都集中在这里 stackexchange：全世界的热心人士都集中在这里提问与回答问题 edge：牛人与牛人对话 PLOS：开源模式在学术界的应用，公共图书馆，你可以免费看到最新的论文；你也可以提交自己的论文 中文汉化： TEDtochina：TED 中文社区，不仅仅是翻译。 网易公开课：有 TED 及各个国外大学的开放课程的翻译。 MyOOPS 开放课程：来自全球顶尖大学的开放式课程，现在由世界各国的数千名义工志工为您翻译成中文。 思维 对思维的深度思考与刻意练习，将拥有较高人生自由度。 图书 一般来说，以下推荐的图书组合，以中文图书为主，top10往往是必读，其他的可以泛读与选读。 心智黑客：如何基于心理学与脑科学为主的科学理论，来提升心智？ 网站 mindhacks：国外著名博客，内容以介绍心理学与脑科学研究为主。 BPS Research Digest：常有最新研究。 写作 中国的作文训练让我们丧失表达欲望。如果不上大学，你可以通过西式的创意写作课程补上这一课。当恢复对写作的热情之后，再从中国传统文化中汲取营养。 开始写作吧：以西方的创意写作课程图书为主。 编程 一种独特的手艺，轻松参与到国际化竞争的手段。 图书 全世界程序员都说好的图书：在 stackoverflow 书单基础上整理，多为经典图书。其中1-12本为stack overflow 推荐，全世界程序员都说好的图书。 Ruby 与 Rails 开发基础书单：涵盖了 Ruby 与 Rails3新手所需要的主要技能。Ruby 与 Rails 的组合，能够快速进入 Web 开发世界。 与小朋友一起学编程：一些入门的计算机编程读物，面向小朋友写的。所以不用担心自己读不懂。涵盖 Python，Ruby与 Arduino 等。 经典开放课程 ios 开发：涵盖 ios5 与 ios4。 斯坦福 iOS2011 年开发教程 斯坦福 iOS2010 年开发教程 Web 开发：Ruby 与 Rails 斯坦福关于用 Ruby 与 Rails 进行 Web 开发的开放课程 更多课程 计算机科学与创业 斯坦福课程。内容相当新，将于2012年开始。请立即注册。 精实创业 科技创业 计算机基础 自然语言处理 机器学习 SAAS 人机交互 概率 博弈论 以下为 MIT 的计算机科学课程： MIT 的计算机科学课程 网站 github.com：世界上最优秀的程序员集中地带。 stackoverflow：世界上最热心的程序员集中地带，你的常见问题都可以在这里找到答案。 设计 设计逐步成为驱动世界发展的三巨头之一：教育、技术与设计。 图书 设计 网站 dribbble： 在这里，可以看到高质量的设计作品。 wherewedesign： 设计不仅仅是 Web 设计。 创新 图书 成为创新者：诞生伟大设计与产品的新模式：以 triz 及创新算法为主。 ReWork 系列图书：一些有干货的创业图书。 Rework 中文：小而美的创业之旅。 网站： paulgraham：一位对创业有独特思考的长者。 Hacker News： 技术创业者集中地带。 部分中文资源索引 除了学习之外，还有什么？ 建立信任！建立信任！建立信任！ 一个公司可以招募低学历的人才，但是，前提是对方信任你！除了学历之外，快速建立信任的方式就是用作品证明自己。 你可以，开一个个人博客。如果是2011年才开建，建议采用 octopress，记录个人的学习成长。 如果你是侧重开发领域发展，那么，立即在 github.com 上提交个人的代码，即使它仅仅是完成某项作业，以及登陆 stackoverflow 回答别人的问题。 如果你是设计师，那么，请登陆 dribbble 分享个人做过的工作。   源文档 <http://blog.csdn.net/wx1127524095/article/details/7011937> ","title":"不上大学 你可以学些什么？"},{"content":"2011-11-23 11:00来源: Starming星光社  如果不上大学，你会面临一些什么样的问题？ 人际：由于中国的畸形发展模式，资源日益集中到一二线城市，正在形成以超大城市集群为核心的发展模式，你将缺少在大型城市，一个正常的人际交往圈子； 技能：没有四年大学缓冲期，用于寻找自身的兴趣与培育技能，尤其是一些需要花费较多时间来培养的技能，可能导致基础不牢； 自信：对自身学习能力不再自信，厌倦学习，最终失去更新知识结构的动力； 敲门砖：多数场合，学历证书的确是迈入职场的敲门砖； 社会舆论：亲朋好友，包括部分爸妈会在不上大学与没有发展前途之间划等号； 未来方向：多数人的大学四年，给了他/她一个基本的职场锚，清楚毕业之后，大致朝哪个方向与领域发展，而你，更多将陷于一场又一场为生存而进行的斗争，最终没有任何职业方向可言，什么能糊口我就干什么，很容易陷入恶性循环，人生自由度日益低下。 如何突破困境？ 请首先使用批判性思维，认真思考上述问题。然后尝试写下你的答案，我如果不上大学，我如何拥有一个合适的人际交往圈子，具备谋生的手艺，以及拥有较高的人生自由度，自信地面对未来？ 以下常规性思维，真的有理吗？ 人际：考试成绩不好=我只配拥有社会上混的的朋友圈 技能：考试成绩不好=我离开学校之后，可以完全放弃学习 自信：考试成绩不好=我恐惧一切与书本相关的内容 敲门砖：考试成绩不好=我可以选择最大众化的发展路线，比如去广州富士康 社会舆论：考试成绩不好=我只能通过赚钱来向亲友证明自己 未来方向：考试成绩不好=我只能屈从于一些容易解决生存压力的工作 在大学诞生前后，无数例子都可以打败以上思维定势。除了成功学虚构的竭斯底里例子之外，世界上还存在一些用时间与耐心来突破这种困境的例子。比如，以下所列： 我是如何从煤矿工成为程序员的 珍妮的故事 Stay Hungry， Stay Foolish ！！ 不上大学，你可以学些什么？ 10年前，没有 TED 与斯坦福公开大学；没有 hacker news 与Y基金；没有 github.com；没有 app store 与 facebook 平台；没有各类天使投资机构与创业者聚会… 今天，接受一流教育理念与卓越智慧如此容易。如果不上大学，可以学些什么？以下资源以图书为主，涉及部分网站。 元资源 通过它，可以找到更多资源。 TED：尖峰创想，启迪心智 斯坦福开放大学：免费读大学，最新的类似于 IOS 开发与 Ruby 开发系列课程，你可以免费找到 MIT 开放课程：世界上最好的理工学院，关于人工智能等议题开启你的好奇心 YouTube：大量的学习视频都集中在这里 stackexchange：全世界的热心人士都集中在这里提问与回答问题 edge：牛人与牛人对话 PLOS：开源模式在学术界的应用，公共图书馆，你可以免费看到最新的论文；你也可以提交自己的论文 中文汉化： TEDtochina：TED 中文社区，不仅仅是翻译。 网易公开课：有 TED 及各个国外大学的开放课程的翻译。 MyOOPS 开放课程：来自全球顶尖大学的开放式课程，现在由世界各国的数千名义工志工为您翻译成中文。 东西网与译言：有部分 edge 翻译材料。 优酷：汉化了不少 YouTube 资源。 思维 对思维的深度思考与刻意练习，将拥有较高人生自由度。 图书 一般来说，以下推荐的图书组合，以中文图书为主，top10往往是必读，其他的可以泛读与选读。 心智黑客：如何基于心理学与脑科学为主的科学理论，来提升心智？ 网站 mindhacks：国外著名博客，内容以介绍心理学与脑科学研究为主。 BPS Research Digest：常有最新研究。 写作 中国的作文训练让我们丧失表达欲望。如果不上大学，你可以通过西式的创意写作课程补上这一课。当恢复对写作的热情之后，再从中国传统文化中汲取营养。 开始写作吧：以西方的创意写作课程图书为主。 编程 一种独特的手艺，轻松参与到国际化竞争的手段。 图书 全世界程序员都说好的图书：在 stackoverflow 书单基础上整理，多为经典图书。其中1-12本为 stack overflow 推荐，全世界程序员都说好的图书。 Ruby 与 Rails 开发基础书单：涵盖了 Ruby 与 Rails3 新手所需要的主要技能。Ruby 与 Rails 的组合，能够快速进入 Web 开发世界。 与小朋友一起学编程：一些入门的计算机编程读物，面向小朋友写的。所以不用担心自己读不懂。涵盖 Python，Ruby 与 Arduino 等。 经典开放课程 ios 开发：涵盖 ios5 与 ios4。 斯坦福 iOS2011 年开发教程 斯坦福 iOS2010 年开发教程 Web 开发：Ruby 与 Rails 斯坦福关于用 Ruby 与 Rails 进行 Web 开发的开放课程 更多课程 计算机科学与创业 斯坦福课程。内容相当新，将于2012年开始。请立即注册。 精实创业 科技创业 计算机基础 自然语言处理 机器学习 SAAS 人机交互 概率 博弈论 以下为 MIT 的计算机科学课程： MIT 的计算机科学课程 网站 github.com：世界上最优秀的程序员集中地带。 stackoverflow：世界上最热心的程序员集中地带，你的常见问题都可以在这里找到答案。 设计 设计逐步成为驱动世界发展的三巨头之一：教育、技术与设计。 图书 设计 网站 dribbble： 在这里，可以看到高质量的设计作品。 wherewedesign： 设计不仅仅是 Web 设计。 创新 图书 成为创新者：诞生伟大设计与产品的新模式：以 triz 及创新算法为主。 ReWork 系列图书：一些有干货的创业图书。 Rework 中文：小而美的创业之旅。 网站： paulgraham：一位对创业有独特思考的长者。 Hacker News： 技术创业者集中地带。 部分中文资源索引 除了学习之外，还有什么？ 建立信任！建立信任！建立信任！ 一个公司可以招募低学历的人才，但是，前提是对方信任你！除了学历之外，快速建立信任的方式就是用作品证明自己。 你可以，开一个个人博客。如果是2011年才开建，建议采用 octopress，记录个人的学习成长。 如果你是侧重开发领域发展，那么，立即在 github.com 上提交个人的代码，即使它仅仅是完成某项作业，以及登陆 stackoverflow 回答别人的问题。 如果你是设计师，那么，请登陆 dribbble 分享个人做过的工作。 诸如此类，在你所感兴趣的领域，找到建立信任的方式。而参加开源项目、公益组织更是突破阶层板结，建立信任的良好方式。","title":"不上大学 你可以学些什么？"},{"content":"如果不上大学，你会面临一些什么样的问题？ 人际：由于中国的畸形发展模式，资源日益集中到一二线城市，正在形成以超大城市集群为核心的发展模式，你将缺少在大型城市，一个正常的人际交往圈子； 技能：没有四年大学缓冲期，用于寻找自身的兴趣与培育技能，尤其是一些需要花费较多时间来培养的技能，可能导致基础不牢； 自信：对自身学习能力不再自信，厌倦学习，最终失去更新知识结构的动力； 敲门砖：多数场合，学历证书的确是迈入职场的敲门砖； 社会舆论：亲朋好友，包括部分爸妈会在不上大学与没有发展前途之间划等号； 未来方向：多数人的大学四年，给了他/她一个基本的职场锚，清楚毕业之后，大致朝哪个方向与领域发展，而你，更多将陷于一场又一场为生存而进行的斗争，最终没有任何职业方向可言，什么能糊口我就干什么，很容易陷入恶性循环，人生自由度日益低下。 如何突破困境？ 请首先使用批判性思维，认真思考上述问题。然后尝试写下你的答案，我如果不上大学，我如何拥有一个合适的人际交往圈子，具备谋生的手艺，以及拥有较高的人生自由度，自信地面对未来？ 以下常规性思维，真的有理吗？ 人际：考试成绩不好=我只配拥有社会上混的的朋友圈 技能：考试成绩不好=我离开学校之后，可以完全放弃学习 自信：考试成绩不好=我恐惧一切与书本相关的内容 敲门砖：考试成绩不好=我可以选择最大众化的发展路线，比如去广州富士康 社会舆论：考试成绩不好=我只能通过赚钱来向亲友证明自己 未来方向：考试成绩不好=我只能屈从于一些容易解决生存压力的工作 在大学诞生前后，无数例子都可以打败以上思维定势。除了成功学虚构的竭斯底里例子之外，世界上还存在一些用时间与耐心来突破这种困境的例子。比如，以下所列： 我是如何从煤矿工成为程序员的 珍妮的故事 Stay Hungry， Stay Foolish ！！ 不上大学，你可以学些什么？ 10年前，没有 TED 与斯坦福公开大学；没有 hacker news 与Y基金；没有 github.com；没有 app store 与 facebook 平台；没有各类天使投资机构与创业者聚会… 今天，接受一流教育理念与卓越智慧如此容易。如果不上大学，可以学些什么？以下资源以图书为主，涉及部分网站。 元资源 通过它，可以找到更多资源。 TED：尖峰创想，启迪心智 斯坦福开放大学：免费读大学，最新的类似于 IOS 开发与 Ruby 开发系列课程，你可以免费找到 MIT 开放课程：世界上最好的理工学院，关于人工智能等议题开启你的好奇心 YouTube：大量的学习视频都集中在这里 stackexchange：全世界的热心人士都集中在这里提问与回答问题 edge：牛人与牛人对话 PLOS：开源模式在学术界的应用，公共图书馆，你可以免费看到最新的论文；你也可以提交自己的论文 中文汉化： TEDtochina：TED 中文社区，不仅仅是翻译。 网易公开课：有 TED 及各个国外大学的开放课程的翻译。 MyOOPS 开放课程：来自全球顶尖大学的开放式课程，现在由世界各国的数千名义工志工为您翻译成中文。 思维 对思维的深度思考与刻意练习，将拥有较高人生自由度。 图书 一般来说，以下推荐的图书组合，以中文图书为主，top10往往是必读，其他的可以泛读与选读。 心智黑客：如何基于心理学与脑科学为主的科学理论，来提升心智？ 网站 mindhacks：国外著名博客，内容以介绍心理学与脑科学研究为主。 BPS Research Digest：常有最新研究。 写作 中国的作文训练让我们丧失表达欲望。如果不上大学，你可以通过西式的创意写作课程补上这一课。当恢复对写作的热情之后，再从中国传统文化中汲取营养。 开始写作吧：以西方的创意写作课程图书为主。 编程 一种独特的手艺，轻松参与到国际化竞争的手段。 图书 全世界程序员都说好的图书：在 stackoverflow 书单基础上整理，多为经典图书。其中1-12本为 stack overflow 推荐，全世界程序员都说好的图书。 Ruby 与 Rails 开发基础书单：涵盖了 Ruby 与 Rails3 新手所需要的主要技能。Ruby 与 Rails 的组合，能够快速进入 Web 开发世界。 与小朋友一起学编程：一些入门的计算机编程读物，面向小朋友写的。所以不用担心自己读不懂。涵盖 Python，Ruby 与 Arduino 等。 经典开放课程 ios 开发：涵盖 ios5 与 ios4。 斯坦福 iOS2011 年开发教程 斯坦福 iOS2010 年开发教程 Web 开发：Ruby 与 Rails 斯坦福关于用 Ruby 与 Rails 进行 Web 开发的开放课程 更多课程 计算机科学与创业 斯坦福课程。内容相当新，将于2012年开始。请立即注册。 精实创业 科技创业 计算机基础 自然语言处理 机器学习 SAAS 人机交互 概率 博弈论 以下为 MIT 的计算机科学课程： MIT 的计算机科学课程 网站 github.com：世界上最优秀的程序员集中地带。 stackoverflow：世界上最热心的程序员集中地带，你的常见问题都可以在这里找到答案。 设计 设计逐步成为驱动世界发展的三巨头之一：教育、技术与设计。 图书 设计 网站 dribbble： 在这里，可以看到高质量的设计作品。 wherewedesign： 设计不仅仅是 Web 设计。 创新 图书 成为创新者：诞生伟大设计与产品的新模式：以 triz 及创新算法为主。 ReWork 系列图书：一些有干货的创业图书。 Rework 中文：小而美的创业之旅。 网站： paulgraham：一位对创业有独特思考的长者。 Hacker News： 技术创业者集中地带。 部分中文资源索引 除了学习之外，还有什么？ 建立信任！建立信任！建立信任！ 一个公司可以招募低学历的人才，但是，前提是对方信任你！除了学历之外，快速建立信任的方式就是用作品证明自己。 你可以，开一个个人博客。如果是2011年才开建，建议采用 octopress，记录个人的学习成长。 如果你是侧重开发领域发展，那么，立即在 github.com 上提交个人的代码，即使它仅仅是完成某项作业，以及登陆 stackoverflow 回答别人的问题。 如果你是设计师，那么，请登陆 dribbble 分享个人做过的工作。","title":"不上大学 你可以学些什么？"},{"content":"Android版Siri，作者为Dexetra团队，开发仅用了8小时。当初这几位只是一群编程疯狂爱好者，觉得做这个东西很好玩。Iris使用Android自带的Google语音搜索来识别问题，再去Wikipedia等网站去找到答案，最后把文本转换成语音。而现在Iris变得越来越Seri-ous，Dexetra刚刚发布了Iris 2.1产品。新版Iris可谓脱胎换骨。   1.0版产品只是一个简答的拼接，通过Android自带的语音识别技术进行简单的搜索，谈不上任何自然语言处理。 2.0版产品则启用了Iris自己的服务器（采用亚马逊的云服务），并提供简单的API服务。但由于访问次数太多，服务器不堪负荷。直到本周，2.1版产品才基本稳定，对API和服务器进行了优化，现在每天可处理数百万次访问请求。 据Dextra团队称，新版Iris最大的特点就是能够识别位置信息，比如咖啡馆、餐厅、ATM、警察局等。   以下是Iris的官方更新日志：   修复强制关闭问题 支持位置识别 支持搜索附近的酒吧、餐馆、赌场等 对时间、天气等返回可视化信息 服务器速速更快 安装之后，发现Iris聪明多了，比如我说“coffee”，Iris立马打开我手机（Nexus S）上的Google地图，然后给出了人民大学附近的咖啡馆。我问：“Do you have a penis？”，她回答：“I am a female robot. Are you a man or woman？”显然比原来的回答“I’m a male”靠谱多了。     Android版的Siri，功能几乎一模一样，就是让Android用户也能体验到智能语音搜索的乐趣，连名字都与Siri反着来。 开启之后向Iris提问，她会为你找到答案，哲学、历史、科学等等都不在话下。 软件需要安装Google语音搜索和TTS语音合成支持，安装好之后才能使用。 需要的软件包下载： http://06peng.com/read.php?74","title":"抢先玩乐Iris发布2.1版 Android版Siri（下载地址）"},{"content":"摘要： 两篇文档是否相关往往不只决定于字面上的词语重复，还取决于文字背后的语义关联。对语义关联的挖掘，可以让我们的搜索更加智能化。本文着重介绍了一个语义挖掘的利器：主题模型。主题模型是对文字隐含主题进行建模的方法。它克服了传统信息检索中文档相似度计算方法的缺点，并且能够在海量互联网数据中自动寻找出文字间的语义主题。近些年来各大互联网公司都开始了这方面的探索和尝试。就让我们看一下究竟吧。 关键词：主题模型 技术领域：搜索技术、自然语言处理 假设有两个句子，我们想知道它们之间是否相关联： 第一个是：“乔布斯离我们而去了。” 第二个是：“苹果价格会不会降？” 如果由人来判断，我们一看就知道，这两个句子之间虽然没有任何公共词语，但仍然是很相关的。这是因为，虽然第二句中的“苹果”可能是指吃的苹果，但是由于第一句里面有了“乔布斯”，我们会很自然的把“苹果”理解为苹果公司的产品。事实上，这种文字语句之间的相关性、相似性问题，在搜索引擎算法中经常遇到。例如，一个用户输入了一个query，我们要从海量的网页库中找出和它最相关的结果。这里就涉及到如何衡量query和网页之间相似度的问题。对于这类问题，人是可以通过上下文语境来判断的。但是，机器可以么？ 在传统信息检索领域里，实际上已经有了很多衡量文档相似性的方法，比如经典的VSM模型。然而这些方法往往基于一个基本假设：文档之间重复的词语越多越可能相似。这一点在实际中并不尽然。很多时候相关程度取决于背后的语义联系，而非表面的词语重复。 那么，这种语义关系应该怎样度量呢？事实上在自然语言处理领域里已经有了很多从词、词组、句子、篇章角度进行衡量的方法。本文要介绍的是其中一个语义挖掘的利器：主题模型。 主题模型是什么？ 主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。还是上面的例子，“苹果”这个词的背后既包含是苹果公司这样一个主题，也包括了水果的主题。当我们和第一句进行比较时，苹果公司这个主题就和“乔布斯”所代表的主题匹配上了，因而我们认为它们是相关的。 在这里，我们先定义一下主题究竟是什么。主题就是一个概念、一个方面。它表现为一系列相关的词语。比如一个文章如果涉及到“百度”这个主题，那么“中文搜索”、“李彦宏”等词语就会以较高的频率出现，而如果涉及到“IBM”这个主题，那么“笔记本”等就会出现的很频繁。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布 。与主题关系越密切的词语，它的条件概率越大，反之则越小。 例如： 通俗来说，一个主题就好像一个“桶”，它装了若干出现概率较高的词语。这些词语和这个主题有很强的相关性，或者说，正是这些词语共同定义了这个主题。对于一段话来说，有些词语可以出自这个“桶”，有些可能来自那个“桶”，一段文本往往是若干个主题的杂合体。我们举个简单的例子，见下图。 以上是从互联网新闻中摘抄下来的一段话。我们划分了4个桶（主题），百度（红色），微软（紫色）、谷歌（蓝色）和市场（绿色）。段落中所包含的每个主题的词语用颜色标识出来了。从颜色分布上我们就可以看出，文字的大意是在讲百度和市场发展。在这里面，谷歌、微软这两个主题也出现了，但不是主要语义。值得注意的是，像“搜索引擎”这样的词语，在百度、微软、谷歌这三个主题上都是很可能出现的，可以认为一个词语放进了多个“桶”。当它在文字中出现的时候，这三个主题均有一定程度的体现。 有了主题的概念，我们不禁要问，究竟如何得到这些主题呢？对文章中的主题又是如何进行分析呢？这正是主题模型要解决的问题。下面我简要介绍一下主题模型是怎样工作的。 主题模型的工作原理 首先，我们用生成模型的视角来看文档和主题这两件事。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到的。那么，如果我们要生成一篇文档，它里面的每个词语出现的概率为： 上面这个式子，可以矩阵乘法来表示，如下图所示： 左边的矩阵表示每篇文章中每次词语出现的概率；中间的Φ矩阵表示的是每个主题中每个词语出现的概率 ，也就是每个“桶 表示的是每篇文档中各个主题出现的概率 ，可以理解为一段话中每个主题所占的比例。 假如我们有很多的文档，比如大量的网页，我们先对所有文档进行分词，得到一个词汇列表。这样每篇文档就可以表示为一个词语的集合。对于每个词语，我们可以用它在文档中出现的次数除以文档中词语的数目作为它在文档中出现的概率 。这样，对任意一篇文档，左边的矩阵是已知的，右边的两个矩阵未知。而主题模型就是用大量已知的“词语－文档”矩阵 ，通过一系列的训练，推理出右边的“词语－主题”矩阵Φ 和“主题文档”矩阵Θ 。 主题模型训练推理的方法主要有两种，一个是pLSA（Probabilistic Latent Semantic Analysis），另一个是LDA（Latent Dirichlet Allocation）。pLSA主要使用的是EM（期望最大化）算法；LDA采用的是Gibbs sampling方法。由于它们都较为复杂且篇幅有限，这里就只简要地介绍一下pLSA的思想，其他具体方法和公式，读者可以查阅相关资料。 pLSA采用的方法叫做EM（期望最大化）算法，它包含两个不断迭代的过程：E（期望）过程和M（最大化）过程。用一个形象的例子来说吧：比如说食堂的大师傅炒了一盘菜，要等分成两份给两个人吃，显然没有必要拿天平去一点点去精确称量，最简单的办法是先随意的把菜分到两个碗中，然后观察是否一样多，把比较多的那一份取出一点放到另一个碗中，这个过程一直重复下去，直到大家看不出两个碗里的菜有什么差别为止。 对于主题模型训练来说，“计算每个主题里的词语分布”和“计算训练文档中的主题分布”就好比是在往两个人碗里分饭。在E过程中，我们通过贝叶斯公式可以由“词语－主题”矩阵计算出“主题－文档”矩阵。在M过程中，我们再用“主题－文档”矩阵重新计算“词语－主题”矩阵。这个过程一直这样迭代下去。EM算法的神奇之处就在于它可以保证这个迭代过程是收敛的。也就是说，我们在反复迭代之后，就一定可以得到趋向于真实值的 Φ和 Θ。 如何使用主题模型？ 有了主题模型，我们该怎么使用它呢？它有什么优点呢？我总结了以下几点： 1）  它可以衡量文档之间的语义相似性。对于一篇文档，我们求出来的主题分布可以看作是对它的一个抽象表示。对于概率分布，我们可以通过一些距离公式（比如KL距离）来计算出两篇文档的语义距离，从而得到它们之间的相似度。 2)它可以解决多义词的问题。回想最开始的例子，“苹果”可能是水果，也可能指苹果公司。通过我们求出来的“词语－主题”概率分布，我们就可以知道“苹果”都属于哪些主题，就可以通过主题的匹配来计算它与其他文字之间的相似度。 3） 它可以排除文档中噪音的影响。一般来说，文档中的噪音往往处于次要主题中，我们可以把它们忽略掉，只保持文档中最主要的主题。 4） 它是无监督的，完全自动化的。我们只需要提供训练文档，它就可以自动训练出各种概率，无需任何人工标注过程。 5） 它是跟语言无关的。任何语言只要能够对它进行分词，就可以进行训练，得到它的主题分布。 综上所述，主题模型是一个能够挖掘语言背后隐含信息的利器。近些年来各大搜索引擎公司都已经开始重视这方面的研发工作。语义分析的技术正在逐步深入到搜索领域的各个产品中去。在不久的将来，我们的搜索将会变得更加智能，让我们拭目以待吧。 原文来源：http://stblog.baidu-tech.com/?p=1190","title":"搜索背后的奥秘——浅谈语义主题计算"},{"content":"搜索技术总结整理 学习搜索有一段时间了，为了复习巩固和提高，特把学习的结果总结一下。本文章搜索只特指小型搜索系统。之所以特指是小型系统，是因为大型小型搜索系统虽然整体处理过程大体相似，但整体架构和要处理的数据量和响应速度是密切相关的，百万量级的和十亿量级的搜索系统是不可同日而语的。 搜索系统处理大体分为：蜘蛛、切词、索引、检索，下面逐个的描述。 1. 蜘蛛 蜘蛛是用来抓取网页的。所谓的抓，其实也就是通过socket向http服务器发送post或者get请求，服务器根据你的请求把页面送给你的过程。大家通常用的IE浏览器就是要先做这个工作，然后在根据返回的Html源码生成你看到的网页。蜘蛛当然不需要去生成网页给人看了，它只是把html存储起来给其它程序用。我看到有人问如何写一个下载动态网页的蜘蛛，其实动态页面和静态页面对于蜘蛛是一样的，都是发送请求和接受，没有实质区别。 搜索技术中，蜘蛛看起来是最容易的。大家学习了一个语言和熟悉TCP/IP后都可以编写一个下载网页的程序了。但是，要写一个适应性强的蜘蛛却是相当难的，因为网络环境太过于复杂，你总是很难想象到别人会发什么给你，也就是蜘蛛要有很强的例外处理能力。最常见的问题是： 网路故障 造成蜘蛛抓取超时，没有抓取到网页或者抓取不全。 网站链接 有些网站对错误的链接或者蜘蛛使用的ip，返回一个很怪的页面，例如它的内部错误信息。 页面问题 页面问题中网页编码是首要问题，各个网站用的编码五花八门，需要你判断和处理合理的编码。有朋友会问，http协议头信息中有charset，html中也有charset，可以方便的判断编码，没错，这是主要的判断方式，但是，如果这两个地方都得不到charset信息呢？ 其次，要提高蜘蛛的效率和速度，这就要涉及到待抓取url的权值值计算，因为并不是每个网页对我们来说都是同等重要的。由于网络的无边无际，所以采用广度搜索和深度搜索都不可取，只有采用种权值计算公式采用减枝方法做有效遍历。也有只依赖权值来决定搜索目标的。 最后是蜘蛛的性能。也就是并行处理方式。单个蜘蛛处理能力有限制，因此多采用多线程和多进程的方法，多个蜘蛛同时运行。另外，采用此方法的另一个重要原因是，网络速度是蜘蛛程序的瓶颈。 2.切词 切词仿佛是最简单的。采用最流行的倒序最大前序最大切词方法基本上就够用的了。但是切词也是让我最迷惑的东西，什么是切词？采用上述方法切出来的并不是标准的词语，更像是字符和分割组合，你查一下百度或者googel就可以看到它们的切词实现效果。切词程序真的不是在切词啊。 切词是深入处理中文信息的基本模块。在自然语言分析中也有切词，切词的基本方法和过程也基本相同，但我觉得在两者的是指导思想有些不一样。自然语言的分析很在意切词的准确性，而索引切词确更注重切词结果的覆盖情况。自然语言处理在切词后，需要词性标注和词法语法分析，因此必须切词准确，对新词、命名实体等的处理相对要求要高，而索引切词后就是做倒排索引了，只要能保证索引时切词和查询时的切词是同样的，基本上就可以正确使用索引，只是为了索引效果，需要尽可能的在符合词语的自然分段的基础上，把词的切割粒度定位在在效率和灵活性的一个调和位置，对新词和命名实体的识别就不那么看重。例如：北京大学，在第一中切词中会切割成一个词语，而在后一种切词中，为了保证能使用北京也可以搜索到，会被切割成北京和大学两个词。也有为了效率而做复合索引的，例如把北京大学切割为 北京大学、北京和大学这3个词，方便用户使用任何一个熟悉的词都能搜索到。 通过上面的讨论，分析到切词系统实际上是根据实际应用而不同的。就索引切词来说，切词的准确性并不重要，切词后的词能够快速准确找到索引，才重要。那么怎么切才能让后面的索引做的快速而且准确呢？想一想，我们为什么要切词呢，即便不切词，我们也可以在字上做索引，也可以在字节上做索引，开源搜索醒目lucence默认处理中国语言的方式就是双字切词，两个字两个字的切，例如：北京大学，切割后为，北京 京大 大学。但要讲到效率，根据搜索到的资料数据比较，还是基于词库的自然词切割是效果较好的。我想原因是：1，切割的完整性，不会遗漏词语，也不会有太多废词，例如 京大。因为使用索引查询的人的查询输入，也基本符合中文语法，基本不会出现 京大这样的词，相对的2字切词的处理方式就会产生废词，就要多做和维护一个基本用不到的索引。2，切割的后的词检索的效率，如果基于字或者字节做索引，虽然中国总数有2万多，但是常用的只有4~6千，要在这个范围内映射千万量级以上的文档，索引就比较大，每个字要对应万量级的文档，同时后期的归并和条件过滤处理工作量也比较大，而对应的词是中文语法结构的基本单位，常用词有5~10万，足以承担亿量级的文档索引分担。做了一段时间的索引程序后，我的感觉是词长在2~4之间比较好。 这里讨论的罗嗦的原因是为了比较说明索引切词并不是严格意义上的切词，只是融合了切词逻辑的字符切割。这种切割字符基本符合词语单位，我想是有必然的潜在的语言规则。我在学习切词时有走过弯路，总想把词切割的准确些，再准确些，结果程序效率下降，准确性也不能有明显提高，现在觉得，如果不涉及更深层次额语义处理，并不必要。 3.索引 对于学习过数据库，熟悉数据库基本原理的朋友们来说，做索引并不是很困难的事。现有的索引系统基本是采用倒排索引。基本含义就是在一个文件中记录下一个词在哪篇文章哪个位置出现，以便解析用户输入后可以直接读取对应的文章id，把对应的文章择要提取出来显示给用户。 我认为索引难做的一点是索引的维护，包括插入和删除。而索引的更新(update)，通常索引系统把它转化为删除(delete)和插入(insert)。信息的变更要尽可能快的反映到索引中，这个对索引系统的压力还是比较大的。想一下数据的删除过程，删除一片文章，要知道这篇文章都在哪些词的索引中，要逐个的清理数据，而这个数据清理，通常为了性能，又不是真实的清理，只是把文章对应的数据抹去（清零或做特定标记），这样索引中就像是留下了一个洞，时间一场，索引也就千疮百孔了，所以索引系统需要根据一定的算法计算索引中的空洞比率，在适合的条件下，整理索引，剔除空洞。 另外，插入索引也需要特别注意，简单的索引插入当然没有问题，因为文档id在索引中一般是排序的（整体排序或者分段排序），这样方便检索时快速处理。因此大批量或者频繁的插入时，就会有性能问题。通常的解决方案是采用大小索引的办法，新插入的索引并不直接插入到大索引中（数据处理量相对大），而是临时写入小索引文件，而检索时，从这两个索引中取值，然后归并就可以了。 另外，索引数据，为了提高读写速度，一般是经过简单压缩的，这时也就涉及到数据安全问题，为了避免万一问题，也可在代码中增加校验功能，保证数据完整，即便某个地方因计算错误而有问题，也要把他限定在最小范围内。 4.检索 我很怀疑google的pagerank计算公式，据说有超过一千的影响因素，这在我简单的脑袋里是不可想象的。我看google黑板报（google的中文官方blog）里曾经说到，简单就是最美的，公式的思想好像与这个相违背。总之pagerank的思想就是：我重要，你和我有友好关系，那么可以简单的推到出，你也重要。根据这样的思想，我们可以创建自己的计算公式。 我比较关心检索的组合，例如检索：测试 软件 程序员 -网站  。这个在检索时会处理成一个逻辑结果，好像，(a and b and c ) - d 。大家都知道这就是集合操作，熟悉数据库的朋友可以想到，这个很像数据查询语句的条件，没错，为了减少数据库和索引操作，分析和优化查询是很有必要的，处理过程完全可以借鉴数据库检索语句分析和优化过程。 总结一下，我写的这些是索引的入门。我对索引的理解是。根据处理的数据类型和数据量，索引系统有很大的不同。我记得百度掌门人说过，搜索人人可作，但要做大做好，就很困难了。具体索引的实现方式，要比描述的复杂写，有兴趣的朋友可以现学习lucence，非常棒的开源java程序，也有.net版本的开源。我对lucence的感觉是，它很经典，它的索引文件有点烦琐，它的检索方式很值得学习。 学习搜索有一段时间了，为了复习巩固和提高，特把学习的结果总结一下。本文章搜索只特指小型搜索系统。之所以特指是小型系统，是因为大型小型搜索系统虽然整体处理过程大体相似，但整体架构和要处理的数据量和响应速度是密切相关的，百万量级的和十亿量级的搜索系统是不可同日而语的。 搜索系统处理大体分为：蜘蛛、切词、索引、检索，下面逐个的描述。 1. 蜘蛛 蜘蛛是用来抓取网页的。所谓的抓，其实也就是通过socket向http服务器发送post或者get请求，服务器根据你的请求把页面送给你的过程。大家通常用的IE浏览器就是要先做这个工作，然后在根据返回的Html源码生成你看到的网页。蜘蛛当然不需要去生成网页给人看了，它只是把html存储起来给其它程序用。我看到有人问如何写一个下载动态网页的蜘蛛，其实动态页面和静态页面对于蜘蛛是一样的，都是发送请求和接受，没有实质区别。 搜索技术中，蜘蛛看起来是最容易的。大家学习了一个语言和熟悉TCP/IP后都可以编写一个下载网页的程序了。但是，要写一个适应性强的蜘蛛却是相当难的，因为网络环境太过于复杂，你总是很难想象到别人会发什么给你，也就是蜘蛛要有很强的例外处理能力。最常见的问题是： 网路故障 造成蜘蛛抓取超时，没有抓取到网页或者抓取不全。 网站链接 有些网站对错误的链接或者蜘蛛使用的ip，返回一个很怪的页面，例如它的内部错误信息。 页面问题 页面问题中网页编码是首要问题，各个网站用的编码五花八门，需要你判断和处理合理的编码。有朋友会问，http协议头信息中有charset，html中也有charset，可以方便的判断编码，没错，这是主要的判断方式，但是，如果这两个地方都得不到charset信息呢？ 其次，要提高蜘蛛的效率和速度，这就要涉及到待抓取url的权值值计算，因为并不是每个网页对我们来说都是同等重要的。由于网络的无边无际，所以采用广度搜索和深度搜索都不可取，只有采用种权值计算公式采用减枝方法做有效遍历。也有只依赖权值来决定搜索目标的。 最后是蜘蛛的性能。也就是并行处理方式。单个蜘蛛处理能力有限制，因此多采用多线程和多进程的方法，多个蜘蛛同时运行。另外，采用此方法的另一个重要原因是，网络速度是蜘蛛程序的瓶颈。 2.切词 切词仿佛是最简单的。采用最流行的倒序最大前序最大切词方法基本上就够用的了。但是切词也是让我最迷惑的东西，什么是切词？采用上述方法切出来的并不是标准的词语，更像是字符和分割组合，你查一下百度或者googel就可以看到它们的切词实现效果。切词程序真的不是在切词啊。 切词是深入处理中文信息的基本模块。在自然语言分析中也有切词，切词的基本方法和过程也基本相同，但我觉得在两者的是指导思想有些不一样。自然语言的分析很在意切词的准确性，而索引切词确更注重切词结果的覆盖情况。自然语言处理在切词后，需要词性标注和词法语法分析，因此必须切词准确，对新词、命名实体等的处理相对要求要高，而索引切词后就是做倒排索引了，只要能保证索引时切词和查询时的切词是同样的，基本上就可以正确使用索引，只是为了索引效果，需要尽可能的在符合词语的自然分段的基础上，把词的切割粒度定位在在效率和灵活性的一个调和位置，对新词和命名实体的识别就不那么看重。例如：北京大学，在第一中切词中会切割成一个词语，而在后一种切词中，为了保证能使用北京也可以搜索到，会被切割成北京和大学两个词。也有为了效率而做复合索引的，例如把北京大学切割为 北京大学、北京和大学这3个词，方便用户使用任何一个熟悉的词都能搜索到。 通过上面的讨论，分析到切词系统实际上是根据实际应用而不同的。就索引切词来说，切词的准确性并不重要，切词后的词能够快速准确找到索引，才重要。那么怎么切才能让后面的索引做的快速而且准确呢？想一想，我们为什么要切词呢，即便不切词，我们也可以在字上做索引，也可以在字节上做索引，开源搜索醒目lucence默认处理中国语言的方式就是双字切词，两个字两个字的切，例如：北京大学，切割后为，北京 京大 大学。但要讲到效率，根据搜索到的资料数据比较，还是基于词库的自然词切割是效果较好的。我想原因是：1，切割的完整性，不会遗漏词语，也不会有太多废词，例如 京大。因为使用索引查询的人的查询输入，也基本符合中文语法，基本不会出现 京大这样的词，相对的2字切词的处理方式就会产生废词，就要多做和维护一个基本用不到的索引。2，切割的后的词检索的效率，如果基于字或者字节做索引，虽然中国总数有2万多，但是常用的只有4~6千，要在这个范围内映射千万量级以上的文档，索引就比较大，每个字要对应万量级的文档，同时后期的归并和条件过滤处理工作量也比较大，而对应的词是中文语法结构的基本单位，常用词有5~10万，足以承担亿量级的文档索引分担。做了一段时间的索引程序后，我的感觉是词长在2~4之间比较好。 这里讨论的罗嗦的原因是为了比较说明索引切词并不是严格意义上的切词，只是融合了切词逻辑的字符切割。这种切割字符基本符合词语单位，我想是有必然的潜在的语言规则。我在学习切词时有走过弯路，总想把词切割的准确些，再准确些，结果程序效率下降，准确性也不能有明显提高，现在觉得，如果不涉及更深层次额语义处理，并不必要。 3.索引 对于学习过数据库，熟悉数据库基本原理的朋友们来说，做索引并不是很困难的事。现有的索引系统基本是采用倒排索引。基本含义就是在一个文件中记录下一个词在哪篇文章哪个位置出现，以便解析用户输入后可以直接读取对应的文章id，把对应的文章择要提取出来显示给用户。 我认为索引难做的一点是索引的维护，包括插入和删除。而索引的更新(update)，通常索引系统把它转化为删除(delete)和插入(insert)。信息的变更要尽可能快的反映到索引中，这个对索引系统的压力还是比较大的。想一下数据的删除过程，删除一片文章，要知道这篇文章都在哪些词的索引中，要逐个的清理数据，而这个数据清理，通常为了性能，又不是真实的清理，只是把文章对应的数据抹去（清零或做特定标记），这样索引中就像是留下了一个洞，时间一场，索引也就千疮百孔了，所以索引系统需要根据一定的算法计算索引中的空洞比率，在适合的条件下，整理索引，剔除空洞。 另外，插入索引也需要特别注意，简单的索引插入当然没有问题，因为文档id在索引中一般是排序的（整体排序或者分段排序），这样方便检索时快速处理。因此大批量或者频繁的插入时，就会有性能问题。通常的解决方案是采用大小索引的办法，新插入的索引并不直接插入到大索引中（数据处理量相对大），而是临时写入小索引文件，而检索时，从这两个索引中取值，然后归并就可以了。 另外，索引数据，为了提高读写速度，一般是经过简单压缩的，这时也就涉及到数据安全问题，为了避免万一问题，也可在代码中增加校验功能，保证数据完整，即便某个地方因计算错误而有问题，也要把他限定在最小范围内。 4.检索 我很怀疑google的pagerank计算公式，据说有超过一千的影响因素，这在我简单的脑袋里是不可想象的。我看google黑板报（google的中文官方blog）里曾经说到，简单就是最美的，公式的思想好像与这个相违背。总之pagerank的思想就是：我重要，你和我有友好关系，那么可以简单的推到出，你也重要。根据这样的思想，我们可以创建自己的计算公式。 我比较关心检索的组合，例如检索：测试 软件 程序员 -网站  。这个在检索时会处理成一个逻辑结果，好像，(a and b and c ) - d 。大家都知道这就是集合操作，熟悉数据库的朋友可以想到，这个很像数据查询语句的条件，没错，为了减少数据库和索引操作，分析和优化查询是很有必要的，处理过程完全可以借鉴数据库检索语句分析和优化过程。 总结一下，我写的这些是索引的入门。我对索引的理解是。根据处理的数据类型和数据量，索引系统有很大的不同。我记得百度掌门人说过，搜索人人可作，但要做大做好，就很困难了。具体索引的实现方式，要比描述的复杂写，有兴趣的朋友可以现学习lucence，非常棒的开源java程序，也有.net版本的开源。我对lucence的感觉是，它很经典，它的索引文件有点烦琐，它的检索方式很值得学习。","title":"搜索技术总结整理"},{"content":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理处理的内容涉及到语言的各个层次，包括字、词、句、段落、篇章和语义。 目前自然语言处理的主要研究和应用方向有： 1、  统计语言模型: 统计语言模型是自然语言处理的主流技术之一。我们研究的主要内容包括各种语言模型的构建、改进以及应用，包括N元文法模型、隐马尔科夫模型、最大熵模型等。 2、  非齐次概率建模：在自然语言处理领域中，各级语言元素（字、词、词性、组块、短句……）因其语法语义属性不同，其可以充当的语言成分不同，因此，其在语言元素序列中出现的位置和范围具有一定的规律性。上述规律性通常对应概率模型中的非齐次性假设，因此又称为语言元素的非齐次属性。语言元素非齐次现象是语言元素的普遍现象，语言元素的非齐次属性是语言元素的本质属性。非齐次概率建模期望将语言元素的非齐次属性进行量化表示并加以利用，从而提高传统概率模型的性能，增强概率模型在自然语言处理各项任务中的应用效果。 3、  汉字处理: 汉字处理解决在计算机及移动设备上输入汉字的问题。研究内容包括音字转换、手写体识别以及键盘输入等问题。 4、  词法分析：词法分析的主要目的是找出词汇的各个词素，从中获得语言学信息。词法分析是很多中文信息处理任务的必要步骤。很多应用，如搜索引擎、机器翻译都需要词法分析的支持。词法分析的主要研究内容包括自动分词、词性标注、歧义消解、新词识别等，采用的方法主要以统计机器学习为主。 5、  命名实体识别：命名实体识别的任务是自动识别文本中的人名、地名、机构名等各种实体。命名实体识别可以提高语言理解的准确性，是信息抽取系统的重要组成部分。命名实体识别的主要研究内容包括识别语料的标注、识别规则的自动抽取、识别模型的构建以及识别特征的自动选取等。 6、  句法分析：句法分析是对句子和短语的结构进行分析。句法分析可分为完全句法分析和浅层句法分析。句法分析是语言学理论和实际的自然语言应用的一个重要桥梁。一个实用的、完备的、准确的句法分析将是计算机真正理解自然语言的基础。 7、  语义分析：在过去，计算语言学研究集中在词法分析和句法分析上，基于规则、基于统计的语法分析技术率先在自然语言处理领域得到广泛运用。目前，随着Internet网络应用的普及和深入，多语种内容信息的理解和处理逐渐受到人们的关注。语义分析的研究，如词义排歧和语义归纳、推理等，开始处于萌芽期并将逐步走向前台，成为下一阶段计算语言学研究的一个亮点。计算机本身没有智能，自然语言的语义分析和内容信息的理解，离不开相应的语义知识库的支持，它是帮助计算机“了解”人类语言的一个媒介和桥梁，也是让计算机逐渐“聪明”起来的一个物质前提。语义分析主要研究基于语义知识库的语义相似度的计算方法、语义知识库的自动构建等内容。 8、  语料库多级加工：语料库语言学是以语料库为基本知识源来研究自然语言规律的学科，其中语料库加工的理论、方法和工具和基于语料库的知识获取是语料库语言学研究的主要内容。语料库是按照一定的原则组织在一起的真实的自然语言数据(包括书面语和口语)的集合，主要用于研究自然语言的规律，特别是统计语言学模型的训练以及相关系统的评价和测试。所谓语料库标注或加工就是对电子语料（包括书面语和口语）进行不同层次的语言学分析，并添加相应的\"显性\"的解释性的语言学信息过程。与不同层次的自然语言分析相对应，语料库的加工主要包括词性标注、句法标注、语义标注、言语标注和语用标注等，由于汉语书写的特殊性，汉语的语料加工还包括分词。   自然语言处理的专业英语： 学习和研究中文分词问题,引起了我对中文分词的极大兴趣,甚至到了无法自拔的地步.我非常希望,能够通过自己的学习和研究,自己开发一套高性能的中文处理系统.但越学习越深入,越发现自己的知识的缺乏.熟练掌握一门编程语言是最基本的,另外涉及到概率论、统计和语言学、汉字编码等诸多复杂问题。因为这些问题同时也是国际上的热点问题，在学习这些内容时不可避免的要遇到一些英语问题。本文将最近所遇到的该领域的专业英语及其含义做一个简单的小结，列举如下。     corpus  语料库。其本义是尸体、文集的意思。记住哦，在统计自然语言处理领域它是语料库的意思。它是文本的集合，这里的“文本”通常是指文本文件，如记事本及其内容。为了便于理解和统计的方便，人们通常会把词汇信息存储在这样的文本中。多个文本便构成一个语料库了。    corpora 语料库集合。它是语料库corpus的复数形式，顾名思义，是指多个文本集合的集合，即多个语料库的集合。    lingusitic competence 语言能力。反映了母语说话者脑海中假设存在的语言结构知识。   linguistic performance 语言性能。它受一系列事物的影响，例如记忆的局限性和环境的传递噪音。   parse 语法分析。给定一个合理的语法，对一个标准的自然语句进行句法分析，句法分析的结果就是parse。   Wordnet 词网。是一个英语电子词典。词汇被组织到一个网络层次中。每个节点由相近意思的词集组成。   word token 词次。指文本的长度，例如在某个语料库中，其文本包含有71370个词次（token）。   word type 词形。指文本中出现多少个不同单词的个数（在英文中是这样，在中文中有所区别）。  根据token和type，我们可以计算其比值，他表示每个type出现的平均次数。   hapax legomena 罕用语。它是希腊语，表示预料库中只出现一次的单词。   bigram 二元组。  KWIC  Keyword In Context。上下文关键词。人们通常用上下文关键词索引程序来产生数据表示，在这样的表示中，所有出现的词汇都被列出，并且词的上下文环境也分别列在它的左右两边。  prior probability 先验概率。  posterior probability 后验概率。  binomial distribution 二项分布。  Bayes optimal decision 贝叶斯最优决策。  mutual information 互信息。  capacity 信道容量。  Perplexity 混乱度。在语音识别领域中，人们通常用混乱度而不是交叉熵来描述一个模型的好坏。  parts of speech ,POS 词性。通常说来词性有三类：名词、动词、形容词。   n-gram n元语法模型。即马尔可夫模型。   stemming 词干化，取词根。   dictionary-based disambiguation 基于词典的消歧。   function fitting 函数拟合，就是说基于一些数据点推断出函数的形态。   Hidden Markov Model， HMM。 隐马尔可夫模型。   rule based 基于规则   corpus based 基于语料库。   conditional probability 条件概率   transitive probability 转移概率   neighboring pairs of words 词语接续对。   maximum likehood estimation 最大似然估计   data sparse 数据稀疏 目前自然语言处理的主要研究和应用方向有： 1、  统计语言模型: 统计语言模型是自然语言处理的主流技术之一。我们研究的主要内容包括各种语言模型的构建、改进以及应用，包括N元文法模型、隐马尔科夫模型、最大熵模型等。 2、  非齐次概率建模：在自然语言处理领域中，各级语言元素（字、词、词性、组块、短句……）因其语法语义属性不同，其可以充当的语言成分不同，因此，其在语言元素序列中出现的位置和范围具有一定的规律性。上述规律性通常对应概率模型中的非齐次性假设，因此又称为语言元素的非齐次属性。语言元素非齐次现象是语言元素的普遍现象，语言元素的非齐次属性是语言元素的本质属性。非齐次概率建模期望将语言元素的非齐次属性进行量化表示并加以利用，从而提高传统概率模型的性能，增强概率模型在自然语言处理各项任务中的应用效果。 3、  汉字处理: 汉字处理解决在计算机及移动设备上输入汉字的问题。研究内容包括音字转换、手写体识别以及键盘输入等问题。 4、  词法分析：词法分析的主要目的是找出词汇的各个词素，从中获得语言学信息。词法分析是很多中文信息处理任务的必要步骤。很多应用，如搜索引擎、机器翻译都需要词法分析的支持。词法分析的主要研究内容包括自动分词、词性标注、歧义消解、新词识别等，采用的方法主要以统计机器学习为主。 5、  命名实体识别：命名实体识别的任务是自动识别文本中的人名、地名、机构名等各种实体。命名实体识别可以提高语言理解的准确性，是信息抽取系统的重要组成部分。命名实体识别的主要研究内容包括识别语料的标注、识别规则的自动抽取、识别模型的构建以及识别特征的自动选取等。 6、  句法分析：句法分析是对句子和短语的结构进行分析。句法分析可分为完全句法分析和浅层句法分析。句法分析是语言学理论和实际的自然语言应用的一个重要桥梁。一个实用的、完备的、准确的句法分析将是计算机真正理解自然语言的基础。 7、  语义分析：在过去，计算语言学研究集中在词法分析和句法分析上，基于规则、基于统计的语法分析技术率先在自然语言处理领域得到广泛运用。目前，随着Internet网络应用的普及和深入，多语种内容信息的理解和处理逐渐受到人们的关注。语义分析的研究，如词义排歧和语义归纳、推理等，开始处于萌芽期并将逐步走向前台，成为下一阶段计算语言学研究的一个亮点。计算机本身没有智能，自然语言的语义分析和内容信息的理解，离不开相应的语义知识库的支持，它是帮助计算机“了解”人类语言的一个媒介和桥梁，也是让计算机逐渐“聪明”起来的一个物质前提。语义分析主要研究基于语义知识库的语义相似度的计算方法、语义知识库的自动构建等内容。 8、  语料库多级加工：语料库语言学是以语料库为基本知识源来研究自然语言规律的学科，其中语料库加工的理论、方法和工具和基于语料库的知识获取是语料库语言学研究的主要内容。语料库是按照一定的原则组织在一起的真实的自然语言数据(包括书面语和口语)的集合，主要用于研究自然语言的规律，特别是统计语言学模型的训练以及相关系统的评价和测试。所谓语料库标注或加工就是对电子语料（包括书面语和口语）进行不同层次的语言学分析，并添加相应的\"显性\"的解释性的语言学信息过程。与不同层次的自然语言分析相对应，语料库的加工主要包括词性标注、句法标注、语义标注、言语标注和语用标注等，由于汉语书写的特殊性，汉语的语料加工还包括分词。   自然语言处理的专业英语： 学习和研究中文分词问题,引起了我对中文分词的极大兴趣,甚至到了无法自拔的地步.我非常希望,能够通过自己的学习和研究,自己开发一套高性能的中文处理系统.但越学习越深入,越发现自己的知识的缺乏.熟练掌握一门编程语言是最基本的,另外涉及到概率论、统计和语言学、汉字编码等诸多复杂问题。因为这些问题同时也是国际上的热点问题，在学习这些内容时不可避免的要遇到一些英语问题。本文将最近所遇到的该领域的专业英语及其含义做一个简单的小结，列举如下。     corpus  语料库。其本义是尸体、文集的意思。记住哦，在统计自然语言处理领域它是语料库的意思。它是文本的集合，这里的“文本”通常是指文本文件，如记事本及其内容。为了便于理解和统计的方便，人们通常会把词汇信息存储在这样的文本中。多个文本便构成一个语料库了。    corpora 语料库集合。它是语料库corpus的复数形式，顾名思义，是指多个文本集合的集合，即多个语料库的集合。    lingusitic competence 语言能力。反映了母语说话者脑海中假设存在的语言结构知识。   linguistic performance 语言性能。它受一系列事物的影响，例如记忆的局限性和环境的传递噪音。   parse 语法分析。给定一个合理的语法，对一个标准的自然语句进行句法分析，句法分析的结果就是parse。   Wordnet 词网。是一个英语电子词典。词汇被组织到一个网络层次中。每个节点由相近意思的词集组成。   word token 词次。指文本的长度，例如在某个语料库中，其文本包含有71370个词次（token）。   word type 词形。指文本中出现多少个不同单词的个数（在英文中是这样，在中文中有所区别）。  根据token和type，我们可以计算其比值，他表示每个type出现的平均次数。   hapax legomena 罕用语。它是希腊语，表示预料库中只出现一次的单词。   bigram 二元组。  KWIC  Keyword In Context。上下文关键词。人们通常用上下文关键词索引程序来产生数据表示，在这样的表示中，所有出现的词汇都被列出，并且词的上下文环境也分别列在它的左右两边。  prior probability 先验概率。  posterior probability 后验概率。  binomial distribution 二项分布。  Bayes optimal decision 贝叶斯最优决策。  mutual information 互信息。  capacity 信道容量。  Perplexity 混乱度。在语音识别领域中，人们通常用混乱度而不是交叉熵来描述一个模型的好坏。  parts of speech ,POS 词性。通常说来词性有三类：名词、动词、形容词。   n-gram n元语法模型。即马尔可夫模型。   stemming 词干化，取词根。   dictionary-based disambiguation 基于词典的消歧。   function fitting 函数拟合，就是说基于一些数据点推断出函数的形态。   Hidden Markov Model， HMM。 隐马尔可夫模型。   rule based 基于规则   corpus based 基于语料库。   conditional probability 条件概率   transitive probability 转移概率   neighboring pairs of words 词语接续对。   maximum likehood estimation 最大似然估计   data sparse 数据稀疏","title":"自然语言处理应用方向和专业英语"},{"content":" 计算机程序设计艺术，机械工业，经典之作 C指针与陷阱 C++大学教程，电子工业 自然语言处理，电子工业 C程序设计题典，夏宽里 高级程序员教程， C语言从入门到精通，陈锐，电子工业出版社， 比较全面， 零基础学数据结构，机械工业 C语言程序设计艺术，机械工业 C语言程序设计语言，机械工业， c语言大学教程，电子工业， 数据结构，严蔚敏 c/C++常用函数与算法速查手册，中国铁道出版社","title":"c语言书籍"},{"content":"转载：http://mindhacks.cn/2008/12/18/how-to-think-straight/ 一年前一个偶然的机会我遇到了一本书——《影响力》，看完这本书之后对我们如何思维产生了极大的兴趣，于是在一年的时间里面密集地阅读了以下一些方面的经典著作：社会心理学、认知科学、神经科学、进化心理学、行为经济学、机器学习、人工智能、自然语言处理、问题求解、辩论法（Argumentation Theory）、Critical Thinking、判断与决策。以及大量的 Wikipedia 条目。 这一年来，对以上这些领域的阅读和思考给我带来了极大的价值，我相信他们也会给你带来巨大的收益。 关于为什么我认为我们都需要学习这方面的知识，我曾在博客中写到： 另外还有一些我认为是 essential knowledge 的例子：分析问题解决问题的思维方法（这个东西很难读一两本书就掌握，需要很长时间的锻炼和反思）、判断与决策的方法（生活中需要进行判断与决策的地方远远多于我们的想象），波普尔曾经说过：All Life is Problem-Solving。而判断与决策又是其中最常见的一类Problem Solving。尽管生活中面临重大决策的时候并不多，但另一方面我们时时刻刻都在进行最重大的决策：如：决定自己的日常时间到底投入到什么地方去。如：你能想象有人宁可天天花时间剪报纸上的优惠券，却对于房价的1%的优惠无动于衷吗？（《别做正常的傻瓜》、《Predictably Irrational》）如：你知道为什么当手头股票的股价不可抑止地滑向深渊时我们却一边揪着头发一边愣是不肯撤出吗？（是的，我们适应远古时代的心理机制根本不适应金融市场。）糟糕的判断与决策令我们的生活变得糟糕，这还不是最关键的，最关键的是我们从来不会去质疑自己的判断，而是总是能“找到”其他为自己辩护的理由（《错不在我（Mistakes were made, but not by me）》）又，现在是一个信息泛滥的时代，于是另一个问题也出现：如何在海洋中有效筛选好的信息，以及避免被不好的信息左右我们的大脑（Critical Thinking）关于以上提到的几点我在豆瓣上有一个专门的豆列（“学会思考”），希望有一天我能够积累出足够多的认识对这个主题展开一些详细介绍。 人类的大脑和思维是目前已知最为复杂的系统，对这个系统的研究不仅自身是一件极其迷人的事情，对于像我们这样的芸芸众生来说即便不去做研究，学习一些这方面的科普知识，对于学会正确地思考有极大的益处。 你的大脑是你唯一的工具，要正确利用这个工具，唯一的途径就是去了解它。与很多人的直觉相反，实际上我们的思维有着各种各样的缺陷和陷阱（keyword: cognitive bias），我们解决日常问题的思维方式也并不总是最优的（keyword: bounded rationality），这里摘抄一段我在豆列上的导言： 我们的思维有很多很多的弱点，我一向认为，正确的思维方式，是一切高效学习的基础。比如参见如下2个例子，错误的思维方式得到的结论有大得多的可能性是谬误。 人总喜欢沿袭以往习得的经验，并通过类比来进行外推。我第一次在一个地铁终点站坐地铁的时候，看着从远方开来的地铁，我心生疑惑——“这车每节车厢都这么长，待会怎么调头呢（我心说没看到铁轨终点有一个大大的供调头的 U 形弯啊）？”，当车开始开的时候我终于意识到原来车是可以往两头方向开的。 人喜欢从关联当中寻找因果，有一次我我老婆去银行取款，到了 ATM 室的自动门口，我开玩笑地拿着手头的饭卡去刷了一下，然后——门居然开了。我顿时来了劲，立即得出一个结论：这个刷卡装置不安全，至少不是能够专门识别银联的卡的。我甚至飞快地泛化出了一个更具一般性的理论来解释这个现象：即可能所有带有磁性的卡都可以用来开门。老婆看我得意洋洋，就泼过来一盘冷水：不一定是你的卡刷开的啊，你不刷卡试试看。我不信，说怎么可能呢，心想我刷卡，门就开了，还有比这更明显的因果关系嘛。但出乎我意料的是，我走出门，这次没刷卡，门也开了——原来是感应门——原先这个 ATM 室的确是刷卡门，但后来改成了感应门，刷卡的那个装置只不过没拆掉残留在那里而已。 总的来说 人类的思维充满着各种各样的捷径，每一条捷径都是一把双刃剑。一方面，它降低了大脑的认知复杂性（笼统的看一个问题要比细致的分析简单得多），有助于迅速做出绝大部分时候都正确的判断；但另一方面，它也常常导致人们把大部分情况下成立的法则当成了放之四海而皆准的。可以说，有多少捷径，就有多少条谬误。 人类的情绪也在很大程度上影响着人的思考。比如，如果你憎恶一个人，你往往就会反对他的所有立场。反之亦成立。 人类大脑经过长时间的进化，先天就具备一些特定的“思维定势”，以使得人类能够在面对进化过程中经常出现的适应性问题时能够不假思索的做出迅速的反应。然而，在现代社会，这类思维定势已经不适应了。 人类不可避免的受着各种各样的偏见的影响，这些偏见有些是有一定适应价值的“思维定势”（如事后聪明式偏见），而有些则是大脑的认知机制的“缺陷”。 以上，构成了人类思维中的种种谬误。而学会思考，就是学会认识到这些谬误。 Critical-Thinking 在西方拥有悠久的历史，早到古希腊时代，亚里士多德就已经对人类语言中的各种各样的谬误有了一定的认识（譬如，“我们无法讨论不存在的东西，所以所有的事物都是真实的”），并对辩论之中存在的各种各样的谬误进行了归类。然而令人遗憾的是，在中国的文化里面，理性思维似乎是一直被抑制的，中国文人传统都是非理性思考者；所谓非理性思考，主要包括联想、比方等形式，这些思维方式作为人类天生具有的思维方式的一种，一方面当然有它的好处（比如在科研方面，联想往往能够启发新思路；类比也有助于用新颖的方式来解决既有问题），然而另一方面，这样的思维方式同样也充满了各种各样致命的谬误。在大众知识领域，自中国古代文人思维习惯流传下来的影响深刻地左右着人们的语言习惯，随处可见的不靠谱的类比和文字游戏就是证明（例如，严格来说，类比的一般形式是，A具有X、Y、Z三个属性，B具有X、Y属性（类似于A），所以B具有Z属性。这个类比要成立，必须要满足一个前提，即X、Y属性对于Z属性的有无必须是有关的。然而这个前提被根本忽视了，详见 False Analogy）。 这个豆列中的书，有一些是介绍人类思维工作的机制的，认识这些机制是正确思考的大前提；有许多是关于人类推理（Reasoning）过程中的形形色色的谬误的，因为唯有认识到 这些谬误，才能避免它们。唯有避免了思维的谬误，才能进行正确的思考。 注： 一个最完整的认知偏见（cognitive bias）列表见：http://en.wikipedia.org/wiki/List_of_cognitive_biases 一个完整的 Fallacies 列表见： http://en.wikipedia.org/wiki/Fallacies Wikipedia 上关于 Critical Thinking 的条目见： http://en.wikipedia.org/wiki/Critical_thinking 另： 人类在思考问题的过程中，自身的思维习惯、性格、知识积累无不都在悄悄地影响着思维的过程，所以，一些心理学的知识也非常有助于帮助正确的思考。更多心理学方面的推荐，参考：http://www.douban.com/doulist/46003/ 文章末尾将贴出的是我这一年来学习的知识结构总揽（用 XMind 画的思维导图）。注：这只是一个整体的知识结构，或者说“寻路图”，其中固然包含一些例子（用 “e.g.” 标出），但最重要的是从各个分支引申出去的延伸阅读，后者包含上百个很有价值的 wikipedia 条目，不下 50 本经典的著作（大部分我已经读过，小部分经过我的仔细考察，正在阅读中或者肯定是有价值的）。 如何获得这些延伸出去的阅读，有两个办法： 1. 在总揽图中抽出关键字到 Wikipedia&Google 上查找，如：informal fallacy，cognitive biases,bounded rationality, critical thinking, argumentation theory, behavioral economics, problem solving等等（以上这些关键字你都会在思维导图中看到）。注：阅读 Wikipedia 时要严重注意每个条目后面的 Reference ，一般来说这些参考资料本身也都非常经典，其价值不亚于 Wikipedia 条目本身。 2. 查看我整理的四个豆列： 【只读经典】心理学改变生活 【只读经典】学会思考 【只读经典】判断与决策 机器学习与人工智能学习资源导引 以上四个豆列中整理的绝大多数都是我阅读过的，你也可以参考我的整个“思维”标签下的书。如何获得这些书（尤其是其中包含大量的无中文翻译版的英文书）请参考李笑来老师的笔记。 这个领域的新知识是如此的纷至沓来，以至于我只有时间不断地阅读和思考，以及不时在我的 Google Notebook 里做一些笔记，而完全没有时间一本书一本书，一个子领域一个子领域地写具体的 Introduction （目前具体的荐书只是在 TopLanguage 上零散的推荐了几本，还没有专题介绍）。既便如此，仍然还是在博客上写了很多相关的东西，它们就是这一年来的学习的收获的证明:-)，因此如果你想快速判断上面列出的一些书籍是否对你有价值，有多大的价值，不妨参考一下我写的这些文章，这些文章很大程度上是在这一年的学习过程当中的感悟或总结。注：第 3 部分（关于学习、记忆与思考）的文章基本上是领域无关的： 关于 Problem Solving 的 《跟波利亚学解题》 《知其所以然地学习（以算法学习为例）》 关于机器学习的（机器学习和人工智能领域对于理解我们的思维方式也提供了极好的参考） 《数学之美番外篇：平凡而又神奇的贝叶斯方法》 《机器学习与人工智能学习资源导引》 关于学习、记忆与思考的 《一直以来伴随我的一些学习习惯》（一，二，三，四） 《方法论、方法论——程序员的阿喀琉斯之踵》 《学习与记忆》 《阅读与思考》 《鱼是最后一个看到水的》 《我不想与我不能》 《学习密度与专注力》 好在我并不打算零星的一本一本推荐:D 所以我就花了点时间将整个的知识体系整理了一番，画了下面这张结构图，请按图索骥，如下（有三个版本，1. 至 xMind Share 的超链接，2. 内嵌在该页面中的幻灯片，如果无法载入请参考 1 。3. 图片版（注：图很大，请下载浏览或打印）） 我在前面写学习习惯的时候曾经提到： 8. 学习一项知识，必须问自己三个重要问题：1. 它的本质是什么。2. 它的第一原则是什么。3. 它的知识结构是怎样的。 有朋友问我具体的例子，好吧，那么这张思维导图便是第三点——知识结构——的一个很好的例子:) 1. 至 XMind Share 的超链接：http://share.xmind.net/pongba/how-to-think-straight-4/ 2. 嵌入的幻灯片（如加载失败请直接点击上面的 XMind Share 超链接至 XMind 浏览）： 3. 图片版（此为缩略版，完整版请至相册下载：google picasa 的 ，或 csdn 相册的）（最后提醒一下，别忘了这幅图只是大量书籍和 Wikipedia 条目的“藏宝图”，如何延伸阅读请参考前文所述的方法）","title":"如何清晰地思考（近一年来业余阅读的关于思维方面的知识结构整理）"},{"content":"        命名实体识别（NE）是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。 　　命名实体识别是信息提取、问答系统、句法分析、机器翻译、面向Semantic Web的元数据标注等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。       一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。其中人名、地名、组织机构名是最常用到的三种。 　　命名实体识别的过程通常包括两部分：（1）实体边界识别；（2） 确定实体类别（人名、地名、机构名或其他）。英语中的命名实体具有比较明显的形式标志（即实体中的每个词的第一个字母要大写），所以实体边界识别相对容 易，任务的重点是确定实体的类别。和英语相比，汉语命名实体识别任务更加复杂，而且相对于实体类别标注子任务，实体边界的识别更加困难。 　　汉语命名实体识别的难点主要存在于： （1）汉语文本没有类似英文文本中空格之类的显式标示词的边界标示符，命名实体识别的第一步就是确定词的边界，即分词； （2）汉语分词和命名实体识别互相影响； （3）除了英语中定义的实体，外国人名译名和地名译名是存在于汉语中的两类特殊实体类型； （4）现代汉语文本，尤其是网络汉语文本，常出现中英文交替使用，这时汉语命名实体识别的任务还包括识别其中的英文命名实体； （5）不同的命名实体具有不同的内部特征，不可能用一个统一的模型来刻画所有的实体内部特征。        组织机构名识别的四大难点：1、中文机构名的用词十分广泛。2、长度极其不稳定。3、含有大量其他的命名实体，其中不乏未登录的地名。4、大多数机构名称都有其简称，简称规则使得难度加大。 http://jhx0129.blog.163.com/blog/static/17897057200852751212473/ 汉语分词、词性标注、命名实体识别","title":"命名实体识别"},{"content":"1.   技术背景     分类问题是人类所面临的一个非常重要且具有普遍意义的问题。将事物正确的分类，有助于人们认识世界，使杂乱无章的现实世界变得有条理。自动文本分类就是对大量的自然语言文本按照一定的主题类别进行自动分类，它是自然语言处理的一个十分重要的问题。文本分类主要应用于信息检索，机器翻译，自动文摘，信息过滤，邮件分类等任务。文本分类的一个关键问题是特征词的选择问题及其权重分配。     在搜索引擎中，文本分类主要有这些用途：相关性排序会根据不同的网页类型做相应的排序规则；根据网页是索引页面还是信息页面，下载调度时候会做不同的调度策略；在做页面信息抽取的时候，会根据页面分类的结果做不同的抽取策略；在做检索意图识别的时候，会根据用户所点击的url所属的类别来推断检索串的类别等等。 2.   自动分类的原理和步骤     在分类的时候首先会遇到文档形式化表示的问题，文档模型有3种：向量空间模型，布尔模型和概率模型，其中我们常用的是向量空间模型。向量空间模型的核心描述如下： 文档（Document）：文本或文本中的片断（句子或段落）。 特征项（Term）：文档内容用它所包含的基本语言单位来表示，基本语言单位包括字、词、词组、短语、句子、段落等，统称为特征项。 特征项权重（Term Weight）：不同的特征项对于文档D的重要程度不同，用特征项Tk附加权重Wk 来进行量化，文档D可表示为（T1，W1；T2，W2；…；Tn，Wn） 向量空间模型（Vector Space Model）：对文档进行简化表示，在忽略特征项之间的相关信息后，一个文本就可以用一个特征向量来表示，也就是特征项空间中的一个点；而一个文本集可以表示成一个矩阵，也就是特征项空间中的一些点的集合。 相似度（Similarity）：相似度Sim（D1，D2）用于度量两个文档D1和D2之间的内容相关程度。当文档被表示为文档空间的向量，就可以利用欧氏距离、内积距离或余弦距离等向量之间的距离计算公式来表示文档间的相似度。     其中特征选取是文本表示的关键， 方法包括：文档频率法（DF）、信息增益法和互信息法等等。     在做特征选取之前，一般还要进行预处理的工作，要对先对网页降噪。另外在实际的分类中，除了利用文档的内容特征之外，可能还会用到实际应用中所特有的特征，比如在网页分类中，可能用到url的特征、html的结构特征和标签特征等信息。     分类的基本步骤是这样的：定义分类体系，将预先分类过的文档作为训练集，从训练集中得出分类模型，然后用训练获得出的分类模型对其它文档加以分类。 3.   常用的分类算法     文档自动分类是学术界研究多年，技术上比较成熟的一个领域。目前分类算法主要分下面这些：     其中比较常用的是：支持向量机（SVM）方法、朴素贝叶斯(NB)方法、神经网络（NN）方法、K近邻（KNN）方法、决策树（Decision Tree）方法等。 支持向量机（Support Vector Machines, SVM）由Vapnik在1995年提出，用于解决二分类模式识别问题。它通过寻找支持向量来确定决策面，并使分类间隔最大。SVM方法提供了解决 “维数灾难”问题的方法。SVM方法较好的理论基础和它在一些领域的应用中表现出来的优秀的泛化性能，尽管SVM算法的性能在许多实际问题的应用中得到了验证，但是该算法在计算上存在着一些问题，包括训练算法速度慢、算法复杂而难以实现以及检测阶段运算量大等等。 朴素贝叶斯(Naive Bayes，NB) 概率分类器是机器学习中很常用的一种方法，其基本思想是利用单词和分类的联合概率来估计给定文档的分类概率。         贝叶斯公式：P(C|X)*P(X)=P(X|C)*P(C)         特征向量：X=(x1,x2,x3…)    C={C1,C2,……}         其中P(C)是每个类别的先验概率，即，互联网上各个分类所占总页面的比例         P(X|C)：条件概率，表示在类别为C的训练集合中，X的分布情况。         P(X)：每个特征值的分布，由于特征值的分布是随机的，所以P(X)相等 神经网络（Neural network，NN）技术是人工智能中的成熟技术。将神经网络用于文档分类时，需要为每个分类建立一个神经网络，通过学习得到从输入单词（或者更复杂的特征词向量）到分类的非线性映射。其计算量和训练时间非常庞大。 KNN是著名的模式识别统计学方法，已经有四十年历史，它是最好的文本分类算法之一。KNN算法相当简单：给定一个测试文档，系统在训练集中查找离它最近的k个邻居，并根据这些邻居的分类来给该文档的候选分类评分。把邻居文档和测试文档的相似度作为邻居文档所在分类的权重。如果这k个邻居中的部分文档属于同一个分类，则该分类中的每个邻居的权重求和并作为该分类和测试文档的相似度。该方法的特点是允许文档可以属于多个分类。KNN通过查询已知类似的例子的情况，来判断新例子与已知例子是否属于同一类。     通过我们对现实网页的分类测试情况看，这些方法中SVM方法的效果是比较好的，但是性能不高; 朴素贝叶斯的分类效果虽然略差于SVM，但是性能上要好很多。 4.   网页分类应用 4.1分类算法 　　实际应用中, 除了分类效果外, 速度是一个需要重点考虑的因素。 4.2分类类别 　　在搜索引擎中, 在不同的应用场景下, 会有不同的分类的标准, 比如在链接调度中需要信息页、索引页这样的分类，不同类型的页面更新调度的周期不一样；排序对分类的要求又不同, 比如按表现形式分图片、视频等；按网站类型分为论坛、博客等，不同类型的页面抽取策略也会不尽相同；再按内容主题分成小说、招聘和下载等类别。对网页从多个维度进行分类，能更好给用户提供更为贴切的检索结果。 4.3 特征选取 　　在学术研究中, 一般比较重视分类算法的研究,在特征选择上比较忽视。传统的特征选择一般是用TF*IDF等方法选择内容关键字等，这也是我们使用的一个重要因子， 但是除内容特征之外，我们还会用到很多其它特征，比如：网站特征、html特征和url特征等，这些特征会明显的提高分类的准确率和召回率。","title":"搜索中得网页分类技术"},{"content":"（这是曾经发在bbs上的文章，现在粘贴过来，作为一个副本。）      万维网毫无疑问是20世纪伟大的发明，但它也有缺陷，不知你是否注意到咱学院很多好网站不容易找到，例如：http://ccst.jlu.edu.cn/JCSB/webRoot/index.html。我们人类搜索信息的主要方式是提问的形式，例如，我把爸爸的妈妈的表弟的儿子的媳妇叫什么？（额～，我承认我有点恶搞），很可惜，你如果这样搜素是得不到答案的，为什么这种方式不行，而必须接受传统搜索引擎的关键字方式。其实，这是因为当前Web的基础结构决定的了它上面的应用是什么样的，所谓“经济”基础决定上层建筑。是的，大牛早就注意到了，他们已经开始轰轰烈烈的语义网运动，在此，我打算用5篇左右的文字来简单的介绍一下语义Web，让本科的学弟学妹们懂得有这么一个方向。由于我刚接触语义Web，如果行文中有错误之处，恳请大家批评指正。好了，我们开始第一篇。      噢，对了，在此我要说明的是，下面的思想归功于Toby Segaran，我只是做了一些本土化工作，但是如果其中引入了错误（或其他问题），那是我的责任，与Toby Segaran无关。 ============================================================================== “People are always asking ’what great technique can I use on this data set?’ when they should be asking ‘what’s the best data set I can get?’ ”        —— Andreas Weigend, former chief scientist at Amazon   引言——先来说一下语义     自然语言，就是我们在生活中与人交流的语言，她是如此的令人惊奇！你能在毫不费力的情况下让一个进入校园的陌生者找到吉林大学计算机学院的大楼——“青楼”；你可以和你的朋友分享你关于音乐、电影方面的知识；你可以去图书馆，那起一本书，从中习得知识，即使作者生活在几十年前，甚至上百年前（^-^，如果你懂古文的话）。此时也许就有人说了，自然语言有什么神秘的，3岁小孩都能学会。神秘吗？是的，确实神秘，之所以我们会忽略她的神秘，而觉得如此平常，是因为我们就生活在充满自然语言的环境中。有句话说的好：“鱼是最后一个知道水的”。实际上，没有比自然语言更好的API了。     下面我们来举个简单的例子，下面是两个“主-谓-宾”结构的句子，这是个非常简单的语法结构： 1， 李雷喜欢韩梅。 2， 韩梅喜欢小狗。     这里的每一个句子都表达了一定的信息。其中词语“李雷”和“韩梅”分别代表了一个人，词语“小狗”代表了一个会“汪汪”叫的动物（注意：不是某个雪饼上的那个东西，你能理解，对吗？如果不知道，请在google上敲入“wangwangxuebing”）。因为你之前知道“喜欢”这个词的含义（额~，你不知道。。。），那么通过读这两句话，你获得了一些关于这个世界的信息。利用这些信息，现在你可以回答一些问题，例如“请问谁喜欢小狗？”（我突然想起了，初中的英语听力，啊，好怀念）这是一个关于语义的例子，其中符号（如“李雷”）代表一个事物或者一个概念，而一个符号组成的序列将会表达一定的含义。     在一个动作中，语义是把含义传达给结果的过程。一个符号序列可以表达一定的含义（信息），并且这个信息将会影响我们的行为。例如，当你在阅读这一页时，你正在将这页文字所表达的含义和你以前的知识整合在一起，如果我所写文字的语义是清晰的，那么你偶尔会微微一笑，并且会帮助你了解语义网这个研究方向（当然只是微小的一部），你甚至会选择语义Web作为未来的研究方向。然后学习语义Web技术去做一些great things（^-^）。     虽然以自然语言作为例子，但是我的目的不是介绍自然语言处理的知识，我的目的是介绍如何利用语义信息在计算机之间表达，组合，共享知识，让机器读懂我们人类的知识，让机器帮我们做更多的事（囧，我们真的好懒）。     如果你编写过一个程序，哪怕它只有一个变量。那么你就接触过语义。作为一个程序员，你知道这个变量代表一个值，并且你编写程序对这些变量的变化作为响应。幸运的是，你可能在你的代码中加入了一些注释，用来解释这个变量所代表的东西，以及如何使用这个变量，这样其他程序员（包括自己）就可以更容易的理解你的代码。变量的值，值的含义，以及程序的动作，这三者之间的关系是非常重要的，但是，它是在系统的设计中隐含的。     付出一点小小的努力，就可以使你的数据之间的语义关系显式的表达出来，并且可以使你的系统的行为随着数据含义的不同而变化。随着语义的显式表达，其他程序，甚至不是你写的，都可以无缝（这个词好啊）的利用你的数据。类似的，当你的程序也理解语义数据时，那么你的程序就可以处理这些数据，即使这些数据在你设计你的程序时并没有考虑。（是不是有点迷糊呢？呵呵，不要紧，接下来，我来看一些简单的例子，帮助你理解什么是语义数据） 未完待续……","title":"戏说语义网——简介"},{"content":"国内计算机类三大中文学报《计算机学报》《软件学报》《计算机研究与发展》投稿的实际体会。 共同点： 都是EI核心来源期刊； 中国计算机学会参与主办是会刊； 科学出版社出版发行； 可接受8000-10000字左右的长文； 稿量大，处理流程大多缓慢，应早投； 国内众星捧月zzz 《计算机学报》 http://cjc.ict.ac.cn/，月刊，中国计算机学会与中国科学院计算技术研究所主办，网站好像最近改版了，但功能有所欠缺。 投稿方式： 网页登记然后Email投稿,中英文均可；初审后通知编号，邮寄打印稿二份及投稿声明等， 审理费：150元。 审稿周期：4个月左右。被拒的或录用的文章给出的意见都比较中肯，感觉审稿人专业啊。 （本人投了一篇被拒历时4个月，另外一个师弟投一篇被录用，历时10个月左右时间，感觉被拒的稿件处理快，可接受的稿件审理较慢） 录用率：不详。 版面费：不详。 发表周期：不详。 其它等事宜未接触不加评论。 《软件学报》http://www.jos.org.cn，月刊,由中国科学院软件研究所和中国计算机学会联合主办，网站功能齐全，投稿处理流程合理。 投稿方式：直接网站在线投稿，中英文均可。初审后通知编号，邮寄审理费。外审通过后再通知其它处理事宜。 审理费：150元 审稿周期：6个月左右。（本人前后总共投了二篇，第一篇3个月给了通知，第2篇6个月给了通知,根据意见逐一修改，且必须给出修改说明，较为严谨）. 录用率：不详 发表周期：录用后的发表周期较长，需要耐心等待，但专刊较快. 版面费标准：180.00元/面，收费比较厚道。 整个处理流程中，编辑部会及时与作者沟通确认，感觉很受尊重。2007年（以后也会有）该刊组织了很多专刊，投专刊的文章被录用后发表周期相对短，但录用率超低（如今年网络专刊为5%）。专刊反映信息较快，是一种不错的方式。 《计算机研究与发展》http://crad.ict.ac.cn/，月刊，由中国科学院计算技术研究所-中国计算机学会联合主办,网站功能较为齐全。 投稿方式：在线投稿或email投稿的同时还要邮寄3份打印稿，投稿声明等。好像不接受英文稿。 审理费:200元。 审理周期：官方说6个月左右（实际体会，投了一篇稿子，催了三次等了8个月给出了通知，给出的审稿意见感觉不乍的呢。） 录用率：不详。 发表周期：不详。 版面费：260元/页（不超过7页），超出的部分，每页400元。 最后必须要提的是：国内最权威的计算机科学与技术类期刊当属英文期刊JCST（中文名：计算机科学技术学报,双月刊,国内外同行评审,中国计算机学会和中科院计算所主办），SCIE和EI双收录. 附JCST简历：《Journal of Computer Science and Technology》(JCST)是中国计算机科学技术领域唯一个英文学术性期刊。JCST为中国计算机学会会刊, 由中国科学院计算技术研究所承办。JCST由数十位国际计算机界的著名专家和学者联袂编审，把握世界计算机科学技术最新发展趋势。目前，JCST正在稳步地发展，其影响不断扩大，知名度日益提高。JCST荟萃了国内外计算机科学技术领域中有指导性和开拓性的学术论著，其中部分文章邀请了著名计算机领域的专家撰写。其内容包括: 计算机科学理论，形式化方法，信息安全，算法与计算复杂性，计算机体系结构与高性能计算，模式识别与图像处理， VLSI设计与测试，软件工程，计算机网络与Internet，分布式计算与网格计算，自然语言处理，生物信息学，计算机图形学与人机交互，人工智能等。","title":"国内计算机类三大中文学报投稿体会"},{"content":" 汉语自动分词研究评述 【作者】孙茂松/邹嘉彦 【作者简介】孙茂松，清华大学     邹嘉彦，香港城市大学     孙茂松，100084　北京.清华大学计算机系　Email:lkc-dcs＠mail.tsinghua.edu.cn     邹嘉彦，香港　香港城市大学　语言资讯科学研究中心 【内容提要】本文首先阐述了汉语自动分词研究的现实性和可能性，接着围绕该研究中的三个基本问题（切分歧义消解、未登录词处理和语言资源建设）展开了重点讨论，并扼要评介了十几年来产生的各种方法。最后就这个领域未来的研究要点发表了一些个人意见。 【关键词】中文信息处理/汉语自动分词/切分歧义消解/未登录词处理/语言资源建设 【正文】     1.汉语自动分词的现实性与可能性     众所周知，中文文本没有类似英文空格之类的显式表标示词的边界标志。汉语自动分词的任务，通俗地说，就是要由机器在中文文本中词与词之间自动加上空格。一提到自动分词，通常会遇到两种比较典型的质疑。一种质疑是来自外行人的：这件事看上去平凡之极，好像一点儿也不“热闹”，会有什么用呢？另一种质疑则是来自内行人的：自动分词研究已经紧锣密鼓地进行了十几年了，可到现在也未见一个经得起考验的系统推出来（与此形成鲜明对照的是，日语同样也存在分词问题，但已经有了圈内人士广泛认同的日语分词系统），这几乎成了中文信息处理中一个“永恒”的话题，那么，到底还有没有希望搞出真正意义上的“门道”来？     第一种质疑关心的是自动分词的现实性问题，其答案是十分明确的。当前的大环境令人鼓舞：中国正在向信息化社会迅速前进，其突出表征是Internet上中文网页的急剧增加和中文电子出版物、中文数字图书馆的迅速普及。以非受限文本为主要对象的中文自然语言处理研究于是也水涨船高，重要性日益显著。而汉语自动分词是任何中文自然语言处理系统都难以回避的第一道基本“工序”，其作用是怎么估计都不会过分。只有逾越这个障碍，中文处理系统才称得上初步打上了“智能”的印记，构建于词平面之上的各种后续语言分析手段才有展示身手的舞台。否则，系统便只能被束缚在字平面上，成不了太大气候。具体来说，自动分词在很多现实应用领域（中文文本的自动检索、过滤、分类及摘要，中文文本的自动校对，汉外机器翻译，汉字识别与汉语语音识别的后处理，汉语语音合成，以句子为单位的汉字键盘输入，汉字简繁体转换等）中都扮演着极为重要的角色（Wu Z.M.and Tseng G.1993;Wu Z. M.　 andTseng G.1995; Nie J.Y.and Brisebois M.et al.1996;Sun M. S. andLin F.Z.,et al.1996）。我们举两个例子直观说明一下。     ［文本检索］     设文本A含句子（1a）而文本B含句子（1b）：     （1）a.和服│务│于三日后裁制完毕，并呈送将军府中。     　　 b.王府饭店的设施│和│服务│是一流的。     显然，文本A讲的是日本“和服”，文本B则与酒店的“服务”有关，两者风马牛不相干。如果不分词或者“和服务”分词有误，都会导致荒谬的检索结果。     ［文语转换］     注意句子（2a）、（2b）中的“查金泰”：     （2）a.他们是来│查│金泰│撞人那件事的。     　　 b.行侠仗义的│查金泰│远近闻名。     句子（2a）中“查”为动词，应读cha，句子（2b）中则为姓氏，应读zha。     第二种质疑直指自动分词的可能性问题。虽然迄今为止我们尚不能下一个完全肯定的结论，但经过圈内学者十几年不懈的探索，这个答案的轮廓还是大体凸显出来了。毕竟词平面上的研究与句法平面和语义平面相比照，本身难度要小得多，并且无论是在计算语言学方面还是在普通语言学方面，所取得的成果也要成熟、扎实得多。现有的工作积累已经达到了可以厚积薄发的程度。如果说面向非受限文本的汉语句法、语义自动分析还是可望而不可即的话，那么，面对相同对象的汉语自动分词，则距凯歌初奏只有几步之遥了（当然即使达到了那个目标，也还不是功德圆满）。Sproat R. and Shih C.L.,et al.（1996）及Sun M. S.and Shen D.Y.,et al.（1997 ）的汉语自动分词原型系统已初具处理非受限文本所需的种种功能，他们沿着正确方向跨了一大步。          本文的重点是第2节，将集中讨论汉语自动分词中的基本问题， 并扼要评介十几年来产生的各种方法（文后的参考文献基本囊括了这一领域比较有代表性的论文）。第3 节则就今后的研究要点发表一些个人意见。     2.汉语自动分词中的基本问题和主要解决方法     2.1切分歧义及其处理方法     2.1.1切分歧义的基本类型     切分歧义是汉语自动分词研究中的一个“拦路虎”。梁南元（1987）最早对这个现象进行了比较系统的考察。他定义了两种基本的切分歧义类型：     定义1　汉字串AJB被称作交集型切分歧义，如果满足AJ、JB同时为词（A、J、B分别为汉字串）。此时汉字串J被称作交集串。     ［例］交集型切分歧义：“结合成”     （3）a.结合│成     　　 b.结│合成     其中A＝“结”，J＝“合”，B＝“成”。     定义2　汉字串AB被称作多义组合型切分歧义，如果满足A、B、 AB同时为词。     ［例］多义组合型切分歧义：“起身”     （4）a.他站│起│身│来。     　　 b.他明天│起身│去北京。     对交集型切分歧义，他还定义了链长：     定义3　 一个交集型切分歧义所拥有的交集串的集合称为交集串链，它的个数称为链长。     如，交集型切分歧义“结合成分子”、“结合”、“合成”、“成分”、“分子”均成词，交集串的集合为｛“合”，“成”，“分”｝，链长为3。     这些定义所涉及的几个概念，基本刻画了汉语切分歧义的结构特点，因而一直沿用下来。     梁南元（1987）对一个48,092字的自然科学、社会科学样本进行了统计：交集型切分歧义518个，多义组合型切分歧义42个。据此推断，中文文本中切分歧义的出现频度约为1.2次／100字，交集型切分歧义与多义组合型切分歧义的出现比例约为12∶1。     有意思的是，刘挺、王开铸（1998）的调查却显示了与梁南元截然相反的结果：中文文本中交集型切分歧义与多义组合型切分歧义的出现比例约为1∶22。造成这种情形的原因在于，定义2有疏漏。Sun M.　S.and Benjamin K.T.（1995）猜测， 加上一条限制才真正反映了梁的本意：     定义2'汉字串AB被称作多义组合型切分歧义，如果满足（1）A、 B、AB同时为词；（2）中文文本中至少存在一个前后语境C，在C 的约束下，A、B在语法和语义上都成立。     例如，汉字串“平淡”符合定义2，但不符合定义2' （因为“平│淡”在文本中不可能成立）。刘、王将“平淡”计入了多义组合型切分歧义，梁并未计入。由于符合定义2 的汉字串数量远远大于符合定义2'的汉字串数量，出现“乾坤颠倒”也就不足为怪了。     仔细分析一下，定义1和定义2都是完全从机器角度加以形式定义的，定义2'则增加了人的判断。孙茂松、黄昌宁等（1997）认为， 定义2中给出的名称“多义组合型切分歧义”是不太科学的（实际上，某些交集型切分歧义也是多义组合的），易引起混淆，与“交集型”这个纯形式的名称相呼应，称作“包孕型”或者“覆盖型”可能更恰当。     董振东（1997）采用了另外一套名称：称交集型切分歧义为“偶发歧义”，称多义组合型切分歧义为“固有歧义”。“两者的区别在于：造成前者歧义的前后语境是非常个性化的、偶然的、难以预测的”，“而后者是可以预测的”。这个表述相当深刻地点出了两类歧义的性质，耐人寻味。但名称的准确性仍有可斟酌之处。 　　　　　　　 视角　 真歧义类　　　　　　 伪歧义类 交集型切分歧义 定义　　　　　　　定义1 　　　　　　　 性质　　　　　　偶发歧义 　　　　　　　 数量　 少量　　　　　　　　 大量 　　　　　　　 例子　 地面积,和平等,的确定 和软件,在建设,部门对 覆盖型切分歧义 定义　 定义2'　　　　　　　 定义2扣除定义2'的外延 　　　　　　　 性质　 固有歧义　　　　　　 偶发歧义 　　　　　　　 数量　 少量　　　　　　　　 大量 　　　　　　　 例子　 起身,把手,一行,三角　平淡,高度,词条,结论         　　　　表1　切分歧义类型表          孙茂松、左正平（1998）指出，切分歧义应进一步区别“真切分歧义”和“伪切分歧义”。譬如：同属交集型，“地面积”为真歧义（“这几块│地│面积│还真不小”“地面│积│了厚厚的雪”），“和软件”则为伪歧义（虽然存在两种不同的切分形式“和软│件”和“和│软件”，但在真实文本中，无一例外地应被切分为“和│软件”）；同属覆盖型，“起身”为真歧义，“平淡”则为伪歧义。       归纳以上论述，本文整理出一张切分歧义类型表（见表1）， 希望对澄清概念上流传已久的混乱有所帮助。       关于切分歧义，还有两点基本观察：     1）根据孙茂松、左正平（1998）对一个1亿字语料库的穷尽式统计，交集型切分歧义长度变化范围为3～14 个字（“提高人民群众生活水平息息相关”），交集串长度变化范围为1～3个字（“如箭在弦上”），链长变化范围为1～9个字（“中国人民生活水平和美化”）；     2） 交集型和覆盖型常常会相互纠缠在一起，这就更增加了变数。如图1中的“提高人民生活水平”共可衍生出19 种可能的形式切分（弧线表示可成词）。     附图     图1　若干基本类型的混合     2.1.2　切分歧义的检测与消解     切分歧义处理包括两部分内容：（1）切分歧义的检测；（2）切分歧义的消解。这两部分在逻辑关系上可分成两个相对独立的步骤。      首先谈谈切分歧义的检测问题。“最大匹配法”（精确的说法应该叫“最长词优先匹配法”）是最早出现、同时也是最基本的汉语自动分词方法，1963年就在《文字改革》杂志上被介绍过（刘涌泉1988）。刘源、梁南元（1986）首次将这个方法大规模应用到汉语自动分词系统中。依扫描句子的方向，又分正向最大匹配MM（从左向右）和逆向最大匹配RMM（从右向左）两种。 最大匹配法实际上将切分歧义检测与消解这两个过程合二为一，对输入句子给出唯一的切分可能性，并以之为解。据梁南元（1987）的实验结果，在词典完备、没有任何其它知识的条件下，最大匹配法的错误切分率为1次／169字～1次／245字，并且具有简单、快速的优点。Guo J.（1997）更对最大匹配法的工作原理作了严格的形式解释。此外，揭春雨、刘源等（1989）比较完整地分析了最大匹配法的结构及其时间效率。       从最大匹配法出发导出了“双向最大匹配法”，即MM＋ RMM。SunM.S. and Benjamin K.T.（1995）注意到：汉语文本中90.0％左右的句子，MM和RMM的切分完全重合且正确，9.0％左右的句子MM和RMM 切分不同，但其中必有一个是正确的（歧义检测成功），只有不到1.0 ％的句子，或者MM和RMM的切分虽重合却是错的，或者MM和RMM切分不同但两个都不对（歧义检测失败）。这正是双向最大匹配法在实用中文信息处理系统中得以广泛使用的原因所在。        显然，双向最大匹配法存在着切分歧义检测盲区。针对切分歧义检测，另外两个有价值的工作是，王晓龙、王开铸等（1989）的“最少分词法”（歧义检测能力较双向最大匹配法要强些，产生的可能切分个数仅略有增加）和马晏（1996）的“全切分法”（穷举所有可能切分，实现了无盲区的切分歧义检测，但代价是导致大量的切分“垃圾”）。这个问题直到今天也没有完全解决——如果把双向最大匹配法视作一个极端（最简单）而全切分法视作另一个极端（最繁杂）的话，我们的目标应该是：在这两极之间寻找一个“删繁就简”的折衷方案，既（几乎）排除了检测盲区，又抑制了可能切分个数的无理膨胀。         接下来讨论切分歧义的消解问题。十几年来，研究人员几乎调动了人工智能领域所有“时髦”的计算手段来对付切分歧义，堪称“八仙过海，各显神通”。典型的手段包括：“松弛法”（Fan C.K. and　Tsai W. H. 1988），“扩充转移网络”（黄祥喜1989）， “短语结构文法”（梁南元1990；姚天顺、张桂平等1990；Yeh C.L. and Lee　H.　J. 1991；韩世欣、王开铸1992），“专家系统”（徐辉、 何克抗等1991），“神经网络”（徐秉铮、詹剑等1993 ）， “有限状态自动机”（Sproat R. and Shih C.L., et al. 1996），“隐Markov 模型”（LaiB.Y. and Sun M.S., et al. 1997；沈达阳、孙茂松等1997a； 孙茂松、左正平等1999a），“Brill式转换法”（Palmer D.D.1997）等。 这些新的探索体现了切分歧义消解计算的不同侧面，在一定范围内取得了各自的效果，但从总体上看，还都嫌粗糙；或者虽然研究比较充分，但模型本身的计算能力偏弱；或者仅仅搭起了一个框架，浅尝辄止；或者实验规模太小，说服力不足。        通过不断的实践，人们越来越深刻地认识到，如果没有足够的语言知识作为支撑，再先进的计算手段也只能是“银样蜡枪头——中看不中用”。切分歧义消解经历了一个由浅及深、由简单到复杂的语言知识利用的演变过程：     1 ）一些系统（尤其是早期系统）主要利用词频以及语素（自由抑或约束）、切分歧义表层结构等简单信息（Fan C.K. and Tsai W.　H.1988；李国臣、刘开瑛等1988；王永成、苏海菊等1990；Chen　K.　J.and Liu S.H. 1992；马晏1996）。     2）Sun M.S. and Lai B.Y., et al. (1992) 揭示了音节信息在自动分词中的作用。     3）何克抗，徐辉等（1991）断言，95.0 ％左右的切分歧义可以借重句法以下的知识解决，只有5.0％必须诉诸语义和语用知识。 基于规则的几个分词系统（黄祥喜1989；梁南元1990；姚天顺、张桂平等1990；Yeh C.L. and Lee H.J.1991；韩世欣、王开铸1992；徐辉、 何克抗等1991）都自觉或不自觉地受到这个结论的支配，切分歧义消解主要诉诸词法与句法规则。存在的缺陷是，规则集由人凭主观编制而成，会受到系统性、有效性、一致性、可维护性等“天然”问题困扰。       4）为克服人工句法规则集的弊端， 一些研究人员开始尝试另一种途径一句法统计。Lai B.Y. and Sun M.S., et al.(1992;　1997) 、Chang C.H. and Chen C.D.(1993)、白拴虎（1995）等将自动分词和基于Markov链的词性自动标注技术结合起来，利用从人工标注语料库中提取出的词性二元统计规律来消解切分歧义（词性标注对分词有反馈作用，两者并行）。初步实验（Lai B.Y.and Sun M.S., et al.1997）表明，同“先做最大匹配分词，再作词性自动标注”（词性标注对分词无反馈作用，两者串行）相比，这种做法的分词精度和词性标注精度分别提高了1.3％和1.4％。     （5）他俩儿谈恋爱是从头年元月开始的。     切分a.…　是　│　从头　│　 年　 │　元月　│　…     　　　　动词　　　副词　　时间量词　 时间词     切分b.…　是　│　从　│　 头年　 │　元月　│　…     　　　　动词　　 介词　　 时间词　　 时间词     虽然“从头”、“年”的词频之积大于“从”、“头年”的词频之积，但词性序列“动词＋副词＋时间量词＋时间词”的概率远小于“动词＋介词＋时间词＋时间词”的概率，所以选择切分b作为结果。     5）Wu A.D. and Jiang Z.X.(1998)走得更远。他们相信， 多数情况下，切分歧义可以在输入句子的局部范围内得到妥善处理，但有些比较复杂的切分歧义，必须在句中更大的范围内才能解决。当遇到这种情况时，他们的系统将对句子做完整的句法分析，如果分析失败，则拒绝相应的切分：     （6）在这些企业中国有企业有十个。     切分a.在│这些│企业│中│国有│企业│有│十│个│。     切分b.在│这些│企业│中国│有│企业│有│十│个│。     切分b得不到可信的句法树，因而被拒绝。     当然，分析的层次越深，机器对知识库质量、规模等的依赖性就越强，所需要的时间、空间代价也就越大（况且面向真实文本的汉语句法分析器在可预期的将来几乎没有实现的可能，这也是应予考虑的因素）。有时不免使人产生一种陷入因果循环般的困惑：消解切分歧义这一相对“简单”的任务似乎不得不倚仗比分词本身困难得多的句法分析才得以完成。这个“悖论”里面其实蕴涵着深刻的“潜台词”，对中文自然语言处理系统的设计很有启发，囿于篇幅，这里就不展开了。       另一个值得一提的工作是，孙茂松、左正平等（1999b）发现， 从一个1亿字真实汉语语料库中抽取出的前4，619 个高频交集型歧义切分覆盖了该语料库中全部交集型歧义切分的59.20 ％（它们对另一个完全独立的语料库的覆盖率为50.85％， 说明高频交集型切分的分布相对不同的领域是比较稳定的），其中4，279个属伪歧义（如“和软件”、“充分发挥”、“情不自禁地”），覆盖率高达53.35％。 鉴于伪歧义的消解与上下文无关，于是他们提出了一个简单却很有效的策略：对伪歧义型高频交集型歧义切分，可以把它们的正确（唯一）切分形式预先记录在一张表中，其歧义消解通过直接查表即可实现。本质上，这是一个基于记忆的模型。     2.2未登录词及其处理     未登录词大致包含两大类：1）新涌现的通用词或专业术语等；2）专有名词，如中国人名、外国译名、地名、机构名（泛指机关、团体和其它企事业单位）等。前一种未登录词理论上是可预期的，能够人工预先添加到词表中（但这也只是理想状态，在真实环境下并不易做到）；后一种未登录词则完全不可预期，无论词表多么庞大，也无法囊括。     孙茂松、邹嘉彦（1995）指出，真实文本中（即便是大众通用领域），未登录词对分词精度的影响超过了歧义切分。未登录词处理在实用型分词系统中占的份量举足轻重。     对第一种未登录词的处理，一般是在大规模语料库的支持下，先由机器根据某种算法自动生成一张候选词表（无监督的机器学习策略），再人工筛选出其中的新词并补充到词表中。鉴于经过精加工的千万字、甚至亿字级的汉语分词语料库目前还是水月镜花，所以这个方向上现有的研究无一不以从极大规模生语料库中提炼出的n元汉字串之分布（n≥2）为基础。Sproat R. and Shih C.L. (1993) 借用信息论中的“互信息”定量描述任意两个汉字之间的结合力。Sun M.S. and Shen D.Y.,et al. (1998)沿这个思路前进了一步，提出了汉字间t- 测试差的概念作为互信息的有益补充。黄萱菁、吴立德等（1996）则引入经典统计论中的“四分联立表”及检验联立表独立性的皮尔逊x[2]－统计量，对长度分别为2字、3字和4字的任意汉字串做内部关联性分析， 继而获得候选词表。Nie J.Y. and Jin W.Y., et al.(1994) ， 刘挺、 吴岩等（1998）的工作仅利用了相对简单的字串频信息。这里提到的几个统计量（互信息、t-测试差、x[2]－统计量、字串频）都是依赖于极大规模语料库的，孙茂松、邹嘉彦（1995）故而称之为全局统计量。     处理第二种未登录词的做法通常是：首先依据从各类专有名词库中总结出的统计知识（如姓氏用字及其频度）和人工归纳出的专有名词的某些结构规则，在输入句子中猜测可能成为专有名词的汉字串并给出其置信度，之后利用对该类专有名词有标识意义的紧邻上下文信息（如称谓），以及全局统计量和局部统计量（参见下文），进行进一步的鉴定。已有的工作涉及了四种常见的专有名词：中国人名的识别（张俊盛、陈舜德等1992；宋柔、朱宏等1993；孙茂松、黄昌宁等1995）、外国译名的识别（孙茂松、张维杰1993）、 中国地名的识别（沈达阳、 孙茂松1995）及机构名的识别（Chen H.H. and Lee J.C. 1994；张小衡、 王玲玲1997）。从各家报告的实验结果来看，外国译名的识别效果最好，中国人名次之，中国地名再次之，机构名最差。而任务本身的难度实质上也是循这个顺序由小增大。     沈达阳、孙茂松等（1997b ）特别强调了局部统计量在未登录词处理中的价值。局部统计量是相对全局统计量而言的，是指从当前文章得到且其有效范围一般仅限于该文章的统计量（通常为字串频）。孙茂松、邹嘉彦（1995）通过下例演示了局部统计量的功效：     （7）河南会员冯俊发愿无偿赠送百日红1000株。     切分a.河南│会员│冯俊发│愿│无偿│赠送│百日红│1000│株│。     切分b.河南│会员│冯俊│发愿│无偿│赠送│百日红│1000│株│。     孤立地看句子（7）， 即使进行句法甚至语义分析也不能判断到底是切分a还是切分b（两者都具合理性）。只有跳出句子界限的束缚，在比句子更大的单位——篇章内才能定夺。譬如，若下文出现“冯俊发”如何如何，则取切分a；出现“冯俊”如何如何，则取切分b。显然，局部统计量与心理学中的“短时记忆”机制或计算机技术中的“缓冲区”机制是“心有灵犀一点通”的。     一般地，未登录词的介入会引起新的切分歧义，从而使分词系统所面临的形势更加复杂化。Sun M.S. and Shen D.Y., et al. (1997) 将切分歧义明确地细分为：1）普通词与普通词之间的切分歧义（第2.1节）；2）普通词与未登录词之间的切分歧义；3）未登录词与未登录词之间的切分歧义。     观察句子（8）：     （8）王林江爱踢足球。     中国人名识别模块猜出的候选者为“王林”、“王林江”、“林江”、“林江爱”、“江爱”，中国地名识别模块猜出的候选者为“林江”。其中中国人名“王林”与“王林江”、“王林”与“林江”、“王林”与“林江爱”、“王林江”与“林江”、“王林江”与“林江爱”、“王林江”与“江爱”、“林江”与“林江爱”、“林江”与“江爱”、“林江爱”与“江爱”之间以及中国人名“林江”与中国地名“林江”之间产生了未登录词与未登录词之间的切分歧义，普通词“爱”与“江爱”、“林江爱”之间则产生了普通词与未登录词之间的切分歧义。     必须说明，目前关于未登录词处理的研究，总的来说还是比较初步，在方法上特别是在局部统计量的计算模型上还要下大气力。这里不加说明地列出两组例子，读者不妨仔细体会个中滋味：     附图     　2.3　语言资源建设         一个好的自动分词系统离不开必要的语言资源的支持。涉及到的最主要的资源有三个：通用词表、经过分词和词性标注的语料库以及极大规模生语料库。一方面，它们为开采分词系统所需要的各类知识提供了“矿藏”丰富的宝山（如：切分歧义的静态分布与采用什么样的词表有关，切分歧义的动态分布及其句法消解模式，乃至隐Markov模型的统计参数，都可从分词和词性标注的语料库中习得，全局统计量则可由极大规模生语料库自动转化而来）；另一方面，分词和词性标注的语料库又可作为测试材料对自动分词系统的性能进行定量评估。因此，语言资源的构造同样是自动分词研究不可或缺的一环。         这个环节上面临的主要困难其实源自汉语语言学研究中悬而未决的一些“经典”问题，如词与语素及短语的界限、词类划分体系及词的具体归类等等。受文章长度的制约，不打算多谈了。这里仅想对第一个问题（其实就是所谓的分词规范）简单讲几句。分词规范直接影响到词表和分词语料库的质量，虽然已经有了国家标准（国家技术监督局1993；刘源等1994），有的单位也制定了自己的规范（黄居仁、陈克健等1997），但这些规范的可操作性都不太强（如国家标准中多次出现的关于“什么是词”的表述：“结合紧密、使用稳定”，就无法操作），很难据之构造出一致性好的词表和分词语料库来（孙茂松1999）。针对这一点，梁南元、刘源等（1991）和孙茂松、张磊（1997）提出了“人机结合、定性与定量并举”的解决思路，并进行了一定规模的实验，但这个思路是否真的可操作，尚言之过早。         顺带提一下，在这个环节上，语言学是大有用武之地的，计算语言学正在以一种迫切、坦诚的心情张开双臂期待着与语言学的拥抱。反过来，语言计算的性质（系统必须覆盖拟处理的一切语言现象）也会逼迫语言学更多地以全面、系统的观点解释、分析语言，从中升华出来的理论可能更贴近语言的真实面貌，更经得起推敲。        3.　今后的研究要点         1995年12月，国家科委组织了863智能机专题自动分词评测， 国内有几个系统参加。开放测试条件下的评测结果是：分词精度最高为89.4％；交集型切分歧义处理的正确率最高为78.0％，覆盖型切分歧义处理的正确率最高为59.0％；而未登录词识别的正确率，人名最高为58.0％，地名最高为65.0％（刘开瑛1997）。1998年3月， 国家科委又搞了第二次评测，结果与第一次差不多。这意味着，即使是对汉语分析最低级、最简单的任务——自动分词，距真正意义的实用还有距离，我们还须付出艰苦、细致的努力。     这个不容乐观的现状并不影响我们在第1 节中对汉语自动分词的可行性做出比较乐观的估计，因为虽然有待完成的工程量还很大，但在任务难度的性质上，自动分词毕竟不属于“挟泰山以超北海”——“非不为也，乃不能也”一类。那么，今后的研究应着重在哪几点上“有所为”，才能有助于达至我们的理想境界呢？结合自己的研究经验，笔者认为大概要抓以下一些工作：         1）尽快建立一个广为接受的、 高质量的通用词表。这是保证其它一切自动分词研究是否扎实、可靠的先决条件；         2）建立一套为学界同仁认同并遵守的汉语自动分词规范和词性标注规范，研制百万字级的经分词、词性标注的平衡语料库以及千万字级的甚至亿字级经分词的通用语料库。各家的工作成果应尽量共享，避免简单重复；          3）在通用词表及极大规模语料库的支持下，系统地发现那些频度高、稳定性好（指与领域基本无关）的切分歧义（或可称为通用切分歧义）并有针对性地给出解决办法；         4 ）对覆盖型切分歧义的研究目前十分薄弱，统计手段似乎鞭长莫及，宜探讨新的对策；          5 ）使已有的各种专有名词识别机制更加精细化，并增设日本人名、少数民族人名识别机制；          6）研究各种专有名词之间的冲突处理机制；          7）继续发掘全局统计量和局部统计量的潜力，同时注意克服其副作用；          8）研究融合词法、 句法甚至部分语义信息，集经验主义（统计形式）与理性主义（规则形式）于一体的分词算法；          9 ）以已有工作为基础（曹焕光、 郑家恒1992 ），构造更加合理的自动分词评测模型，争取评测工作的权威化、公开化、持续化；         10）在机器学习理论的指导下，研究从线性或半结构化语言单位序列中获取结构化语言知识的途径，以及有监督学习和无监督学习的互补互动策略，最大限度地提高自动分词系统对复杂开放环境的自适应能力。   【参考文献】     Chang, C.H. and Chen C.D. 1993.　A　study　on　integratingChinese　 word　 segmentation　and　part- of- speech　tagging.Communications of COLIPS 3.2.69—77.     Chen, H.H. and Lee J. C.　 1994.　 The　identification　oforganization names in Chinese texts. Communications of　COLIPS4.2.131—142.     Chen, K. J. and Liu S.H. 1992.　 Word　identification　for　Mandarin　 Chinese　sentences.　 Proceedings　of　the　 14th International　Conference on Computational Linguistics,　 101—107.Nantes.      Fan,　 C. K.　 and　Tsai　W. H.　 1988.　 Automatic　wordidentification　in　Chinese　sentences　 by　 the　 relaxationtechnique.　 Computer　Processing　of　Chinese　and　 OrientalLanguages 4.1.33—56.     Guo, J.　1997. Critical tokenization and　its　properties. Computational Linguistics 23.4.569—59.     Lai,B.Y.,Sun M.S.,et al.1992.Tagging- based　first　 orderMarkov model approach to Chinese word identification.     　　　Proceedings　of　1992　International　Conference　onComputer Processing of Chinese and Oriental Languages, Florida.     ----.1997.Chinese word segmentation　and　part- of- speechtagging　in one step.Proceedings of　International　Conference:1997 Research on Computational Linguistics,229—236.Taipei.     Nie,J.Y. , Brisebois　M. , et　al. 1996. On　Chinese　wordsegmentation and word- based　text　retrieval. Proceedings　ofInternational Conference on Chinese Computing 1996, 405 —412.Singapore.     Nie,J.Y.,Jin W.Y.,et al.1994.A hybrid approach to　unknownword detection and segmentation of Chinese.     　Proceedings　of　International　Conference　on　 Chinese Computing 1994,405—412.Singapore.     Palmer,D.D.1997.A trainable rule- based Algorithm for word segmentation.Proceedings of the 35th　Annual Meeting　of　ACL and 8th Conference of the European Chapter of ACL.Madrid.     Sproat,R.and Shih　C. L. 1993. A　statistical　method　forfinding word boundaries in Chinese　text. Computer　Processing of Chinese and Oriental Languages 4.4.336—249.     Sproat, R.,Shih C.L.,et al.1996.A stochastic　finite-stateword　segmentation　 algorithm　 for　 Chinese.　ComputationalLinguistics 22.3.377—404.     Sun,M.S.and Benjamin K. T. 1995. Ambiguity　resolution　inChinese　word　segmentation. Proceedings　of　the　10th　 AsiaConference　on Language,Information and Computation, 121 —126.Hong Kong.     Sun, M.S., Lai B.Y. ,　 et　al.　 1992.　 Some　issues　onstatistical　 approach　to　 Chinese　 word　 identification.Proceedings of the 3rd　International　Conference　on　ChineseInformation Processing, 246—253. Beijing.     Sun, M.S., Lin F.Z., et al. 1996.　 Linguistic　processingfor Chinese OCR & TTS. Proceedings of the　 2nd　InternationalConference of Virtual Systems and Multimedia,27—42.Gifu.     Sun,M.S.,Shen D.Y.,et al.1997.Cseg & Tag 1.0: A　practicalword segmenter and POS tagger for Chinese texts.　 Proceedingsof the 5th Conference on　Applied Natural Language　Processing,119—126.Washington D.C.     ----.1998.Chinese word segmentation without using　lexiconand hand-crafted training data.Proceedings of the 36th　AnnualMeeting of Association of Computational　Linguistics　and　the17th　International Conference　on　Computational　Linguistics,1265—1271.Montreal.     Wu,A.D.and Jiang Z.X.1998. Word segmentation　in　sentenceanalysis.Proceedings of the 1998 International　Conference　onChinese Information Processing,169—180.Beijing.     Wu,Z.M.and Tseng G. 1993. Chinese　text　segmentation　fortext　retrieval: achievements　and　problems. Journal　of　theAmerican Society for Information Science 44.9.532—542.     ----.1995.ACTS: An　automatic　Chinese　text　segmentationsystem　for　full　text　retrieval. Journal　of　the　AmericanSociety for Information Science 46.1.83—96.     Yeh,C.L.and Lee H.J.1991.Rule- based　word　identificationfor　Mandarin　Chinese　sentences — a　unification　 approach.Computer Processing of Chinese and Oriental Languages 5.2. 97—118.     白拴虎，1995，汉语词切分及词性标注一体化方法。《计算语言学进展与应用》北京：清华大学出版社，56—61页。     曹焕光、郑家恒，1992，自动分词软件质量的评价模型。《中文信息学报》第4期，57—61页。     董振东， 1997，汉语分词研究漫谈。 《语言文字应用》第1 期，107—112页。     国家技术监督局，1993，中华人民共和国国家标准GB/T　13715 —92。《信息处理用现代汉语分词规范》北京：中国标准出版社。     黄居仁、陈克健等，1997，“资讯处理用中文分词规范”设计理念及规范内容。《语言文字应用》第1期，92—100页。     黄萱菁、吴立德等，1996，基于机器学习的无需人工编制词典的切词系统。《模式识别与人工智能》第4期，297—303页。     黄祥喜，1989，书面汉语自动分词的“生成—测试”方法。《中文信息学报》第4期，42—49页。     韩世欣、王开铸，1992，基于短语结构文法的分词研究。《中文信息学报》第3期，48—53页。     何克抗、徐辉等，1991，书面汉语自动分词专家系统设计原理。《中文信息学报》第2期，1—14页。     揭春雨、刘源等，1989，论汉语自动分词方法。《中文信息学报》第1期，1—9页。     李国臣、刘开瑛等，1988，汉语自动分词及歧义组合结构的处理。《中文信息学报》第3期，27—33页。     梁南元，1987，书面汉语自动分词系统——CDWS。《中文信息学报》第2期，44—52页。     ——，1990，汉语计算机自动分词知识。《中文信息学报》第2 期，29—33页。     梁南元、刘源等，1991，制订《信息处理用现代汉语常用词词表》的原则与问题讨论。《中文信息学报》第3期，26—37页。     刘开瑛，1997，现代汉语自动分词评测技术研究。《语言文字应用》第1期，101—106页。     刘挺、吴岩等，1998，串频统计和词匹配相结合的汉语自动分词系统。《中文信息学报》第1期，17—25页。     刘挺、王开铸，1998，关于歧义字段切分的思考与实验。《中文信息学报》第2期，63—64页。     刘涌泉，1988，再谈词的问题。《中文信息学报》第2期，47 —50页。     刘源、梁南元，1986，汉语处理的基础工程——现代汉语词频统计。《中文信息学报》第1期，17—25页。     刘源等，1994，《信息处理用现代汉语分词规范及自动分词方法》北京：清华大学出版社及广西科学技术出版社。     马晏，1996，基于评价的汉语自动分词系统的研究与实现。《语言信息处理专论》北京：清华大学出版社及广西科学技术出版社，2 —36页。     沈达阳、孙茂松，1995，中国地名的自动辨识。《计算语言学进展与应用》北京：清华大学出版社，68—74页。     沈达阳、孙茂松等，1997a， 汉语分词系统中的信息集成和最佳路径搜索方法。《中文信息学报》第2期，34—47页。     ——， 1997b，局部统计在汉语未登录词辨识中应用和实现方法。《语言工程》北京：清华大学出版社，127—132页。     宋柔、朱宏等，1993，基于语料库和规则库的人名识别法。《计算语言学研究与应用》北京：北京语言学院出版社，150—154页。     孙茂松，1999，谈谈汉语分词语料库的一致性问题。《语言文字应用》第2期，87—90页。     孙茂松、黄昌宁等，1995，中文姓名的自动辨识。《中文信息学报》第2期，16—27页。     ——，1997，利用汉字二元语法关系解决汉语自动分词中的交集型歧义。《计算机研究与发展》第5期，332—339页。     孙茂松、张维杰，1993，英语姓名译名的自动识别。《计算语言学研究与应用》，北京：北京语言学院出版社，144—149页。     孙茂松、张磊，1997，人机共存，质量合一——谈谈制定信息处理用汉语词表的策略。《语言文字应用》第1期，79—86页。     孙茂松、邹嘉彦，1995，汉语自动分词研究中的若干理论问题。《语言文字应用》第4期，40—46页。     孙茂松、左正平，1998，汉语真实文本中的交集型切分歧义。《汉语计量与计算研究》香港：香港城市大学出版社，323—338页。     ——，1999a，消解中文三字长交集型分词歧义的算法。 《清华大学学报》第5期，101—103页。     孙茂松、左正平等，1999b， 高频最大交集型歧义切分字段在汉语自动分词中的作用。《中文信息学报》第1期，27—34页。     王晓龙、王开铸等，1989，最少分词问题及其解法。《科学通报》第13期，1030—1032页。     王永成、苏海菊等，1990，中文词的自动处理。《中文信息学报》第4期，1—10页。     姚天顺、张桂平等，1990，基于规则的汉语自动分词系统。《中文信息学报》第1期，37—43页。     徐秉铮、詹剑等，1993，基于神经网络的分词方法。《中文信息学报》第2期，36—44页。     徐辉、何克抗等，1991，书面汉语自动分词专家系统的实现。《中文信息学报》第3期，38—47页。     张俊盛、陈舜德等，1992，多语料库作法之中文姓名辨识。《中文信息学报》第3期，7—15页。     张小衡、王玲玲，1997，中文机构名称的识别与分析。《中文信息学报》第4期，21—32页。","title":"汉语自动分词研究评述"},{"content":"  统计自然语言处理---信息论基础                          李亚超     2010-10-29 1 简介         信息论产生于20世纪，最早由Shannon(Claude Shannon)提出。 那时Shannon在研究如何在由噪音的通信链路上尽量提高数据的传输量，为了能够从理论上求出最大的数据压缩律，Shannon提出了熵(Entropy)的概念。注意这个概念很重要，是信息论的基本理论，以后会多次用到。这里我要 介绍的包括熵(Entropy)，联合熵和交叉熵(Joint entropy and conditional entropy)，互信息(Mutual information )，噪声信道模型(The noisy channel model)，相对熵(Relative entropy or Kullback-Leibler divergence)。这些信息论上的理论在用到统计自然语言处理时，会产生意想不到效果。 2 信息论 2.1 熵(Entropy)       熵(自信息,self-information)是度量信息量的一种方法。一条信息的信息量大小和他的不确定性有直接的关系，比如我们要搞清楚一个不一无所知的问题需要大量的信息，相反如果要我们已经对一件事有所了解，那么就不需要太多的信息就可以把它搞清楚。从这个意义上说熵就是平均不确定性的多少。       熵的大小用bits来衡量，这里我们用得到数学公式log是以2为基底，并且定义和log0=0。       离散型随机变量X包含一系列值{x1,x2,...,xn}的熵H为： H(X) =E(I (X) ) 。 E是期望值， I 是X的信息内容，I(X)是自我随机变量(I(X) is itself a random variable)，如果p表示x的概率的质量函数( probability mass function)，那么熵可以写为：         b是对数的底，通常为2，欧拉数e和10。当b=2时熵的单位为bit，b=e是熵的单位为nat，b=10时熵的单位为dit(gigit)。我们通常用的b=2。 2.2 联合熵和条件熵(Joint entropy and conditional entropy)        联合熵是对一对离散的随机变量的信息量的度量，比如X,Y~P(x,y)        2.3 互信息(Mutual information)   “ 互信息 ” 是信息熵的引申概念，它是对两个随机事件相关性的度量。下图表示熵和互信息之间的关系。     图1：The relationship between mutual information  I and entropy  H.                                因此互信息的公式可以表述为：                 2.3.1 互信息的用途— 词义的二义性( 歧义 )          在自然语言处理中，经常要度量一些语言现象的相关性。比如在机器翻译中，最难的问题是词义的二义性（歧义性）问题。比如 Bush 一词可以是美国总统的名字,也可以是灌木丛。 至今为止，没有一种语法能很好解决这个问题，真正实用的方法是使用互信息。                  具体的解决办法大致如下：首先从大量文本中找出和总统布什一起出现的互信息最大的一些词，比如总统、美国、国会、华盛顿等等，当 然，再用同样的方法找出和灌木丛一起出现的互信息最大的词，比如土壤、植物、野生等等。有了这两组词，在翻译  Bush  时，看看上下文中哪类相关的词多就可以了。这种方法最初是由吉尔 (Gale) ，丘奇(Church) 和雅让斯基 (Yarowsky) 提出的。 2.3.1 互信息的用途---求两个事件之间的独立性         从图1可以看出，互信息表示两个变量之间的依赖性。除了这些，互信息可以更好地用于测量两个变量之间的独立性，因为：        1 当两个变量独立时，互信息为0        2 对于两个因变量，互信息的增长，不仅依赖于两个变量的依赖程度，而且和熵有关系。        从I(X:Y)=H(X)-H(X|Y) = H(X)+H(Y)-H(X,Y)                       因为H(X|X)=0,可以得出H(X)=H(X)-H(X|X)=I(X:X), 这就是为什么把熵称为自信息(self-information )，为什么两个完全不同的变量的互信息并不是固定的，而是取决于他们的熵。 2.4 相对熵(Relative entropy or Kullback-Leibler divergence)                  对于两个概率质量函数(pmf)P、 Q ,他们的相对熵可以表示为：                      我们定义log(0/Q)=0,log(P/0)= ꝏ 。在 信息论中这是除了熵以外另一个重要的概念， 在有些文献中它被称为成 “ 交叉熵 ” 。在英语中是 Kullback-Leibler Divergence， 是以它的两个提出者库尔贝克和莱伯勒的名字命名的。相对熵用来衡量两个正函数是否相似，对于两个完全相同的函数，它们的相对熵等于零。在用到两个概率分布函数时，要求由相同的概率空间。        在自然语言处理中可 以用相对熵来衡量两个常用词（在语法上和语义上）是否同义，或者两篇文章的内容是否相近等等。利用相对熵，我们可以用到信息检索中最重要的一个概念：词频率-逆向文档频率（TF/IDF)。下面我们简单介绍一下。             词频率-逆向文档频率（TF/IDF)最早用在信息检索中，用来确定信息和查询的相关性。简单的说，就是用户输入一个或多个关键词，然后系统统计所存贮的文本信息包含这个关键字的多少，凭直觉如果一个文本中出现所要求的关键字越多那么这个文本和用户所要求的信息相关性就越大。比如用户输入“西北民族大学”，如果在某个1000个词的网页中，西北出现了15词，民族出现了3次，大学出现了6次。那么词频分别为，0.015,0.003,0.006.这三个数相加就是原始预料和查询“西北民族大学”的简单的相关性度量。                 概括的说，如果一个查询包括关键词w1,w2,w3...wn.他们在一个网页中出现的词数分别为TF1,TF2,TF3...TFn。(TF:term frequency),那么这个查询和网页的相关性就是：TF1+TF2+TF3+...+TFn。          通过观察很容易发现，如果一个关键词在网页很少出现，那么通过他可以很快锁定目标，那么它的权重应该大一些，反之相反。如果一个关键词w在Dw个网页中出现过，那么Dw越大，w的权重越小，反之亦然。在信息检索中使用最大的权重是“逆文本频率指数” (Inverse document frequency 缩写为IDF)，它的公式为log(D/Dw)。利用 IDF,上述相关性计算个公式就由词频的简单求和变成了加权求和,即 TF1*IDF1 + TF2*IDF2 +... +TFN*IDFN。 其实IDF 的概念就是一个特定条件下、关键词的概率分布的交叉熵(Kullback-Leibler Divergence) 这样,信息检索相关性的度量,又回到了信息论。 2.5 噪音信道模型和HMM模型        噪音信道模型由信息论的祖师 Shannon提出，目的是为了在噪音的信道下，以最少的编码空间来发送信息，并且还要根据接收到的信号恢复出原信息。模型如下。 s1 , s2 , s3... 表示信息源发出的信号。 o1,o2,o3 ... 是接受器接收到的信号。通信中的解码就是根据接收到的信号 o1 , o2 , o3 ... 还原出发送的信号 s1 , s2 , s3... 。       其实人们日常的语言交流也可以看成为信息的发送和接受。 很多自然语言处理问题都可以等同于通信系统中的解码问题 ： 一个人根据接收到的信息，去猜测发话人要表达的意思。这其实就象 通信 中，我们根据接收端收到的信号去分析、理解、还原发送端传送过来的信息 。对应到机器翻译中这就是 翻译模型 。      那么，怎么根据接收到的信息推测出原信息呢。 我们可以 用隐马克尔克夫模型 （ HMM, Hidden Markov Model 来解决这个问题。 以 机器 为例，当我们观测到 藏语句子 o1,o2,o3时，我们要根据这组 词语来推测汉语句子  s1,s2,s3。显然，我们应该在所有可能的句子中找最有可能性的一个。用数学语言来描述，就是在已知 o1,o2,o3,...的情况下，求使得条件概率P(s1,s2,s3,...|o1,o2,o3....) 达到最大值的那个句子 s1,s2,s3,...      当然，上面的概率不容易直接求出，于是我们可以间接地计算它。利用贝叶斯公式 证明如下， P(o|s)=P(o)P(s|o)/P(s)，因为在给定条件下P(S)是个常数，所以Max(P(o|s))=Max(P(o)P(s|o)) 所以 可以把上述公式等价变换成 P(o1,o2,o3,...|s1,s2,s3....) * P(s1,s2,s3,...) P(o1,o2,o3,...|s1,s2,s3....) 表示某句话 s1,s2,s3...被读成 o1,o2,o3,...的可能性, 而 P(s1,s2,s3,...) 表示字串 s1,s2,s3,...本身能够成为一个合乎情理的句子的可能性，所以这个公式的意义是用发送信号为 s1,s2,s3...这个数列的可能性乘以 s1,s2,s3...本身可以一个句子的可能性，得出概率。 （读者读到这里也许会问，你现在是不是把问题变得更复杂了，因为公式越写越长了。别着急，我们现在就来简化这个问题。）我们在这里做两个假设： 第一，s1,s2,s3,... 是一个马尔可夫链，也就是说，si 只由 si-1 决定 第二，第 i 时刻的接收信号 oi 只由发送信号 si 决定（又称为独立输出假设, 即 P(o1,o2,o3,...|s1,s2,s3....) = P(o1|s1) * P(o2|s2)*P(o3|s3)...。 那么我们就可以很容易利用算法 Viterbi 找出上面式子的最大值，进而找出要识别的句s1,s2,s3,...。        满足上述两个假设的模型就叫隐含马尔可夫模型。我们之所以用“隐含”这个词，是因为状态 s1,s2,s3,...是无法直接观测到的。       隐含马尔可夫模型 (HMM) 的应用远不只在 机器翻译 中。在上面的公式中，如果我们把 s1,s2,s3,...当成中文，把 o1,o2,o3,...当成对应的 语音信号 ，那么我们就能利用这个模型解决 语音识别 问题； 如果我们把o1,o2,o3,...当成扫描文字得到的图像特征，就能利用这个模型解决印刷体和手写体的识别。       P(o1,o2,o3,...|s1,s2,s3....)根据应用的不同而又不同的名称，在语音识别中它被称为“声学模型”(Acoustic Model)， 在机器翻译中是“翻译模型” (Translation Model) 而在拼写校正中是“纠错模型” (Correction Model)。而P (s1,s2,s3,...) 就是我们在系列一中提到的语言模型。","title":"统计自然语言处理---信息论基础"},{"content":"  统计自然语言处理简介                                                                                                                         李亚超    2010-10-28 简介        语言学家的任务是描述和解释存在我们周围的众多语言现象，比如交谈、写作、以及其他的媒体形式。这就需要一方面确定人类是怎么认知世界、以及怎样获取、产生、理解语言，另一方面要理解语言的结构。对于后者的理解，人们做了很久的工作，构建出一系列的复杂的句法、语言规则，试图来概括所有语言结构，但事实证明，这个设想存在很大的局限性，重要的是能否完全概括所有的语法现象和准确度问题。 几十年过去了，基于规则的方法在自然语言处理上基毫无突破。   Edward Sapir 很早就发现的这个问题，后人总结了他的观点为一为句话“ All grammars leak”.在这里我不知道该怎么翻译，是直接翻译为“所有语法都有泄漏”，还是翻译为“不能概括所有的语法”，于是就直接把原话放在这里。       我要介绍的是用统计的方法来解决上述问题，又称为统计自然语言理解。在这里，我们不用首先把句子分为合乎语法的和不合语法规则的。我们用到的模型是统计，因此这个方法的理论基础为概率论。这里，我不对概率论做过深的介绍，因为我们的工作重点不在这里。我们的工作是建立语言的概率模型，利用这些模型来解决自然语言问题。 理性主义和经验主义        一些语言学者和许多自然语言处理工作者的兴趣重点在文字上，没有考虑到语言的心里表征和和语言的书面形式之间的关系。一些人可能会寻找以往的经验是怎么在人脑里建立模型的。这一章简要介绍这个问题中包含的哲学问题。              在1960年到1985年之间，大多数的语言学家、人工智能专家、自然语言处理学者信奉理性注意，他们认为人类大脑里知识的重要组成部分不是由后天获得的，而是由先天遗传的。经验主义者认为，小孩可以通过不断的学习，或者语言的详细结构、语言的模式。这种方法在1920年至1960年占统治地位，不过今年来才重新流行起来。        以上的观点太难懂，下面用比较通俗的例子来比喻。举例来说，两个人，一个从来没上过学，另一个受过良好的教育，当人了前提是两个人的智力都是正常人。在交流中，他们都能和别人进行日常交流和相互进行交流。她们之间，受过良好教育的，获得了良好的语法、句法知识，因而可以于人进行交流。而，从没有上过学的人，同样可以与人交流。他没有接受过专业的语法、句法学习，但是通过日常于不断学习，根据经验可以判断那些话是对的，那些是话说这是错的。这样的例子不太准确，但是可以从一个方面说明了理性主义和经验主义的差别。 NLP的科学内涵             语言研究工作者要关注什么问题？这是个很重要的问题。在这里我们首先回答两个问题。       1 人们说的话究竟是什么东西       2 对于现实世界，这些东西是怎么说的/问的       看到这个问题，人们会很快想到以下问题，人们是怎样获得知识的，人们是怎样时实的获取和产生语言句子。对于，以上的两个问题，第一个包含了语言中的所有结构，第二个集中于语义、段落、话语是怎么与现实世界表现联系的。在统计自然语言处理中主要集中于第一个问题“究竟人么说的是什么东西”         传统的语言学家把人们日常说的话分为合乎语法的和不合乎语法的，这种方法由很多局限性，比如判断一个句子的标准是是否“结构良好”。Chomsky给出一个例子来说明局限性“Colorless green ideas sleep furiously ”。这句话合乎语法，但是不合人们的逻辑。 语言的非范畴现象。除了以上句子语法判断的困难外，如果深入的研究一下，就可以看到语言范畴假设的失败之处。这说明了语言范畴观点对于很多工作来说是足够的，但是我们必须把他看成一个近似值，就像牛顿定理用在很多地方，但是有他的局限性。 语言与认知的概率现象       认为概率作为理解语言的科学依据者认为人类的认知就是一种概率。而语言作为认知的重要组成部分，也必须是概率的。在我们日常生活中，大多数的语句不全部是合乎语法，也不是全部不合乎语法的。在大多数的时间，词的使用仅仅是作为言论的一部分，没有混合。如果语言和认知从整体上解释为概率是较好的，那么概率论将是解释语言的理论依据。 对于认知的概率现象的支持者认为，我们生活的世界充满了不确定和不完整的信息。为了与这个世界进行交流，我们必须能够处理这些信息的不确定性。接收到各个词，形成这些词的中心意思，然后根据这些信息进行判断。所以这些观点的要点在于对于语言的认知处理和其他形式的感觉输入和其他的知识。这些认知处理可以很好的归结为概率处理，重要的是能够处理不确定和不完整信息。以上很好的支持了人类认知是一种概率的观点。 对于有些人怀疑统计自然语言理解是否能够处理语义问题。对于这个问题的回答的难点在于怎么定义“语义”。在一些语言的实际应用中，把语义定义为符号表达(symbolic expressions )。就像把英语用在SQL语句中。对于自然语言处理语义就是： 文章所包含单词和各种表达的分布情况。相似的观点为一个词的意思由它所在的语境决定。 为什么自然语言处理很难              在当前的大多数自然语言处理系统中，语义分析要在句法分析之后。这就意味着，随着句子的长度增加和语法的复杂性，句法分析的歧义会越来越严重。 歧义处理自然语言处理要面对的首要问题。 统计自然语言处理要做什么 简单的统计 对于统计自然语言要做的工作主要包括以下 常用词统计 。在英语中，常用的词由长度比较短的词,比如限定词、介词、补充词(complementizers)，这些词通常包含重要的语法功能，称为 功能词(function words) 。统计时，需要的信息位，词，频率，所用的词性。        文章的总词数 。对于这个问题可以解释为两种方式，一种是问文本的长度，即文本包含多少个词。另一个是，文本包含多少个不同的词，即包含了多少个不同的词类型。需要的信息为，词频，词频的频率(词类型出现的频率)。词频分布极不平均。少数词类型占据了大部分的词频。 以上是简单的词统计，下面介绍些比较高级的统计方法。 Zipf 定理 Zipf在他的一本书中提到了，一个普遍规则”最小省力原则“，并认为这适用于整个人类。最小省力原则认为人们会尽量的采取行动，来减少其工作量。(people will act so as to minimize their probable average rate of work)。以下将要介绍他的几个实验语言规律。 Zipf定理。如果把一个规模足够大的语料库(英文)中的各个词出现的频率进行排序，我们会发现一个现象一个词的频率f和其在句子中出现的位置之间r的关系，称为Rank r。        Zipf定理：f=1/r ， 也就是说r和f的乘积是个常数k，k=f*r。打个比喻来说，如果出现频率为50的词，在句子中出现的词数是出现频率为150的次的位置次数的三倍，这是个平均值。这种句子中词的频率和位置关系称为Zipf定理。        Zipf定理是对人类语言文字出现频率的粗略描述。对于我们来说有用的是，我们观测数据中的大多数词的用法很少，用法很多的词并不多。为了更为精确的描述秩r和频率f的关系，提出以下公式。 1.1        或者是两边取对数   其中P,B, ρ 是文本的参数,这个公式用来衡量文本用词丰富程度。对于具体的语言模型，要测量P,B, ρ的具体参数。 其他经验性理论 词频率 与 词含义数量关系 。Zipf 认为一个词的意思数量和这个词出现的频率有关。提出了以下公式，一个词的意思数量 m 遵守以下公式：         即：  Zipf通过实验证明了他的结论，秩频率 r 为 10000 的词平均有 2.1 个意思， 5000 的平均有 3 个， 2000 的平均有 4.6 个。 另一个现象是 实词的集中出现趋势 。对于一个词，人们可以计算出，在文章中出现的每个词之间的行数或页数。然后，可以计算出不同的区间间隔(I) 出现的频率 (F) 。据 Zipf统计，对于间隔 的数量和间隔的距离成 反比。   ， ρ 大约为1  – 1.3 。 词语搭配 词语搭配为各个词之间组成一个完整的意义单位，比如New York 。这对于汉语来说就是中文分词。主要是统计各个词的搭配频率。 对于词语搭配统计可以用在 中文分词 上。比如一个句子可以分为三种方法，A1,A2,A3,A4; B1,B2,B3; C1,C2,C3; 怎样确定那个那种分词方法好。如果一个好的分词方法，分出的句子的概率应该是最大的，因此可以求出分词句子的概率，可以利用统计语言模型分别计算出每个句子的概率，那么概率最大的就是最好的分词结果。 词语 模式搭配 除了要统计词语出现的频率，有个重要的数据是词语模式搭配频率。比如统计一个词在语法特定语法用途下出现的频率，比如在一个语料库中，New York  以 AN 结构出现11487 次， Los   Angeles 以 NN结构出现 5412 次。 还有是统计第一词相同，第二个词不同的词语搭配的频率，比如 showed off , showed the  等。 收集这些动词形态的搭配频率，可以用在统计分析器上。统计自然语言处理很大一部分的工作是通过处理大量的数据，包括，索引行( concordance lines )、候选搭配方案 (lists of candidates for collocations) 。     http://blog.csdn.net/harry_lyc/article/details/5972316","title":"统计自然语言处理简介"},{"content":"  语言计算:信息科学技术中长期发展的战略制高点 (2010-10-10 17:31:25) 标签： 校园 分类： 工作篇 自然语言处理，简单地说，就是用计算机来处理人类的语言(英文、中文等)。由于语言是人类区别于动物的根本标志，没有语言, 人类的思维也就无从谈起，所以自然语言处理体现了人工智能的最高任务与境界（只有当计算机具有处理语言的能力时，机器才可能通过图灵测试）。 自然语言处理带有很强的多学科交叉的性质, 涉及计算机科学、语言学、数学（尤其是统计学）、逻辑学、认知科学等多个领域。针对中文的自然语言处理，也被称作“中文信息处理”。主要研究内容包括：语言计算（语音与音位、词法、句法、语义、语用等各个层面上的计算），语言资源建设（计算词汇学、术语学、电子词典、语料库、知识本体等），机器翻译或机器辅助翻译，汉语和少数民族语言文字输入输出及其智能处理，中文手写和印刷体识别，中文语音识别及文语转换，信息检索，信息抽取与过滤，文本分类、中文搜索引擎，以自然语言为枢纽的多媒体检索，与语言处理相关的数据挖掘、机器学习、知识获取、知识工程、人工智能研究，与语言计算相关的语言学研究等。 中文信息处理（包括对汉语以及少数民族语言的信息处理）在我国信息领域科学技术进步与产业发展中占有特殊位置。历史地看，中文信息处理技术对推动我国信息科技与产业发展的贡献是巨大的。在一定程度上可以这么说，没有王选的汉字激光照排（两次获得国家科技进步一等奖），今天的方正集团就不会存在；没有倪光南的汉卡（获国家科技进步一等奖），也可能没有今天的联想集团；没有汉语拼音方案和王永民的五笔字型输入法，我国社会信息化范围就不会象今天这样广泛；没有刘迎建的汉王汉字输入系统（获国家科技进步一等奖），我们今天使用手机、PDA等就不会这么方便。类似的成功例子还有不少，如施水才的TRS信息检索系统，唐亚伟的速录机（获国家技术发明二等奖），陈肇雄的机器翻译系统（获国家科技进步一等奖）、丁晓青的清华文通汉字OCR系统（获国家科技进步二等奖）等等。这些无一不体现着鲜明的自主创新精神的成果，既是我国中文信息处理事业发展历程的见证，同时也将为其未来的继续蓬勃发展提供了宝贵的精神财富。 我们已经进入以互联网为主要标志的海量信息时代。一个与此相关的严峻事实是，数字信息有效利用已成为制约信息技术发展的一个全局性瓶颈问题。语言信息处理无可避免地成为信息科学技术中长期发展的一个新的战略制高点。《国家中长期科学和技术发展规划纲要》指出，我国将促进“以图像和自然语言理解为基础的‘以人为中心’的信息技术发展,推动多领域的创新”。目前，我们正处于两个基本背景之中：第一，以Google为典型代表的基于信息\\知识处理的国际公司的崛起，正在形成比微软有过之而无不及的商业垄断以及对信息\\知识有效利用的持续性、战略性控制；另一方面，经过长期的研究积累与技术沉淀，中文信息处理正处于酝酿重大突破的前夜。中文信息处理领域能否并且如何抓住这个新的历史机遇，迎接挑战，在新的历史条件下，发扬优良传统，争取更大光荣，将是在这个领域中辛勤耕耘着的我国科技工作者必须回答的一个重大问题。 清华大学计算机系自然语言处理课题组早在上个世纪七十年代末，就在黄昌宁教授的带领下从事这方面的研究工作，是国内开展相关研究最早、深具影响力的科研单位，同时也是中国中文信息学会（全国一级学会）计算语言学专业委员会的挂靠单位。现任学科带头人孙茂松教授任该专业委员会的主任（同时任中国中文信息学会副理事长）。   语言计算:信息科学技术中长期发展的战略制高点 清华大学智能技术与系统国家重点实验室 孙茂松   一、基于语义的内容计算 随着互联网以及大规模数据存储体系的迅猛发展，人类已经进入名副其实的海量信息时代。例如，著名的搜索引擎Google 的检索范围已达80 多亿张网页，允许对近三十种语言进行搜索（包括英语、主要欧洲国家语言、日语、中文简繁体、朝鲜语等）。人类知识更新的步伐日新月异。据激光打印机发明人GaryStarkweather 博士称：在1750～1950 年中，知识增长的速度是150 年翻一番，而1950～1960 年间，10 年就翻了一番，1960～1992 年间，翻番时间已缩短到5 年。期望到2020 年，信息量每73 天就将翻一番。绝大多数新产生出来的信息都是数字化的，同时旧的信息也正在通过大型的数字图书馆计划不断地在被数字化中。 可以设想，在不远的将来，互联网上将集聚人类有史以来创造的几乎全部知识。 然而，拥有海量数据仅仅意味着人类拥有全面、深入、方便地驾驭这些海量数据中所蕴涵知识的潜在可能性，但可能性与现实性有天壤之别。现实状况是： 目前对海量数据的操作主要还在信息检索阶段，根本谈不上构建于其上的知识组织、总结及分析。即使是信息检索这个比较初级的任务，效果也很不理想：TREC2004 Terabyte Track 的测试结果显示，文本信息检索的最高精度不超过30%。而对声音、图象、视像等的搜索能力就更差了。就目前状况而言，互联网这个知识海洋颇像虚拟世界中巨大无比的“黑洞”，大多数宝物都被默默地埋藏于幽深的海底难见天日，而我们却缺乏有效手段实现随心所欲的“大海捞针”，只好无奈地“望洋兴叹”。人类正面临着一种前所未有的尴尬与困惑的局面：对数字信息利用的有效率极其低下。换个形象的说法, 互联网象个大茶壶,它的壶体正在急剧膨胀,颇有“醉里乾坤大, 壶中日月长”的味道,但茶壶嘴几乎没有扩张,虽然大肚能容,有货却倒不出来。 必须指出，计算机的运算速度、磁盘容量、存取效率、网络带宽等因素与解决这个问题并无实质性关系（著名的摩尔定律指出，计算机的性能每18个月翻一番。目前的发展实际超越了摩尔定律，如3年内图形处理能力提高了100 倍，网络带宽增加了64 倍）。彻底扭转此被动局面的唯一途径是，信息处理必须跨越到基于语义的内容计算。 这一跨越在信息处理的研究与应用两大方面都将是无与伦比的，一旦得以完成，将会导致信息技术出现一场全新的革命，推动人类从虚拟世界的必然王国走进自由王国，其重大意义无论怎么讲都不过分，经济效益和社会效益不可估量： （1）科学意义：实现以信息为中心的计算（Information-centric Computing）。 放大人类的智能，而非简单地放大人类的工具。 （2）国家基础设施建设：从Web走向互联网发明人暨W3C 主席Tim Berbers Lee1998年提出的语义Web(虽然笔者认为, 在中近期实现严格意义的语义Web近乎天方夜谈,但其变体,如面向特定应用的小型语义Web 却是可能的)，提升信息的质量与系统性，实现知识的有效组织与利用。 （3）国家经济建设：建设与工程体系相配合的、以“软科学”为特征的非工程体系，提供全面、强大的决策支持。 （4）国家安全：敏感信息的准确检测与过滤（例如军事、政治敏感信息）。目前基于IP 地址及基于关键词匹配的策略只能是权宜之计，防不胜防。 （5）人民生活质量与文化素质的提高：网络的各种个性化服务及按需服务。 （6）网络色情的围堵：有效制止其恶性泛滥（已成为网络上的首要公害）。 虽然要圆“基于语义的内容计算”之梦，人类还需要走非常漫长的路,但在这个圆梦之路的不同阶段所产生的一些阶段性重要成果，仍足以促使信息技术发生深刻变革及带动相关产业的升级。 由于自然语言文本占据了互联网的大半河山, 同时,在可预期的将来,对声音、影像、图片的检索仍将严重依赖自然语言分析技术(正如近两年Google 推出的图象与视像搜索引擎所做的那样),语言计算的重要性也就不言而喻了。可以预期,它将无可避免地成为信息科学技术中长期发展的战略制高点。   二、目前的主要任务 语言计算是一项长期的艰巨任务，不可能一蹴而就。那么,在现阶段,我们应该抓的主要任务有那些呢?笔者认为,以下诸多方面的研究应该成为我们关心的焦点。 （1）在人工智能、机器学习、数学、语言学等理论交叉指导下，进行面向超大规模文本等真实复杂环境的方法与原型研究。尤其要注意研究算法在这一条件下的性质。 （2）面向互联网的汉语自动分词研究英文的信息处理一起步就是在词平面上。而中文信息处理起步是在字平面上。现有的中文搜索引擎，虽然几乎使都用了汉语分词系统，但由于分词系统的性能存在严重缺陷，导致检索性能不佳，更堪担忧的是，中文搜索引擎将无法向更高级的形态发展。许嘉璐先生曾一针见血地指出：“到目前为止，中文信息处理基本上还停留在‘字处理阶段’，也就是说计算机对汉语的‘认知’是一个字一个字地进行”“如果我们说得‘宽宏’一些，最多可以说现在是处在‘字和词处理之间’阶段”“中文信息处理技术虽然在有些方面有所进步，但至今还没有跨上‘语言处理’这个台阶”。要从字平面跨越到词平面，汉语自动分词是必由之途。观察表明,现有的分词系统对互联网文本的处理能力远远不够。这个貌似简单的任务其实十分困难，不以大工程的态度对待，断无成功之理： ● 建立“信息处理用现代汉语通用分词词表”,与国家标准“信息处理用现代汉语分词规范”相互衔接。这个通用词表将成为构造语义Web 所需的通用ontology的基础。 ● 建立各个主要应用领域的分词词表（词数当在数百万级），并制订相关规范。这些领域分词词表将成为各领域ontologies的基础。 ● “来自互联网”: 在通用词表与领域词表的支持下，以互联网上的中文文本集合为基本对象，进行汉语分词歧义等的大规模调查，据之设计有效的分词歧义消解算法,并进行新词汇自动发现的研究。 ● “面向互联网”: 实现一个可驾御互联网的实用型汉语自动分词系统。研究当分词必然存在一定错误率的条件下中文搜索引擎设计的健壮性问题。 （3）应用驱动的浅层句法分析技术的研究。 （4）借鉴WordNet与HowNet，进行大规模汉语语义资源的整合与建设。并且以之为基础，进行汉语语义计算的研究。 （5）词法、句法、语义一体化的汉语分析模型的研究。 （6）进行领域ontology的研究，并建立一个示范性ontology。制订相关的标准。 （7）研究在海量文本中自动发现词与词之间关系的算法。 （8）研究高精度的汉语文本自动分类算法,建立Web逻辑地图。 （9）将自然语言处理、OCR、语音识别等技术融合于基于内容的图像、视像处理研究中，以显著提高图像和视像的智能化处理能力。 （10）完成对文本、声音、图像和视像均具有很强判断能力的关键性应用系统（典型如色情和军事、政治敏感信息的自动过滤）。 （11）促进大规模语言计算资源共享平台与机制的建设。 （12）将上述成果集成起来，设计并实现实用型工具软件,可以将任意一个普通网站经过若干步深层次处理后自动转换成一个智能型网站，从而被赋予一定的知识管理能力。 （13）建立并完善我们自己的搜索引擎,与Google 抗衡。 （14）在内容计算的基础上，研发各类知识服务系统,如基于Web 的预警系统。   三、中文信息处理与中华文化 中国已成为世界上仅次于美国的第二大网络大国。据中国互联网络信息中心《第十五次中国互联网络发展状况分析报告》统计，截止到2004 年12 月31 日，我国网民数量已经达到9400 万，上网计算机总数已达4160 万台,WWW 站点数约668,900 个。中国大陆IP 地址总数已达59,945,728 个。中文网上资源呈雪崩式发展的态势。 于是, 语言计算无可避免地被赋加了更多一层的特殊意义：面向中国人的语言计算, 以确保国民掌控及利用海量中文信息的能力，同时，使中华文化能够借重这种强有力的技术手段，在全球网络一体化，多文明、多文化共存乃至激烈碰撞的考验下，岿然屹立于世界文化之林，并且历久弥新，发扬光大。令人深思的一个例子是,2004 年12 月，Google 宣布将对纽约公共图书馆及四家知名大学图书馆（牛津大学、哈佛大学、斯坦福大学及密歇根大学）的上百万图书进行扫描(含45 亿页文字材料)，实现图书内容的网上搜索及浏览。法国人对此作出了强烈反应。他们担心：一旦这个网上图书馆建成，就可能意味着美国的声音将对人 们今后的世界观施加压倒性的影响,而法国曾经创造的优美语言和光辉思想将越来越少为人所知，最终成为被世界遗忘的角落。法总统府在一份声明中写道：“总统先生将与其他欧洲领导人采取行动，以谋求加强在这一领域的协作。当今的世界正在掀起一场知识数字化的革命，拥有独一无二文化遗产的法国和欧洲理应占有一席之地，以便让人们了解欧洲的智慧、历史以及文化遗产”。这也理应成为我们对待中文信息处理的一个新的视角。 （本文发表于《语言文字应用》2005 年第3 期, 第38-40 页   中文自然语言处理开放平台4-[课程讲义] 中文自然语言处理的研究现状和发展趋势, (2009-01-11). 5-[应用系统] 基于网络查询的问答系统, (2008-04-21). 6-[会议信息] 第一届全国知网研讨会征文 ... http://www.nlp.org.cn/ 自然语言处理_百度百科自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、 ... http://baike.baidu.com/view/18784.htm 我爱自然语言处理中午在CSDN看到这个不幸的消息：[逝者]自然语言处理大师Fred Jelinek，之后水木自然语言处理版也有nlper转载了英文的相关信息。我读了一下Language Log里的文章，印象 ... http://www.52nlp.cn/ 北京理工大学自然语言处理实验室首页北京理工大学自然语言处理实验室还很年轻，我们希望加大和外界同仁的交流，共同推进中文信息处理技术的提高，也希望有志青年加入我们实验室共同奋斗！ ... http://nlp.bit.edu.cn/ 智能技术与自然语言处理研究室哈尔滨工业大学计算机学院智能技术与自然语言处理研究室（ITNLP）是国内较早从事自然语言处理研究的科研团体之一。研究室从八十年代初期以来，先后开展了机器翻译、 ... http://www.insun.hit.edu.cn/ Natural Language Processing - Microsoft Research - [ 翻译此页 ]Building a computer system that will analyze, understand, and generate natural languages. research.microsoft.com/en-us/groups/nlp/ 欢迎光临HNC自然语言理解处理网站！中国科学院声学研究所主办，介绍HNC知识，报道HNC动态。 http://www.hncnlp.com 自然语言处理导论-课程网页课程名称：自然语言处理导论, 任课教师: 詹卫东* 刘扬王厚峰常宝宝**. 电子邮件：zwd@pku.edu.cn （詹卫东） liuyang@pku.edu.cn （刘扬）, 电话：62765810 (办公室) ... http://ccl.pku.edu.cn/alcourse/nlp/ 北京大学计算语言学研究所 http://icl.pku.edu.cn/ 分享 0 顶 阅读(33)┊ 评论 (0)┊ 收藏(0) ┊禁止转载 ┊ 顶▼ ┊打印┊举报 已投稿到： 排行榜 加载中，请稍候...... 前一篇：相关系数 后一篇：书籍:自然语言处理、计算语言学与中文信息处理","title":"语言计算:信息科学技术中长期发展的战略制高点"},{"content":"  《浪潮之巅》（IT巨头们的兴衰之路） 定价：55.00 元 快书包价：35.80 元 折扣：65折 节省：19.2元 <!-- 价：按会员组优惠方案 元--> 作者：吴军 译者： 出版社：电子工业出版社 出版时间：2011-07-01 品牌： ISBN：9787121139512 开本：16开 页数：584 字数：510000 装订：平装 印刷：黑白   有货：北京,西安,成都,天津,长沙 无货： 购买数量： 件 立即购买 加入收藏 经常一起购买的商品（bought together） 《暗时间》（部分签名本... 22.80 | 35.00 《史蒂夫•乔布斯》(乔... 51.00 | 68.00 一路向前（星巴克创始人... 25.80 | 45.00 太平天国（史景迁作品） 37.10 | 56.00 作者：吴军 吴军博士，毕业于清华大学计算机系（本科）、电子工程系（硕士）和美国约翰 · 霍普金斯大学计算机科学系（博士）。在清华大学和约翰 · 霍普金斯大学期间，吴军博士致力于语音识别、自然语言处理，特别是统计语言模型的研究。他曾获得1995年全国人机语音智能接口会议的最佳论文奖和2000年Eurospeech的最佳论文奖。 吴军博士于2002年加入Google公司。在Google，他和Amit Singhal（Google院士，世界著名搜索专家）、Matt Cutts（Google反作弊官方发言人）等三位同事一起开创了网络搜索反作弊的研究领域，并因此获得Google工程奖。2003年，他和Google全球架构的总工程师朱会灿博士等共同成立了中日韩文搜索部门。吴军博士是当前Google中日韩文搜索算法的主要设计者。在Google其间，他还领导了许多研发项目，包括许多与中文相关的产品和自然语言处理的项目，并得到了当时公司首席执行官埃里克 · 施密特和创始人谢尔盖 · 布林的高度评价。此外，他还在谷歌黑板报上发表了《数学之美》系列博客。 吴军博士在国内外发表过数十篇论文，并获得和申请了十余项美国和国际专利。他于2005年起，当选为约翰 · 霍普金斯大学计算机系董事会董事。2007起担任风险投资基金中国世纪基金的董事。 2010年，吴军博士离开Google，加盟腾讯公司，担任负责搜索业务的副总裁。并担任国家重大专项“新一代搜索引擎和浏览器”项目的总负责人。 书的由来、内容、形式与风格： 快书包贴士：  《浪潮之巅》第一、二次印刷勘误表 近一百多年来，总有一些公司很幸运地、有意识或无意识地站在技术革命的浪尖之上。在这十几年间，它们代表着科技的浪潮，直到下一波浪潮的来临。 从一百年前算起，AT&T 公司、IBM 公司、苹果公司、英特尔公司、微软公司、思科公司、雅虎公司和Google公司都先后被幸运地推到了浪尖。虽然，它们来自不同的领域，中间有些已经衰落或正在衰落，但是它们都极度辉煌过。《浪潮之巅》系统地介绍了技术革命的浪尖之上些公司成功的本质原因及科技工业一百多年的发展。 在极度商业化的今天，科技的进步和商机是分不开的。因此，《浪潮之巅》也系统地介绍了影响到科技浪潮的风险投资公司，诸如 KPCB 和红杉资本，以及百年来为科技捧场的投资银行，例如高盛公司，等等。 在这些公司兴衰的背后，有着它必然的规律。《浪潮之巅》不仅讲述科技工业的历史，更重在揭示它的规律性。 翻翻目录： 序言（李开复） 前言　有幸见证历史 第 1 章　帝国的余辉—AT&T 公司 AT&T 100年来发展得非常健康。虽然它一直受反垄断法的约束，但是美国政府司法部并没有真正要过它的命，每一次反垄断其实是帮助AT&T修枝剪叶，然后让它发展得更好。 1　百年帝国 2　几度繁荣 3　利令智昏 4　外来冲击 结束语 第 2 章　蓝色巨人—IBM 公司 郭士纳在到IBM以前也是做(芯)片的，但是，是土豆芯片。 1　赶上机械革命的最后一次浪潮 2　领导电子技术革命的浪潮 3　错过全球信息化的大潮 4　他也是做（芯）片的 5　保守的创新者 6　内部的优胜劣汰 7　后金融危机时代 结束语 第 3 章　“水果”公司的复兴—乔布斯和苹果公司 在每一次技术革命中，新技术必须比老的技术有数量 级的进步才能站住脚。 1　传奇小子 2　迷失方向 3　再创辉煌 4　大难不死 5　i 十年 结束语 第 4 章　计算机工业的生态链 一个IT公司如果今天和18个月前卖掉同样多的、同样的产品，它的营业额就要降一半。 1　摩尔定律 2　安迪–比尔定律 3　反摩尔定律 结束语 第 5 章　奔腾的芯 — 英特尔公司 英特尔的CEO格罗夫虽然是学者出身，但他同时也是微机时代最优秀的领导者和管理者，数次被评为世界上最好的CEO。 1　时势造英雄 2　英特尔、摩托罗拉之战 3　指令集之争 4　英特尔和AMD的关系 5　举步艰难 结束语 第 6 章　IT 领域的罗马帝国—微软公司 当乔布斯给盖茨看了新设计的麦金托什个人电脑，以及漂亮的基于图形界面的操作系统时，盖茨惊呆了。那一年，乔布斯和盖茨都是26岁。 1　双雄会 2　亡羊补牢 3　人民战争 4　帝国的诞生 5　当世拿破仑 6　尾大不掉 7　条顿堡之战 8　客厅争夺战 结束语 第 7 章　互联网的金门大桥—思科公司 据说斯坦福两个系的计算中心主管莱昂纳多•波萨卡和桑迪•勒纳要在计算机上写情书，由于各自管理的网络不同，设备又是乱七八糟，什么厂家的、什么协议的都有，互不兼容，情书传递起来很不方便，于是两人干脆发明了一种能支持各种网络服务器、各种网络协议的路由器。于是思科公司赖以生存的“多协议路由器”便诞生了。 1　好风凭借力 2　持续发展的绝招 3　竞争者 4　诺威格定律的宿命 结束语 第 8 章　英名不朽—杨致远、菲洛和雅虎公司 一百年后，如果人们只记得两个对互联网贡献最大的人，那么这两个人很可能就是杨致远和菲洛。 1　当世福特 2　流量、流量、流量 3　成也萧何，败也萧何 4　既生瑜，何生亮 5　红巨星 6　自废武功 结束语 第 9 章　硅谷的见证人—惠普公司 作为硅谷最早的公司，惠普见证了硅谷发展的全过程，从无到有，从硬件到软件，惠普的历史从某种程度上讲就是硅谷历史的缩影。 1　昔日硅谷之星 2　有争议的生死抉择 3　最有争议的CEO 4　亚洲制造的冲击 5　峰回路转 结束语 第 10 章　没落的贵族—摩托罗拉公司 如果我们认为公司之中也有所谓的贵族，摩托罗拉无疑可以算一个。曾几何时，摩托罗拉就是无线通信的代名词，同时它还是技术和品质的结晶。 1　二战的品牌 2　黄金时代 3　基因决定定律 4　铱星计划 5　全线溃败 6　回天乏力 结束语 第11章　硅谷的另一面 美国的硅谷只占国土面积万分之五，却创造了无数的商业神话。在这里，大约每10天便有一家公司上市。美国前100强的公司中，硅谷占了四成。 1　成王败寇 2　嗜血的地方 3　机会均等 4　硅含量不断降低 5　亘古而常青结束语 第12章　短暂的春秋—与机会失之交臂的公司 在人类命运降临的伟大瞬间，市民的一切美德—小心、顺从、勤勉、谨慎，都无济于事，它始终只要求天才人物，并且将他造就成不朽的形象。命运鄙视地把畏首畏尾的人拒之门外。命运—这世上的另一位神，只愿意用热烈的双臂把勇敢者高高举起，送上英雄们的天堂。 1　太阳公司 2　Novell公司 3　网景公司 4　RealNetworks 结束语 第13章　幕后的英雄—风险投资 对于想找投资的新创业公司，红杉资本有一些基本要求—公司的业务要能几句话就讲得清楚。红杉资本的投资人会给你一张名片，看你能不能在名片背面的一点点地方把你想做的事情写清楚。 1　风投的起源 2　风投的结构 3　风投的过程 4　投资的决策和公司的估价 5　风投的角色 6　著名的风投公司 结束语 第14章　信息产业的规律性 人类的文明和技术是不断进步的，旧的不去，新的不来，只有清除掉阻碍我们进步的那些庞大的恐龙，才能为人类提供新的发展空间。从这个角度讲，一个昔日跨国公司的衰亡，也许是它为我们这个社会做的最后一次贡献。 170–20–10律 2　诺威格定律 3　基因决定定律 结束语 第15章　硅谷的摇篮—斯坦福大学 二战后，帮助斯坦福大学解决财政危机的是它的一位教授弗里德里克•特曼，他后来被称为“硅谷之父”。他仔细研究了斯坦福夫妇的遗嘱，发现里面没有限制大学出租土地，于是他兴奋地声称找到了解决问题的秘密武器—建立斯坦福科技园。 1　充满传奇的大学 2　硅谷的支柱 3　纽曼加洪堡的教育模式 4　创业的孵化器 结束语 第16章　科技公司的吹鼓手—投资银行 华尔街的贪婪既会捧起，也会扼杀一个科技新星。 1　华尔街和美国的金融体系 2　著名的投资公司 3　科技公司的上市过程 4　成也萧何，败也萧何 5　华尔街与微软、雅虎和Google的三国演义 结束语 第17章　挑战者—Google公司 Google是个奇怪的地方。也许是因为Google的年轻人太多，他们不懂得传统也不拘泥传统，只要认准了对公司对社会有用，就大胆去干了。 1　历史上最轰动的IPO 2　早期岁月 3　商业模式 4　个人英雄主义和群众路线 5　绝代双骄 6　感谢上帝，今天是星期五（TGIF） 7　不作恶 8　不败的神话 9　秘密军团 10　云计算和数据中心 11　Google的新气象 12　3G时代 13　进攻，永远是最好的防守 结束语 第18章　成功的转基因—诺基亚、3M、GE公司 由于科学技术是最革命、发展最快的生产力，一家科技公司要想在几次技术革命大潮中都能够立于浪潮之巅是一件极不容易的事。 1　从木工厂到手机之王（诺基亚公司） 2　道琼斯指数中的常青树（3M公司） 3　世界最大的联合体（GE公司） 结束语 第19章　印钞机—最佳的商业模式 所有成功的大公司都有好的商业模式，很多大公司的兴起，不是靠技术的革新而是靠商业模式的转变。 1　Google的广告系统 2　eBay和亚马逊的在线市场 3　戴尔的虚拟工厂 4　腾讯的虚拟物品和服务 结束语 第20章　互联网2.0 互联网2.0最重要的是提供了一个开放的平台，让用户可以在平台上开发自己的应用程序，并且提供给其他用户使用。 1　互联网的前世今生 2　互联网2.0的特征 3　著名的互联网2.0公司 4　是革命还是泡沫 结束语 第21章　金融风暴的冲击 虽然全世界在2008年的最后一个季度里陷入严重的衰退，同时人们的恐惧心理加重了这场危机，虽然在更长一些时间里我们仍将处于衰退，但是，明天仍然会好起来。今后的44年里我们的经济、我们的社会都将获得长足的发展，就如同过去的44年一样。 —沃伦•巴菲特 1　金融危机的成因 2　瑞雪兆丰年：优胜劣汰 3　潜在的商机到处都是 4　格局的变迁 结束语 第22章　云计算 云计算保证用户可以随时随地访问和处理信息，并且可以非常方便地与人共享信息。它的好处是让全社会的计算资源得到最有效的利用。 1　云计算的起源 2　云计算的本质 3　云计算的核心技术和工程 4　对IT产业链的颠覆 结束语 第23章　下一个Google 虽然我们不知道下一个Google在哪里，但是可以肯 定它不在搜索领域，这就如同几年前我们寻找的“下一个微软”不会是一家软件公司，而最终是一家互联网公司一样。 1　千亿俱乐部 2　岁岁年年人不同 3　新领域 4　关注亚太地区 结束语 537 后记 541 索引   看看书评： . 读读书摘： 第1 章　帝国的余晖 AT&T 公司 1　百年帝国 图1-1 是在上世纪90 年代拍摄的美国新泽西州弗伦翰公园（FlorhamPark）日落时的照片。弗伦翰公园占地十几平方公里，大多是芳草地和森林，在森林中央，是一片中等规模临湖的工业园— 这是笔者见到的最美丽的工业园。在那里，每天都能看到天鹅在湖中悠闲地游荡，还不时可以见到野鹿出没。这里原是石油巨头埃克森美孚（Exxon–Mobil）的地产，1996 年，这里来了一个新主人— 美国电报和电话公司（AT&T）实验室。 1995 年，如日中天的AT&T 公司重组，分裂成AT&T、朗讯和NCR 三家公司。AT&T 下属的举世闻名的科研机构贝尔实验室也被一分为二。朗讯公司获得了一半的科研机构和贝尔实验室的名称。划归AT&T 的一半研究室组成了AT&T 实验室（后来更名为香农实验室Shannon Labs），从原来的默里山（Murray Hill）搬到了弗伦翰公园。在那里，出过11 位诺贝尔奖获得者的AT&T 实验室，像一颗进入晚年的恒星，爆发出极强的、但也是最后的光辉，然后就迅速地暗淡下来。10 年后，AT&T 和朗讯（Lucent Technologies）公司分别被SBC 公司和法国的阿尔卡特（Alcatel）公司并购。 1997 年，我在AT&T 实验室实习，当时大家的情绪都很高，实验室的气氛很像10 年后的Google 和今天的Facebook。不少人的座位旁都放着上面那张美丽的夕阳照。现在想起来，它似乎预示着一个帝国的黄昏。 说起美国电话和电报公司，即AT&T 公司，在美国乃至在世界上几乎无人不知、无人不晓。该公司由电话之父亚历山大·贝尔（Alexander Bell）创立于1877 年，最初叫做贝尔电话公司。为了避免以后混淆，我们这里统一采用AT&T 的称呼。电话的发明和AT&T 公司的建立，第一次实现了人类远程实时的交互通信（虽然电报比电话出现得早，但它不是实时交互通信），并且促进了社会的进步。 AT&T 从它创立的第一天起，就是龙头老大，直到它被收购的那一天。但是，AT&T 起初的扩展速度远比今天的人想象的慢得多。它用了15 年（到1892 年）才将生意从纽约地区扩展到美国中部芝加哥地区（当时从纽约到芝加哥一分钟的通话费是2 美元，而当时的1 美元的购买力相当于今天的50 美元。今天在美国打国际长途，也不过10 美分一分钟）。38 年后（1915年），它的生意扩展到全国（但是从纽约到旧金山的电话费高达7 美元一分钟）。50 年后的1927 年，AT&T 的长途电话业务扩展到欧洲。 1925 年，AT&T 公司成立了研发机构—贝尔实验室（Bell Laboratories，简称Bell Labs）。贝尔实验室是历史上最大、最成功的私有实验室。由于AT&T 公司从电信业获得了巨大的垄断利润，它拿出了产值的3% 用于贝尔实验室的研发工作。在很长时间里，贝尔实验室的人总是用“无须为经费发愁”这一条理由来吸引优秀的科学家到该实验室工作，这使得贝尔实验室不仅在通信领域长期执牛耳，而且在射电天文学、晶体管和半导体、计算机科学等领域领先于世界。它著名的发明除电话本身外，还包括射电天文望远镜、晶体管、数字交换机、计算机的Unix 操作系统和C 语言等。 此外，贝尔实验室还发现了电子的波动性，发明了信息论，组织发射了第一颗通信卫星，铺设了第一条商用光纤。在相当长的时间内，贝尔实验室不仅仅是信息领域科学家最向往的工作单位，也是基础研究领域学者梦寐以求的地方。那个时代进入贝尔实验室的人是很幸运的。如果确有才华，他可以成为业界的领袖，甚至得到诺贝尔奖、香农奖或图灵奖。即使是一般的研究员和工程师，也会有很好的收入、可靠的退休保障及受人尊重的社会地位。 AT&T 在很长时间内垄断美国并且（通过北电）控制加拿大的电话业务。1984 年，根据联邦反垄断法的要求，AT&T 的市话业务被分出去，根据地区划分成7 个小的贝尔公司。这时贝尔电话公司才正式更名为AT&T公司。 根据当年的划分原则，7 家小贝尔（Baby Bells）公司从事市话业务，而AT&T 公司（被称为老祖母）从事长途电话业务和通信设备的制造。贝尔实验室划给了AT&T，并从贝尔实验室分出一部分，称为贝尔核心（Bell Core），划给7 家小贝尔公司。不久，贝尔核心因为7 个和尚没水喝，很快就退出了历史舞台，这当然是后话了。 现在，大多数人认为，这是AT&T 走向衰落的开始。但我认为，AT&T并没有因此而伤筋动骨。事实上，在接下来的10 年里，AT&T 的业务得到长足的发展。虽然丢掉了市话服务，但是，它作为一家通信设备供应商，依然是市话通信设备几乎唯一的供货商，在海外市场它依然是龙头老大。在长途电话业务中，虽然有MCI 和Sprint 两个竞争者，AT&T 仍然控制着美国大部分市场，利润十分可观，足以维持贝尔实验室高额的研发费用，使得AT&T 在通信和半导体技术上仍然领先于世界。到1994 年，它的营业额达到近700 亿美元，大致等同于2008 年金融危机前它和SBC 合并后的总营业额。 这一年，贝尔实验室的总裁梅毅强（John Mayer）博士率大规模的代表团访华，中国国家主席江泽民亲自接见了他，足以说明对AT&T 的重视。中国国家主席接见一个公司下属机构的总裁，这次可能是空前绝后的。AT&T当时可以说风光到了极点。 既然1984 年那次分家并没有使AT&T 公司伤筋动骨，那么又是什么原因造成了它的衰落呢？ 上榜纪录： . 浏览此商品的顾客最终购买了（ultimately bought） 《浪潮之巅》（IT巨头们的兴衰之路） 35.80 | 55.00 一切皆有价（继畅销书《怪诞行为学》之后，一本观点更具颠覆性、争议性和畅销性的通俗经济学读物！） 25.30 | 42.00 文案训练手册（来自美国顶尖撰稿人的终极秘诀：如何写出有销售力的广告文案） 29.70 | 48.00 百年孤独（魔幻现实主义文学经典，加西亚·马尔克斯巅峰杰作，中文版全球首次正式授权） 29.60 | 39.50","title":"《浪潮之巅》（IT巨头们的兴衰之路）"},{"content":"谈谈机器学习(Machine Learning)大家 (full version)   送交者: HiT, 2005年8月23日08:58:07 于 [教育与学术]http://www.bbsland.com      闲着无事，想写点一些我所了解的machine learning大家。由于学识浅薄，见识有限，并且仅局限于某些领域，一些在NLP及最近很热的生物信息领域活跃的学者我就浅陋无知，所以不对的地方大家仅当一笑。 Machine Learning 大家(1)：M. I. Jordan (http://www.cs.berkeley.edu/~jordan/) 在我的眼里，M Jordan无疑是武林中的泰山北斗。他师出MIT，现在在berkeley坐镇一方，在附近的两所名校（加stanford）中都可以说无出其右者，stanford的Daphne Koller虽然也声名遐迩，但是和Jordan比还是有一段距离。 Jordan身兼stat和cs两个系的教授，从他身上可以看出Stat和ML的融合。 Jordan最先专注于mixtures of experts，并迅速奠定了自己的地位，我们哈尔滨工业大学的校友徐雷跟他做博后期间，也在这个方向上沾光不少。Jordan和他的弟子在很多方面作出了开创性的成果，如spectral clustering， Graphical model和nonparametric Bayesian。现在后两者在ML领域是非常炙手可热的两个方向，可以说很大程度上是Jordan的lab一手推动的。 更难能可贵的是，Jordan不仅自己武艺高强，并且揽钱有法，教育有方，手下门徒众多且很多人成了大器，隐然成为江湖大帮派。他的弟子中有10多人任教授，个人认为他现在的弟子中最出色的是stanford的Andrew Ng，不过由于资历原因，现在还是assistant professor，不过成为大教授指日可待；另外Tommi Jaakkola和David Blei也非常厉害，其中Tommi Jaakkola在mit任教而David Blei在cmu做博后，数次获得NIPS最佳论文奖，把SVM的最大间隔方法和Markov network的structure结构结合起来，赫赫有名。还有一个博后是来自于toronto的Yee Whye Teh，非常不错，有幸跟他打过几次交道，人非常nice。另外还有一个博后居然在做生物信息方面的东西，看来jordan在这方面也捞了钱。这方面他有一个中国学生Eric P. Xing(清华大学校友)，现在在cmu做assistant professor。 总的说来，我觉得Jordan现在做的主要还是graphical model和Bayesian learning，他去年写了一本关于graphical model的书，今年由mit press出版，应该是这个领域里程碑式的著作。3月份曾经有人答应给我一本打印本看看，因为Jordan不让他传播电子版，但后来好像没放在心上（可见美国人也不是很守信的），人不熟我也不好意思问着要，可以说是一大遗憾. 另外发现一个有趣的现象就是Jordan对hierarchical情有独钟，相当多的文章都是关于hierarchical的，所以能hierarchical大家赶快hierarchical，否则就让他给抢了。 用我朋友话说看jordan牛不牛，看他主页下面的Past students and postdocs就知道了。 Machine Learning大家（2）：D. Koller (http://ai.stanford.edu/~koller/) D. Koller是1999年美国青年科学家总统奖(PECASE)得主，IJCAI 2001 Computers and Thought Award(IJCAI计算机与思维奖，这是国际人工智能界35岁以下青年学者的最高奖)得主，2004 World Technology Award得主。 最先知道D koller是因为她得了一个大奖，2001年IJCAI计算机与思维奖。Koller因她在概率推理的理论和实践、机器学习、计算博弈论等领域的重要贡献，成为继Terry Winograd、David Marr、Tom Mitchell、Rodney Brooks等人之后的第18位获奖者。说起这个奖挺有意思的，IJCAI终身成就奖（IJCAI Award for Research Excellence），是国际人工智能界的最高荣誉; IJCAI计算机与思维奖是国际人工智能界35岁以下青年学者的最高荣誉。早期AI研究将推理置于至高无上的地位; 但是1991年牛人Rodney Brooks对推理全面否定，指出机器只能独立学习而得到了IJCAI计算机与思维奖; 但是koller却因提出了Probabilistic Relational Models 而证明机器可以推理论知而又得到了这个奖，可见世事无绝对，科学有轮回。 D koller的Probabilistic Relational Models在nips和icml等各种牛会上活跃了相当长的一段时间，并且至少在实验室里证明了它在信息搜索上的价值，这也导致了她的很多学生进入了google。虽然进入google可能没有在牛校当faculty名声响亮，但要知道google的很多员工现在可都是百万富翁，在全美大肆买房买车的主。 Koller的研究主要都集中在probabilistic graphical model，如Bayesian网络，但这玩意我没有接触过，我只看过几篇他们的markov network的文章，但看了也就看了，一点想法都没有，这滩水有点深，不是我这种非科班出身的能趟的，并且感觉难以应用到我现在这个领域中。 Koller才从教10年，所以学生还没有涌现出太多的牛人，这也是她不能跟Jordan比拟的地方，并且由于在stanford的关系，很多学生直接去硅谷赚大钱去了，而没有在学术界开江湖大帮派的影响，但在stanford这可能太难以办到，因为金钱的诱惑实在太大了。不过Koller的一个学生我非常崇拜，叫Ben Taskar，就是我在（1）中所提到的Jordan的博后，是好几个牛会的最佳论文奖，他把SVM的最大间隔方法和Markov network结合起来，可以说是对structure data处理的一种标准工具，也把最大间隔方法带入了一个新的热潮，近几年很多牛会都有这样的workshop。 我最开始上Ben Taskar的在stanford的个人网页时，正赶上他刚毕业，他的顶上有这么一句话：流言变成了现实，我终于毕业了！ 可见Koller是很变态的，把自己的学生关得这么郁闷，这恐怕也是大多数女faculty的通病吧，并且估计还非常的push！ Machine learning 大家（3）:J. D. Lafferty 大家都知道NIPS和ICML向来都是由大大小小的山头所割据，而John Lafferty无疑是里面相当高的一座高山，这一点可从他的publication list里的NIPS和ICML数目得到明证。虽然江湖传说计算机重镇CMU现在在走向衰落，但这无碍Lafferty拥有越来越大的影响力，翻开AI兵器谱排名第一的journal of machine learning research的很多文章，我们都能发现author或者editor中赫然有Lafferty的名字。 Lafferty给人留下的最大的印象似乎是他2001年的conditional random fields，这篇文章后来被疯狂引用，广泛地应用在语言和图像处理，并随之出现了很多的变体，如Kumar的discriminative random fields等。虽然大家都知道discriminative learning好，但很久没有找到好的discriminative方法去处理这些具有丰富的contextual inxxxxation的数据，直到Lafferty的出现。 而现在Lafferty做的东西好像很杂，semi－supervised learning， kernel learning，graphical models甚至manifold learning都有涉及，可能就是像武侠里一样只要学会了九阳神功，那么其它的武功就可以一窥而知其精髓了。这里面我最喜欢的是semi－supervised learning，因为随着要处理的数据越来越多，进行全部label过于困难，而完全unsupervised的方法又让人不太放心，在这种情况下semi－supervised learning就成了最好的。这没有一个比较清晰的认识，不过这也给了江湖后辈成名的可乘之机。到现在为止，我觉得cmu的semi－supervised是做得最好的，以前是KAMAL NIGAM做了开创性的工作，而现在Lafferty和他的弟子作出了很多总结和创新。 Lafferty的弟子好像不是很多，并且好像都不是很有名。不过今年毕业了一个中国人，Xiaojin Zhu(上海交通大学校友)，就是做semi－supervised的那个人，现在在wisconsin-madison做assistant professor。他做了迄今为止最全面的Semi-supervised learning literature survey， 大家可以从他的个人主页中找到。这人看着很憨厚，估计是很好的陶瓷对象。另外我在（1）中所说的Jordan的牛弟子D Blei今年也投奔Lafferty做博后，就足见Lafferty的牛了。 Lafferty做NLP是很好的，著名的Link Grammar Parser还有很多别的应用。其中language model在IR中应用，这方面他的另一个中国学生ChengXiang Zhai(南京大学校友，2004年美国青年科学家总统奖(PECASE)得主)，现在在uiuc做assistant professor。 Machine learning 大家（4):Peter L. Bartlett 鄙人浅薄之见，Jordan比起同在berkeley的Peter Bartlett还是要差一个层次。Bartlett主要的成就都是在learning theory方面，也就是ML最本质的东西。他的几篇开创性理论分析的论文，当然还有他的书Neural Network Learning: Theoretical Foundations。 UC Berkeley的统计系在强手如林的北美高校中一直是top3， 这就足以证明其肯定是群星荟萃，而其中，Peter L. Bartlett是相当亮的一颗星。关于他的研究，我想可以从他的一本书里得到答案：Neural Network Learning: Theoretical Foundations。也就是说，他主要做的是Theoretical Foundations。基础理论虽然没有一些直接可面向应用的算法那样引人注目，但对科学的发展实际上起着更大的作用。试想vapnik要不是在VC维的理论上辛苦了这么多年，怎么可能有SVM的问世。不过阳春白雪固是高雅，但大多数人只能听懂下里巴人，所以Bartlett的文章大多只能在做理论的那个圈子里产生影响，而不能为大多数人所广泛引用。 Bartlett在最近两年做了大量的Large margin classifiers方面的工作，如其convergence rate和generalization bound等。并且很多是与jordan合作，足见两人的工作有很多相通之处。不过我发现Bartlett的大多数文章都是自己为第一作者，估计是在教育上存在问题吧，没带出特别牛的学生出来。 Bartlett的个人主页的talk里有很多值得一看的slides,如Large Margin Classifiers: Convexity and Classification；Large Margin Methods for Structured Classification: Exponentiated Gradient Algorithms。大家有兴趣的话可以去下来看看。 Machine learning 大家（5): Michael Collins Michael Collins (http://people.csail.mit.edu/mcollins/ 自然语言处理(NLP)江湖的第一高人。出身Upenn，靠一身叫做Collins Parser的武功在江湖上展露头脚。当然除了资质好之外，其出身也帮了不少忙。早年一个叫做Mitchell P. Marcus的师傅传授了他一本葵花宝典-Penn Treebank。从此，Collins整日沉迷于此，终于练成盖世神功。 学成之后，Collins告别师傅开始闯荡江湖，投入了一个叫AT&T Labs Research的帮会，并有幸结识了Robert Schapire、Yoram Singer等众多高手。大家不要小瞧这个叫AT&T Labs Research的帮会，如果谁没有听过它的大名总该知道它的同父异母的兄弟Bell Labs吧。 言归正传，话说Collins在这里度过了3年快乐的时光。其间也奠定了其NLP江湖老大的地位。并且练就了Discriminative Reranking， Convolution Kernels，Discriminative Training Methods for Hidden Markov Models等多种绝技。然而，世事难料，怎奈由于帮会经营不善，这帮大牛又不会为帮会拼杀，终于被一脚踢开，大家如鸟兽散了。Schapire去了Princeton， Singer 也回老家以色列了。Collins来到了MIT，成为了武林第一大帮的六袋长老，并教授一门叫做的Machine Learning Approaches for NLP (http://www.ai.mit.edu/courses/6.891-nlp/的功夫。虽然这一地位与其功力极不相符，但是这并没有打消Collins的积极性，通过其刻苦打拼，终于得到了一个叫Sloan Research Fellow的头衔，并于今年7月，光荣的升任7袋Associate Professor。 在其下山短短7年时间内，Collins共获得了4次世界级武道大会冠军(EMNLP2002, 2004, UAI2004, 2005)。相信年轻的他，总有一天会一统丐帮，甚至整个江湖。 看过Collins和别人合作的一篇文章，用conditional random fields 做object recogntion。还这么年轻，admire to death！","title":"http://www1.bbsland.com/education/messages/236421.html"},{"content":"编者按：本文为淘宝广告技术部广告算法负责人、淘宝网研究员吴雪军在8月3日CTO俱乐部沙龙演讲实录，全文如下： 我今天演讲的题目是自然语言处理技术在搜索和广告中的应用。搜索和广告是技术非常密集两个互联网产品，它们前端都非常简单，但后台系统架构极其复杂。 今天主要讲三个方面的内容：第一、主要是介绍一比较典型的互联网应用体系；第二、主要介绍NLP技术在搜索中的应用；第三、介绍NLP技术在物联网广告中的应用。 NLP技术体系 首先介绍NLP技术体系，NLP技术体系在不同的应用需求、不同的领域下，拥有不同的组织形式。下图是一种比较典型的面向互联网应用的技术体系。 在05年之前NLP技术在实际应用中，特别是互联网应用还中比较多。07年时我参加自然语言学者技术研讨会。会上很多人都是国内做NLP技术、自然语言处理技术的前沿代表人物，当时我们讨论的主要问题就是NLP技术在实际应用中有没有价值？ 底层为数据层，包含三种类型的数据，1.词典，词条译本在分词或一些词法分析内可用到；2.知识库，内包含一些语言，语义分析处理是比较重要的功能；3.统计的数据，主要是词汇共现、ngram数据。以上比较有代表性的三个数据。 第二层是Term级。分为词法分析、Term语义表和Term关系。1.词法分析包含分词分词、词性标注和未登录词识别；2.Term语义表包含属性/类别和语义的表示；3.Term关系包含同义关系、词汇见关系和知识库构建。 第三个层短串涉及一些变化，分为短串解析、短串语义表示和短串变换。1.短串解析分为结构分析/浅层句法分析和Term重要性分析；2.短串语义表示包含短串主题分类短串语义表示；3.短串变换包含同义词替换、语义归一化以及省略纠错 第四个层为篇章级，分为单文档分析和多文档分析，在研究领域应用较多，在分析的领域，有诸如PLSA、LDA这样海量的文本分析技术。 NLP技术在搜索中的应用 侧重介绍在NPL在搜索引擎和互联网广告的应用，下图为一个简单的搜索引擎基础架构，第一块为最基本的网页抓取，第二块是网页的分析、索引。第三大块为一个查询。蓝色的三块是NLP技术应用比较多的三个方向，我将介绍这三个方向中NLP的应用。 query分析/Rank 一、短串分析技术，涉及到结构的分析，Term重要性的标注，对短串进行初步的处理。为后续查询和语义的相关度计算做一些基础的分析。由于查询需求有很多不同的表示方法，我们会对query进行改写，使其能比较好的召回。这其中其实最主要的技术是短串的语义相关性。    二、语义规化，即相同语义用不同方法表示，这种语义规化技术在搜索引擎中应用广泛。语义短串在这里能很好的被应用，用一种相同的形式表示，然后计算它们之间的关联。 三、纠错，我们需要分析用户需要什么，对query需求的识别能针对性的满足用户不同的需求，或者整合成特定的数据库用来满足精确的需求。快速需求识别是比较重要的应用，其中的技术可被理解为对query语义类别的识别，即短串的分类。 在排序上的应用，NLP技术在这里面体现是相关性计算，query和网页相关性。在搜索内Rank代表两个体系，像百度、谷歌规则为主的系统以及像微软、雅虎用这种机器学习的Rank系统。在机器学习的Rank起到的作用还是最基本的query文本的语义表示，这些特征。规则系统里面会涉及到一些语义的计算、相关性的识别和相关性计算。 在网页分析和索引中应用主要涉及对象是网页title、Term权重计算、网页语义表示和网页类别识别。 互联网广告技术整体架构 NLP在互联网广告技术中应用包含了三种类型的广告技术：用户行为分析、网页内容分析/站点分析和Query分析，涉及到很多基本的广告库分析，索引。 如果能定向广告会涉及到用户行为分析。要把用户行为表成一个能去检索广告的形式，如果用户行为很多，这里面会涉及到行为排序。如果广告库很小，我们可能对这个行为的表示可能会抽象一些。如果整个广告集合很多，侯选广告很多可以分层次。对于内容广告而言，会涉及比较多的对网页内容的分析，这方面主要涉及到最基本网页主题的提取，另外还会涉及到关联内容分析，因为广告一般都具有商业价值的内容，我们会把广告内容关联到有商业价值的内容上去，做到对广告的匹配。 对于搜索广告而言，请求分析主要是query分析，与搜索大体一样。对用户的基本请求解析完之后，会涉及到怎么去匹配广告如果广告集合很大我们不可能对每个广告做相关性计算，所以先要保证能够把相关的广告召回做一个集合，然后对这个集合进行相关性计算。    最后一方面的应用是广告排序，收费在搜索广告里是很重要的形式，按点击量来收费，所以排序最主要的是预测其点击率作为排序的依据。预测点击率后我们根据其价格算出其基本的收益。其最核心的技术是预估点击率，CTR是排序的核心。语义特征的表示，以及相关性的机损，广告可以有一部分特征语义来表示。  ","title":"淘宝吴雪军：自然语言处理技术在搜索和广告中的应用"},{"content":"  《Javascript权威指南》   《Javascript高级程序设计》（约三分之一）   《锋利的jQuery》   《AspectJ Cooking》   《MySQL开发者权威指南》（约三分之二）   《Spring in action第二版》（约二分之一）   《Java编程思想》   《Java核心技术第八版I、II卷》   《重构--改善既有代码的结构》（约三分之二）   《Java与设计模式》（目前只读了约三分一）   《UML与Rose2003从入门到精通》（正在读）   《时间管理幸福学》（正在读）   《简化你的时间》（正在读） 、《spring技术内幕》 计算机程序设计艺术，机械工业，经典之作 C指针与陷阱 C++大学教程，电子工业 自然语言处理，电子工业 C程序设计题典，夏宽里 高级程序员教程， C语言从入门到精通，陈锐，电子工业出版社， 比较全面， 零基础学数据结构，机械工业 C语言程序设计艺术，机械工业 C语言程序设计语言，机械工业， c语言大学教程，电子工业， 数据结构，严蔚敏 c/C++常用函数与算法速查手册，中国铁道出版社 还有好多vc之类的图书，像孙鑫的，还有翻译过来的vc入门经典，windows程序设计，等等 看看这些书，对于大家的编程能力有一定的提高。特别是希望提高算法的人员","title":"看别人的总结自己的"},{"content":"  搜索技术总结整理 2006/12/05  作者：Hontlong from Hour41 (www.hour41.com) 学习搜索有一段时间了，为了复习巩固和提高，特把学习的结 果总结一下。本文章搜索只特指小型搜索系统。之所以特指是小型系统，是因为大型小型搜索系统虽然整体处理过程大体相似，但整体架构和要处理的数据量和响应 速度是密切相关的，百万量级的和十亿量级的搜索系统是不可同日而语的。 搜索系统处理大体分为：蜘蛛、切词、索引、检索，下面逐个的描述。 1. 蜘蛛 蜘蛛是用来抓取网页的。所谓的抓，其实也就是通过 socket向http服务器发送post或者get请求，服务器根据你的请求把页面送给你的过程。大家通常用的IE浏览器就是要先做这个工作，然后在根 据返回的Html源码生成你看到的网页。蜘蛛当然不需要去生成网页给人看了，它只是把html存储起来给其它程序用。我看到有人问如何写一个下载动态网页 的蜘蛛，其实动态页面和静态页面对于蜘蛛是一样的，都是发送请求和接受，没有实质区别。 搜索技术中，蜘蛛看起来是最容易的。大家学习了一个语言和熟悉TCP/IP后都可以编写一个下载网页的程序了。但是，要写一个适应性强的蜘蛛却是相当难的，因为网络环境太过于复杂，你总是很难想象到别人会发什么给你，也就是蜘蛛要有很强的例外处理能力。最常见的问题是： 网路故障 造成蜘蛛抓取超时，没有抓取到网页或者抓取不全。 网站链接 有些网站对错误的链接或者蜘蛛使用的ip，返回一个很怪的页面，例如它的内部错误信息。 页面问题 页面问题中网页编码是首要问题，各个网站用的编码五花八门，需要你判断和处理合理的编码。有朋友会问，http协议头信息中有charset，html中 也有charset，可以方便的判断编码，没错，这是主要的判断方式，但是，如果这两个地方都得不到charset信息呢？ 其次，要提高蜘蛛的效率和速度，这就要涉及到待抓取url的权值值计算，因为并不是每个网页对我们来说都是同等重要的。由于网络的无边无际，所以采用广度搜索和深度搜索都不可取，只有采用种权值计算公式采用减枝方法做有效遍历。也有只依赖权值来决定搜索目标的。 最后是蜘蛛的性能。也就是并行处理方式。单个蜘蛛处理能力有限制，因此多采用多线程和多进程的方法，多个蜘蛛同时运行。另外，采用此方法的另一个重要原因是，网络速度是蜘蛛程序的瓶颈。 2.切词 切词仿佛是最简单的。采用最流行的倒序最大前序最大切词方 法基本上就够用的了。但是切词也是让我最迷惑的东西，什么是切词？采用上述方法切出来的并不是标准的词语，更像是字符和分割组合，你查一下百度或者 googel就可以看到它们的切词实现效果。切词程序真的不是在切词啊。 切词是深入处理中文信息的基本模块。在自然语言分析中也有 切词，切词的基本方法和过程也基本相同，但我觉得在两者的是指导思想有些不一样。自然语言的分析很在意切词的准确性，而索引切词确更注重切词结果的覆盖情 况。自然语言处理在切词后，需要词性标注和词法语法分析，因此必须切词准确，对新词、命名实体等的处理相对要求要高，而索引切词后就是做倒排索引了，只要 能保证索引时切词和查询时的切词是同样的，基本上就可以正确使用索引，只是为了索引效果，需要尽可能的在符合词语的自然分段的基础上，把词的切割粒度定位 在在效率和灵活性的一个调和位置，对新词和命名实体的识别就不那么看重。例如：北京大学，在第一中切词中会切割成一个词语，而在后一种切词中，为了保证能 使用北京也可以搜索到，会被切割成北京和大学两个词。也有为了效率而做复合索引的，例如把北京大学切割为 北京大学、北京和大学这3个词，方便用户使用任何一个熟悉的词都能搜索到。 通过上面的讨论，分析到切词系统实际上是根据实际应用而不 同的。就索引切词来说，切词的准确性并不重要，切词后的词能够快速准确找到索引，才重要。那么怎么切才能让后面的索引做的快速而且准确呢？想一想，我们为 什么要切词呢，即便不切词，我们也可以在字上做索引，也可以在字节上做索引，开源搜索醒目lucence默认处理中国语言的方式就是双字切词，两个字两个 字的切，例如：北京大学，切割后为，北京 京大 大学。但要讲到效率，根据搜索到的资料数据比较，还是基于词库的自然词切割是效果较好的。我想原因是：1，切割的完整性，不会遗漏词语，也不会有太多废 词，例如 京大。因为使用索引查询的人的查询输入，也基本符合中文语法，基本不会出现 京大这样的词，相对的2字切词的处理方式就会产生废词，就要多做和维护一个基本用不到的索引。2，切割的后的词检索的效率，如果基于字或者字节做索引，虽 然中国总数有2万多，但是常用的只有4~6千，要在这个范围内映射千万量级以上的文档，索引就比较大，每个字要对应万量级的文档，同时后期的归并和条件过 滤处理工作量也比较大，而对应的词是中文语法结构的基本单位，常用词有5~10万，足以承担亿量级的文档索引分担。做了一段时间的索引程序后，我的感觉是 词长在2~4之间比较好。 这里讨论的罗嗦的原因是为了比较说明索引切词并不是严格意 义上的切词，只是融合了切词逻辑的字符切割。这种切割字符基本符合词语单位，我想是有必然的潜在的语言规则。我在学习切词时有走过弯路，总想把词切割的准 确些，再准确些，结果程序效率下降，准确性也不能有明显提高，现在觉得，如果不涉及更深层次额语义处理，并不必要。 3.索引 对于学习过数据库，熟悉数据库基本原理的朋友们来说，做索引并不是很困难的事。现有的索引系统基本是采用倒排索引。基本含义就是在一个文件中记录下一个词在哪篇文章哪个位置出现，以便解析用户输入后可以直接读取对应的文章id，把对应的文章择要提取出来显示给用户。 我认为索引难做的一点是索引的维护，包括插入和删除。而索 引的更新(update)，通常索引系统把它转化为删除(delete)和插入(insert)。信息的变更要尽可能快的反映到索引中，这个对索引系统的 压力还是比较大的。想一下数据的删除过程，删除一片文章，要知道这篇文章都在哪些词的索引中，要逐个的清理数据，而这个数据清理，通常为了性能，又不是真 实的清理，只是把文章对应的数据抹去（清零或做特定标记），这样索引中就像是留下了一个洞，时间一场，索引也就千疮百孔了，所以索引系统需要根据一定的算 法计算索引中的空洞比率，在适合的条件下，整理索引，剔除空洞。 另外，插入索引也需要特别注意，简单的索引插入当然没有问 题，因为文档id在索引中一般是排序的（整体排序或者分段排序），这样方便检索时快速处理。因此大批量或者频繁的插入时，就会有性能问题。通常的解决方案 是采用大小索引的办法，新插入的索引并不直接插入到大索引中（数据处理量相对大），而是临时写入小索引文件，而检索时，从这两个索引中取值，然后归并就可 以了。 另外，索引数据，为了提高读写速度，一般是经过简单压缩的，这时也就涉及到数据安全问题，为了避免万一问题，也可在代码中增加校验功能，保证数据完整，即便某个地方因计算错误而有问题，也要把他限定在最小范围内。 4.检索 我很怀疑google的pagerank计算公式，据说有超过一千的影响因素，这在我简单的脑袋里是不可想象的。我看google黑板报 （google的中文官方blog）里曾经说到，简单就是最美的，公式的思想好像与这个相违背。总之pagerank的思想就是：我重要，你和我有友好关 系，那么可以简单的推到出，你也重要。根据这样的思想，我们可以创建自己的计算公式。 我比较关心检索的组合，例如检索：测试 软件 程序员 -网站  。这个在检索时会处理成一个逻辑结果，好像，(a and b and c ) - d 。大家都知道这就是集合操作，熟悉数据库的朋友可以想到，这个很像数据查询语句的条件，没错，为了减少数据库和索引操作，分析和优化查询是很有必要的，处 理过程完全可以借鉴数据库检索语句分析和优化过程。 总结一下，我写的这些是索引的入门。我对索引的理解是。根 据处理的数据类型和数据量，索引系统有很大的不同。我记得百度掌门人说过，搜索人人可作，但要做大做好，就很困难了。具体索引的实现方式，要比描述的复杂 写，有兴趣的朋友可以现学习lucence，非常棒的开源java程序，也有.net版本的开源。我对lucence的感觉是，它很经典，它的索引文件有 点烦琐，它的检索方式很值得学习。","title":"搜索技术总结整理"},{"content":"主题: 机器学习相关书籍推荐 1. 《Programming Collective Intelligence》，近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的:P 2. Peter Norvig 的《AI, Modern Approach 2nd》（无争议的领域经典）。 3. 《The Elements of Statistical Learning》，数学性比较强，可以做参考了。 4. 《Foundations of Statistical Natural Language Processing》，自然语言处理领域公认经典。 5. 《Data Mining, Concepts and Techniques》，华裔科学家写的书，相当深入浅出。 6. 《Managing Gigabytes》，信息检索好书。 7. 《Information Theory：Inference and Learning Algorithms》，参考书吧，比较深。 相关数学基础（参考书，不适合拿来通读）： 1. 线性代数：这个参考书就不列了，很多。 2. 矩阵数学：《矩阵分析》，Roger Horn。矩阵分析领域无争议的经典。 3. 概率论与统计：《概率论及其应用》，威廉·费勒。也是极牛的书，可数学味道太重，不适合做机器学习的。于是讨论组里的 Du Lei 同学推荐了《All Of Statistics》并说到 机器学习这个方向，统计学也一样非常重要。推荐All of statistics，这是CMU的一本很简洁的教科书，注重概念，简化计算，简化与Machine Learning无关的概念和统计内容，可以说是很好的快速入门材料。 4. 最优化方法：《Nonlinear Programming, 2nd》非线性规划的参考书。《Convex Optimization》凸优化的参考书。此外还有一些书可以参考 wikipedia 上的最优化方法条目。要深入理解机器学习方法的技术细节很多时候（如SVM）需要最优化方法作为铺垫。","title":"主题: 机器学习相关书籍推荐"},{"content":"1 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 International journal of computer vision 3 机器学习： AAAI         April 15, 2011: Notification of acceptance or rejection  ICML          April 19, 2011: Notification of acceptance or rejection      IJCAI          Notification of acceptance/rejection: March 31, 2011 NIPS          Notification of acceptance/rejection: 02 Aug 2011  ECML        Author notification: 3 Jun. 2011  WWW        Jan 17, 2011  机器视觉： CVPR        Decisions Announced: Feb.18 2011 ICCV         Author notification: End of June 2011  http://www.cnblogs.com/lqcdkj/archive/2009/04/28/1445438.html 下面列一些CS的顶级会议和期刊    有些是网上查到的，有些是某些人用SCI的IF排序做出来的： Computer Vision Conf.:   Best:     ICCV, Inter. Conf. on Computer Vision     CVPR, Inter. Conf. on Computer Vision and Pattern Recognition   Good:     ECCV, Euro. Conf. on Comp. Vision     ICIP, Inter. Conf. on Image Processing     ICPR, Inter. Conf. on Pattern Recognition     ACCV, Asia Conf. on Comp. Vision Computer Vision  Jour.:   Best:     PAMI, IEEE Trans. on Patt. Analysis and Machine Intelligence     IJCV, Inter. Jour. on Comp. Vision   Good:     CVIU, Computer Vision and Image Understanding PR, Pattern Reco. Network Conf.:     ACM/SigCOMM     ACM Special Interest Group of Communication     ACM/SigMetric Info Com Globe Com Network Jour.:     ToN (ACM/IEEE Transaction on Network) A.I.Conf.:     AAAI: American Association for Artificial Intelligence     ACM/SigIR IJCAI: International Joint Conference on Artificial Intelligence     NIPS: Neural Information Processing Systems     ICML: International Conference on Machine Learning A.I.Jour.:     Machine Learning     NEURAL COMPUTATION     ARTIFICIAL INTELLIGENCE PAMI     IEEE TRANSACTIONS ON FUZZY SYSTEMS     IEEE TRANSACTIONS ON NEURAL NETWORKS AI MAGAZINE     NEURAL NETWORKS     PATTERN RECOGNITION     IMAGE AND VISION COMPUTING     IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING     APPLIED INTELLIGENCE OS,System Conf.:     SOSP: The ACM Symposium on Operating Systems Principles     OSDI: USENIX Symposium on Operating Systems Design and Implementation Database Conf.:     ACM SIGMOD     VLDB:International Conference on Very Large Data Bases     ICDE:International Conference on Data Engineering Security Conf.:     IEEE Security and Privacy     CCS: ACM Computer and Communications Security NDSS (Network and Distributed Systems Security) Web Conf.:     WWW(International World Wide Web Conference) Theory Conf.:     STOC FOCS  EDA Conf.:  Best:      DAC: IEEE/ACM Design Automation Conference     ICCAD: IEEE International Conference on Computer Aided Design Good:     ISCAS: IEEE International Symposium on Circuits And Systems     ISPD: IEEE International Symposium on Physical Design     ICCD: IEEE International Conference on Computer Design     ASP-DAC: European Design Automation Conference     E-DAC: Asia and South Pacific Design Automation Conference Graphics Conf.:   Best:     Siggraph: ACM SigGraph   Good:     Euro Graph Jour.: IEEE(ACM) Trans. on Graphics     IEEE Trans. on Visualization and Computer Graphics CAD    Jour.: CAD CAGD Softe Engineering: conf.:      ICSE The International Conference on Software Engineering     FSE The Foundations of Software Engineering Conferences     ICASE IEEE International Conference on Automated Software Engineering     COMPSAC International Computer Software and Applications Conferences     ESEC The European Software Engineering Conferences Jour.:     SEN ACM SIGSOFT Software Engineering Notes     TSE IEEE Transactions on Software Engineering     ASE Automated Software Engineering SPE Software-Practice and Experience   EI收录的中国期刊：     来自http://www.ei.org.cn/twice/coverage.jsp ISSN 期 刊 名 相关链接 0567-7718 Acta Mechanica Sinica 1006-7191 Acta Metallurgica Sinica (English Letters) 0253-4827 Applied Mathematics and Mechanics (English Edition) 0890-5487 China Ocean Engineering 1004-5341 China Welding 1004-9541 Chinese Journal of Chemical Engineering 1022-4653 Chinese Journal of Electronics 1000-9345 Chinese Journal of Mechanical Engineering (English Edition) 学报网站 1671-7694 Chinese Optics Letters 学报网站 1673-7350 Frontiers of Computer Science in China 期刊网址 1006-6748 High Technology Letters 1674-4799 International Journal of Minerals, Metallurgy and Materials 1004-0579 Journal of Beijing Institute of Technology (English Edition) 学报编辑部 1005-9784 Journal of Central South University of Technology 学报网站 1672-5220 Journal of Donghua University (English Edition) 1005-9113 Journal of Harbin Institute of Technology (New Series) 1001-6058 Journal of Hydrodynamics 1005-0302 Journal of Materials Science and Technology 1002-0721 Journal of Rare Earths 1674-4926 Journal of Semiconductors 学报编辑部 1007-1172 Journal of Shanghai Jiaotong University (Science) 1003-7985 Journal of Southeast University (English Edition) 1004-4132 Journal of Systems Engineering and Electronics 1009-6124 Journal of Systems Science and Complexity 1003-2169 Journal of Thermal Science 1000-2413 Journal of Wuhan University of Technology -Materials Science Edition 1673-565X Journal of Zhejiang University SCIENCE A 1674-5264 Mining Science and Technology 1001-0521 Rare Metals 1006-9291 Science in China, Series B: Chemistry 1672-1799 Science in China, Series G: Physics, Astronomy 1005-8885 The Journal of China Universities of Posts and Telecommunications 1005-1120 Transactions of Nanjing University of Aeronautics and Astronautics 1003-6326 Transactions of Nonferrous Metals Society of China 1006-4982 Transactions of Tianjin University 1007-0214 Tsinghua Science and Technology Editor Information 1001-1455 爆炸与冲击 0254-0037 北京工业大学学报 1001-5965 北京航空航天大学学报 学报编辑部 1001-053X 北京科技大学学报 学报编辑部 1001-0645 北京理工大学学报 学报编辑部 1007-5321 北京邮电大学学报 学报编辑部 1000-1093 兵工学报 1001-4381 材料工程 1005-0299 材料科学与工艺 1009-6264 材料热处理学报 学报网站 1005-3093 材料研究学报 1001-1595 测绘学报 学报编辑部 1007-7294 船舶力学 1000-8608 大连理工大学学报 1004-499X 弹道学报 1000-2383 地球科学 学报网站 1005-0388 电波科学学报 1000-6753 电工技术学报 1007-449X 电机与控制学报 1000-1026 电力系统自动化 学报网站 1006-6047 电力自动化设备 1001-0548 电子科技大学学报 0372-2112 电子学报 1009-5896 电子与信息学报 1005-3026 东北大学学报　(自然科学版) 1001-0505 东南大学学报 (自然科学版) 1000-3851 复合材料学报 1003-6520 高电压技术 1000-7555 高分子材料科学与工程 1002-0470 高技术通讯 1003-9015 高校化学工程学报 1000-5773 高压物理学报 1000-4750 工程力学 0253-231X 工程热物理学报 1001-9731 功能材料 学报网站 1006-2793 固体火箭技术 0254-7805 固体力学学报 1005-0086 光电子.激光 1000-0593 光谱学与光谱分析 1004-924X 光学精密工程 学报网站 0253-2239 光学学报 学报网站 0454-5648 硅酸盐学报 1001-2486 国防科技大学学报 1006-7043 哈尔滨工程大学学报 学报网站 0367-6234 哈尔滨工业大学学报 0253-360X 焊接学报 1005-5053 航空材料学报 1000-8055 航空动力学报 编辑部网站 1000-6893 航空学报 学报网站 0258-0926 核动力工程 1001-9014 红外与毫米波学报 1000-2472 湖南大学学报 (自然科学版) 1000-565X 华南理工大学学报(自然科学版) 编辑部网站 1671-4512 华中科技大学学报(自然科学版) 0438-1157 化工学报 1002-0446 机器人 学报网站 0577-6686 机械工程学报 学报网站 1671-5497 吉林大学学报(工学版) 学报编辑部 1003-9775 计算机辅助设计与图形学学报 1006-5911 计算机集成制造系统 编辑部网站 0254-4164 计算机学报 1000-1239 计算机研究与发展 学报网站 1007-4708 计算力学学报 1001-246X 计算物理 1007-9629 建筑材料学报 1000-6869 建筑结构学报 1671-7775 江苏大学学报（自然科学版） 1009-3443 解放军理工大学学报（自然科学版） 0412-1961 金属学报 0258-1825 空气动力学学报 1000-8152 控制理论与应用 学报网站 1001-0920 控制与决策 0459-1879 力学学报 学报网站 0253-9993 煤炭学报 学报网站 1003-6059 模式识别与人工智能 1004-0595 摩擦学学报 1672-6030 纳米技术与精密工程 1005-2615 南京航空航天大学学报 1005-9830 南京理工大学学报 (自然科学版) 1000-0925 内燃机工程 1000-0909 内燃机学报 1002-6819 农业工程学报 学报编辑部 1000-1298 农业机械学报 学报编辑部 1001-4322 强激光与粒子束 学报编辑部 1000-0054 清华大学学报 (自然科学版) 0253-2409 燃料化学学报 1006-8740 燃烧科学与技术 1000-985X 人工晶体学报 无机材料期刊网 1000-9825 软件学报 学报编辑部 1006-2467 上海交通大学学报 1000-2618 深圳大学学报（理工版） 0371-0025 声学学报 1000-7210 石油地球物理勘探 1000-0747 石油勘探与开发 0253-2697 石油学报 1001-8719 石油学报:石油加工 学报网站 1672-9897 实验流体力学 学报网站 1001-6791 水科学进展 0559-9350 水利学报 学报编辑部 1003-1243 水力发电学报 1009-3087 四川大学学报(工程科学版) 学报编辑部 0254-0096 太阳能学报 学报编辑部 0493-2137 天津大学学报 学报编辑部 1001-8360 铁道学报 1000-436X 通信学报 0253-374X 同济大学学报 (自然科学版) 1000-131X 土木工程学报 学报网站 1674-4764 土木建筑与环境工程 学报编辑部 1001-4055 推进技术 1000-324X 无机材料学报 1671-8860 武汉大学学报(信息科学版) 1001-2400 西安电子科技大学学报 学报网站 0253-987X 西安交通大学学报 1000-2758 西北工业大学学报 0258-2724 西南交通大学学报 学报网站 1002-185X 稀有金属材料与工程 1000-6788 系统工程理论与实践 1001-506X 系统工程与电子技术 1007-8827 新型炭材料 1000-6915 岩石力学与工程学报 学报网站 1000-4548 岩土工程学报 1000-7598 岩土力学 期刊编辑部 0254-3087 仪器仪表学报 1005-0930 应用基础与工程科学学报 学报网站 1000-6931 原子能科学技术 1008-973X 浙江大学学报 (工学版) 1672-7126 真空科学与技术学报 1004-6801 振动测试与诊断 1004-4523 振动工程学报 1000-3835 振动与冲击 学报网站 0258-8013 中国电机工程学报 1001-7372 中国公路学报 0258-7025 中国激光 学报网站 1000-1964 中国矿业大学学报 1673-5005 中国石油大学学报 (自然科学版) 1001-4632 中国铁道科学 1004-0609 中国有色金属学报 学报网站 1672-7207 中南大学学报（自然科学版） 学报网站 0254-4156 自动化学报 学报网站   中科院计算所推荐国际会议 序号 会议名称 会议介绍 代表领域 1 ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左 右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录 用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。< /p> 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。 GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千 人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会 议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左 右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一 次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一 次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu.  http://hpdc13.cs.ucsb.edu 高性能计算 42  NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/  高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications  高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括 technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing  该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems  由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing  This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下 半年排名。 高性能计算 48 ACM International Conference on Supercomputing  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing  IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 　 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57  FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶 尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59  SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60  IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62  IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等 概念。 自主计算 63  Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64  International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65  [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66  IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67  USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68  IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69  International Symposium on Computer Architecture（ISCA ） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很 难 系统结构 70  International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72  IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73  Annual ACM International Conference on Supercomputing（ICS）  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 74  Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其 困难 操作系统 75  ACM Symposium on Operating Systems Principles （SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要 中极其困难 操作系统 76  Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困 难 操作系统，程序语言 77  Workshop on Hot Topics in Operating Systems （HOTOS） 操作系统最好的会议之一 操作系统 78  Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79  Annual IEEE Conference on Local Computer Networks（LCN） 　 网络 80  International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影 响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收 率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会 议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影 响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收 率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。< /p> 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统 等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影 响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术 121 ECOOP - European Conference on Object-Oriented Programming 关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。 编译技术 122 ESOP - European Symposium on Programming ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。< /p> 编译技术 123 Euro-Par - European Conference on Parallel Computing 关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。 　 124 SAS - International Static Analysis Symposium 关注程序的静态分析的权威会议。 编译技术 125 CAV - Computer Aided Verification Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。 编译技术 126 FASE - Fundamental Approaches to Software Engineering ETAPS的member conference，主要关注Software Science，影响因子0.91。 编译技术 127 TACAS - Tools and Algorithms for the Construction and Analysis of Systems ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24 编译技术 128 VMCAI - Verification, Model Checking and Abstract Interpretation Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。 编译技术 129  ACL: The Association for Computational Linguistics 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。 人工智能 计算语言学 130 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索方面最好的会议, ACM 主办, 每年开。19％左右 信息检索技术 131 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining 数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右 　 132 WWW: The ACM International World Wide Web Conference 应用和媒体领域顶级国际会议 万维网 133 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库领域顶级国际 数据管理 134 CIKM: The ACM Conference on Information and Knowledge Management 数据库领域知名国际会议 数据管理 135 COLING: International Conference on Computational Linguistics 计算语言学知名国际会议 计算语言学 136 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 137 IEEE ICDM: International Conference on Data Mining 数据挖掘领域顶级国际会议 　 138 IJCAI: International Joint Conference on Artificial Intelligence 人工智能领域顶级国际会议，论文接受率18％左右 人工智能 139 VLDB: The ACM International Conference on Very Large Data Bases 数据库领域顶级国际 数据库 140 SIGGRAPH: ACM SIGGRAPH Conference 计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％ 计算机图形学 141 EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics 欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20% 计算机图形学 142 AAAI: American Association for Artificial Intelligence 美国人工智能学会AAAI的年会，使该领域的顶级会议 人工智能 143 ACM Conference　on　Computer and　Communications Security ACM通信和计算健全领域顶级学术会议 信息安全 144 ACM SIGCOMM: Special Interest Group on Data Communications 　 数据通信 145 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索领域的重要会议 信息检索 146 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。 通信与网络 147 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems ACM性能建模与评价领域顶级学术会议 通信与网络 148 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。 数据管理 149 ASPLOS: Architectural Support for Programming Languages and Operating Systems 编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。 体系结构 150 CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid 网格计算国际会议，网格平台、中间件 　 151 CIKM: The ACM Conference on Information and Knowledge Management 信息检索领域的会议，录用率为15% 信息检索 152 CLUSTER - IEEE International Conference on Cluster Computing 集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。 集群计算 153 CPM: Combinatorial Pattern Matching Symposium 组合模式匹配年会，是字符串匹配、模式匹配较好的会议。 模式匹配 154 FAST4: Third USENIX Conference on File and Storage Technologies, USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。 文件与存储 155 Grid : IEEE/ACM International Workshop on Grid Computing 网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。 网格计算 156 HPC: IEEE International Conference for High Performance Computing 　 　 157 HPDC: International Symposium on High Performance Distributed Computing 高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。< /p> 高性能计算 158 ICDCS: IEEE International Conference on Distributed Computing Systems IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS 举办了26届，录用率为15%。 分布式计算 159 ICML: International Conference on Machine Learning 机器学习领域中的顶级会议 机器学习 160 ICWS: IEEE International Conference on Web Services Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。 Web服务 161 IEEE CSB: Computer Society Bioinformatics 　 　 162 IEEE ICDM: International Conference on Data Mining 数据挖掘领域的著名会议，率用率为14%。 数据挖掘 163 IEEE ICNP: International Conference on Network Protocols IEEE 网络通信领域顶级学术会议，录用率在10%左右。 网络 164 IEEE ICON: IEEE International Conference on Networks 　 　 165 IEEE INFOCOM: conference on computer communications IEEE网络通信领域著名会议，领域广泛。 网络 166 IEEE IPCCC: International Performance Computing and Communications Conference IEEE性能领域著名学术会议，主要关注性能评价。 网络性能 167 IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval 字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。 字符串处理信息检索 168 IJCAI: International Joint Conference on AI 人工智能领域的顶级会议。 人工智能 169 IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。 170 International Symposium on High Performance Computer Architecture(HPCA) 　 服务计算 171 IPDPS: IEEE International Parallel and Distributed Processing Symposium IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%， 会议的优秀论文可能发表于JDPC杂志上。 并行计算分布式计算 172 ISMB: International conference on Intelligent Systems for Molecular Biology 　 无线 173 MobiCom: ACM/IEEE Conference on Mobile Computing and Networking 始于1995，无线、移动计算方面比较有历史和重要的会议。 安全 174 MobiSys: The International Conference on Mobile Systems, Applications, and Services 无线方面，2006年第4名。 无线 175 OSDI: USENIX Symposium on Operating Systems Design and Implementation USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。 操作系统 176 PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining 　 　 177 PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies 关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。 并行计算分布式计算 178 PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases 数据挖掘领域的重要会议，录用率为14%。 数据挖掘 179 SCC: IEEE International Conference on Services Computing 服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。< /p> 服务计算 180 SDM: SIAM International Conference on Data Mining 数据挖掘领域的重要会议，录用率为14% 数据挖据 181 SOSE: IEEE International Workshop on Service-Oriented System Engineering 　 　 182 USENIX Sec: USENIX Security Symposium USENIX安全领域重要会议，侧重安全技术。 安全 183 USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI) USENIX网络领域重要会议，设计网络涉及各方面内容。 网络 184 VLDB: The ACM International Conference on Very Large Data Bases 　 数据管理 185 WWW: The ACM International World Wide Web Conference ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。 Internet 186 RAID International Symposium on Recent Advances in Intrusion Detection 数据库顶级国际会议 　 187 IJCAI: International Joint Conference on Artificial Intelligence 人工智能顶级国际会议 人工智能 188 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据库 189 ICML: International Conference on Machine Learning 机器学习顶级国际会议 机器学习 190 PRICAI: Pacific Rim International Conference on Artificial Intelligence 亚太人工智能国际会议 人工智能 191 IFIP ICIIP: IFIP International Conference on Intelligent Information Processing IFIP智能信息处理国际会议 智能信息处理 192 NIPS: Neural Information Processing Systems 神经信息处理领域顶级国际会议 神经计算，机器学习 193 ISCA: International Symposium on Computer Architecture   体系结构领域的顶级会议 微处理器设计 194 International Symposium on Microarchitecture 体系结构领域的顶级会议 微处理器设计 195 HPCA：International Symposium on High Performance Computer Architecture 体系结构领域的顶级会议 微处理器设计 196 APCSAC: Asia-Pacific Computer Systems Architecture Conference  体系结构方面的重要会议 微处理器设计 197 ISLPED: International Symposium on Low Power Electronics and Design 低功耗设计的重要会议 微处理器设计 198 OSDI: Operation System Design & Implementation  操作系统方面的重要会议 微处理器设计 199 ASPLOS: Architecture Support for Programming Languages and Operation  体系结构方面的顶尖会议 微处理器设计 200 ICCD: IEEE International Conference on Computer Design 体系结构方面的顶尖会议 微处理器设计 201 DAC: Design Automation Conference  设计自动化领域的顶级会议 微处理器设计 202 IEEE/ACM International Conference on Computer Aided Design(ICCAD) 集成电路设计自动化方面的顶尖会议 微处理器设计 203 ASP-DAC: Asia and South Pacific Design Automation Conference  设计自动化领域的重要会议 微处理器设计 204 ISSCC: IEEE International Solid-State Circuits Conference 设计自动化领域的重要会议 微处理器设计 205 CICC: Custom Integrated Circuits Conference 集成电路设计方面的顶尖会议（公认排名第二） 微处理器设计 206 ESSCIRC: European Conference on Solid-State Circuits 集成电路设计方面的顶尖会议 微处理器设计 207 Symposium on VLSI Circuits 集成电路设计方面的顶尖会议 微处理器设计 208 IEEE International ASIC/SOC Conference 集成电路设计方面的重要会议 微处理器设计 209 Symposium on VLSI Technology 集成电路设计方面的重要会议 微处理器设计 210 ASSCC: Asian Conference on Solid-State Circuits 集成电路领域重要会议 微处理器设计 211 MWSCAS: Midwest Symposium on Circuits and Systems 集成电路领域重要会议 微处理器设计 212 ICECS: IEEE International Conference on Electronics, Circuits and Systems： 集成电路领域重要会议 微处理器设计 213 ISCAS: International Symposium Circuit and System  电路与系统方面的重要会议 微处理器设计 214 RFIC: IEEE Symposium on Radio Frequency Integrated Circuits  射频集成电路领域顶尖会议 微处理器设计 215 ACM RECOMB: Int. Conference on Research in Computational Molecular Biology RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在 20%左右。 计算生物学 216 IEEE CSB: Computer Society Bioinformatics CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办。 计算生物学 217 International Conference of the IEEE Engineering in Medicine and Biology Society （EMBC） 生物信息学与计算生物学国际研讨会 计算生物学 218 PSB: Pacific Symposium on Biocomputing  PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举 办。 计算生物学 219 WABI:Workshop on Algorithms in Bioinformatics WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方 面。 计算生物学 220 CSB: IEEE Computational Systems Bioinformatics Conference  CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办 计算生物学 221 ISMB：Annual International Conference on Intelligent Systems for Molecular Biology ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为 Bioinformatics杂志的专刊发表，近年来录取率为15%左右。 生物信息 222 ECCB：European Conference on Computational Biology ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为 Bioinformatics杂志的专刊发表，近年录取率在20%左右。 生物信息 223 APBC：Asia Pacific Bioinformatics Conference APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。< /p> 生物信息 224 COCOON：Annual International Computing and Combinatorics Conference COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左 右。 生物信息 225 CPM：Annual Symposium on Combinatorial Pattern Matching CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左 右。 生物信息 226 EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千 计。 生物信息 227 Geospatial Information and Technology Association（GITA） Annual Conference 地球空间信息与技术协会年会 遥感与空间信息处理 228 International Geoscience and Remote Sensing Symposium （IGRSS） 地球科学与遥感国际会议 遥感与空间信息处理 229 International Society for Photogrammetry and Remote Sensing （ISPRS）Technical Commission Symposium 国际摄影测量与遥感学会专业委员会会议 遥感与空间信息处理 230 International Conference on Geoinformatics 地球信息国际会议 遥感与空间信息处理 231 IEEE SKG (Semantics, Knowledge and Grid) 由计算所发起的IEEE国际会议，每年有100人参加。 知识网格 232 WWW: The ACM International World Wide Web Conference Internet领域顶级国际会议 Internet 233 International Semantic Web Conference Semantic Web领域顶级会议，录用率17% Semantic Web 234 ACM SIGMOD: ACM SIGMOD Conf on Management of Data ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。 数据管理 235 ACM PODS Conference ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。 数据管理 236 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据管理 237 IEEE ICDE - International Conference on Data Engineering 数据库顶级国际会议 数据管理  ","title":"计算机某些领域的顶级期刊和会议"},{"content":"  统计自然语言处理--数学基础                          李亚超   2010-10-28 1简介  这一部分介绍统计自然语言处理的数学基础，主要内容包括概率论和数理统计。要把所有要点详细的介绍完，很难。在这里，我只是把以后在自然语言处理中会用得到比较重要的数学知识做个介绍。单纯的数学公式是很枯燥的，比如在上中学时学的余弦定理，那时候就是一个单纯的数学公式，但是现在自然语言处理上，比如新闻分类，那就生动多了。多以在这里我想说的是数学是百科之母。 2概率基础 2.1 基础概率简介 概率论是统计自然语言处理的理论基础，有些知识我们在中学或者是大学已经学过了，在这里就不在做过多的赘述。比如，概率空间( Probability spaces )、条件概率和独立性(Conditional probability and independence)、随机变量、期望和方差。      在这里我只是介绍一下条件概率，因为这个概率模型以后会很多次提到，并且用途也很广。比如最大熵问题，HMM模型，语言模型中求字符串的概率。用的比较多的是多参数的链式法则。 在机器翻译中这个模型是语言模型。 例如，S表示特定排列的词串w1,w2,w3,w4,w5...wn。如果要求出S在文中出现的概率，即用P(S)表示S的概率，利用条件概率这个S的概率等于S中各个词的概率乘积，于是P(S)可以展开为： P(S)=P(w1*w2*w3...wn)=P(w1)P(w2|w1)P(w3|w1w2)...P(wn|w1w2w3...wn-1) 其中P(w1)表示词w1在文中出现的频率，P(w2|w1)表示已知第一个词的前提下，第二个词出现的频率，以后的一次类推。 在实际应用中，如果词串太长后面的计算量太大，用 N 元语法模型或者是 马尔可夫假设 ，即任意一个词 wi 出现的概率只有前一个 wi-1 词决定 , 这样前面的公式就变得简单多了， P(S)=P(w1)P(w2|w1)P(w3|w2)...P(wn|wn-1)  ，这里用的是二元语法模型，也可以用三元，四元模型，不过随着数量的增加计算的复杂度也相应的增加。 接下来的问题是，如果计算 P(wi|wi-1) ， 只要数一数这对词（ wi-1,wi)  在统计的文本中出现了多少次，以及  wi-1  本身在同样的文本中前后相邻出现了多少次 ，然后用两个数一除就可以了 ,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1) 。 也许人们不相信，就是这个简单的数学公式，可以解决复杂的语音识别、机器翻译、中文分词消歧问题。 李开复用统计语言模型把  997  词语音识别的问题简化成了一个  20  词的识别问题，实现了有史以来第一次大词汇量非特定人连续语音的识别。 Google 的中英文自动翻译也是主要基于这个统计语言模型。 2.2  贝叶斯定理 (Bayes’theorem) 简单地说 Bayes 是个逆概率问题， 比如，我们要可以把求P(A/B) ，的问题转换为求 P(B/A) 的问题。 当正向概率很复杂时，这个公式就显示出他的作用，以后我们会经常用到的。他由条件概率的定义引出。 Bayes定理：P(B|A)=P(AB)/P(A) = P(A|B)P(B)/P(A) 这样在给定A的条件下，求P(B|A)，我们可以忽略A的只，因为他在所有的情况下都是常数。 所以，argmax(P(A|B)P(B)/P(A)) = argmax(P(A|B)P(B))，这个公式在机器翻译中用的很广泛，比如用在 翻译模型 。  除了这些，Bayes公式在自然语言处理中用的很广泛，以后会经常用到。 Bayes统计 2.2.1 Bayes更新 Bayes统计是用来测量信任度，可以通过计算以前的数据，在遇到新的证据时，通过Bayes定理重新更新以前的数据。 2.2.2 Bayes决策理论 Bayes决策理论是Bayes定理的一个应用，Bayes本身就是求逆概率问题，所以可以通过事后数据来决定那个模型更好地符合实际。 2.3 Bayes 贝叶斯网络       我们在以前提到马尔科夫链(MarkovChain),它描述了这样的一个 状态 序列，每个状态值取决于其前面的有限的状态。 这种模型，对很多实际问题来讲是一种很粗略的简化。在现实生活中，很多事物相互的关系并不能用一条链来串起来。它们之间的关系可能是交叉的、错综复杂的。 不能用一条链来描述。 如下图： 我们可以把上述的 有向图看成一个网络， 它就是贝叶斯网络。其中每个圆圈表示一个状态。状态之间的连线表示它们的因果关系。比如从心血管疾病出发到吸烟的弧线表示心血管疾病可能和吸烟有关。当然，这些关系可以有一个量化的可信度(belief)，用一个概率描述。我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名。由于网络的每个弧有一个可信度，贝叶斯网络也被称作信念网络(belief networks)。       和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述事件之间的相关性。可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。       使用贝叶斯网络必须知道各个状态之间相关的概率。得到这些参数的过程叫做训练。和训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据。比如在训练上面的网络，需要知道一些心血管疾病和吸烟、家族病史等有关的情况。       贝叶斯网络在图像处理、文字处理、支持决策等方面有很多应用。在文字处理方面，语义相近的词之间的关系可以用一个贝叶斯网络来描述。我们利用贝叶斯网络，可以找出近义词和相关的词，在Google 搜索和Google 广告中都有直接的应用。 对于机器翻译可以应用在找出同义词或歧义消除。","title":"统计自然语言处理 之 数学基础"},{"content":"  自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理处理的内容涉及到语言的各个层次，包括字、词、句、段落、篇章和语义。 目前自然语言处理的主要研究和应用方向有： 1、  统计语言模型: 统计语言模型是自然语言处理的主流技术之一。我们研究的主要内容包括各种语言模型的构建、改进以及应用，包括N元文法模型、隐马尔科夫模型、最大熵模型等。 2、  非齐次概率建模：在自然语言处理领域中，各级语言元素（字、词、词性、组块、短句……）因其语法语义属性不同，其可以充当的语言成分不同，因此，其在语言元素序列中出现的位置和范围具有一定的规律性。上述规律性通常对应概率模型中的非齐次性假设，因此又称为语言元素的非齐次属性。语言元素非齐次现象是语言元素的普遍现象，语言元素的非齐次属性是语言元素的本质属性。非齐次概率建模期望将语言元素的非齐次属性进行量化表示并加以利用，从而提高传统概率模型的性能，增强概率模型在自然语言处理各项任务中的应用效果。 3、  汉字处理: 汉字处理解决在计算机及移动设备上输入汉字的问题。研究内容包括音字转换、手写体识别以及键盘输入等问题。 4、  词法分析：词法分析的主要目的是找出词汇的各个词素，从中获得语言学信息。词法分析是很多中文信息处理任务的必要步骤。很多应用，如搜索引擎、机器翻译都需要词法分析的支持。词法分析的主要研究内容包括自动分词、词性标注、歧义消解、新词识别等，采用的方法主要以统计机器学习为主。 5、  命名实体识别：命名实体识别的任务是自动识别文本中的人名、地名、机构名等各种实体。命名实体识别可以提高语言理解的准确性，是信息抽取系统的重要组成部分。命名实体识别的主要研究内容包括识别语料的标注、识别规则的自动抽取、识别模型的构建以及识别特征的自动选取等。 6、  句法分析：句法分析是对句子和短语的结构进行分析。句法分析可分为完全句法分析和浅层句法分析。句法分析是语言学理论和实际的自然语言应用的一个重要桥梁。一个实用的、完备的、准确的句法分析将是计算机真正理解自然语言的基础。 7、  语义分析：在过去，计算语言学研究集中在词法分析和句法分析上，基于规则、基于统计的语法分析技术率先在自然语言处理领域得到广泛运用。目前，随着Internet网络应用的普及和深入，多语种内容信息的理解和处理逐渐受到人们的关注。语义分析的研究，如词义排歧和语义归纳、推理等，开始处于萌芽期并将逐步走向前台，成为下一阶段计算语言学研究的一个亮点。计算机本身没有智能，自然语言的语义分析和内容信息的理解，离不开相应的语义知识库的支持，它是帮助计算机“了解”人类语言的一个媒介和桥梁，也是让计算机逐渐“聪明”起来的一个物质前提。语义分析主要研究基于语义知识库的语义相似度的计算方法、语义知识库的自动构建等内容。 8、  语料库多级加工：语料库语言学是以语料库为基本知识源来研究自然语言规律的学科，其中语料库加工的理论、方法和工具和基于语料库的知识获取是语料库语言学研究的主要内容。语料库是按照一定的原则组织在一起的真实的自然语言数据(包括书面语和口语)的集合，主要用于研究自然语言的规律，特别是统计语言学模型的训练以及相关系统的评价和测试。所谓语料库标注或加工就是对电子语料（包括书面语和口语）进行不同层次的语言学分析，并添加相应的\"显性\"的解释性的语言学信息过程。与不同层次的自然语言分析相对应，语料库的加工主要包括词性标注、句法标注、语义标注、言语标注和语用标注等，由于汉语书写的特殊性，汉语的语料加工还包括分词。   自然语言处理的专业英语： 学习和研究中文分词问题,引起了我对中文分词的极大兴趣,甚至到了无法自拔的地步.我非常希望,能够通过自己的学习和研究,自己开发一套高性能的中文处理系统.但越学习越深入,越发现自己的知识的缺乏.熟练掌握一门编程语言是最基本的,另外涉及到概率论、统计和语言学、汉字编码等诸多复杂问题。因为这些问题同时也是国际上的热点问题，在学习这些内容时不可避免的要遇到一些英语问题。本文将最近所遇到的该领域的专业英语及其含义做一个简单的小结，列举如下。     corpus  语料库。其本义是尸体、文集的意思。记住哦，在统计自然语言处理领域它是语料库的意思。它是文本的集合，这里的“文本”通常是指文本文件，如记事本及其内容。为了便于理解和统计的方便，人们通常会把词汇信息存储在这样的文本中。多个文本便构成一个语料库了。    corpora 语料库集合。它是语料库corpus的复数形式，顾名思义，是指多个文本集合的集合，即多个语料库的集合。    lingusitic competence 语言能力。反映了母语说话者脑海中假设存在的语言结构知识。   linguistic performance 语言性能。它受一系列事物的影响，例如记忆的局限性和环境的传递噪音。   parse 语法分析。给定一个合理的语法，对一个标准的自然语句进行句法分析，句法分析的结果就是parse。   Wordnet 词网。是一个英语电子词典。词汇被组织到一个网络层次中。每个节点由相近意思的词集组成。   word token 词次。指文本的长度，例如在某个语料库中，其文本包含有71370个词次（token）。   word type 词形。指文本中出现多少个不同单词的个数（在英文中是这样，在中文中有所区别）。  根据token和type，我们可以计算其比值，他表示每个type出现的平均次数。   hapax legomena 罕用语。它是希腊语，表示预料库中只出现一次的单词。   bigram 二元组。  KWIC  Keyword In Context。上下文关键词。人们通常用上下文关键词索引程序来产生数据表示，在这样的表示中，所有出现的词汇都被列出，并且词的上下文环境也分别列在它的左右两边。  prior probability 先验概率。  posterior probability 后验概率。  binomial distribution 二项分布。  Bayes optimal decision 贝叶斯最优决策。  mutual information 互信息。  capacity 信道容量。  Perplexity 混乱度。在语音识别领域中，人们通常用混乱度而不是交叉熵来描述一个模型的好坏。  parts of speech ,POS 词性。通常说来词性有三类：名词、动词、形容词。   n-gram n元语法模型。即马尔可夫模型。   stemming 词干化，取词根。   dictionary-based disambiguation 基于词典的消歧。   function fitting 函数拟合，就是说基于一些数据点推断出函数的形态。   Hidden Markov Model， HMM。 隐马尔可夫模型。   rule based 基于规则   corpus based 基于语料库。   conditional probability 条件概率   transitive probability 转移概率   neighboring pairs of words 词语接续对。   maximum likehood estimation 最大似然估计   data sparse 数据稀疏","title":"自然语言处理应用方向和专业英语"},{"content":"IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个领域就没几篇了，象machine learning、computer vision这么大的领域每次大概也就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司的\"IJCAI Inc.\"主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer  & Thoughts Award,   前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member去找 reviewer 来审, 而不象一般会议的PC member其实就是reviewer. 为了制约这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可以给到1+, 也可以给到1-或者2+, 总的来说我给它\"1\". 这是因为它的开法完全受IJCAI制约: 每年开, 但如果这一年的IJCAI在北美举行, 那么就停开. 所以, 偶数年里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些,特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比 IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: \"一小群数学家在开会\". 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. IEEE一直有个倾向, 要把会办成\"盛会\", 历史上已经有些会被它从quality很好的会办成\"盛会\"了. CVPR搞不好也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的    介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会,会开完后第2年才出论文集, 也就是说, NIPS'05的论文集是06年出. 会议的名字是\"Advances in Neural Information Processing Systems\", 所以, 与ICML\\ECML这样的\"标准的\"机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael  Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给\"外人\"的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说,ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选理事, 有资格提名的人包括近三年在ICML\\ECML\\COLT发过文章的人, NIPS则被排除在外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of    Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI)    最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列在tier-1里面, 一方面是名声远不及其他的top conference响亮, 另一方面是相对容易被录用. 但现在它被列在tier-1应该是毫无疑问的事情了.    另: 参见sir和lucky的介绍. UAI (1-): 名字叫\"人工智能中的不确定性\", 涉及表示\\推理\\学习等很多方面, AUAI(Association of UAI) 主办, 每年开. Computer Vision Conf.: Best:       ICCV, Inter. Conf. on Computer Vision      CVPR, IEEE Conference on Computer Vision and Pattern Recognition Good:      ECCV, Euro. Conf. on Comp. Vision      ICIP, Inter. Conf. on Image Processing      ICPR, Inter. Conf. on Pattern Recognition      ACCV Asia Conf. on Comp. Vision  IVCNZ ,Image and Vision Computing New Zealand（IEEE Conferences） ================================================== Computer Vision Jour.:  Best:      PAMI, IEEE Transactions on Pattern Analysis and Machine Intelligence IJCV, International Journal of Computer Vision Good:      CVIU, Computer Vision and Image Understanding PR, Pattern Reco. NIPS， Advances in Neural Information Processing Systems  ICML, International Conference on Machine Learning MM，acm multimedia elsevier系列 image and vision computing journal of visual communication and image processing signal processing signal processing:image communication pattern recognition pattern recognition letters computer vision and image understanding medical image analysis","title":"几个模式识别领域的会议"},{"content":"编者按：做一个好的科学家需要具备什么样的素质呢？微软亚洲研究院互联网搜索与挖掘组高级研究员李航博士总结出了六点特质，并以九名在机器学习、统计学等领域成就卓绝的科学家的真实故事为例，与读者分享这些特质所体现的耐人寻味的品格。 作者：李航   曾经有同学问我：“做一个好的科学家需要有什么样的素质？” 这是一个不好回答的问题。让我总结，大概有以下几点：（1）酷爱研究，（2）勤奋好学，（3）思想深刻，（4）想象丰富，（5）功底扎实，（6）为人谦和。 我接触过许多世界一流的科学家，他们几乎无一例外。下面我就以自己经历过的、听过的，或读过的几个小故事为证，分享给大家，愿与大家共勉。   “数学太有趣了”——伊藤清 伊藤清（Kiyoshi Ito）是当代最伟大的数学家之一。他将微积分方法扩展到随机过程，建立了伊藤微积分。该方法被广泛应用到随机微分方程、金融数学等领域。我在日本京都大学读本科生时，正值伊藤先生从京大退休。去聆听了他的“最终讲义”。伊藤先生报告的结束语让我至今记忆犹新。他说：“我做数学，是因为数学太有趣了。”话音一落，全场响起了经久不息的掌声。   让孩子们喜欢数学——里奥•布雷曼 里奥•布雷曼（Leo Breiman）是杰出的统计学家，曾发明CART与Bagging算法。布雷曼认为现在的数学教育大多是失败的。重要的是要让孩子们喜欢数学，因为数学是非常有趣的。他曾亲自去小学五年级教数学。布雷曼生前回忆那段经历时说：“我们一起做了各种游戏。他们在游戏中学习了解析几何、代数以及其他各种复杂的东西。” [1]   魔术师出身的数学家——佩尔西·戴康尼斯 佩尔西·戴康尼斯（Persi Diaconis）是著名的统计学家。他14岁开始当魔术师，18岁时买了一本概率书，发现根本读不懂，于是24岁时开始在夜校学习数学。后来他在哈佛大学拿到数学博士，成为斯坦福大学教授，还当上了美国科学院院士。戴康尼斯的一个著名研究成果是他证明洗扑克牌七次才能洗干净，不愧是魔术师出身的数学家。听过戴康尼斯的关于排序学习的讲演。印象深的是他很投入，讲演中会时不时地闭上眼睛，完全进入自己的世界。   童心未泯，充满好奇心——拉凯什·阿格拉瓦 拉凯什·阿格拉瓦（Rakesh Agrawal）提出了许多数据挖掘的重要概念与方法，例如关联规则挖掘算法。他说：“我做研究选题时一定考虑会不会对十年以后产生影响。”有一次陪他去北京一家餐厅吃饭。刚进门，就见他放慢了脚步，像小孩一样好奇地探着头，凑到旁边的玻璃柜前。原来那里摆着许多穿着民族服装的玩偶。拉凯什在那儿端详了半天，用带着印度口音的英语说：“It is nice （这很漂亮）.”，脸上露出了开心的笑容。像许多科学家一样，拉凯什也是一个精力旺盛，充满好奇心的人。   每晚睡前读一篇论文——迈克尔·乔丹 迈克尔·乔丹（Michael I. Jordan）是机器学习领域最有影响的学者之一，贝叶斯学习的代表人物。乔丹培养出了许多优秀的学生，他和学生们做的许多工作都是里程碑性的。乔丹并非出身名校，但是靠自己的聪明才智与刻苦努力，成为麻省理工学院(MIT)、伯克利的教授，2011年还当选为美国科学院院士。他多年来养成了一个习惯——晚上就寝前一定要读一篇论文。   一切发现都在情理之中——罗伯特·夏皮尔 罗伯特·夏皮尔（Robert Schapire）因为发明了著名的机器学习算法AdaBoost与Yoav Freund一起获得了计算理论界最高奖歌德尔奖。我曾问罗伯特：“你们当初是怎么想到AdaBoost的？”他的回答非常简单：“因为我们之前证明了可以把弱分类器提升为强分类器，所以后来就想到了AdaBoost。”好像一切都在情理之中。他的表情也像平时一样从容淡定。一流科学家思想都达到了极高境界，很多发明发现都是他们通过自然推理，而非灵光一现，产生出来的。   Idea如泉涌，当今的爱因斯坦——爱德华·威滕 有人说爱德华·威滕（Edward Witten）是有史以来最伟大的理论物理学家，还有人说他是当今的爱因斯坦。威滕建立和发展了诸多物理学理论，特别是超弦理论（Superstring Theory）。他曾撰写了两百多篇论文，在物理学中论文引用H指数排名第一。他获得过多个大奖，包括数学的菲尔兹奖。威滕思维敏捷，经常会思若泉涌，想出很多idea。正因如此，他也从不吝惜自己的idea，只将最好的idea付诸于研究。他说：“我年轻的时候，每天早上起来都会有一种感觉，今天会有从来没有过的好idea。”[2]   深厚的理论功底——彼得·巴特利特 彼得·巴特利特（Peter Bartlett）创立了学习泛化能力的分析手法，是机器学习理论的领军人物。彼得的机器学习讲义思路清晰、证明严谨、叙述简明，读起来让人觉得是一种享受。有一次，彼得儿子的学校请他给小学生们讲一堂课，内容是微积分！他说花了很大功夫备课，但也可见他理论功底之深。   绅士风度，为人谦和——罗斯·昆兰 罗斯·昆兰（Ross Quinlan） 2011年获得了数据挖掘领域最高荣誉奖KDD创新奖。十多年前听过昆兰先生的报告。即将结束时，一位听众冒昧地问：“什么是方差？”这位相貌酷似林肯的澳大利亚绅士并没有显出不耐烦，相反他耐心地解释说：“我不是统计学家。方差是......”八十年代末九十年代初，当人们还在用手工的方法构建专家系统时，昆兰发明了著名决策树学习算法ID3、C4.5，闯出一条数据驱动、统计学习的新路，实在难能可贵。   参考文献 [1] News at UC Berkeley, http://berkeley.edu/news/media/releases/2005/07/07_breiman.shtml. [2] The Puzzle of Genius, Newsweek 1993, http://www.thedailybeast.com/newsweek/1993/06/27/the-puzzle-of-genius.html. (本文图片均来自于网络) 作者介绍 李航博士： 微软亚洲研究院互联网搜索与挖掘组高级研究员及主任研究员。李航博士是日本京都大学电气工程系毕业，日本东京大学计算机科学博士。曾任职于日本NEC公司中央研究所，2001年至今任职于微软亚洲研究院，现任高级研究员及主任研究员。北京大学，南开大学，西安交通大学客座教授。李航的研究方向包括信息检索，自然语言处理，统计机器学习，及数据挖掘。李航一直活跃在相关学术领域，并在重要国际学术杂志和国际学术会议上发表过近百篇学术论文。","title":"怎样成为一名优秀的科学家"},{"content":"  线程也有上下文，当线程被抢先时，必定发生线程的上下文切换。线程没有地址空间，它是包含在进程的地址空间之中。 实际上线程上下文只包含一个堆栈、一个寄存器组和优先权，寄存器组包含程序或者指令指针以及堆栈指针。   一个进程中的所有线程称为同位体(peer)，所有线程共享进程中的资源，进程不拥有任何资源。由任何线程创建的任何资源都可以被他的同位体共享，线程可以在进程中挂起、恢复和终止其他线程。          某进程的逻辑布局             具有次线程的某进程的逻辑布局                                          进程的逻辑布局与线程的逻辑布局的对比 多线程实现的一个典型的例子是执行自然语言处理（natural language processing, NLP）。   线程与进程的区别P52 线程没有自己的address space，如果进程创建多个线程所有线程都将包好在它的地址空间之中，这是为什么它们如此容易共享资源。对变量的修改不会影响父进程的数据。为了让父子进程间共享内存，必须创建一个共享内存区域，父子进程必须使用进程间通信机制（如管道）在两者通信和传输数据。   进程内的线程被看做同位体，而且处于相等的级别。不管是哪个线程创建了哪个线程，进程内的任何线程都可以销毁、挂起、恢复或者更改其他线程的优先权，线程也对要整个进程施加控制。进程内的任何线程可以通过销毁主线程来销毁该进程。销毁主线程将导致该进程的所有线程都被销毁   对进程优先权的更改将改变进程内继承了优先权但是没有作修改的所有线程的优先权。   子进程不能对父进程施加控制；   线程的优点： 1、  当进程有许多并发子任务时，多线程就可以提供子任务的异步执行。这种方式对上下文切换所花的开销相对较少，而并发多进程（一进程一线程）则需要上下文的切换的开销较大。 2、  多线程可以增加应用程序的吞吐能力。通过一个线程，一个I/O请求就可以停止整个进程，通过多线程，当一个线程等待I/O请求时，程序可以继续执行，当一个线程阻塞时，另一个线程可以执行 3、  线程不需要子任务间的特殊通信机制。线程可以在任务之间轻松的传递与接收数据，节省系统资源。   线程的缺点： 1、  进程是孤立的。一个线程可能产生影响其他线程的不良数据，线程导致的错误比进程产生的错误付出的代价将会更大。 2、  进程更孤立。一个应用程序可以将任务分为多个进程来做，这些进程打包成模块，这些模块可以用于其他的应用程序之中，其他的应用程序中的模块可以导入新的应用程序，而线程不能退出到创建它的进程之外的。 3、  进程可以保护资源不被其他进程随意访问，线程与进程中所有线程共享资源。、       参数结构和线程ID不应该是局部声明变量。在新创建线程的生存期内，调用线程可能会约出作用域，如果这种情况发生，线程的堆栈将被释放，而且销毁变量。   进程内的一个线程可一个强迫另一个线程终止。线程不能强迫自己终止。   分离进程为异步子进程，它不继承父进程的任何属性，他们用作后台进程，不需要键盘输入或者屏幕输出。分离进程在终止时不反悔到父进程，分离进程在这一点与分离进程相似。   远程线程：     改变线程优先权 高优先类线程比较低优先类线程接受更多的处理器时间，因为他们执行的时间更加频繁些，看起来这对低优先类的线程不公平，较高优先类线程主宰处理器，剥夺其他优先类的线程的处理器时间，成为线程饥饿。    线程的优先权包括一个优先类（priority class）和一个优先级（priority level）。","title":"《C++面向对象多线程编程》笔记--1"},{"content":"转自：http://www.zhizhihu.com/html/y2011/3489.html 刚看完HMM，因为有个ME-HMM方法，所以再看看最大熵模型，最后再把CRF模型看看，这一系列理论大体消化一下，补充一下自己的大脑，方便面试什么的能够应付一些问题。 多读书，多思考，肚子里才有东西。 ========== 什么是熵？咱们这里只看信息以及自然界的熵吧。《Big Bang Theory》中Sheldon也经常把这个熵挂在嘴边。在咱们的生活中，你打碎了一块玻璃，或者洒落了一盒火柴，很自然的事情就是玻璃碎的一塌糊涂，根本没有规律可言。火柴也是，很乱，你难道从中找到规律么？规律是什么东西？规律的反面是什么？其实很有意思的事情就是自然界的东西尽可能的互补以及平衡，火柴很乱，那就规律性很小。 乱+序=1. 既然=1，那么这个乱也能描述啦？这就是熵的概念，熵是描述事物无序性的参数，熵越大则无序性越强。 我们更关注的是信息熵，怎么用熵来描述信息，不确定性等等。怎么用数学式子进行形式化描述呢？前人已经做了很多工作了： 设随机变量ξ ，他有A1 、A2 ....An 共n 个不同的结果，每个结果出现的概率为p1 ，p2 ....pn ，那么ξ 的不确定度，即信息熵为： H(ξ)=∑i=1npilog1pi=−∑i=1npilogpi 熵越大，越不确定。熵为0，事件是确定的。例如抛硬币，每次事件发生的概率都是1/2的话，那么熵=1：H(X)=-(0.5log0.5+0.5log0.5)=1。 那么这个式子是怎么来的呢？为什么会用log表示？我也不知道啊，查查文献。不过【参考5】中举了几个简单的例子来说明一下过程，这里引用下。 ========== 例子：称硬币的问题，说有5个硬币，其中有一个是假的，这个假硬币的重量很轻，所以打算用一个天平称称，问需要最少称几次就能够保证把这个假硬币给找出来？这个问题其实是一个很经典的问题，也有另外一个类似的问题是毒水和白鼠的问题，5瓶水其中一瓶有毒，用最少几只白鼠能够保证把毒水找出来？ 其实这个问题有个统一的解法就是对半分呗，二叉树，二进制等。 拿硬币的例子，可以取四个放在天平两端，如果相等那么剩下的那个就是假的。如果不相等，把轻的一端的两个硬币再称一次就知道假的了。因为这样称两次就能够保证把假硬币找出来了。这里称的事件是有三个结果的：左边重、相等、右边重。 拿小白鼠的例子，小白鼠只有活着和中毒两种状态，咱们这里人性一点儿，有解药可以解毒的，只要实验达到目的就行。那么把水分成两组，一组两瓶，一组三瓶，让一只小白鼠和一组，如果中毒，假设是三瓶的那一组，那么再递归的讲这三瓶分组，最坏情况下是用3只小白鼠。这里小白鼠的事件只有两个结果：中毒、健康。 我们假设x是那瓶毒水的序号，x∈X={1,2,3,4,5} ，y是第i只小白鼠的症状，y∈Y={1,2} ,，1表示健康，2表示中毒。 用二进制的思想的话就是设计编码y1y2...yn 使他能够把x全部表示出来。因为一个y只有两个状态，所以要有三个y并列起来才能表示2×2×2=23=|Y|3=8>5 。所以是用三只小白鼠。上面称硬币的问题由于一个y可以表示三个状态，所以需要两个3∗3=9>5 就可以表示完所有的x了。 思想是这样的，从上面的分析可以看出，我们只用到的是x ，y 的状态，而没有用x ，y 的内容以及意义。也就是说只用了X 的“总不确定度”以及Y 的“描述能力”。 拿小白鼠和毒水的例子，X 的\"总不确定度\":H(X)=log|X|=log5 。Y 的“描述能力”为：H(Y)=log|Y|=log2 。 所以至少要用多少个Y才能够完全准确的把X表示出来呢？ H(X)H(Y)=log5log2=2.31 所以得用三只小白鼠。称硬币那个问题由于Y 的表示能力强啊，log3 的表示能力，所以表示X 的时候仅仅需要1.46的y就行了，所以就是称2次。 那么为什么用log 来表示“不确定度”和“描述能力”呢？前面已经讲过了，假设一个Y 的表达能力是H(Y) 。显然，H(Y) 与Y 的具体内容无关，只与|Y| 有关。所以像是log|Y|n 这种形式，把n就可以拿出来了，因为关系不大所以扔掉n就剩下log|Y| 了。 “不确定度”和“描述能力”都表达了一个变量所能变化的程度。在这个变量是用来表示别的变量的时候，这个程度是表达能力。在这个变量是被表示变量的时候，这个程度是不确定度。而这个可变化程度，就是一个变量的熵（Entropy）。显然：熵与变量本身含义无关，仅与变量的可能取值范围有关。 ========== 下面看称硬币以及小白鼠毒水问题的一个变种： （1）已知第一个硬币是假硬币的概率是三分之一；第二个硬币是假硬币的概率也是三分之一，其他硬币是假硬币的概率都是九分之一。（2）毒水也是，第一瓶是毒水的概率是1/3。。。以此类推。 最后求称次数或者小白鼠数量n的期望。因为第一个、第二个硬币是假硬币的概率是三分之一，比其他硬币的概率大，我们首先“怀疑”这两个。第一次可以把这两个做比较。成功的概率是三分之二。失败的概率是三分之一。如果失败了，第二次称剩下的三个。所以，期望值是： 13×log3log3+13×log3log3+19×log9log3+19×log9log3+19×log9log3=43 小白鼠的也可以同理求出来。为什么分子会有log3 、log9 呢？其实分子的log3 、log9 表示的都是“不确定度”。事件发生的确定性为1/3，那么不确定度可以理解为log3=log11/3 ，再除以y的“表达能力”，就是每一次猜测的输出结果了，再根据期望公式∑ixipi 就可以求一下期望。不知道理解的对不对？ ========== 更广泛的，如果一个随机变量x的可能取值为X={x1,x2,...,xk} ，要用n位y:y1y2...yn 表示出X来，那么n的期望是： ∑i=1kp(x=xi)log1p(x=xi)log∣∣Y∣∣=∑i=1kp(x=xi)log1p(x=xi)log∣∣Y∣∣ 其实分子式不确定度，分母就是表达能力。那么X 的信息量为： H(X)=∑i=1kp(x=xi)log1p(x=xi) 这就是熵的定义了是吧？我们就算凑出来了。X的具体内容跟信息量无关，我们只关心概率分布，于是H(X)可以写成： H(X)=∑i=1kp(x)log1p(x) ========== 有时候我们知道x,y变量不是相互独立的，y的作用会影响x的发生，举个例子就是监督学习了，有了标记y之后肯定会对x的分布有影响，生成x的概率就会发生变化，x的信息量也会变化。那么此时X的不确定度怎么表示呢？ H(X|Y)=∑(x,y)∈X×Yp(x,y)log1p(x∣∣y) 这个其实就是条件熵Conditional Entropy。很显然，Y加入进来进行了标记之后，就引入了知识了，所以会减小X的不确定性，也就是减小了熵。所以知识能够减小熵。 那么有了部分标记，我们就有了知识，就可以预测一部分模型，这个模型对未知的知识还是保留着熵，只是这个熵被减少了。但是我们知道熵越大，数据分布越均匀，越趋向于自然。 所以我们就想，能够弄出个模型，在符合已知知识的前提下，对未知事物不做任何假设，没有任何偏见。也就是让未知数据尽可能的自然。这就是最大熵模型(Maximum Entropy Models)了。 ========== 【【未完待续：MaxEnt: 最大熵模型(Maximum Entropy Models)(二)】】     上面《MaxEnt: 最大熵模型(Maximum Entropy Models)(一)》其实就是讲了下熵，下面我们继续最大熵模型(Maximum Entropy Models)。 最大熵原理指出，当我们需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。说白了，就是要保留全部的不确定性，将风险降到最小。----摘自《Google黑板报》作者：吴军 link ======== 继续用【参考5】的例子。 “学习”这个词可能是动词，也可能是名词。可以可以被标为主语、谓语、宾语、定语…… 令x1 表示“学习”被标为名词， x2 表示“学习”被标为动词。令y1 表示“学习”被标为主语， y2 表示被标为谓语，y3 表示宾语， y4 表示定语。得到下面的表示： p(x1)+p(x2)=1∑i=14p(yi)=1 如果没有其他的知识，根据信息熵的理论，概率趋向于均匀。所以有： p(x1)=p(x2)=0.5p(y1)=p(y2)=p(y3)=p(y4)=0.25 但是在实际情况中，“学习”被标为定语的可能性很小，只有0.05。我们引入这个新的知识：p(y4)=0.05 ，在满足了这个约束的情况下，其他的事件我们尽可能的让他们符合自然，符合均匀分布： p(x1)=p(x2)=0.5p(y1)=p(y2)=p(y3)=0.953 嗯，如果再加入一个知识，当“学习”被标作动词的时候，它被标作谓语的概率为0.95，这个其实是很自然的事情。都已经是动词了，那么是谓语的可能性就很大了： p(y2|x1)=0.95 已经有了两个知识了，第二个还是条件概率的知识，那么其他的我们尽可能的让他们不受约束，不受影响，分布的均匀一些，现在应该怎么让他们符合尽可能的均匀分布呢？ 其实就是使熵尽可能的大就行了。也就是有个分布p，他尽可能的把训练集中的知识表示出来，损失最小，并且还能够保证p的熵最大： p∗=argmaxpH(p) 那约束是什么呢？ 用概率分布的极大似然对训练语料表示如下，其中是Count(x,y) 在语料中出现的次数，N为总次数： p¯(x,y)=1N×Count(x,y) 在实际问题中，由于条件x和结果y取值比较多样化，为模型表示方便起见，通常我们将条件x和结果y表示为一些二制特征。对于一个特征(x0,y0) ，定义特征函数： f(x,y)={1:y=y0&x=x00:others 特征函数在样本中的期望值为： p¯(f)=∑(xi,yi)p¯(xi,yi)f(xi,yi) 其中p¯(x,y) 在前面已经数了，数数次数就行。 有了训练集，我们就能够训练一个模型出来，特征f在这个模型中的期望值为： p(f)=∑(xi,yi)p(xi,yi)f(xi,yi)=∑(xi,yi)p(yi|xi)p(xi)f(xi,yi)=∑(xi,yi)p(yi|xi)p¯(xi)f(xi,yi) 其中p¯(xi) 为x出现的概率，数数归一化就行。 好了，约束来了，有了样本的分布，有了模型，那么对每一个特征(x,y)，模型所建立的条件概率分布要与训练样本表现出来的分布相同： p(f)=p¯(f) ========== 目标函数有了，约束有了，归纳一下最大熵模型(Maximum Entropy Models)。 p∗=argmaxp∈PH(Y|X) P={p|p是y|x的概率分布并且满足下面的条件}，对训练样本，对任意给定的特征fi ： p(fi)=p¯(fi) 展开： p∗=argmaxp∈P∑(x,y)p(y|x)p¯(x)log1p(y∣∣x) 约束P为： P=⎧⎩⎨⎪⎪⎪⎪p(y|x)∣∣∣∣∣∀fi:∑(x,y)p(y|x)p¯(x)fi(x,y)=∑(x,y)p¯(x,y)fi(x,y)∀x:∑yp(y|x)=1⎫⎭⎬⎪⎪⎪⎪ ======== 都齐了，该求解了吧？哈哈，有没有看过wiki上的关于拉格朗日乘子Lagrange Multiplier的问题，恰好这里面有个例子就是求最大熵的，哈哈。所以我们可以用拉格朗日乘子法来求解。 对于有k个约束的优化问题：   maxH(p)s.t.:Ci(p)=bi,i=1,2,...,k   可以引入k个拉格朗日算子   Λ=[λ1,λ2,...,λk]T   ，那么拉格朗日函数为：   L(p,λ)=H(p)+∑i=1kλi[Ci(p)−bi]   OK，咱们开始一步一步的带入求解∂L∂p=0 。 由于约束中有两部分组成，对于第二部分，我们引入拉格朗日算子为γ :   L(p,Λ,γ)=∑(x,y)p(y|x)p¯(x)log1p(y|x)+    ∑i=1kλi∑(x,y)(p(y|x)p¯(x)fi(x,y)−p¯(x,y)fi(x,y))+   γ⎛⎝∑yp(y|x)−1⎞⎠   下面就是就偏微分=0计算最优解了：   ∂L∂p(y|x)=p¯(x)(log1p(y|x)−1)+∑i=1kλip¯(x)fi(x,y)+γ=0   求得最优解的参数形式：   p∗(y|x)=e∑iλifi(x,y)+γp¯(x)−1   但是里面还有参数，所以我们必须求得γ∗ 和Λ∗ 。 巧妙的是我们发现最后节的后面部分有个类似的常数项：   e∑iλifi(x,y)+γp¯(x)−1=e∑iλifi(x,y)eγp¯(x)−1   而且有意思的是，前面问题的第二个约束中有：   ∀x:∑yp(y|x)=1   从而：   ∑yp∗(y|x)=∑y(e∑iλifi(x,y)eγp¯(x)−1)=1eγp¯(x)−1∑ye∑iλifi(x,y)=1eγp¯(x)−1=1∑ye∑iλifi(x,y)=Z(x)   也就是式子中的关于γ 的常数项我们用关于Λ 的常数项进行代替了，这样参数就剩下一个了：   p∗(y|x)=Z(x)e∑iλifi(x,y)Z(x)=1∑ye∑iλifi(x,y)   那么剩下的一个参数Λ 应该怎么进行求解呢？ 解法以及最大熵模型与最大似然估计的关系在参考5中很详细，还有GIS以及IIS的方法进行求解，以后再写《MaxEnt: 最大熵模型(Maximum Entropy Models)(三)》。 有什么问题请留言。 参考： 1、A maximum entropy approach to natural language processing (Adam Berger) 2、A Brief MaxEnt Tutorial (Adam Berger) 3、Learning to parse natural language with maximum entropy models (Adwait Ratnaparkhi) 4、中科院刘群教授《计算语言学-词法分析（四）》 5、《最大熵模型与自然语言处理》：laputa，NLP Group, AI Lab, Tsinghua Univ.  ","title":"MaxEnt: 最大熵模型(Maximum Entropy Models)"},{"content":"怎样成为一名优秀的科学家？   (2011-10-25 23:02:57) 转载 标签：  优秀科学家   机器学习   统计学   数学   勤奋   扎实   深刻   想象力   谦和 it 分类： 人才 编者按：做一个好的科学家需要具备什么样的素质呢？微软亚洲研究院互联网搜索与挖掘组高级研究员李航博士总结出了六点特质，并以九名在机器学习、统计学等领域成就卓绝的科学家的真实故事为例，与读者分享这些特质所体现的耐人寻味的品格。 作者：李航   曾经有同学问我：“做一个好的科学家需要有什么样的素质？” 这是一个不好回答的问题。让我总结，大概有以下几点：（1）酷爱研究，（2）勤奋好学，（3）思想深刻，（4）想象丰富，（5）功底扎实，（6）为人谦和。 我接触过许多世界一流的科学家，他们几乎无一例外。下面我就以自己经历过的、听过的，或读过的几个小故事为证，分享给大家，愿与大家共勉。   “数学太有趣了”——伊藤清 伊藤清（Kiyoshi Ito）是当代最伟大的数学家之一。他将微积分方法扩展到随机过程，建立了伊藤微积分。该方法被广泛应用到随机微分方程、金融数学等领域。我在日本京都大学读本科生时，正值伊藤先生从京大退休。去聆听了他的“最终讲义”。伊藤先生报告的结束语让我至今记忆犹新。他说：“我做数学，是因为数学太有趣了。”话音一落，全场响起了经久不息的掌声。   让孩子们喜欢数学——里奥•布雷曼 里奥•布雷曼（Leo Breiman）是杰出的统计学家，曾发明CART与Bagging算法。Breiman认为现在的数学教育大多是失败的。重要的是要让孩子们喜欢数学，因为数学是非常有趣的。他曾亲自去小学五年级教数学。Breiman生前回忆那段经历时说：“我们一起做了各种游戏。他们在游戏中学习了解析几何、代数以及其他各种复杂的东西。” [1]   魔术师出身的数学家——佩尔西·戴康尼斯 佩尔西·戴康尼斯（Persi Diaconis）是著名的统计学家。他14岁开始当魔术师，18岁时买了一本概率书，发现根本读不懂，于是24岁时开始在夜校学习数学。后来他在哈佛大学拿到数学博士，成为斯坦福大学教授，还当上了美国科学院院士。戴康尼斯的一个著名研究成果是他证明洗扑克牌七次才能洗干净，不愧是魔术师出身的数学家。听过戴康尼斯的关于排序学习的讲演。印象深的是他很投入，讲演中会时不时地闭上眼睛，完全进入自己的世界。   童心未泯，充满好奇心——拉凯什·阿格拉瓦 拉凯什·阿格拉瓦（Rakesh Agrawal）提出了许多数据挖掘的重要概念与方法，例如关联规则挖掘算法。他说：“我做研究选题时一定考虑会不会对十年以后产生影响。”有一次陪他去北京一家餐厅吃饭。刚进门，就见他放慢了脚步，像小孩一样好奇地探着头，凑到旁边的玻璃柜前。原来那里摆着许多穿着民族服装的玩偶。拉凯什在那儿端详了半天，用带着印度口音的英语说：“It is nice （这很漂亮）.”，脸上露出了开心的笑容。像许多科学家一样，拉凯什也是一个精力旺盛，充满好奇心的人。   每晚睡前读一篇论文——迈克尔·乔丹 迈克尔·乔丹（Michael I. Jordan）是机器学习领域最有影响的学者之一，贝叶斯学习的代表人物。乔丹培养出了许多优秀的学生，他和学生们做的许多工作都是里程碑性的。乔丹并非出身名校，但是靠自己的聪明才智与刻苦努力，成为麻省理工学院(MIT)、伯克利的教授，2011年还当选为美国科学院院士。他多年来养成了一个习惯——晚上就寝前一定要读一篇论文。   一切发现都在情理之中——罗伯特·夏皮尔 罗伯特·夏皮尔（Robert Schapire）因为发明了著名的机器学习算法AdaBoost与Yoav Freund一起获得了计算理论界最高奖歌德尔奖。我曾问罗伯特：“你们当初是怎么想到AdaBoost的？”他的回答非常简单：“因为我们之前证明了可以把弱分类器提升为强分类器，所以后来就想到了AdaBoost。”好像一切都在情理之中。他的表情也像平时一样从容淡定。一流科学家思想都达到了极高境界，很多发明发现都是他们通过自然推理，而非灵光一现，产生出来的。   Idea如泉涌，当今的爱因斯坦——爱德华·威滕 有人说爱德华·威滕（Edward Witten）是有史以来最伟大的理论物理学家，还有人说他是当今的爱因斯坦。威滕建立和发展了诸多物理学理论，特别是超弦理论（Superstring Theory）。他曾撰写了两百多篇论文，在物理学中论文引用H指数排名第一。他获得过多个大奖，包括数学的菲尔兹奖。威滕思维敏捷，经常会思若泉涌，想出很多idea。正因如此，他也从不吝惜自己的idea，只将最好的idea付诸于研究。他说：“我年轻的时候，每天早上起来都会有一种感觉，今天会有从来没有过的好idea。”[2]   深厚的理论功底——彼得·巴特利特 彼得·巴特利特（Peter Bartlett）创立了学习泛化能力的分析手法，是机器学习理论的领军人物。彼得的机器学习讲义思路清晰、证明严谨、叙述简明，读起来让人觉得是一种享受。有一次，彼得儿子的学校请他给小学生们讲一堂课，内容是微积分！他说花了很大功夫备课，但也可见他理论功底之深。   绅士风度，为人谦和——罗斯·昆兰 罗斯·昆兰（Ross Quinlan） 2011年获得了数据挖掘领域最高荣誉奖KDD创新奖。十多年前听过昆兰先生的报告。即将结束时，一位听众冒昧地问：“什么是方差？”这位相貌酷似林肯的澳大利亚绅士并没有显出不耐烦，相反他耐心地解释说：“我不是统计学家。方差是......”八十年代末九十年代初，当人们还在用手工的方法构建专家系统时，昆兰发明了著名决策树学习算法ID3、C4.5，闯出一条数据驱动、统计学习的新路，实在难能可贵。   参考文献 [1] News at UC Berkeley, http://berkeley.edu/news/media/releases/2005/07/07_breiman.shtml. [2] The Puzzle of Genius, Newsweek 1993, http://www.thedailybeast.com/newsweek/1993/06/27/the-puzzle-of-genius.html. (本文图片均来自于网络) 作者介绍 李航博士： 微软亚洲研究院互联网搜索与挖掘组高级研究员及主任研究员。李航博士是日本京都大学电气工程系毕业，日本东京大学计算机科学博士。曾任职于日本NEC公司中央研究所，2001年至今任职于微软亚洲研究院，现任高级研究员及主任研究员。北京大学，南开大学，西安交通大学客座教授。李航的研究方向包括信息检索，自然语言处理，统计机器学习，及数据挖掘。李航一直活跃在相关学术领域，并在重要国际学术杂志和国际学术会议上发表过近百篇学术论文。","title":"怎样成为一名优秀的科学家？（转）"},{"content":"    　1、 英国剑桥大学自然语言和信息处理研究小组Cambridge The Natural Language and Information Processing (NLIP) Research Group, http://www.cl.cam.ac.uk/Research/NL/　 　　2、 英国格拉斯哥大学信息检索研究小组Glasgow Information Retrieval Group, http://www.dcs.gla.ac.uk/ir/ 　　3、 谷歌实验室Google Labs, http://labs.google.com/　 　　4、 卡内基梅隆大学语言技术研究院CMU (Carnegie Mellon University) Language Technologies Institute,LTI, 　　http://www.lti.cs.cmu.edu/　 　　5、 美国马塞诸塞大学智能信息检索中心Massachusetts CIIR, The Center for Intelligent Information Retrieval, http://ciir.cs.umass.edu/　 　　6、 微软亚洲研究院网络检索与数据挖掘小组MSR Asia, Web Search & Data Mining Group http://research.microsoft.com/wsm/　 　　7、 美国斯坦福大学信息实验室Standford InfoLab, http://infolab.stanford.edu/　 　　8、美国伊利诺伊州立大学香槟分校信息检索小组UIUC Information Retrieval Group, 　　http://sifaka.cs.uiuc.edu/ir/　 　　9、北京大学搜索引擎与互联网信息挖掘组（天网组） http://sewm.pku.edu.cn/　 　　10、北京大学计算语言学研究所, http://icl.pku.edu.cn/　 　　11、复旦大学信息检索和自然语言处理组, http://www.cs.fudan.edu.cn/mcwil/irnlp/　 　　12、哈尔滨工业大学信息检索组, http://ir.hit.edu.cn/　 　　13、清华大学智能技术与系统国家重点实验室, http://www.csai.tsinghua.edu.cn/　 　　14、中科院大规模内容计算组, http://159.226.40.18/　 　　15、信息检索专门兴趣小组(Special Interest Group on Information Retrieval,SIGIR) 　　http://www.sigir.org/　 　　16、芬兰赫尔辛基信息技术研究院Helsinki Institute for Information Technology 　　17、IBM研究院信息检索与分析小组The Information Retrieval and Analysis Group 　　http://www.research.ibm.com/irgroup/　 　　18、美国伊利诺伊州技术研究所信息检索实验室The Illinois Institute of Technology Information Retrieval Laboratory 　　http://ir.iit.edu/　 　　19、信息检索研究室Information Retrieval Facility (IRF) http://www.ir-facility.org/　 　　20、微软研究院信息检索与分析小组http://research.microsoft.com/en-us/groups/ir/　 　　21、 美国匹兹堡大学信息科学学院信息检索小组School of Information Science Information Retrieval, University of Pittsburgh 　　http://www2.sis.pitt.edu/~ir/　 　　22、伦敦玛丽皇后大学信息检索小组Information Retrieval Group at the Queen Mary University of London (UK) 　　http://www.dcs.qmul.ac.uk/researchgp/ir/　 　　23、英国谢菲尔德大学信息研究部信息检索小组The IR group in the Information Studies department of the University of Sheffield 　　http://ir.shef.ac.uk/　 　　24、信息检索专家小组 Information Retrieval Specialist Group 　　http://irsg.bcs.org/　 　　25、匈牙利信息检索中心Centre for Information Retrieval (Hungary) 　　http://www.dcs.vein.hu/CIR　 　　26、英国伦敦城市大学交互系统研究中心Centre for Interactive Systems Research at City University, London (UK) 　　http://www.soi.city.ac.uk/is/research/cisr　 　　27、瑞士纳沙泰尔大学信息检索小组Information Retrieval Group at Université de Neuchâtel (Switzerland) 　　http://www2.unine.ch/iiun/page4246.html　 　　28、美国宾夕法尼亚州立大学智能系统研究实验室PSU Intelligent Systems Research Laboratory http://iis.ist.psu.edu/　 　　29、荷兰阿姆斯特丹大学信息和语言处理系统研究小组Information and Language Processing Systems Group at the University of Amsterdam http://ilps.science.uva.nl/　 　　30、西班牙拉科鲁尼亚大学信息检索实验室Information Retrieval Lab at the University of A Coruña (Spain) 　　http://www.dc.fi.udc.es/ir/　 　　31、美国密歇根大学计算语言学和信息检索小组Computational Linguistics and Information Retrieval (CLAIR) Group 　　http://tangra.si.umich.edu/clair/　 　　32、英国国家文本挖掘研究中心National Centre for Text Mining (UK) http://www.nactem.ac.uk/　 　　33、加拿大信息技术研究院National Research Council – Institute for Information Technology (Canada) 　　http://iit-iti.nrc-cnrc.gc.ca/r-d/analysis-info-analyse_e.html　 　　34、 计算机协会信息检索特别兴趣小组Association for Computing Machinery Special Interest Group on Information Retrieval http://www.acm.org/sigs/sigir/　 　　35、 网络分析学联合会Web Analytics Association 　　http://www.webanalyticsassociation.org/index.asp　 　　36、 美国圣何塞州立大学信息科学专业协会 Professional Associations in the Information Sciences 　　http://slisweb.sjsu.edu/resources/orgs.htm　 　　37、 美国信息科学与技术协会ASIS&T: The American Society for Information Science and Technology http://www.asis.org/　 　　38、 计算机语言学学会ACL – Association for Computational Linguistics 　　http://www.aclweb.org/　 　　39、 国际万维网会议筹备委员会 World Wide Web http://www.iw3c2.org/　 　　40、 数字图书馆联盟Digital Library Federation http://www.diglib.org/　   引自:http://jpkc.whu.edu.cn/jpkc2010/xxjs/course/Content.asp?c=27&a=98&todo=show","title":"信息检索资源"},{"content":" 3.2 PAIRS AND STRIPES   在MapReduce程序中同步的一个普遍做法是构建复杂的键和值，通过这种方式使得执行框架可以将需要计算的数据自然的放到一起。我们在之前的章节中涉及到这个技术，即把部分总数和计数“打包”成一个复合值（例如pair），依次从mapper传到combiner再传到reducer。以之前的出版物为基础【54，94】，这节介绍两个常见的设计模式，我们称为pairs（对）和strips（条纹）。   作为一个运行时的例子，我们关注于在大型数据上建立单词同现矩阵，这是语料语言学（corpus linguistics）和统计自然语言处理的共同任务。正式来说，语料库中的同现矩阵是一个在语料库中以n个不同单词（即词汇量）为基础的n×n矩阵。一个mij单元包含单词wi与wj在具体语境（像句子，段落，文档或某些窗口上的m词，m词是应用程序依赖的属性）下共同出现的次数。矩阵的上下三角形是同样的因为同现是一个对称关系，虽然一般来说单词之间的关系不必相对称。例如，一个同现矩阵M，mij是单词wi和单词wj同现的次数，它通常不能均衡。   这个任务在文本处理和为其它算法提供初始数据时很普遍，例如，逐点信息交互的统计，无人监管的辨别聚集，还有很多，词典语义的大部分工作是基于词语的分布式情景模式，追溯到1950年和1960年的Firth [55] 和 Harris [69]。这个任务也可以应用于信息检索（例如，同义词词典构建和填充），另外一些相关的领域例如文本挖掘。更重要的是这些问题代表着一个从大量观测值中的不相关的joint事件的分布的任务的特殊实例，统计自然语言处理的一个共同任务是MapReduce的解决方案。实际上这里展示的观念在第六章讨论最大期望值算法时也会用到。   除了文本处理，很多应用领域的问题都有相同的特性。例如，大的零售商会分析销售点的交易记录来识别出购买的产品之间的关系（例如，顾客们买这个的话就会想买那个），这有助于库存管理和产品在货架上的摆放位置。同样地，一个智能的分析希望分辨重复的金融交易，它将提供恶意买卖的线索。这节讨论的算法可以解决类似的问题。   很明显，单词同现问题的算法复杂度是O(n2)， 其中n是词库大大小，现实中的英语单词全部加起来可能有10万多个，在web规模中甚至达到10亿个。如果把整个单词同现矩阵放到内存中，计算这个矩阵是非常容易的，然而，由于这个矩阵太大以致内存放不下，一种在单机上很慢无经验的实现是把内存保存到磁盘上。虽然压缩计数能够提高单机构建单词同现矩阵的规模，但是它明显存在限制伸缩性的问题。我们会为这个任务提供两个MapReduce算法来使其能适用于大规模的数据集。 1: class Mapper 2:     method Map(docid a, doc d) 3:              for all term w ∈ doc d do 4:                        for all term u ∈ Neighbors(w) do 5:                                 Emit(pair (w, u), count 1) //Emit count for each co-occurrence 1: class Reducer 2:     method Reduce(pair p, counts [c1,c2,...]) 3:              s ← 0 4:              for all count c ∈ counts [c1,c2,...] do 5:                        s ← s + c    //Sum co-occurrence counts 6:               Emit(pair p, count s) 图 3.8: 大数据集中计算单词的同现矩阵的伪代码   图3.8展示了我们称之为“pairs”的第一个算法的伪代码。像往常一样，文档的id和相关的内容组成输入的键值对。Mapper处理每一个输入文档和发送同现词对作为键1（即计数）作为值的中间键值对。这由两个嵌套循环来完成：外循环遍历每一个词语（pair中的左元素），内循环遍历第一个词语（pair中的右元素）的所有邻接词。邻接词可以通过滑动窗口或者句子等文章单元来定义。MapReduce执行框架保证同一键的所有值都会在reducer中集合。因此，在这种情况下reducer只是用同一单词同现键值对获得文档中joint事件的绝对数量来进行简单的统计，这些值将作为最终键值对发送出去。每一个键值对相当于单词同现矩阵的一个值。这个算法说明了使用复杂的键来协调分布式计算。   1: class Mapper 2:     method Map(docid a, doc d) 3:              for all term w ∈ doc d do 4:                         H ← new AssociativeArray 5:                         for all term u ∈ Neighbors(w) do 6:                         H{u}← H{u}+ 1 7:              Emit(Term w, Stripe H) 1: class Reducer 2:     method Reduce(term w, stripes [H1,H2,H3,...]) 3:              Hf ← new AssociativeArray 4:              for all stripe H ∈ stripes [H1,H2,H3,...] do 5:                        Sum(Hf ,H)            //Element-wise sum 6:              Emit(term w, stripe Hf ) 图 3.9:用stripes的方法来计算单词的同现矩阵， 注：element wise就是按元素进行运算，将两个不同矩阵内部的对应元素相乘   图3.9展示了另一个可选的方法---“stripes”方法。和pairs方法一样，同现词的键值对由两个嵌套循环来生成。然而，和之前方法主要的不同是，同现的信息首先被存放在关联数组H中而不是发送每一个同现词对的中间键值对。Mapper用词语作为key并把对应的关联数组作为value发送出去，每一个关联数组记录着某一个词语的相邻元素（如：它的上下文中出现的词语）的同现次数。MapReduce执行框架会使所有相同key的关联数组到reduce阶段一起处理。Reducer根据相同的key来进行统计运算（element-wise sum），积累的计数相当于同现矩阵中的同一个单元（cell）。最后的关联数组以相同的词作为主键发送出去。相比于pairs方法，stripes方法中每个最终键值对包含同现矩阵中的一行。   显然，pairs算法相对stripes算法来说要生成很多键值对。Stripes表现得更加紧凑，因为pairs算法中的左元素代表着每一同现词对。Stripes方法则生成更加少而短的中间键，因此，在框架中执行时不需要太多的排序。但是，stripes中的值更加复杂，也比pairs算法有着更多的序列化与反序列化操作。   这两种算法都得益于使用combiners，因为它们运行在reducers（额外和元素智能的关联数组的个数）的程序都是可交换和可结合的。然而，stripes方法中的combiners有更多的机会执行本地聚集，因为主要是词库占用空间，关联数组能在mapper多次遇到某个单词时被更新。相比之下，pairs的主要占用空间的是它自己和词典相交的空间，一个mapper观察到多次同一同现对时只能计数只能聚集（它和在stripes中观察一个单词的多次出现不同）。   对这两种算法而言，之前章节提到的in-mapper combining优化方法也可以被调用；因为这个修改比较简单我们把它留给读者作为练习。然而，前面提到的警告依然是：因为缺少中间值的存储空间，pairs方法将有比较少的机会做到部分聚集。缺少空间也限制了in-mapper combining的效率，因为在所有文档都被处理之前mapper就有可能已经用完了内存，这样就必须周期地发送出键值对（更多地限制执行部分聚集的机会）。同样地，对stripes方法来说，它的内存的管理对于简单的单词统计例子来说更加复杂。对于常见的词语，关联数组会变得特别大，需要一些方法去周期地清除内存中的数据。   考虑到每个算法潜在的伸缩性瓶颈是重要的。Stripes方法假定，在任何时候，每一个关联数组都要足够小来使之适合内存---否则，内存分页（将内存中的数据暂放到磁盘上）会显著地影响性能。关联数组的大小受限于词典大小，而词典大小和文档的大小无关（回忆之前讨论过的内存不足问题）。因此，当文档的大小增加时，这将成为一个紧迫的问题---可能对于GB级别的数据还没有什么，但可以肯定未来将常见到的TB和PB级别的数据一定会遇到。Pairs方法，在另一方面，没有这种限制，因为它不需要在内存中保存中间数据。   鉴于此讨论，那一个方法更快呢？这里我们重现已发表的结论[94]来回答这个问题。我们在Hadoop中执行了这两个算法，并把它们应用在美联社Worldstream栏目（APW）中总计5.7GB的由2.27百万个文档组成的文档集中。在Hadoop中运行之前，文档集需先做下面处理：把所有XML标记移除，然后是用Lucene搜索引擎提供的基本工具来做分词和去除停止词。为了更有效的编码所有分词都被唯一的整数代替。图3.10对比了pairs和stripes在同一文档集中运行时的不同分数，这个实验是执行在有19个节点的Hadoop集群中，每个节点有一个双核处理器和两个磁盘。 图3.10 这个结果说明了stripes方法比pairs方法要快很多：处理5.7GB的数据分别用666秒（约11分钟）和3758秒（约62分钟）。Pairs方法中的mappers生成26亿个总计31.2GB的中间键值对。经过combiners处理后，减少到11亿个键值对，这确定了需要通过网络传输的中间数据的数据量。最后，reducers总共发送1.42亿个最终键值对（同现矩阵中不为零的值的数量）。在另一方法，Pairs方法中的mappers生成6.53亿个总计48.1GB的中间键值对。经过combiners处理后，只剩0.288亿个键值对，最后reducers总共发送169万个最终键值对（同现矩阵中的行数）。像我们期望那样，stripes方法提供更多的机会来让combiners聚集中间结果，因此大大的减少了清洗（shuffle）和排序时的网络传输。图3.10也看到了两种算法展现出的高伸缩性---输入数据数量的线性。这由运行时间的线性回归决定，它产生出的R2 值接近1。     图 3.11:（左边）用不同规模的EC2服务器组成的Hadoop集群来测试stripes算法在APW文集中执行的时间。（右边）根据增加Hadoop集群的规模得到的标度特征（相对的运行速度的提升）。   额外的一系列实验探索了stripes方法另一方面的伸缩性：集群的数量。这个实验可以用亚马逊的EC2服务来做，它允许用户很快地获得不同数量的集群。EC2中的虚拟化计算单元被称为实例，用户根据实例的使用时间来交费。图3.11（左）展示了stripes算法的时间（同一数据集，与之前相同的设置），在不同数量的集群上，从20个节点的“小”实例到80个节点的实例（沿着x坐标）。运行时间由实心方块标示。图3.11（右）重构同样的结果来说明伸缩性。圆圈标出在EC2实验中规模的大小和增速，关于20个节点的集群。这个结果展示了非常理想的线性标度特征（即加倍集群数量使任务时间加快一倍）。这由线性回归中R2值接近1决定的。   从抽象层面看，pairs和stripes算法代表两个计算大量观测值中的重现事件的不同方法。这两个算法抓住了很多算法的特点，包括文本处理，数据挖掘和分析复杂生物资料。由于这个原因，这两种设计模式可以广泛而且频繁地用在不同的程序中。 总的来说，pairs方法分别记录每个同现事件，stripes方法记录所有拥有不同条件事件的同现事件。中间可能被记录的同现事件的一个子集。我们把整个词典拆分成b个部分（即通过哈希查找），wi的同现词会分成b个小的“子stripes”，与10个不同的键分开(wi; 1)， (wi; 2) …(wi; b)。这是应对stripes方法中内存限制的合理方法，因为每个子stripes会更小。对b=|V|而言，|V|是词典大小，这与pairs方法是相同的。对b=1而言，这与标准的stripes方法相同。","title":"Data-Intensive Text Processing with MapReduce 第三章（2）——PAIRS AND STRIPES"},{"content":"  通信类权威会议， A类会议：本学科最顶尖级水平的国际会议； B类会议：学术水平较高、组织工作成熟、按一定时间间隔系列性召开的国际会议。 A类会议（序号不表示优先顺序） 序号/英文名称/英文简称/中文名称/备注 1    IEEE International Conference on Acoustics, Speech and Signal Processing/ ICASAP/   IEEE声学、语音和信号处理国际会议     2    IEEE International Conference on Image Processing/ ICIP/    IEEE图像处理国际会议     3   International Conference on Pattern Recognition/    ICPR/   模式识别国际会议     4   IEEE International Conference on Communications/    ICC/   IEEE通信国际会议    （这个和下面经常被老师挂在嘴边，对我们来说只是传说啦） 5    IEEEGlobal Telecommunications Conference/    Globecom/    IEEE全球电信会议     6    IEEEInternational Conference on Intelligent TransportationSystem/    ITSC/   IEEE智能交通系统国际会议     7   Annual IEEE Conference on Computer Communications/    IEEEINFOCOM/    IEEE计算机通信会议    8    IEEERadar Conference/    IEEE雷达会议     B类会议（序号不表示优先顺序） 序号/英文名称/英文简称/中文名称/备注 1   International Conference On Natural Language Processing/   ICON/   自然语言处理国际会议     2   International Conference on Telecommunications/　ICT/    电信国际会议     3   International Geoscience and Remote Sensing Symposium/   IGARSS/   地球科学与遥感国际研讨会     4   Picture Coding Symposium/ PCS/   图像编码研讨会     5   ACM Conference on Computer and Communications Security/ CS/   ACM计算机与通信安全会议  6    IEEEMilitary Communications Conference/    MILCOM/   IEEE军事通信会议     7   International Broadcasting Convention/    IBC/   国际广播会议     8   IEEE Wireless Communications & Networking Conference/   WCNC/    IEEE无线通信和网络会议    9    SPIEConference on Visual Communications and Image Processing/   VCIP/    SPIE视觉通信和图像处理会议     10   International Symposium on Wireless Personal MultimediaCommunications/    WPMC/    无线个人多媒体通信国际研讨会    11   IEEE International Conference on Third Generation Wireless and Beyond/3G andBeyond/IEEE第三代及以上无线通信国际会议   12    ACMMobicom/    ACM/移动通信会议     13   International Conference on Network Protocol/ ICNP/   网络协议国际会议     14   IEEE Speech Coding Workshop     15   International Conference on Speech and Language Processing/   ICSLP/   语音语言处理国际会议 16   International Symposium on Chinese Spoken LanguageProcessing/    ISCSLP/   中文口语语言处理国际会议 17   MOBI COM & MOBI HOC/    移动Ad hoc移动通信会议/    Ad hoc的顶级年会 18   Vehicular Technology Conference/ VTC/   国际传输技术会议/    与产业界结合比较紧密的会2次/年 19    ACMConference on Embedded Networked Sensor Systems Sensys/嵌入式网络传感系统/ WSN的顶级年会（Single Track的小会） 20   Global Navigation Satellite Systems/    ION/IEEEGNSS/   全球导航卫星系统会议/    IEEE和美国导航学会联合召开的年会 21   International conference on Radar/   ICR/  英美法中澳五国轮流召开 22   IEEE Conference on Computer Vision and Pattern Recognition/    CVPR/   计算机视觉与模式识别会议 23   IEEE International Conference on Multimedia & Expo/   ICME/   多媒体IEEE 国际会议及展览会/    每年召开 24   IEEE International conference on Computer Vision/   ICCV/   计算机视觉IEEE国际会议/     25   International Conference on Document Analysis and Recognition/ICDAR/文档分析和识别国际会议/文字识别领域最重要的会议，每两年召开一次，07年是第九届 通信一些期刊的影响因子： 1 IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS 1.328  2 IEEE COMMUNICATIONS MAGAZINE 1.291  3 IEEE NETWORK 1.288  4 RADIO SCIENCE 1.059  5 IEEE TRANSACTIONS ON ANTENNAS AND PROPAGATION1.011  6 IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY 0.812  7 IEEE TRANSACTIONS ON COMMUNICATIONS 0.681  8 TELECOMMUNICATIONS POLICY 0.586  9 IEE PROCEEDINGS-OPTOELECTRONICS 0.545  10 BT TECHNOLOGY JOURNAL 0.454  11 IEEE TRANSACTIONS ON ELECTROMAGNETICCOMPATIBILITY 0.421  12 IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONICSYSTEMS 0.381  13 IEE PROCEEDINGS-MICROWAVES ANTENNAS ANDPROPAGATION 0.380  14 IEEE TRANSACTIONS ON BROADCASTING 0.353  15 IEICE TRANSACTIONS ON COMMUNICATIONS 0.350  16 IEE PROCEEDINGS-RADAR SONAR AND NAVIGATION0.313  17 SMPTE JOURNAL 0.265  18 IEEE TRANSACTIONS ON CONSUMER ELECTRONICS 0.233  19 ELECTRONICS & COMMUNICATION ENGINEERINGJOURNAL 0.208  20 ANNALES DES TELECOMMUNICATIONS-ANNALS OFTELECOMMUNICATIONS 0.105  21 JOURNAL OF COMMUNICATIONS TECHNOLOGY ANDELECTRONICS 0.084","title":"通信类重要学术会议以及期刊"},{"content":"第一部分 1. Google&Wiki（遇到问题做的第一件事情，也是学习某个东西做功课（homework）最先用到的东西。 2. 看书挑剔，只看经典。如何选择经典，可以到网上做做功课，看看评价，综合分析一下。 3. 做读书笔记。一是将自己阅读的时候的思考（包括闪念）总结下来，二是将书中的好例子摘抄下来。（这个习惯是一年前才养成的，发现受益极大。）有了google note，笔记可以加上tag，非常便于回顾，加深理解。我觉得，人与人学习的差距不在资质上，而在花在思考的时间和思考的深度上（后两者常常也是相关的）。 4. 提到思考，我有一个小习惯。利用走路和吃饭的时候思考，还有睡觉前必然要弄一个问题放在脑子里面，在思考中迷糊入睡。发现这样一来往往在不知不觉中多出来大量的思考时间。 4a. 将思考成为习惯还有一个很大的好处——避免焦虑。卡耐基用一整本书讲了一个有效的做法来避免焦虑——底线思考。然而实际上还有另一个有效的做法，就是投入地做另一件事情。不去想\"喜马拉雅山上的猴子\"的方法并不是使劲的告诉自己不去想\"喜马拉雅山上的猴子\"，因为那样等于就是脑袋里想了那只猴子，正确的做法是真的不去想那猴子，而是想别的。用别的东西充满工作记忆，其他的神经活动自然会被抑制（神经科学基本事实）。所以，感到焦虑的时候不妨思考吧，甚至完全可以去理性分析和思考导致焦虑的问题本身，将其拆解，分析来源，在不知不觉中，大脑的工作重心就从情绪模块转向了推理模块了，而且这思考也可能顺带更有效地解决了导致焦虑的问题呢:) 5. 重要的事情优先（详见史蒂芬·柯维的《高效能人士的七个习惯》或《要事优先》）。尽量避免琐事骚扰，不重要的事情能不做就不做。有时候，紧急的事情往往只是当事人觉得必须马上做完才显得紧急或者干脆就是紧他人之急，最糟糕的就是纯属性格上原因觉得每件事情都得第一时间完成，很多看上去紧急的事情实际上并不是真的\"不能再拖了\"，有的干脆就并不需要或值得去做。有很多事情都是可以先放一放甚至完全let go的，否则的话就整天被所谓\"紧急\"的事情牵着鼻子走了。 6. 重要的事情营造比较大的时间块来完成。比如一本好书，或者一个重要的知识点，最好不要切得太琐碎了看，否则看了后面忘了前面。不利于知识的组织&联系。 7. 多看心理学与思维的书，因为它们是跨学科的。知识分两种，一是我们通常所谓的知识，即领域知识。二是关于我们的大脑吸收知识的机制的知识，后者不妨称为元知识。虽说这也是领域知识，但跟其它的领域知识不同的是，它指导着我们学习其它所有的领域知识。 8. 学习一项知识，必须问自己三个重要问题：1. 它的本质是什么。2. 它的第一原则是什么。3. 它的知识结构是怎样的。 9. 获得的多少并不取决于读了多少，而取决于思考了多少、多深。 10. 善于利用小块时间，也就是《奇特的一生》中所说的“时间下脚料”，如何利用前面有几个方法。同时，也善于创造整块时间（如通过要事优先）。 11. 关于习惯的养成，必须要说明的：经常看到有些人评论说，说说容易，做起来哪有那么容易啊（另一个无关习惯的“说起来容易做起来难”则是因为纸上谈兵不可能算计到所有现实中的因素，但那是另一个问题）。对此我要说的是，做起来当然不容易，所谓江山易改，本性难移。人的性格和认识事物的框架是长期积累养成的，并且人们非常珍视自己的信念（英语里面表达不相信某个东西叫做“I don't buy it”）。从进化心理学上这是有依据的，一个经过时间检验的信念往往是更靠谱的。只不过可惜的是靠谱不代表最佳，一个信念能让你活下来并不代表能让你活得最好（详见《Mene Genes》，更多的例子参见《How we know what isn't so》）。我们评判一个信念的标准是satisficing原则（即足够，能行就好，这个术语不是我提的，是大牛Herbert Simon提的），并不是optimizing原则。话说回来，为什么说起来容易做起来难，是因为“说”只是理性上承认正确，并没有考虑到我们每个人大脑中居住的那个非理性自我。这个自我以强大的情绪力量为动机，以习惯为己任，每时每刻都驱使着我们的行为。因为它掌握了“情绪”这个武器，所以我们只能时时拿它当大爷。不记得是哪位哲学家说的了，理性是感性的奴隶。那么，是不是就是说无法克服既有习惯了？以我的经验（以及观察到的别人的经验），还是可以的。第一条就是认识到习惯的改变绝不是一天两天的事情，承认它的难度。第二条就是如果你真想改掉习惯，就需要在过程中常常注意观察自己的行为，否则习惯会以一种你根本觉察不到的方式左右你的行为让你功亏一篑。有一个认知技巧也许可以缓解更改习惯过程中的不适：即把居住在内心的那个非理性自我当成你自己的孩子（你要去培养他），或者你的对手（你要去打败他）也行。总之不能当成自己，因为每个人都不想改变自己。这里转一个认知技巧的例子：李笑来老师在《把时间当作朋友》（顺便也推荐这本开放电子书）中提到他一个朋友用另一个认知技巧来克服背单词的枯燥的： 因为，一共要搞定20,000个单词，而因此可能获得的奖学金是每年40,000美元左右——并且连续四年没有失业可能（后来的事实是，他直到五年之后才获得了博士学位）。当时的美元兑换人民币的汇率差不多是8:1，所以，大约应该相当于320,000元人民币。而如果一年的税后收入是320,000元人民币的话，那么税前就要赚取差不多400,000元人民币。那么，每个单词应该大约值20元人民币——这还只不过是这算了一年的收入而已。 所以，他终于明白背单词是非常快乐的。他每天都强迫自己背下200个单词。而到了晚上验收效果的时候，每在确定记住了的单词前面画上一个勾的时候，他就要想象一下刚刚数过一张20元人民币的钞票。每天睡觉的时候总感觉心满意足，因为今天又赚了4000块！ 注意，这跟自我欺骗不同。一来，我们的情绪系统只能这般对付（《Synaptic Self》中提到，大脑中的新皮层（neocortex，所谓“理性”居住的地方，尤其是前额叶）在进化历史上是较为新近的年代才进化出来的，跟底层较原始的模块（如主管情绪的杏仁核）之间的神经网络沟通并不是合作无间，这就解释了为什么有些事情我们明明知道是对的，但就是不能说服自己，情绪还是在那里不依不挠的驱使你去做另一样事情）。二来，我们知道在干什么，所以不能算欺骗:P 总之，对于习惯的更改，除了最重要的一日三省，加上一些认知技巧（其实每个人都是自己的心理学家，你可以自己看看能不能想出什么法子）。其实是没有什么速效银弹的。但是，知难而不退嘛，值得做的事情几乎总是如此:) 第二部分 1. 学习和思考的过程中常问自己的几个问题： 你的问题到底是什么？（提醒自己思考不要偏离问题。） OK，到现在为止，我到底有了什么收获呢？（提醒自己时不时去总结，整理学习的东西）。 设想自己正在将东西讲给别人听（有声思考；能否讲出来是判断是否真正理解的最佳办法）。 3.1 设想需要讲给一个不懂的人听。（迫使自己去挖掘知识背后最本质、往往也是最简单的解释）。 时常反省和注意自己的思维过程。尤其是当遇到无法理解或解决的问题之后，最需要将原先的思维过程回顾一遍，看看到底哪个环节被阻塞住了妨碍了理解。问题到底出在哪里。并分析以后需要加强哪方面的思维习惯，才能够不在同样或类似的时候被绊住。对此，将思维的大致脉络写下来是一个很好的习惯。 养成反驳自己的想法的习惯：在有一个想法的时候，习惯性地去反驳它，问自己“这个难道就一定成立吗？”、“有没有反例或例外？”、“果真如此吗？”之类的问题。（参见Critical Thinking）  人的思维天生就是极易流于表面来理解事物的（参见《Psychology of Problem Solving》第11章）。觉得自己理解了一个问题了么？条件反射性地问自己：你真的理解了吗？你真的理解了问题的本质了？问题的本质到底是什么？目前我的理解是什么？我对这个理解感到满意吗？这样的理解到底有什么建设性呢？等等。 2. 重视知识的本质：对于程序员来说这一点尤其重要，程序员行业的知识芜杂海量，而且总是在增长变化。很多人感叹跟不上新技术。应对这个问题的办法只能是：抓住不变量。大量的新技术其实只是一层皮，背后的支撑技术其实都是十来年不变的东西。底层知识永远都不过时。算法数据结构永远都不过时。基本的程序设计理论永远都不过时。良好的编码习惯永远都不过时。分析问题和解决问题的能力永远都不过时。强大的学习能力和旺盛的求知欲永远都不过时。你大脑的思维方式永远都不过时。 3. 重视积累的强大力量，万事提前准备：计划订长一点，自然就可以多获得准备的时间。设想你若干年后会在做什么事情，需要哪些技能，现在就开始准备。一个5年计划便可以让你获得从现在开始的5年准备时间。5年中每天腾出半个到一个小时专心于某一件事情，认准一个方向，每次走一点，其实不要说5年，两年就会发现会起到宏大的效应。长期订阅我的Blog的朋友们也一定注意到我基本上不写东西，一般一个月写上2篇就算多的了。但总结一段时间的学习和思考的习惯却一直都没有停止（博客文章对我来说是学习和思考的副产品，我并不为写文章而写文章），所以5年下来竟也写了不少东西。所以这就是一个简单的例子。你大致还可以从我的Blog看出来我一段时间关注的东西，一般来说，一段比较长的时间（少则半年至一年——譬如对心理学与思维的关注；多则几年——譬如对编程技术的关注），在这段时间内，我的业余时间会被一个主题所充斥。反之，如果不知道目的是什么，就不知道往哪个方向上使劲，就容易产生无用功。 4. 抬起头来：人的思维是非常容易只见树木不见森林的（否则这个成语从哪来的呢？）。时不时抬起头来审视一下自己正在做的事情，问一问它（对现在或未来）有什么价值，是不是你真正希望做的。你学到的东西到底是什么？它们重要吗？你需要在这个时候学习这些吗？（见第2条）。你的时间就是你的资源，你投入这些资源来掌握知识，所以到底用来掌握哪些知识是一个很重要的问题。仅仅遵循兴趣是不够的，人会对很多次要的东西产生兴趣，并一头钻进去浪费好多时间。所以判断一个东西值不值得学习是很重要的。 杂项 1. 退订RSS：RSS Reader是个时间黑洞。就算mark all as read，在有大量feed的情况下，也会无形中消耗掉大量的时间。我们一旦订阅了某个RSS之后就会倾向于不肯退订它，心想也许某天有个重要的信息会从那里得到。这其实是源于人不肯\"关上一扇门（即便门内的收益概率极小）\"的心理（参见《Predictably Irrational》）；而实际上，关上一扇门，有时能够增大收益期望。仔细观察一下reader里面的feeds，有哪些是真正有价值的，把那些没价值的或者价值很小乃至于不值得每天被它骚扰的，全都退订掉。不要舍不得，那些一个星期都没出现让你眼睛一亮的内容的feed，很大的可能是永远也不会出现。就算可能，也别担心你会漏掉什么宝贵信息，真正宝贵的信息，在其他来源你也会接触到的。一开始我的Greader里面每天都有大量的新内容，每天都是1000+，但一段时间之后发现除了信息焦虑，实际上有价值的内容不多，现在，我很高兴地发现自己摆脱了这种状况，我持续不断地退订feeds，留下的内容越来越少，也越来越精，带来的阅读焦虑也越来越少。（顺便推荐一个东西，aideRSS，初步使用，感觉对订阅reddit这样的每天更新大量内容的feed很有用）。 2. 有时间吗？总结总结最近得到的新知识吧。一般来说，我在一段时间内学习的一些东西总是会在这段时间内一直在脑子里打转，一有时间空隙（譬如走路，吃饭）它们就会自己蹦出来，促使我去进一步思考和总结。永远不要认为对一个知识的把握足够深刻，“理解”的感觉很多时候只是假象。学会反问自己对知识到底把握了多少，是很有价值的。（如何反问，前面的总结中有提到）。 3. 有时间吗？看本书吧。（传统的）阅读和思考永远优于所谓的在互联网上汲取新知识，后者往往浅表、不系统、乃至根本没价值。 4. 制定简要的阅读计划：选出最近认为对你最有价值的书，先总览一下，决定阅读的顺序（哪些章节可以优先阅读）。然后每天看一点。并利用走路、吃饭、乘车或其他不适合带着书和笔的时间来总结看过的内容，建立知识结构，抽取知识本质，与以往的大脑中的知识建立联系。（参见《奇特的一生》） 第三部分 这篇主要写一些学习（尤其是阅读）的基本方法。 1. 趁着对一件事情有热情的时候，一股脑儿把万事开头那个最难的阶段熬过去。万事开头难，因为从不了解到了解基本的一些事实，是一个新知识暴涨的阶段，这个时候的困难是最大的。有人熬不过去，觉得困难太大就放弃了。不过，狂热的兴趣可以抵消对困难的感觉，所以趁着对一件事情有热情的时候，开一个好头是很重要的。（当然，这并不是说持之以恒就不重要了）。当然，也许这个是因人而异的，对我来说我会在对一件事情有浓厚兴趣的时候非常专注地学习，把很多 groundworks 做掉。后面就会顺利一些了。 2. 根据主题来查阅资料，而不是根据资料来查阅主题。以前读书的时候是一本一本的读，眼里看到的是一本一本的书，现在则是一章、甚至一节一节的读，眼中看到的不是一本一本的书，而是一堆一堆的章节，一个一个的知识主题，按照主题来阅读，你会发现读的时候不再是老老实实地一本书看完看另一本，而是非常频繁地从一本书跳到另一本书，从一处资料跳到另一处资料，从而来获得多个不同的人对同一个主题是如何讲解的。比如最近我发现在看蒙特卡罗算法时就查了十来处资料，其中有三四篇 paper 和六七本书；这是因为即便是经典的书，你也不能指望它对其中每一个主题的介绍都是尽善尽美的，有些书对某个主题（知识点）的介绍比较到位，有些书则对另一些知识点介绍得比较到位。而有时候一篇紧凑的 paper 比一本书上讲得还要好。我硬盘里面的书按主题分类，每个主题下面都有一堆书，当我需要学习某个主题的知识时（譬如贝叶斯学习或者神经网络），我会把里面涉及这个主题的书都翻开来，索引到相关章节，然后挑讲得好的看。那么，如何判断一个资料是好资料还是坏资料呢？ 3. 好资料，坏资料。好资料的特点：从问题出发；重点介绍方法背后的理念（ rationale ），注重直观解释，而不是方法的技术细节；按照方法被发明的时间流程来介绍（先是遇到了什么什么问题，然后怎样分析，推理，最后发现目前所使用的方法）。坏资料的特点是好资料的反面：上来就讲方法细节，仿佛某方法是从天上掉下来的，他们往往这样写“我们定义... 我们称... 我们进行以下几个步骤... ”。根本不讲为什么要用这个方法，人们最初是因为面对什么问题才想到这个方法的，其间又是怎样才想出了这么个方法的，方法背后的直观思想又是什么。实际上一个方法如果将其最终最简洁的形式直接表达出来往往丢失掉了绝大多数信息，这个丢掉的信息就是问题解决背后的思维过程。至于为什么大多数书做不到这一点，我在这里试着分析过。 4. 学习一个东西之前，首先在大脑中积累充分的“疑惑感”。即弄清面临的问题到底是什么，在浏览方法本身之前，最好先使劲问问自己能想到什么方法。一个公认的事实是，你对问题的疑惑越大，在之前做的自己的思考越多，当看到解答之后印象就越深刻。记得大学里面的课本总是瀑布式地把整个知识结构一览无余地放在面前，读的过程倒是挺爽，连连点头，读完了很快又忘掉了，为什么？因为没有带着疑问去学习。 5. 有选择地阅读。很多人觉得我读书速度很快，其实我只是有选择地阅读。这里的选择体现在两个地方，一是选择一本书中感兴趣的章节优先阅读。二是对一本书中技术性较弱或信息密度较低的部分快速地略读。一般来说，除了技术性非常强的书之外，大多数书的信息密度很低，有很多废话。一般来说在阅读的时候应该这样来切分内容：1. 问题是什么？2. 方案是什么？3. 例子是什么？如果是需要解释一个现象的（譬如《黑天鹅》），那么1. 现象是什么？2. 解释是什么？3. 支撑这个解释的理由是什么？4. 例子是什么？一般来说，这一二三四用不了多少字就可以写完了（如果假设只举一到两个精到的例子的话），这样的无废话著作的典型是《合作的进化》；那为什么有些书，明明核心观点就那点东西（顶多加上几个精要的例子罢了）却写得长得要命呢？因为人的思维都有一个“联想”的特点，写着写着就容易旁逸斜出，而且作者自己也往往觉得引申出去挺牛逼，有时候很多与主题无关的废话就掺和进来了；那么，阅读的时候就应该有选择性地滤掉这些不相干的废话；此外还有一种可能性就是大量冗余的例子。一般来说组织得比较好的书会有详细且一目了然的目录和索引，根据目录首先就可以滤掉一部分（比如某个子章节的内容你以前是看过的），然后有时候作者还会举很多冗余的例子，如果你已经觉得印象够深刻了这些例子完全可以不看（一些书就非常厚道地对每个观点只辅以一两个最最经典的例子，譬如《与众不同的心理学——如何正视心理学》，这样的书我最是喜欢）。 6. 为什么看不懂？如果看不懂一个知识，一般有如下几个可能的原因：1. 你看得不够使劲。对此古人总结过——书读百遍其义自现。虽然这个规律不是任何时候都成立的，但是从认知科学的角度看是完全可以解释的，我们在阅读的时候，注意力往往会有选择性地关注其中的某一些“点”，而忽略了另一些“点”，于是一遍看下来可能因为某一些忽略导致无法理解整体。或者干脆看的时候就没注意其中一些细节但重要的东西。此外，大脑理解一个东西需要一定的处理时间，人脑的处理速度很慢，神经冲动每秒传输速度不过百米，所以不能指望看到哪懂到哪。最后，我们可能因为思维定势的原因会从某个特定的角度去看一句话而忽略了从不同角度去理解的可能性。对于这类情况，仔仔细细地再多读两遍，多试着去理解两遍，往往会“哦！原来这样。”地恍然大悟。2. 其中涉及到了你不懂的概念。这是技术性的不理解。这种情况就需要 Cross Reference 。如果一句话中用到了你不懂的概念，那就去查，现在很多书都是电子书，直接搜索一下，或者，对于纸书，看一下书后面的索引就行了。奇怪的是很多人看不懂也不分析一下为什么不懂，就直接放弃了。正如解决问题一样，问题卡住解决不了，第一时间要做的就是分析到底为什么解决不了，而不是直接求救。3. 作者讲述的顺序不对，你接着往下看，也许看到后面就明白了前面的了。   杂项 7. 如何在阅读之前就能获得对一本书质量的大致评估。在深入阅读之前能够迅速评估一本书的质量可以节省很多时间。基本上有几个线索：1. 看作者。牛作者写的书一般都不错。2. 看目录和简介。一份好的目录和简介能够透露这本书质量的相当一部分信息。目录结构是否清晰，是否直白（而不是装神弄鬼），都是衡量的线索。3. 看 Amazon 上的评价，这里要注意的是，除了看整体打分之外，更要看打分最低的人是怎么说的，因为小众意见往往有可能来自那些真正懂行的人（除了来踢馆的），如果在打分最低的意见里面看不到真正有价值的反驳意见的话就相当肯定书是不错的了。4. 看样章。Amazon 上一般都可以随机浏览一些章节的，表达是否清晰，论证是否严谨，内容是否深刻，基本是几页纸就能看出来的。 8. 如何搜寻到好书。几个线索：1. 同作者的著作。2. Amazon 相关推荐和主题相关的书列（类似豆瓣的豆列）。3. 一本好的著作（或一份好的资料——不管是书还是网页）在参考资料里面重点提到的其他著作。4. 有时对于一个主题，可以搜索到好心人总结的参考资源导引，那是最好不过的。 第四部分 自从建立了 TopLanguage 以来，发现在上面待的时间越来越多，与高手讨论问题是个粘性十足的事情，一方面，分享自己的认识是整理不成熟的想法的极好途径，另一方面，互相之间视角不同，所以往往自己忽视的地方会被别人发现。在讨论中不断精化既有的知识体系。以下这段基本上摘抄自（略有整理和添加）在 TopLanguage 上的发言： 抓住不变量 我喜欢把知识分为essential的和non-essential的。对于前者采取提前深入掌握牢靠的办法，对于后者采取待用到的时刻RTM (Read the manual)方法（用本）。 如何区分essential和non-essential的知识想必绝大多数时候大家心里都有数，我举几个例子：对程序员来说，硬件体系结构是essential的，操作系统的一些重要的实现机制是essential的，主流编程范式（OO、FP）是为了满足什么需求出现的（出现是为了解决什么问题），是怎么解决的，自身又引入了哪些新的问题，从而适用哪些场景）。 这些我认为都是essential的。我想补充一点的是，并不是说硬件体系结构就要了解到逻辑门、晶体管层面才行（其实要了解到这个层面代价也很小，一两本好书就行了），也并不是说就要通读《Computer Architecture: Quantitative Approach》才行。而是关键要了解那些重要的思想（很长时间不变的东西），而不是很细的技术细节（易变的东西）。《Computer Systems: A Programmer’s Perspective》就是为此目的，针对程序员的需求总结出那些essential knowledge的好书。 再来说一下为什么需要预先牢靠掌握这些essential的知识： 根据Joel Spolsky同学的说法（原文），编程语言技术是对底层设备的封装，然而封装总是会出现漏洞的，于是程序员被迫下到“下水道”当中去解决问题，一旦往下走，漂亮的OO、N层抽象就不复存在了，这时候不具备坚硬的底层知识就会无法解决问题。简而言之就是这些底层知识会无可避免的需要用到，既然肯定会被用到那还是预先掌握的好，否则一来用到的时候再查是来不及的，因为essential的知识也往往正是那些需要较长时间消化掌握的东西，不像Ruby的mixin或closure这种翻一下manual就能掌握的东西。（英语也是这样的essential knowledge——上次在PyCN上看到一个招Python开发人员的帖子将英语列为必备技能，却并不将自然语言处理列为必备技能，正是因为英语不是可以临阵磨枪的东西，而且作为知识的主要载体，任何时候都少不了它，如果不具备英语能力，这个就会成为个人知识结构的短板或瓶颈，而且由于需要长时间才能获得这项能力，所以这个瓶颈将持续很长时间存在。我们曾经在 TopLanguage 上讨论过如何花最少的时间掌握英语）另一方面，在问题解决当中，如果不具备必要的知识，是根本无从思考的，再好的分析能力也并不是每个问题都能分析出该用哪些知识然后再去查手册的，很多时候是在工具和问题之间比较，联想，试探性的拼凑来解决问题；这就使得一个好的既有知识基变得至关重要。（实际上以上这个是一个较大的话题，希望有一天我能够把它详细展开说清:)） 如果你不知道某个工具的存在，遇到问题的时候是很难想到需要使用这么样一个工具的，essential knowldge就是使用最为广泛的工具，编程当中遇到某些问题之后，如果缺乏底层知识，你甚至都不知道需要去补充哪些底层知识才能解决这个问题。 你必须首先熟悉你的工具，才能有效地使用它（须知工具的强是无敌的，但这一切得以“了解你的工具”为前提，甚至得以“了解目前可能有哪些工具适合你的问题”为前提）。一门语言，你必须了解它的适用场景，不适用场景（比如继承能解决你的问题不代表继承就是解决你的问题的最适合的方案，须知问题是一个复杂系统，解决方案总是常常引入新的问题）。你必须了解它支持的主要编程范式，此外你还必须了解它的traps和pitfalls（缺陷和陷阱，如果不知道陷阱的存在，掉进去也不知道怎么掉的。）这些都是essential knowledge，如果不事先掌握，指望用的时候查manual，是很浪费时间的，而且正如第2点所说，正因为你不知道这些知识（如适用场景），从而用sub-optimal的方式使用了一门语言自己可能还不知道（最小白的例子是，如果你不知道语言支持foreach，那么可能每次都要写一个冗长的循环，较常见的例子是不知道有很方便的库设施可以解决手头的问题所以傻乎乎的自己写了一堆代码），因为人的评价标准常常是：只要解决了最醒目的问题并且引入的新问题尚能忍受，就行。注意，熟悉并非指熟悉所有细节，而是那些重要的，或者无法在需要用到的时候按需查找的知识。比如上面提到的：适用场景不适用场景，编程范式，主要语言特性，缺陷和陷阱。 当然，以上作为程序员的essential knowledge列表并不完备，关键是自己在学习新知识的时候带着第三只眼来敏锐地判断这个知识是否是不变量，或不易变的量，是否完全可以在用的时候查手册即可，还是需要提前掌握（一些判断方法在上文也有所提及）。并且学会在纷繁的知识中抽象出那些重要的，本质的，不变的东西。我在之前的part里面也提到我在学习新知识的时候常常问自己三个问题：该知识的（体系或层次）结构是什么、本质是什么、第一原则是什么。 另外还有一些我认为是essential knowledge的例子：分析问题解决问题的思维方法（这个东西很难读一两本书就掌握，需要很长时间的锻炼和反思）、判断与决策的方法（生活中需要进行判断与决策的地方远远多于我们的想象），波普尔曾经说过：All Life is Problem-Solving。而判断与决策又是其中最常见的一类Problem Solving。尽管生活中面临重大决策的时候并不多，但另一方面我们时时刻刻都在进行最重大的决策：如：决定自己的日常时间到底投入到什么地方去。如：你能想象有人宁可天天花时间剪报纸上的优惠券，却对于房价的1%的优惠无动于衷吗？（《别做正常的傻瓜》、《Predictably Irrational》）如：你知道为什么当手头股票的股价不可抑止地滑向深渊时我们却一边揪着头发一边愣是不肯撤出吗？（是的，我们适应远古时代的心理机制根本不适应金融市场。）糟糕的判断与决策令我们的生活变得糟糕，这还不是最关键的，最关键的是我们从来不会去质疑自己的判断，而是总是能“找到”其他为自己辩护的理由（《错不在我（Mistakes were made, but not by me）》）又，现在是一个信息泛滥的时代，于是另一个问题也出现：如何在海洋中有效筛选好的信息，以及避免被不好的信息左右我们的大脑（Critical Thinking）关于以上提到的几点我在豆瓣上有一个专门的豆列（“学会思考”），希望有一天我能够积累出足够多的认识对这个主题展开一些详细介绍。 最后分享一个学习小Tip： 学习一个小领域的时候，时时把“最终能够写出一篇漂亮的Survey”放在大脑中提醒自己，就能有助于在阅读和实践的时候有意无意地整理知识的结构、本质和重点，经过整理之后的知识理解更深刻，更不容易忘记，更容易被提取。 杨军在 TopLanguage 上也曾分享了三篇非常棒的学习心得的文章，字字珠玑： [1] 有些事情做起来比想象中容易 [2] 有关读书方法的一点想法 [3] 一件事情如果你没有说清楚，十有八九不能做好 最后告知大家，TopLanguage 最近经历了一次很大的管理策略修订，可以预期将彻底摆脱这两个月来的噪音问题，未来的讨论质量将会越来越高。详情可参见这里。  ","title":"一直以来伴随我的一些学习习惯 ——转自刘未鹏"},{"content":"  导读：原文作者Alan Skorkin是一名软件开发人员，他在博客中分享对软件开发相关的心得，其中有很多优秀的文章，本文就是其中一篇，作者认为：成为优秀的开发人员，可以没有数学技能，但成为卓越的开发人员，不能没有。以下是文章内容，译文来自伯乐在线，CSDN编辑对其中的误译做了必要的校正。 Alan Skorkin 不久之前，我开始考虑数学。你也知道，到目前为止，我编写软件也有几年了。老实说，在我的工作当中，还没发现需要用到数学的。我要学习和掌握许多新东西，包括语言、框架、工具、流程、沟通技巧和没完没了、形形色色的各种库，对这些东西，数学真没啥用处。当然了，这不足为奇，我所做的工作，大部分都是某种类型的CRUD（编注：CRUD是Create、Read、Update和Delete的首字母缩写，这里指数据库相关的编程）。在互联网时代，我们多数开发人员所做的大部分工作都是如此。你是独立顾问？你恐怕主要就是在做网站。你在大公司上班？你主要还是在做网站。你是自由职业者？你主要还是在做网站。 终于有一天你对此有些厌倦了，就像我一样。别误会我，这可以是项有趣并有挑战性的工作，有机会解决问题，并和有趣的人一起互动，在工作时间做这个，我高兴。但在我个人时间中搭建更多的网站，就没太大意思了，于是你开始寻找一些更加有趣/酷/好玩的事情，像我一样。（所以，）有些人转移到前端和图形化技术——能有直观的反馈是非常诱人的。但我并不是其中一员（虽然我和别人一样都喜爱前端，但它真的不能让我兴奋。）这就是遇到一些搜索相关的问题后，我为什么决定深入挖掘的原因了。这把我带回到故事的一开始，因为一旦我抓到第一把充满搜索的铁铲，一旦我“撞到”数学时，我才真正意识到，我的技能恶化的程度。数学并不像骑自行车，长期不用就会忘记。  拓展视野  多对搜索的一些了解，让我接触到各种有趣的软件和计算机科学相关的事情和问题（包括机器学习、自然语言处理、算法分析等）。现在，在我接触的各方面，我都看到了数学，所以我更加强烈地感觉到自己技能缺乏。我已经意识到，如果你想利用计算机做又酷又有趣的事，你需要达到一个像样的数学能力水平。除了上面说的三个，还有一些，如：密码学、游戏人工智能、压缩算法、遗传算法、3D图形算法等。在理解之后，如果你想要编写我们正讨论的那些库和工具，而不是仅仅使用它们（即：做一个“消费者”，而不是“生产者”），那你需要数学（知识）来理解这些领域背后的你能应用的理论。即便如果你不想编写任何库，当你真正理解事情的原理，你在构建软件时，它能给带来更多的成就感，绝非仅仅把它们连起来，就希望它们去做任何它们应该能做的。  虽然大多数开发人员会告诉你，他们在工作中从来不需要数学(就像我前面说的)，但是经过一番沉思后，我有了个想法（突发灵感）：就是反马斯洛的锤子理论。你知道这个吧，当你有一把锤子，你会把一切看成是钉子。这是一个隐喻，也就是说人们乐于使用自己钟爱的工具，即便这并不是手中工作的最好工具。数学就是我们的一个相反的锤子。我们知道有这个锤子，但并不太子的如何使用。所以，当我们遇到问题，我们的锤子是解决问题的最佳工具时，我们却从未认真考虑过它。对我祖父而言，螺丝刀够用了；对我父亲来说，也很好；对我来说，同样如此。谁还需要锤子？数学的技巧在于，人们惧怕它，甚至大多数程序员，你认为我们不会怕，但我们确实怕。所以，我们把自己的话转变为可以自我实现的预言。这并不是我在工作中不需要数学，这只是我真的不知道，即便我知道，我也不知道如何使用它。所以我并没有使用它，当缺少某些东西时，如果你长期将就，不久后你甚至不会察觉它的缺失，所以对其需要更少了，这自我实现的预言。  针对思索接近我们内心世界，这里有一些的“粮食”——学习新技术。作为一名协作世界的开发人员，你努力成为一名通才型的专才（如果你不知道我在说什么，可以看看《我编程，我快乐》这本书）。你尽力在多数事情上做的体面，并在有些事情上做的优秀。但是你擅长什么？一般来说，人们会选择一两个框架或一门语言，然后与之相伴，这样是不错。但是要看到，框架和较小范围内的语言都有保质期。如果你要做一名Hibernate、Rails或Struts专家（使用 Struts的朋友现在真的应该担忧一下了），当新框架取代当前的框架时，你在几年内将不得不重新洗牌。所以，这也许是你真正的最好投资，但也可能不是。另一方面，数学是不会很快消逝的。在我们领域中所做的一切，都是建立在稳固的数学原理之上（算法和数据结构正是这样的例证），所以用在数学上的时间绝不是浪费，这不可辩论。再重复一次，总结起来就是：要真正理解东西，而不是非死记硬背地使用。当涉及到计算机时，数学能有助你更深入地理解你所做的。事实上，正如Steve Yegge（著名技术博客，Google程序员）所言，作为程序员我们所做的事很像数学，只是我们甚至都没有意识到这一点。  什么/谁造就了与众不同？ （Knuth） 你不相信我？那请你想想：在我们的领域中，几乎人人普遍尊敬的卓越程序员同样也是大数学家。我是说像 Donald Knuth（图灵奖得主，计算机科学大师），Edsger W. Dijkstra（图灵奖得主，计算机科学大师），Noam Chomsky（著名语言学家），Peter Norvig（Google研究院总监）这一类人。但是这些家伙并非真正的开发人员，他们是计算机科学家，这能真正算数么？我再一次觉得，在我们写出的纯代码行数能达到这些人所写的十分之一之前，也许我们不应该再去讨论这些问题了。当然，不当科学家，你也能获得成功和名誉，大家都听过Gavin King（Hibernate创始人）或DHH（Ruby on Rails创始人）不是。这当然没错，但是“听说过”和普遍尊敬是不同的，这种差别就如同创建一个框架，和在你的领域中为人类知识所做出的全部重大推动两者之间的差别。（不要误会我，我尊重Gavin和David，他们所做的事，远远超过我，但是这不能影响我所说的事实）。所有的这些重要么？我不知道，可能不重要。但我认为还是应该说出来，因为我们都爱反思不是。  如今的世界正充满着数据，每日都增加更多的数据。而在以前，我们在相对少量的数据下享受工作。我们今日编写的软件必须高效处理海量数据。甚至在协作世界，这也是愈加明显的事实。这也就是说，你越来越不可能只是让程序跑起来，再来考虑如何运作，因为你要处理的数据量将困住你，除非你非常了解它。我的预测是：算法分析将对于普通程序员越来越重要了，并不是说以前不重要，但未来将越来越重要。如果要成为靠谱的算法设计专家，需要什么？你猜到了，是一些数学技能。  所以，我该怎么办呢？嗯，我已决定一点一点地建立或恢复我的数学技能，虽然还有大量的书要看，大量的代码要写，但我会尽力抽时间放在数学上，这就像锻炼，时不常的锻炼总聊胜于无（再次引用Steve Yegge的话）。说到数学，我袖中当然还藏有一张王牌，它对我有利，但很幸运，有这个博客，我们都会受益的。  你在5年内的规划如何？ 那么，数学对所有事都有利么？这事先很难说，我对我现在的处境十分满意，或许你也如此，但这都和潜能有关系。如果你是协作世界的一名开发人员，你真的不需要数学。如果你乐于你的整个职业生涯是这样的：在工作时间中做企业CRUD应用，或在闲暇时间滑翔跳伞或极限水上滑板（或其他各种时髦的极客运动），也分配较多时间在Spring、Hibernate、Visual Studio或其它东西上。那些特殊的职位并没有真正限制你的潜力，你能变得极具价值，甚至可深入追求。但是如果你想为多样化的职业生涯而奋斗，想要有能力尝试几乎所有涉及代码的事，从信息检索到Linux内核。总之，如果你想成为一个开发人员、程序员和计算机科学家的完美组合，你必须确保你的数学技能达到标准。长话短说，如果你在数学方面有一定天赋，那在软件开发领域中所有的大门都是向你敞开的，如果没有，那你就安安心心地做CRUD型工作吧！ 原文链接：http://www.skorks.com 译文链接：http://www.jobbole.com/entry.php/444 ","title":"数学是成就卓越开发人员的必备技能"},{"content":" 如果你刚接触自然语言处理并对她感兴趣，最好读几本这方面的书籍，除了能让你知道自然语言处理各个领域是干什么的外，还能培养一下NLP的感觉。以下四本书是我读研期间阅读和接触过的，如果您还有好书推荐，欢迎补充。 1、 《自然语言处理综论》（Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition） 　　这本书的权威自不用说，译者是冯志伟老师和孙乐老师，当年读这本书的时候，还不知道冯老师是谁，但是读起来感觉非常好，想想如果没有在这个领域积攒多年的实力，是不可能翻译的这么顺畅的。这本书在国内外的评价都比较好，对自然语言处理的两个学派（语言学派和统计学派）所关注的内容都有所包含，但因此也失去一些侧重点。从我的角度来说更偏向于统计部分，所以需要了解统计自然语言处理的读者下面两本书更适合做基础阅读。不过这本书的N-gram语言模型部分写得非常不错，是SRILM的推荐阅读参考。 2、《统计自然语言处理基础》（Foundations of Statistical Natural Language Processing） 　　我比较喜欢这本书，这两年的学习过程中也经常拿这本书作为参考，可能与我做统计机器翻译有关吧。看china pub上的评论说这本书的翻译比较差，自己的感觉是还行。当然，这是国内翻译图书的一个通病：除了很难有翻译的非常好的书外，另外一个原因就是滞后性。如果e文足够好的坏，就及时看英文版吧。这本书在统计基本部分的介绍很不错，另外n元语法部分讲得也比较好，也是SRILM的推荐阅读。 3、《统计自然语言处理》 　　这是北京自动化所宗成庆老师今年5月出版的一本专著，我有幸较早的阅读了这本书的很多章节。一个很强的感觉是：如果你想了解相关领域的国内外最新进展，这本书非常值得一读。上面两本书在由于出版稍早的缘故，很多领域最新的方法都没有介绍。而这本书刚刚出版，宗老师对国内外现状把握的也比较好，因此书中充分体现了这方面的信息。另外统计机器翻译这一部分写得很详细很不错，这可能与宗老师亦是这个领域的研究者有关吧。 4、《计算机自然语言处理》 　　这是我最早看的一部自然语言处理方面的书籍，和上面几部大部头的书籍相比，这本书很薄，可以很快的看完。书的内容我都有点忘了，但是印象中可以在每个章节看到国内这个领域的研究历史和相关单位。这时才发现母校HIT在这个领域的超强实力，只是可惜这时候已经离开冰城了。 　　这些书籍怎么读都行，泛览也罢，精读也行，只要有时间，多读书是没坏处的。我自己的经验是，先泛泛的浏览或阅读一篇，对于比较晦涩的部分可以先跳过去，然后对自己感兴趣的领域或者将要从事的领域的相关章节进行精读，当然，书籍一般在开始的几个章节讲些基础性的知识，这部分最好也仔细揣摩一下。真正要对自己研究的领域深刻了解，还得好好读一下本领域的相关论文。 卓越网：自然语言处理书籍 当当网：自然语言处理书籍 注：原创文章，转载请注明出处“我爱自然语言处理”：www.52nlp.cn","title":"自然语言处理入门书"},{"content":"吴军博士毕业于清华大学计算机系（本科）和电子工程系（硕士），并于1993-1996年在清华任讲师。他于1996年起在美国约翰霍普金斯大学攻读博士，并于2002年获得计算机科学博士学位。在清华和约翰霍普金斯大学期间，吴军博士致力于语音识别、自然语言处理，特别是统计语言模型的研究。他曾获得1995年的全国人机语音智能接口会议的最佳论文奖和2000年Eurospeech的最佳论文奖。 吴军博士于2002年加入Google公司，现任Google研究院资深研究员。到Google不久，他和三个同事们开创了网络搜索反作弊的研究领域，并因此获得工程奖。2003年，他和两个同事共同成立了中日韩文搜索部门。吴军博士是当前Google中日韩文搜索算法的主要设计者。在Google其间，他领导了许多研发项目，包括许多与中文相关的产品和自然语言处理的项目，并得到了公司首席执行官埃里克.施密特的高度评价。 吴军博士在国内外发表过数十篇论文并获得和申请了近十项美国和国际专利。他于2005年起，当选为约翰霍普金斯大学计算机系董事会董事。 他是一个还不错的摄影爱好者，一个古典音乐迷，喜欢享受高质量的影视，比如Blu-ray的电影。平时偶尔会修理园子，甚至做点木匠活。每年旅游很多次。当然，还时不时地为Google China Blog写东西。以前读书很多，现在事情太多读得就少了","title":"Google（谷歌）研究员 吴军"},{"content":"   摘 要：Linux环境中，大部分基于短语的统计机器翻译系统可以很方便的利用GIZA++训练得出的词语对齐结果。然而，对于许多习惯在Windows系统中从事科学研究的科研工作者却享受不到这种便利。虽然我们可以使用Cygwin或者虚拟机在Windows系统下模拟Linux系统，但是有时这样来回的切换很不方便。本文主要是想和大家分享一下我们如何在Windows系统中使用GIZA++进行词语对齐的一些经验。  正文：       词语对齐是自然语言处理领域的一个基本的问题，许多基于双语语料库的应用（比如统计机器翻译（SMT)、基于实例的机器翻译（EBMT)、词义消歧（WSD)、词典编撰）都需要词汇级别的对齐。一般来讲,对齐可以有篇章(section)、段落(paragraph)、句子(sentence)、短语(phrase)、词语(word)等不同的级别的对齐，其目的就是从双语互译的文本中找出互译的片段。其中篇章、段落、句子的对齐技术主要用于语料库的整理，而短语和词语对齐，就是要找出相互翻译的文本中对应的词与词、词与短语、短语和短语之间的相互翻译对。现今的基于短语的统计机器翻译系统中，很大一部分程度依赖于词语对齐（word alignment）[Och et al.，2000；Yarowsky et al.，2000]。现在使用最多的词语对齐方法就是使用双语语料库来抽取词语对齐[Smadja et al.，1996；Melamed，2000]，其中典型的代表也是本文中将使用的对齐软件就是GIZA++[Och，2000；Och et al.，2003]。     GIZA++的前身是GIZA（GIZA是由是统计机器翻译工具Egypt的一部分，Egypt是在1999年约翰霍普金斯大学统计机器翻译夏季讨论班上，由一些研究人员共同合作开发的统计机器翻译工具包。[1]），GIZA++实现了IBM公司提出的5个模型[Brown et al.，1993]和隐马尔科夫模型（HMM)[Och et al.，2003]，其主要思想是利用EM算法对双语语料库进行迭代训练，由句子对齐得到词语对齐。由于GIZA++不依赖于具体的语言对，现在的统计机器翻译系统中几乎都使用了GIZA++进行词语对齐。表1是从GIZA++对齐文件中取出的一个稍加改进的例子。其中x是目标语言句子、y是源语言句子、a是对齐结果，比如“3-2”的意思就是说中文句子的第二个单词“在”对齐到英文的第四个单词“in”（英文句子从0开始标注）。 表 1. 词语对齐示例 x I0 am1 studying2 in3 the4 university5 of6 Macau7 .8 y NULL ({ 4 }) 我 ({ 0 }) 在({ 3 }) 澳门大学 ({ 5 6 7 }) 读书 ({ 1 2 })  。({ 8 }) a 4-0 0-1 3-2 5-3 6-3 7-3 1-4 2-4 8-5   在基于短语的统计机器翻译系统中，比如Moses[Koehn et al.，2007]词语对齐就显得非常重要，究其原因就是统计机器翻译的质量很大程度取决于一个叫做短语翻译（phrase table）的文件，而这个短语翻译表就来自于GIZA++的词语对齐结果。词语对齐工具GIZA++通过EM算法对给定的双语语料库进行双向对齐，通过交集（intersection）和并（union）的操作最终得到较好的词语对齐结果。通过以下两个条件，统计机器翻译系统利用短语对齐的结果进行短语的抽取[Galbrun，2009]：   （1）分别从源语言句子f中和目标语言句子e中抽取连续的单词序列f' 和e'，并且单词序列的长度不能超过k个单词。 （2）连续的单词序列f' 和e' 的对齐信息a' 要由源语言和目标语言的对齐信息a构建而来，其中a' 至少要在a中包含一个连接。              表2给出了一个利用GIZA++中英对齐结果进行短语抽取的例子。           其中对齐为：<I-我><am-能够><able-能够><to-能够><do-做><it-它><well-好><.-。> 表 2. 利用GIZA++对其结果进行短语抽取示例 词语对齐   短语表 (我 ||| I); (我 能够 ||| I am able to ); (我 能够 做 ||| I am able to do); (我 能够 做 好 它 ||| I am able to do it well); (我 能够 做 好 它 。||| I am able to do it well .); (能够 ||| am able to ); (能够 做 ||| am able to do); (能够 做 好 它 ||| am able to do it well); (能够 做 好 它 。||| am able to do it well .); (做 ||| do); (做 好 它 |||  do it well); (做 好 它 。 |||  do it well .); (好 ||| well); (好 它 ||| it well); (好 它 。||| it well .); (它 ||| it); (它 。 ||| it .); (。||| .)         现在步入正题：在实际的科研过程中，很多科研工作者经常使用Windows系统进行科学研究，而GIZA++现在运行于Linux, Irix and SUN等这些操作系统中，对Windows系统还不支持，这就给广大的科研工作者，尤其是熟悉Windows环境的学者带来了不方便。虽然中国有个网名叫做Blue Gene的人成功的把GIZA++移植到了Windows系统[2]，可是仍然要像在Linux环境下一样在DOS界面中输入好多命令，并且要修改很多代码，很是费时。当然我们也可以借助于虚拟机在Windows中安装Linux系统，也可以借用Linux模拟工具Cygwin来操作GIZA++，可是仍然离不开命令行参数的输入。本文中我们的目的就是利用GIZA++生成的可执行程序来直接进行词语对齐，这样我们既可以完整的保留GIZA++的完整性，也可以节省了开发时间提高了科研效率。     系统开发过程中，我们利用Cygwin生成GIZA++的可执行文件，然后借用批处理技术，再加上微软的Visual Studio 2008，最终开发出了一个方便操作的词语对齐系统。    具体的方法就是：      首先使用Cygwin在windows中，编译GIZA++ ,生成可执行文件，主要需要：mkcls.exe,plain2snt.out, snt2cooc.out, GIZA++.exe     然后，把这些编译好的可执行文件，放入一个批处理文件中，表3给了一个例子：其中，chinese和english是我所采用的双语语料库文件名。 表 3. giza.bat批处理文件内容示例 giza.bat批处理文件命令行参数 mkcls.exe -c80 -n10 -pchinese -Vchinese.vcb.classes opt mkcls.exe -c80 -n10 -p english -V english vcb.classes opt plain2snt.out chinese english GIZA++.exe -S chinese.vcb -T english.vcb -C chinese_english.snt   再其次就是使用VS 2008进行界面设计了。这里面的关键是如何去调用这些可执行文件。下面给出实用的函数等。 表4. 使用shellExecute()函数调用可执行文件 控制shellExecute()函数是否运行结束的方法         SHELLEXECUTEINFO sei; //定义变量        memset(&sei, 0, sizeof(SHELLEXECUTEINFO)); //在内存中填充数值        sei.cbSize = sizeof(SHELLEXECUTEINFO);        sei.fMask = SEE_MASK_NOCLOSEPROCESS;        sei.lpVerb = _T(\"open\"); //打开可执行程序        sei.lpFile =_T( \"GIZA++.exe\"); //调用的程序名        sei.nShow = SW_HIDE; //调用的过程中不显示GIZA++的运行界面        ShellExecuteEx(&sei); //执行调用可执行文件        WaitForSingleObject(sei.hProcess, INFINITE); //等待程序运行后才执行其它程序        CloseHandle(sei.hProcess);   //关闭进程               通过这种方法，就可以把GIZA++弄到Windows中来使用了，就看你自己设计的界面美观程度了，直接一个按钮就可以使用GIZA++所有在Linux中的命令和效果。其中图一是我设计的最初效果，这里是完全使用GIZA++，这种方法的优点是使用简单，设计起来较容易。缺点就是速度比Linux下的GIZA++稍微慢点。当然了我现在设计的词语对齐就不在使用GIZA++了，图二是最新的效果，这种离开了GIZA++限制的对齐速度和效果要好多了。随着开发的完善，实验室准备免费提供给用户使用，希望对windows下需要的用户给予帮助。   图一，GIZA++ Windows 移植方法的界面，最初的设计。 图二，最新的不再采用GIZA++的对齐界面，这是多语句的对齐     1 http://www-i6.informatik.rwth-aachen.de/Colleagues/och/software/GIZA++.html 2 http://www.pudn.com/downloads135/sourcecode/windows/detail575412.html","title":"运行于Windows中的GIZA++( GIZA++ working inWindows Platform）"},{"content":" 学计算机11年 从2000年进入大学至今学习计算机已经有11年余，本科的专业是计算机科学与技术，初接触计算机学的内容是c程序设计和汇编语言，好多专业术语都不懂，当时大家普遍感到吃力的很，因为之前并没有接触过计算机，没有学习过计算机基础，连基本的操作都不会，当时的计算机还没有现在这么普及。于是，在国庆节期间，找老师补习了几天的计算机基础，在前2-3个月学习汇编，和c程序设计，感觉都是似懂非懂，不懂的专业术语赶快找宿舍的冲哥问，随着时间流逝，慢慢才入了门。第一学期可以说是非常艰难。到了第二学期，我们开设的是数据结构，操作系统，由于在前一学期c语言没有学好，基础不扎实，数据结构用的是严蔚敏的教材，里面的代码不是真正的c语言，尽管现在看来，严的教材是非常经典，但是在当时对于初学数据结构的我们来说，是不适合的，我们又买了一些参考书，下了好大一番功夫，渐渐有所领悟。但是，真正对数据结构，c语言比较熟悉的时候是从大二结束后的暑假，当时，我买了，两本老外的c语言和c++语言，整整看了一个暑假，之后，才真正算是熟悉了c语言和数据结构。紧接着，我和同班同学参加了计算机等级考试四级和计算机等级考试三级pc技术，很顺利通过了考试。在此期间，我的理论和实践水平有了很大提高，经常到图书馆看书，晚上上机调试程序，那时候，是非常充实的。 到了大四，我又参加了软件水平考试高级程序员，也顺利得以通过。至此，我的大学本科计算机的学习基本算是结束了。 参加工作了两年，我又考入了西北大学继续学习计算机，专业是计算机应用技术，方向是自然语言处理，这期间，主要开阔了视野，提高了自己的理论水平，发表了两篇核心期刊论文，分别发表在计算机应用和计算机工程与应用上。期间，在我们实验室主要是看论文、参考文献，做了一些小的项目，并取得了研究生奖学金。 在研究生期间，结识了不少计算机方面的高手，也对计算机领域有了比较深的认识，自己的vc技术也有了相应的提高，并出版了自己的著作《零基础学数据结构》和《C语言从入门到精通》（电子工业出版社）。 大学生活是值得回味的，在学习上，我在本科的时候，有个计算机软件、硬件水平都比较高的赵同学，他不但对各种比较难的程序设计非常精通，还自己设计电路，当时的数字电路和模拟电路也非常好，汇编都是满分，确实水平非常高，但是，这种人不多。研究生期间，有不少在软件公司工作过的，还有高校的老师，都成为我的同学。也算是人生的一段经历。 现在，我的研究生阶段的好友，有两个在读博士，最近要我跟他一块过去，至今犹豫不决，不知道是否该过去呢？至今，觉得我们学校的耿国华教授的数据结构与算法方面确实很牛的。 虽然，我研究生的方向是自然语言处理，但是我现在给自己的研究兴趣是数据结构与算法，希望在这个领域有点小小的收获吧。 希望喜欢这篇文章的朋友和我成为朋友，经常沟通与交流。","title":"我学计算机11年"},{"content":"怎样选题   前文曾提到科学研究的层次，并分了6个层级。此处所说的选题指的是从C到E三个层次上的选择问题，即：C. 研究方向、D. 子方向、E. 课题。选择研究方向是实验室(Lab)主任们需要重点思考的事情，选择子方向是研究小组(Group)的组长们需要重点思考的事情，选择课题是研究生们需要重点思考的事情。   选择太多，很容易让人困惑，要想理出一个头绪来，需要一些基本的原则。微软的许峰雄来访时谈到了他选择课题的三个标准：有足够的兴趣，能成为世界第一，能赚钱。（！）兴趣，这个原则是非常重要的，我赞同，获得国家最高科技奖的“黄土之父”刘东生院士是搞地球环境科学的，经常在野外作业，按常人推断，这该是多么枯燥艰苦的工作啊，但他说：“枯燥？不！因为经常有新发现，其中的乐趣难以形容”。我坚信任何一个成功的科学家的直接工作动源都是兴趣，而不是意志。（2）成为世界第一，不容易，但是应该作为一种判断标准，如果某个领域已经非常成熟，很难有什么创新了，或者大牛云集，已经打破头了，则应该有所回避。（3）赚钱，许峰雄是在工业研究院中工作，比较注重实用，因此他强调了“赚钱”，我是在工科大学里工作，也比较偏重应用，因此是赞同“能赚钱”这个标准的。不过，“能赚钱”不等于立即赚钱，5年、10年，20年后能够赚钱的研究课题都是值得关注的。   谈谈我选择课题的一些体会： 1、 要有实际需求 一个课题必须有实际需求，可能是现实的需求，也可能是潜在的需求；可能是直接的需求，也可能是间接的需求，总之是的的确确被人们所需要的。据个反例，比如自动文摘，自动文摘是我的博士论文课题，但是实际应用需求始终不清楚，自动文摘的结果用于编辑出版，质量肯定无法保证，用于帮助人们快速浏览资料吧，Google提供的包含查询词的简单的Snippet就起到了这个作用，因此，至今基于全文分析的单文档自动文摘到底用到哪里，仍然不清楚，这方面的研究已经有50多年的历史了，仍然是不死不活，总是找不到应用就无法得到政府和企业界的持续性支持，以往的付出成为鸡肋。我觉得单自动文摘不是一个好课题，目前阶段多文档文摘，或者说对某个题目的自动综述分析是非常好的题目。   2、 有较大的未知空间 以手写体汉字识别为例，市场上已经大面积应用了，在研究上就不宜再展开。   3、 与自己以往的工作有关联 如果你觉得自己的研究领域太窄，或者竞争对手太多，或者自己缺乏兴趣，则可以适当扩展研究方向，但最好是相关性地扩展，比如从自然语言处理(NLP)扩展到信息检索(IR)，IR要用到NLP的技术，这种扩展是从底层技术到应用系统的扩展，很自然。再比如从图片检索扩展到视频检索，只是处理对象有变化，很多原有的技术优势仍然能够发挥。如果跳跃性太大，比如搞NLP，忽然发现做数据挖掘有前途，于是单纯地转向数据库中数据挖掘，和文本处理完全脱节，这种做法一方面无法发挥既有的技术积累，另一方面也让同行感觉你不够专注，不容易得到认可。最要命的是有的人根本就没有自己的方向，什么课题都敢接，这样的人可以一时间让人觉得风风火火，经费也很充足，但过不了多久就会摔落下去，因为缺乏积累，学术形象不清，公鸡下蛋，干了自己不擅长的事情，在学术圈还怎么混？   4、 有可能得到国家的支持 对于资深学者，他选定一个课题后，可以写出立项建议，去说服政府或军方支持他的工作，从而填补国家空白，成为国内这个方向的先驱。哈工大的杨孝宗老师借鉴CMU在wearable computing方面的研究成果，在国内率先提出穿戴计算机的概念，坚持多年，就获得了军方的认可。对于刚出道的年轻人，无力直接影响政府，那只有自己预先判定一个几年后可能成为热点的方向，先走一步，做出一些成绩来，等到大气候适宜的时候，由于他已经取得了一定的成果，也有可能被认可为这个领域的先行者，得到国家的支持。   课题的类型   对一个课题的类型要有一个判断，是研究型的还是开发型的，如果是研究型的，要组织博士生们来攻关，鼓励大家大胆尝试，提出创见；如果是开发型的，要更多地召集硕士生们来做，强调利用一切现有的技术手段把技术或系统做到实用可靠。这两者要分的比较清楚，既不能通过各种打补丁的方法，或者说一大堆小技巧来对付研究型的课题，因为那样是做不出突破性进展的，也不能在开发类课题上总是异想天开，尝试还很不成熟的技术。   如果是研究型课题，还要区别是基础研究还是应用研究，基础研究的结果不能直接被用户使用，类似重工业，应用研究的结果最终用户直接就能够用上，类似轻工业。对于基础研究，可以抛开具体应用的约束，专注于一些科学原理技术原理的突破。对于应用研究，则需要考虑用户的需求。   课题还有长期(long term)和短期(short term)之分，长期研究的课题往往难度大，研究结果难以预料，短期项目则比较好预测，可以速战速决。","title":"怎么做研究（三）"},{"content":"什么是科学   科学是分科的学问，客观地说，是起源于西方的。中国只有经验科学，典型的如中医。我的母亲是学中医的，我从小就对中医耳濡目染，生了病，妈妈就会请他的老师来，一贴小药下去，我的病就好了。因此，我对中医一直是很信服的。然而，近些年来，中医多受批评，发展也越来越缓慢，究其原因，中医不是科学，或者说只是经验科学，而非实证科学。中药的成分以及生化功效不曾用实验进行深入的分析，望闻问切的诊断方法完全凭经验而无法量化，阴阳五行的理论似是而非，祖传秘方的传承方式与知识共享的现代思维背道而驰。因此，尽管中医有诊治的整体观和方剂的个性化两大优点，但其停留于经验层面，而迟迟不能进入科学的殿堂，因此在现代社会中的发展必然步履维艰。   中医不是科学，那到底什么是科学呢？科学（自然科学）是人们用来认识和改造自然世界的思维武器，科学研究可以分为基础研究（理论研究）和应用研究（技术研发）。     基础研究   万事万物皆有其规律，掌握并且利用这些规律就能够为人类造福，这些规律是隐蔽在纷繁复杂的现象背后的，要识破大自然的奥秘，读懂上帝的天书，非要下一番深入观察和探究的功夫不可。以揭示规律为目的的研究活动属于基础研究，从事这些活动的学者是科学家。规律不是被创造出来的，而是早已存在的，人们只有认识规律的权利，而没有创造规律的可能。   从根本上讲，推动基础研究的也是人们在生产生活中的一些实际需要，但是随着基础研究的深入，理论已经成为一个庞大的体系，理论研究早已开始按照它自有的逻辑独立发展，而不必时时刻刻联系实际需要，比如著名的歌德巴赫猜想，可能在百年之后，发现其有重大的应用价值，但是目前到底有什么用，谁也说不清楚。理论的价值在今天这个非常讲求短期功利的社会中常常被忽视，现在有一种倾向认为只有产生实际经济效益的科研工作才有价值，这种极端化的观点显然是错误的，我们必须承认并高度尊重理论研究者的成就。   理论研究的直接动力是科学家的好奇心，以及他们对科学荣誉的渴望。越是单纯的科学家越有希望发现真理，他们的科学探索有点像迷宫探宝或者海边拾贝，伟大的科学家都是没有丧失童趣的人，他们在实验室里是宁静而愉快的，他们是乐此不疲的，很多在常人看来难以忍受的寂寞在他们看来却是一种幸福。越是找不到答案，越是激发探索的热情，在一次次的失败中积累着烦闷与紧张，在终于取得突破后兴奋异常。与此同时，也必须承认科学荣誉也是激励科学家们前进的重要动力，只要别把荣誉看得高于真理，货真价实的荣誉仍然是值得追求的。   理论上的突破对应用研究产生持续不断的推动力，在模式识别领域，神经网络、支持向量机、条件随机域等等机器学习技术不断出现，每当一项理论出现，应用研究者们争相将其应用于自己的研究课题中，于是基于神经网络、基于支持向量机、基于条件随机域的某某研究就成为一个标准的论文题目。首先把某项理论应用于某个实际课题的研究工作应该说还是具有一定的创新性的，毕竟用一个新的思路、新的模型去观察了一个旧的课题，HMM在语音识别上的成功应用就是一例。有人比喻说，理论工具仿佛是锤子，实际课题好比是钉子，一个新的锤子被打造出来，大家都借用过来砸一砸自己手头的钉子，确属常理。不过，需要注意的事，如果拿一个硕大无比的汽锤去砸一个纤细的大头针就荒诞可笑了，不注意思考问题与理论的适配关系而盲目跟风的事情在学术界也是司空见惯，比如我们就曾用HMM试图解决词义消歧的问题，而每个多义词的词义跟它前后一两个词并没有紧密的关系，因此词义消歧貌似和词性标注一样属于线性序列标注问题，其实是有根本差别的。     应用研究   我们是搞计算机的，计算机是一门应用科学，应用科学是由应用驱动的。时至今日，数学定理和物理学定律似乎已经被先哲们发现的差不多了，因此整个科学界中纯粹搞理论研究的人越来越少，很多大学教授都和工业界有着密切的联系，很多大企业也开办企业研究院，这些导致应用科学的研究如火如荼。最近，国家863设立了一个“中文为核心的多语言信息处理”重点项目，总经费7000万，这在多年前的大陆语言处理界完全是不可想象的。   应用驱动，也可以说是市场驱动。市场是一个精灵古怪的家伙，搞应用研究的人如果对市场的未来没有一个基本准确地判断，往往会导致选题上的偏差。二十年前，国内一些研究者开始研究汉字手写输入技术，开始人们觉得从键盘输入汉字很困难，手写输入一定有前途，但是很快，拼音输入法大面积普及，而且拼音输入的速度远比在手写板上输入汉字快得多，于是汉字手写输入套件根本卖不动，前景黯淡。有人开始犹豫，有人开始转向搞印刷体汉字识别等，但忽然有一天，集成了手写功能的商务通大量热销，人们忽然发现原来在手持设备上由于键盘太小，输入不便，给手写功能留下了很大的应用空间。一直专注于手写识别的汉王公司也借着商务通的热销而把多年的科研成果成功地产业化了。再举一个例子：5年前，我认为以图像为输入的图像检索没有什么应用价值，问这些技术的倡导者，他们也只说能够在数码相册中可以找到一些应用，但近来听了微软一些学者们的演讲，他们提到可以用手机拍下一个植物的图片，传回服务器，在大量植物图片库中检索，找到最相似的植物，并给出植物的名称，特点等。哈哈，这对于我这个五谷不分的人来说实在是太有帮助了，可见对于一项技术是否有用实在要仔细思考，不要早下断言。   技术和市场是一个互动的关系，有人认为技术严格地从用户的现实需求出发，这个观点总的来说没有错，但是忽视了技术创造需求的一面。大多数用户往往并不了解技术发展到了什么程度，他们提不出需求来，这时技术专家们需要把技术和产品做出来给人们看，刺激、引领用户的需求，比如数码相机，5年前我想大多数用户和我一样并没有淘汰胶卷相机的强烈要求，但当数码相机进入市场后，人人都意识到：原来我需要这个东东。   在市场与技术的互动中，总的来说，还是市场在引导和拉动技术的发展。市场需要的是产品，产品往往集成了多项技术，因此一项被市场接受的产品能够推动多项技术的进步。比如搜索引擎，它拉动了自然语言处理、并行计算、海量存储设备、数据挖掘等等多项技术的发展。最近中国计算机学会设立了王选奖，在中国真正有市场眼光，能够发明一项技术，拉动一个行业的计算机专家，王选是第一人。怎样根据市场选择研究方向，设计产品，调整技术形态，我在后面还有详细阐述。     科学技术的力量   科学技术的力量是巨大的，爱因斯坦给出的公式E=M*C2，C是光速啊，质量乘以光速的平方，这是多么巨大的能量啊，爱因斯坦的理论直接导致了原子能的利用与开发。基因图谱的发现以及后基因组时代对基因图谱的深入分析必将为人类征服疾病提供一条崭新的解决道路，通过对损坏的基因进行修复，将使无数患者得以康复，无数家庭重拾幸福。互联网的发明，把全世界连为一体，过不了多久，石头里也会嵌入芯片，在这个世界上有生命的、无生命的各种物质之间都可能进行通讯，人们的生活面貌已经彻底改变了。   当然，科学也是双刃剑：原子弹爆炸了，核战争始终威胁着人类；在对基因组这套上帝给出生命密码没有全面理解以前，任何盲动都可能导致基因污染，以至于玩火自焚；互联网上的虚拟生存让人们感到更加孤独。","title":"(转载)怎么做研究（一）"},{"content":"  中国计算机国家重点实验室 　　 　　中国科学院软件研究所：实验室名字就叫做“计算机科学国家重点实验室”。目前，该实验室是国内唯一一个以从事计算机科学和软件方法与技术的基础研究为主的国家重点实验室。 　　 　　中国科学院自动化研究所：模式识别国家重点实验室。实验室以模式识别基础理论、图像处理与计算机视觉以及语音语言信息处理为主要研究方向，研究人类模式识别的机理以及有效的计算方法，为开发智能系统提供关键技术，为探求人类智力的本质提供科学依据。 　　 　　清华大学：体系结构和智能工程方面。起初学校有《计算机系统结构》和《模式识别与智能系统》两个国家级重点学科，现已拥有计算机科学与技术的国家重点学科。（后者属自动化一级学科） 　　 　　北京大学：软件和应用两个方向可谓全国第一，有王选院士和杨芙清院士坐镇。拥有“视觉听觉信息处理”和“文字信息处理”国家级重点实验室（国内首开Case先河的青鸟系统出于此）想必方正软件和北大青鸟大家都如雷灌耳吧！再次，北大拥有计算机科学与技术的国家重点学科。 　　 　　浙江大学：拥有CAD/CG（计算机图形学和计算机辅助设计）国家级重点实验室。拥有计算机应用技术专业的国家级重点学科。 　　 　　北京航空航天大学：软件开发环境国家重点实验室。是在计算机软件理论、技术和开发环境等方面开展基础、应用基础与竞争前高技术的开放式研究基地，是创新型高层次人才的培养基地。也拥有计算机科学与技术的国家重点学科。 　　 　　国防科技大学：一开始只拥有系统结构专业的国家级重点学科，现在以拥有计算机科学与技术的国家重点学科。银河系列大型机也许正是我们选择国防科大最充分的理由。 　　 　　哈尔滨工业大学：拥有全国第一个的计算机应用专业的国家级重点学科（当然了，现在很多学校都有了），现在哈工大以拥有计算机科学与技术的国家重点学科。学校智能机器人是相当有名的。现还拥有计算机信息内容安全国家重点实验室、计算机接口技术与接口系统国家重点专业实验室。 　　 　　南京大学：起初拥有计算机软件与理论专业的国家级重点学科和计算机软件新技术国家级重点实验室。现在也拥有计算机科学与技术的国家重点学科。学校的软件专业甚至涉足操作系统软件的开发研究。 　　 　　东北大学：拥有软件工程国家级工程研究中心。朋友们听说过东大阿尔派软件吗？水平可见一斑。数据库技术也是该校的强项。东北大学也拥有计算机应用技术的国家级重点学科。 　　 　　东南大学：拥有计算机应用技术专业的国家级重点学科。网络方向是其特色，已故的顾冠群院士是中国第一个计算机网络方向的院士。 　　 　　上海交通大学：本拥有模式识别和智能系统的国家重点学科和计算机软件与理论的国家重点学科。现以拥有计算机科学与技术的国家重点学科。 　　 　　西安电子科技大学：拥有ISN(综合业务网）的国家级重点实验室，信号处理专业力量很强，如密码学等。 　　 　　中国科技大学：拥有计算机软件与理论学科国家级重点学科。 　　 　　吉林大学：拥有计算机软件与理论的国家重点学科。 　　 　　武汉大学：软件工程国家重点实验室，计算机软件与理论学科国家级重点学科。 　　 　　华中科技大学：拥有计算机系统结构专业的国家重点学科。 　　 　　西北工业大学：拥有计算机应用技术的国家重点学科。 　　 　　复旦大学：拥有计算机软件与理论的国家重点学科。 　　 　　四川大学：拥有计算机应用技术的国家重点学科。 　　 　　安徽大学：名不见经传的她也拥有计算机应用技术的国家级重点学科。 　　 　　 　　总结： 拥有计算机方面国家级重点学科的高校如下： 　　 　　计算机科学与技术（一级学科国家重点学科，即以下三个方向都是国家重点学科，这7个学校都是07年新增的）： 　　北京大学、北京航空航天大学、国防科学技术大学、哈尔滨工业大学、南京大学、清华大学、上海交通大学； 　　（计算机科学与技术国家级重点学科的高校以下三个门类的学科均默认为国家重点，不再重复罗列） 　　 　　计算机系统结构：华中科技大学； 　　 　　计算机软件与理论：吉林大学、中国科技大学、武汉大学、复旦大学；（浙江大学和重庆大学正在培育中 - _ -!） 　　 　　计算机应用技术：东北大学、东南大学、浙江大学、安徽大学、西北工业大学、四川大学；（电子科技大学正在培育中 - -!） 美国计算机专业前20名学校 　　 　　　　一、4个最为优秀的CS Program: Stanford, UC. Berkeley, MIT, CMU 　　 　　　　二、6个其他前10的： UIUC， Cornell, University of Washington,Princeton, University of Texas-Austin 和 University of Wisconsin-Madison，其中UIUC, Cornell, University of Washington和UW-Madison几乎从未出过前10。 　　 　　　　三、其他非常非常优秀的CS：CalTech, University of Maryland at CP, UCLA, Brown, Harvard, Yale, GIT, Purdue, Rice, 和 University of Michigan. 　　 　　Stanford　　　URL: http://www.stanford.edu/ 　　 　　　　Stanford的CS是个很大个的CS，拥有40人以上的Faculty成员，其中不乏响当当硬梆梆的图灵奖得主(Edward A.Feigenbaum, John McCarthy) 和各个学科领域的大腕人物，比如理论方面的权威 Donald E. Knuth; 　　 　　　　数据库方面的大牛Jeffrey D. Ullman(他还写过那本著名的编译原理，此人出自Princeton)；以及RISC技术挑头人之一的John Hennessy。相信CS的同学对此并不陌生。该系每年毕业30多名Ph.D.以及更多的Master。学生的出路自然是如鱼得水，无论学术界还是工业界，Stanford的学生倍受青睐。几乎所有前10的CS中都有Stanford的毕业生在充当教授。当然同样享有如此地位的还包括其他三头巨牛：UC.Berkeley, MIT 和 CMU. 　　 　　　　毕业于University of Utah的Jim Clark 曾经在Stanford CS当教授。后来就是这个人创办了高性能计算机和科学计算可视化方面巨牛的SGI公司。SUN公司名字的来历是：Stanford University Network.。顺便提一下，创办 YAHOO的华人杨致远曾在斯坦福的EE攻读博士，后来中途辍学办了YAHOO。 　　 　　　 CS科研方面，斯坦福无论在理论，数据库，软件，硬件，AI 等各个领域都是实力强劲的顶级高手。斯坦福的RISC技术后来成为SGI/MIPS的 Rx000系列微处理器的核心技术； DASH，FLASH 项目更是多处理器并行计算机研究的前沿；SUIF并行化编译器成为国家资助的重点项目，在国际学术论文中SUIF编译器的提及似乎也为某些平庸的论文平添几分姿色。 　　 　　　 Stanford有学生14000多，其中研究生7000多。CS有175人攻读博士， 350人攻读硕士，每年招的学生数不详，估计少不了，但不要忘了，每年申请CS的申请学生接近千人。申请费高达 $80。 　　 　　　　斯坦福大学位于信息世界的心脏地带－－硅谷。加州宜人的气候，美丽的风景使得Stanford堪称CS的天堂。33.1平方公里的校园面积怕是够学子们翻江蹈海，叱姹风云的了。 　　 　　　　申请斯坦福是很难成功的，但也并非不可为之。去斯坦福这样的牛校，运气很重要，牛人的推荐也很重要。 　　 　　MIT　URL: http://www.mit.edu/ 　　 　　　　MIT 招生好象不看GRE成绩。但MIT的CS是巨牛的，99年最新排名上它和斯坦福被打了5.0 的满分，并列第一。MIT的CS曾为CS的发展作出不可磨灭的贡献，数据流计算的思想和数据流计算机、人工智能方面的许多重大成就，以及影响了整个 UNIX界的X-Window……MIT和斯坦福，CMU， UC. BERKELEY一样，都是几乎在CS界样样巨牛的学校。 　　 　　　　MIT的Media Arts and Sciences其知名度不在Computer Department下。主要是多媒体技术，信息处理，人工智能……有一大批著名的教授，如Marvin Minsky (Turing Award)…… 　　 　　UC.Berkeley 　　URL: http://www.berkeley.edu/ 　　 　　　　同样地处旧金山湾畔，硅谷地带，离Stanford只有大约 50公里的加州大学伯克利校区：UC.Berkeley是美国最激进的学校之一。60年代的嬉皮文化，反越战，东方神秘主义，回归自然文化都起源于此。诗人爱伦金斯堡是当年 Berkeley的代言人。 　　 　　　　在当今高科技领域C. Berkeley 在缔造新的神话，在文学，数学，化学，新闻等20多个大的学科领域中位居前3. 16个诺贝尔奖得主，总数近200的科学院院士、工程院院士，连同众多在硅谷商战中成为亿万富翁的伯克利人撑起了一面汇集天下之英才的大旗。INTEL总裁AndrewGrove毕业于UC. Berkeley。 　　 　　　 BSD版的UNIX影响了整个OS界，伯克利的RISC技术后来成为了SUN公司SPARC微处理器的核心技术，巨牛人物David Patterson接下了一个6亿美元的项目用于新型计算机体系结构，特别是IRAM的研究开发。 　　 　　　　UC. Berkeley有学生30000多，研究生超过8500。申请费和其他加州大学的分校一样，40$。据一项最近的调查，伯克利已经成为美国大学生最向往的研究生院，高居榜首，其申请的难度可想而知。UC.Berkeley的 DEADLINE一般很早，12月中就截至了，其内部的实际DEADLINE其实要迟一些。 　　 　　　 Berkeley的CS是个大系，Faculty中有图灵奖得主以及象 Patterson这样的巨牛。学生的出路同Stanford，MIT，CMU一样，光辉灿烂，前程锦绣，这里不再赘述。CS科研方面，Berkeley也是样样强，门门巨牛。 　　 　　　 旧金山湛蓝起伏的海湾，苍翠绵延的山峦，舒心宜人的气候，以及近在咫尺的硅谷…… 这一切的一切不也使得UC.Berkeley 俨然一个CS 学子的世外桃源么？ 　　 　　CMU　　URL: http://www.cmu.edu/ 　　 　　　 CMU是个位于匹兹堡的不大的学校，学生7000多，校园好象也不大。但这个学校在工程及其他一些领域却是顶尖的学堂。 CMU的 CS 不单单是个系，而是一个学院，其规模之大，可能只有Stanford, UIUC可比。教师学生的情况同前面3个类似，不再赘述。Mach操作系统，PVM，C.mmp等都有CMU的巨大贡献。 　　 　　　　申请CMU的难度很大，因为尽管CMU的 CS Faculty很多，但每年只招不足30人的研究生队伍。 　　 　　Cornell　　URL: http://www.(cs.)cornell.edu 　　 　　　　作为 IVY LEAGUE的成员和一所私立学校，Cornell有其独到的优势。在美国，私立学校一般比公立学校难进，其学生也是经过很严的选拔才录取的，Cornell的CS学生入校后多能享受FELLOW的待遇，其个人经济条件非公立学校可比，加上贵族式校友的提拔，私立学校的出路是很诱人的。 　　 　　　 康乃尔在理论计算机方面一直是顶级高手，但在其他CS领域并不总能在前10。 Cornell学生18000多，研究生过5000。CS每年招攻读Ph.D.的学生25 人左右。 　　 　　UIUC　　URL: http://www.uiuc.edu/ 　　 　　　 UIUC的工程院在全美堪称至尊级的巨牛，其CS，ECE，EE在历史上都屡建战功。在CS方面，从早期的超级计算机ILLIAC I, II, III, IV到后来的 CEDAR，都是CS发展史上，特别是并行计算机发展史上的重要事件，影响，引导了很长时期的发展。 David Kuck曾是并行处理界的一代先驱。 　　 　　　超级计算机研究开发中心：CSRD，美国国家超级计算及应用中心：NCSA等众多的机构，使得UIUC的CS常常成为研发的领军头领。 　　 　　　　大家可能还记得，Netscape-Navigator 的最初开发人员中有个Marc,Anderssen。这位来自WISCONSIN的小伙在UIUC读本科，大四的时候在NCSA参与编写了MOSAIC，后来他去了硅谷，并在那里遇到了前面提到过的大牛: Jim Clark,SGI的前创始人，两人一见如故，联手创办了著名的网景，并一度在浏览器市场上独霸武林。 　　 　　　　随着一代代至尊大师的离去，UIUC 的 Faculty看上去似乎并不引人注目。但得提醒你，UIUC的CS向来以实干著称。我期待着他们下一个惊世之举。 　　 　　　 UIUC是个大学校，学生数过35000，研究生院的近万。UIUC的CS很大个，40余个Faculty提供了全面的CS教育和科研项目。每年30多个博士的毕业数目似乎只有斯坦福可以匹敌。 　　 　　　 UIUC的Polaris并行化编译器是这个领域和斯坦福的SUIF直接叫板的拳头产品。清华开发并行编程环境时选用了这个系统。只是代码庞大，运作缓慢的Polaris搞的清华有那么一点点瘪西西... 　　 　　　　UIUC 在计算机硬件，软件，AI，DB，等各个领域都相当巨牛。特别是硬件，前面提到的ILLIAC，CEDAR.....事实上，UIUC在超级计算机系统的研究开发方面决不逊于CS四大天王中的任何一个，甚至有过之而无不及。NCSA建立在UIUC这一事实本身就是佐证。 　　 　　　　UIUC-CS 的学生毕业后去学术界的不少，Stanford, Berkeley...都有UIUC的博士挑大梁。但更多UIUC-CS学人还是进入业界，成为业界实干的中坚。 　　 　　University of Washington　　URL: http://www.washington.edu/ 　　 　　　　位于 Seattle的 UW 得天独厚--计算机界的巨牛MS就在西雅图，而且 更为要命的是，Bill Gates就是那里儿的人。这位Harvard的辍学者给了哈福许多MONEY, 但同给UW的钱财相比，实在是小巫见大巫。 　　 　　　　University of Washington位于分光秀丽的WASHINGTON湖畔，气候四季如春。33000多学生中研究生有8000。Seattle最令人厌恶的地方可能就是一年有160天会降水。 　　 　　　 UW的CS较大，30多名Faculty成员，每年近20个优质博士毕业，以及大量的Master。估计每年的招生数应该不低，UW的CS在各个方面比较均衡，最强的软件排名第5，而其他领域也一般都能位居前10，好象没有明显弱的地方。 　　 　　　　图灵奖得主 Dick Karp从Berkeley告老还乡后又被返聘到了UW的CS。U. of Washington的 CS要求很高，Ph.D.学生入学的平均 GPA 高达 3.86, GRE2160+，加上一般较早的DEADLINE，申请UW是相当有难度的。 　　 　　Princeton　　URL: http://www.princeton.edu/ 　　 　　　　Princeton是个令人神往的地方，这里曾经是科学的世界中心。Princeton的CS不大， 18个Faculty成员，学生数也不算多。科研上除了排名第5的理论，似乎俺还没注意到其他闪光点，望知情人补充。但是，Princeton无疑培养出了大量计算机界的优秀人物，Jeffrey D. Ullman, John McCarthy等巨牛人物均出自大名鼎鼎的Princeton. 在Princeton领受的教育是最好的教育熏陶。 　　 　　　　Princeton学校不大，只有6000多学生，研究生不过1700。Princeton 的 CS录取很严，虽然已有不少华人学生就读 Princeton。 　　 　　UW-Madison　　URL: http://www.(cs.)wisc.edu 　　 　　　　UW-Madison的CS较大，35个Faculty, 200多个研究生，每年招60-70 个新生。 目前几乎 1/4 的 Faculty 来自Berkeley，博士生毕业后有去 Stanford, Berkeley等牛校挑大梁的，但和UIUC类似，似乎进入业界的更多些。然而要在这里拿到博士学位可不容易。超过7成的人，会在中途找到比较理想的工作后，拿着硕士文凭撒丫子就跑，免得被那些无穷无尽的科研项目给整瘪了。一位WISCONSIN的哥们在回答我关于 “该做些什么准备”的提问时说：尽情欢乐享受吧，这样可以 Bring A healthy and energetic you to Madison to survive those projects。 　　 　　　　UW-Madison的数据库一直在前 3 位，经常是第1位。这里的数据库由于在设计实现DBMS系统上的传统优势，使得其在业界的声誉相当崇高， MicroSoft 里据说有一帮WISCONSIN的校友从中兴风作浪，Oracle也格外青睐WISCONSIN-Madison的学子。可惜，偶似乎对数据库并不是很感兴趣。 　　 　　　 WISCONSIN的硬件，计算机体系结构实力巨牛，99排名第 6，对业界相当有影响力。微处理器中的超标量技术（SuperScalar）源于此地；多处理机CACHE一致性的总线侦听SNOOPING协议，IEEE SCI协议等，都是源于此地。正在研究开发中的MultiScalar技术和DataScalar技术据吹可以把微处理器每个时钟周期的指令发射数提到10以上，大大地提高微处理器的计算能力。WISCONSIN的软件99排名第 7。主要是在系统软件方面做OS的设计与实现，WEB上的CACHE策略，支持共享主存和消息传递两种并行编程模式及其混合的并行程序设计语言和编译器，以及由MIDSHIP项目挑起的关于并行与分布式计算，OODB，科学数据库，支持图象查询的新型查询语言以及图象处理等方面的研究。由于美国有大量的卫星图象需要及时处理，加上迫切需要GIS系统的研究开发，这方面的研发使得UW-Madison 捞到了不少经费。 　　 　　　 WISCONSIN和UIUC的CS理论都是10名左右。WISCONSIN的Carl de Boor 是逼近理论方面的大牛。 　　 　　　　University of Wisconsin-Madison是个大型综合性的学校，40000 学生中研究生院的超过10000,这万人中有博士生5000，硕士生3500，法学院、医学院、护院、兽医院的职业学生2000人。2200多 Faculty中有多位诺贝尔奖 得主, 52个院士，18个工程院院士。130个科系几乎涵盖了所有科研领域。科研经费常年位居全美前 4。 wisconsin的研究生院稳居 TOP20，而且由于它的大而全，在科研排名上能进前10。UW-Madison在95年NRC 的41项评价中，16项位居TOP10，35项排进了TOP25。 　　 　　　　University of WISCONSIN-Madison的校园位于风景如画的湖畔林荫中. 现代化风格和古典欧美风格的建筑物在平缓起伏的湖岸上交相呼应。学校自吹拥为世界上最美的校园之一。偶不知道其他校园的场景，单从他们在网页上提供的照片来看，的确很美。WISCONSIN的冬天很冷，很长，而且大雪纷飞，寒风凛冽。 　　 　　　 需要注意的是，WISCONSIN的CS有点不同于许多其他学校， 它隶属于College of Letters & Science. 而不在College of Engineering下面，因此许多偏硬件的项目，比如嵌入式系统，网络硬件、路由，多媒体，通信，自控以及数字信号处理及等项目不在CS Dept， 而是在工程院 下的 Dept. of Electrical & ComputerEngineering 即 ECE系。那个系也挺大个，比CS还要大不少。98年在工程类排名的计算机工程一项上也排了第9。但偶将来怕跟他们没多少来往。伊拉的网址： 　　 　　工程院：http://www.engr.wisc.edu 　　 　　ECE系：http://www.ece.wisc.edu 　　 　　URL: http://www.(cs.)utexas.edu/ 　　 　　　 UT-Austin的CS较大。Faculty中好象有个图灵奖得主。( Edsger Wybe Dijkstra，是那个搞算法的)。好象该系发展比较平衡，最好的AI 排第5，其他几个专业也多能挤进前十。 　　 　　　 UT-Austin是个巨大的学校，5万学生，研究生院的可能有1.3万。但学校的主校区却好象面积不足，仅140公顷，按美国大学的标准，太不足了。偶曾见到一张照片，校园周围高楼林立，可能是位于市中心的缘故吧。 　　 　　　　总的来说，前10的CS由于在当前国际计算机行业普遍热门的情况下，很难申请，但决不是不可为之的! 　　 　　CalTech　　URL: http://www.caltech.edu/ 　　 　　　 CalTech的CS很小，只有大约5位教授，每年招很少的学生。虽然申请CalTech是免费的，但建议轻易不要尝试。(也别让我这话给吓趴下了) 　　 　　　　由于系太小，CalTech 好象只是在计算机硬件，和科学计算的可视化方面很强。该系多年以来一直稳坐 NO.11,12几乎没动过窝, 类似的情况 还有斯坦福、MIT，稳居NO.1,2, Cornell稳居NO.5, UW-Madison, 稳居 No.9,10.CalTech的CS和其他系，比如数学，物理，生物等需要大量科学计算的部门联系很紧密。 　　 　　　　CalTech 学校也很小，2000名学生中研究生占1100人。 Faculty人数也不多，但几乎个个是巨牛，按平均水平看，CalTech 可能是世界上最牛的学校了。偶好象就没见到来自大陆的学子在该系，可能是偶孤陋寡闻吧。 　　 　　　 这是一个实力相当强劲的CS，软件(8)，数据库(4)，AI(9) 三个专业都挤进了前10URL: http://www.ucla.edu/ 　　 　　　　历史上 UCLA 的CS曾经一度辉煌，上到过第6 (NRC'82)，但近年来一直徘徊在13－15。而且CS的各个专业细目几乎没有一个能进前10。尽管如此，UCLA的CS还是十分强大的。 　　 　　　 UCLA辉煌的历史可能在于它对Internet的发展，所作出的巨大贡献。六十年代美国的 ARPA在搞网络互连的开创性研究， ARPA网的四个节点是：UCLA，Stanford 的 SRI,UCSB 和 U. of Utah。此时一位来自美国新英格兰地区的青年: Vinton Cerf不去离家咫尺的 Yale大学，远涉千里，来到了加州。他先在Stanford获得数学学士，然后到UCLA拿下了CS 的硕士和博士。 　　 　　　毕业后Cerf一直在SRI从事ARPA网的研究，特别是如何让它无法正常工作。几年后，Cerf与MIT的一位到业界闯荡的数学教授Kyhn合作研发，搞出了一套软件系统用于网络互连(1973)。这就是TCP/IP协议的诞生。 　　 　　　　UCLA 作为 INTERNET 的先驱，地处阳光灿烂的南加州，应当成为CS学生的乐土。加州的学校的确难申请，但也是可以一试的。 　　 　　　　UCLA有学生33000人，其中研究院的占 9900人。地处落杉矶的 UCLA,周围几乎有玩不尽的地方，DISNEY，HOLLYWOOD..... 由于位于大城市，校园不是很大，但风景似乎非常美丽。UCLA的CS较大，规模应该和 University of Washington 和 UW-Madison类似。 　　 　　University of Michigan　　URL: http://www.(eecs.)umich.edu 　　 　　　　University of Michigan是个非常了不起的学校。在BIG TEN里，从综合的角度上说它可算的上是领头羊了，当然UW-Madison，UIUC 也紧随其后。 　　 　　　 这里的CS偏硬的更利害些，硬件排在第9，而计算机工程(7)，EE(5) 都是前10中的巨牛。 MICHIGAN 的 CS 和EE合在一起称为 EECS系。是个相当大个的系，每年招的学生当不在少数。 　　 　　　　MICHIGAN的CS估计在历史上也相当牛，UW-Madison CS里的两位来自umich的教授都是院士，在其他CS系里，比如UIUC的，也大有UMICH的牛人在。如前述，UIUC的CS在硬件上极强，而UMICH的CS中有许多UIUC的哥们在那里当老师。 　　 　　GIT　　URL：http://www.git.edu/ 　　 　　　　GIT是个较大的学校，具体数字记不清了。GIT的工程院很利害，研发经费仅次于MIT，和UIUC, Umich差不多。 CS系的数据库第7，GUI第4。其他没有列在前10，偶也没有去仔细了解过，就一概的不清楚了。 　　 　　Brown　　URL：http://www.brown.edu/ 　　 　　　　Brown的规模不大，具体数字记不清了。这所 IVY LEAGUE 的私立学校可能拥有一些类似于CORNELL的优势。CS的GUI可以列在NO.6，好象还有许多关于语音识别等偏人工智能方面的研发项目。 　　 　　Harvard　　URL: http://www.harvard.edu 　　 　　　在CS的早期发展史上，Harvard曾经是泰斗级的人物，毕竟CS和数学，物理的渊源太深太长了。可惜Harvard并不重视工程化的东西，现在伊的CS已不能和圈里的巨牛，甚至伊的当初相提并论了。好象王安是这里出来的，Bill Gates也是这里出来的， Harvard毕竟是Harvard，总是名人辈出。毕竟Harvard总是可以招到最优秀的人，甚至是在它很瘪的领域里。但千万别以为哈佛人人牛。据说美国人的调侃中，专门有一条是说哈佛的某些学生是如何令人叹为观止的愚蠢……偶还没有身在美国，不知是真是假。 　　 　　　　Harvard 不喜欢带工程色彩的东西，CS 是挂在Arts & Science学院下面的Division of Engineering and Applied Science,连独立的一个系好象都不是。除了理论可以排进前10，其他项目怕也拿不出多少货色了。 　　 　　　但是，如果给我一次机会的话，我一定申请Harvard。因为这里是Harvard，你可以学到许多在别的地方难以学到的东西。专业知识并不是全部，况且哈佛的教育是不会差的，虽然它在CS 的科研上没什么好吹的。哈佛的研究生每年超过 20000$ 的FELLOWSHIP是你安心寒窗苦读的强大后盾。 　　 　　　哈佛大学学生18000人，其中研究生院的11000人。Harvard大学拥有世界上最多的诺贝尔奖得主，150多个美国国家科学院院士.......哈佛是个巨牛云集的超级牛圈。哈佛的 CS 估计不会是个大个子， 招的学生可能也不会多，申请的难度应当很大。 　　 　　Purdue　　URL: http://www.purdue.edu/ 　　 　　　　可能许多人还不知道，Purdue 的计算机系是美国最早成立的计算机系。建系之初一直处于TOP10。在70年代由于本人不甚了解的原因，没落了。Purdue的排名也不太稳定从13到 30的排法偶似乎都见过。Purdue是个大个的学校，有35000学生。其工程院很出名。 　　 　　Rice　　URL: http://www.rice.edu/ 　　 　　　　Rice是个位于休斯顿的小学校， 4000个学生，研究生有1600左右吧。CS也不大。优势在于软件，排在第9。别的情况偶不了解，但偶特别想告诉大家的是，该系的 KEN KENNEDY是个巨牛的人物。伊是美国 HPCC 常委的关键人物之一,好象还是总统在信息科学方面的特别顾问。KENNEDY是并行计算领域的大牛牛。前几年,伊义无反顾地承担起高性能 FORTRAN 语言(HPF)的编译器研制工作，项目之大，投入人力之巨，加上伊的权威地位，被人们普遍寄予厚望。可惜后来项目失败了。从此并行计算界陷入了一阵低潮。这几年 KENNEDY 好象转向去作针对特定处理器的后端优化技术了。Rice CS 学生的出路相当好。 　　 　　YALE　　URL: http://www.yale.edu/ 　　 　　　　YALE 曾经也进过前 10，NRC'82 的排名上，是 YALE和 UCLA而不是 Princeton和UT-Austin 位于前 10 的榜上.YALE的 CS不大，十几个老师加上为数不多的学生，每年只招六个博士研究生。 　　 　　　　和 Harvard这样很重文理的学校一样，YALE 的 CS 在理论上比较强。但不同于哈佛，YALE有独立的CS系，受到较高的重视。YALE-CS 在 AI，软件方面比较强。著名的 LINDA 并行编程模式是在这里提出并实现的。 YALE 的毕业生中到学术界的比到业界的似乎要多，哈佛似乎也是这样。 　　 　　　 这里只随便罗列了一些俺顺口拈来的东西，仅供参考。 其实 CS其他很好的学校还有很多，比如： UCSD，USC，Columbia，UNC-CH，DUKE，University of Penn等等。 Columbia在AI，语音识别，自然语言处理等方面颇有造诣，而北卡: University of North Carolina at Chapel-Hill 和 University of Utah方面则是顶级牛校。","title":"中国计算机国家重点实验室、美国计算机前20名学校"},{"content":"AI 顶级会议列表 对AI领域的会议的评点 注: 本文为小百合BBS的daniel所写 The First Class: 今天先谈谈AI里面tier-1的conferences, 其实基本上就是AI里面大家比较公认的top conference. 下面同分的按字母序排列. IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI实在太 大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个领域就没几篇 了，象machine learning、computer vision这么大的领域每次大概也就10篇左右, 所以难 度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内行人都会掂掂分量, 没希望 的就别浪费reviewer的时间了. 最近中国大陆投往国际会议的文章象潮水一样, 而且因为国 内很少有能自己把关的研究组, 所以很多会议都在complain说中国的低质量文章严重妨碍了 PC的工作效率. 在这种情况下, 估计这几年国际会议的录用率都会降下去. 另外, 以前的 IJCAI是没有poster的, 03年开始, 为了减少被误杀的好人, 增加了2页纸的poster.值得一 提的是, IJCAI是由貌似一个公司的\"IJCAI Inc.\"主办的(当然实际上并不是公司, 实际上是 个基金会), 每次会议上要发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以 AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member去找 reviewer 来审, 而不 象一般会议的PC member其实就是 reviewer. 为了制约这种权力, IJCAI的审稿程序是每篇 文章分配2位PC member, primary PC member去找3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可以给到 1+, 也可以给到1-或者2+, 总的来说我给它\"1\". 这是因为它的开法完全受IJCAI制约: 每年 开, 但如果这一年的IJCAI在北美举行, 那么就停开. 所以, 偶数年里因为没有IJCAI, 它就 是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比 IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱一点, 基本上在1和1+之间; 在奇 数年, 如果IJCAI不在北美, AAAI自然就变成了比IJCAI低一级的会议(1-或2+), 例如2005年 既有IJCAI又有AAAI, 两个会议就进行了协调, 使得IJCAI的录用通知时间比AAAI的 deadline早那么几天, 这样IJCAI落选的文章可以投往AAAI.在审稿时IJCAI 的 PC chair也 在一直催, 说大家一定要快, 因为AAAI那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦 了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以 看成理论计算机科学和机器学习的交叉, 所以这个会被一些人看成是理论计算机科学的会而 不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: \"一小群数学家在开会\". 因为 COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便提一件有趣的事, 因为最近 国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出论文集, LNCS/LNAI基本上已经被搞 臭了, 但很不幸的是, LNCS/LNAI中有一些很好的会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题目上 有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识别最好的会 议, 而在计算机视觉方面, 还有ICCV与之相当. IEEE一直有个倾向, 要把会办成\"盛会\", 历 史上已经有些会被它从quality很好的会办成\"盛会\"了. CVPR搞不好也要走这条路. 这几年 录的文章已经不少了. 最近负责CVPR会议的TC的chair发信说, 对这个community来说, 让好 人被误杀比被坏人漏网更糟糕, 所以我们是不是要减少好人被误杀的机会啊? 所以我估计明 年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的介 绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会每 年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会,会开 完后第2年才出论文集, 也就是说, NIPS'05的论文集是06年出. 会议的名字是\"Advances in Neural Information Processing Systems\", 所以, 与ICMLECML这样的\"标准的\"机器学 习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有一定的距离. 但由于会议 的主体内容是机器学习, 或者说与机器学习关系紧密, 所以不少人把NIPS看成是机器学习方 面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对 Jordan系的人来说, 发NIPS并不是难事, 一些未必很强的工作也能发上去, 但对这个圈子之 外的人来说, 想发一篇实在很难, 因为留给\"外人\"的口子很小. 所以对Jordan系以外的人来 说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有些人( 特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事, 但因为 Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选理事, 有资格 提名的人包括近三年在ICMLECMLCOLT发过文章的人, NIPS则被排除在外了. 无论如何, 这 是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI)最好 的会议之一. KR Inc.主办, 现在是偶数昕? SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来越重 . 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至有点机器学 习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短, 毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列在tier-1里 面, 一方面是名声远不及其他的top conference响亮, 另一方面是相对容易被录用. 但现在 它被列在tier-1应该是毫无疑问的事情了. 另: 参见sir和lucky的介绍. UAI (1-): 名字叫\"人工智能中的不确定性\", 涉及表示推理学习等很多方面, AUAI (Association of UAI) 主办, 每年开. The Second Class: tier-2的会议列得不全, 我熟悉的领域比较全一些. AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念, 几乎所有 AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能升级到1- 去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了 . 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常 明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上 升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在 CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因 为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV 和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着, 很难往上升 . ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去 就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但 我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来 , 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来. 但因为ICDM和SDM, 这 已经不太可能了. 所以今年的PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以 同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被PKDD接受). The Third Class: 列得很不全. 另外, 因为AI的相关会议非常多, 所以能列在tier-3也算不错了, 基本上能进 到所有AI会议中的前30%吧 ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的 quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其 实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域 的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC/FUZZ-IEEE这三个会议是计 算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和 其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有 quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名 就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个 session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合 型会议太多, 所以很难上升. Combined List: 说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令人尊敬的 ,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge Discovery in Databases tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fu Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence","title":"[转载]模式识别领域的期刊会议"},{"content":"近期在弄语言模型的一些东西，这篇论文感觉不错,介绍了语言模型里词表自动生成的知识。故翻译一下，加深对论文的理解。这里跟大家分享下。 由于csdn博客上写公式比较麻烦，索性直接截图放上来，有需要的话可以  猛击这里下载原文和我的翻译pdf。 原文名：Lexicon Optimization for Chinese Language Modeling， 原作者：Zhao Jun 翻译：雨石 参考文献没有列出，有兴趣的可以查看原论文。","title":"中文语言模型的词典优化"},{"content":"之前一段时间弄过文本分类的事情，现在发个文总结一下。 文本分类问题的定义是根据一篇文档的内容，从预定义的类别标号里选择相应的类别。 中文文本分类的基本步骤是中文分词、特征提取、训练模型、预测类别等步骤，需要说明的是，基于统计的文本分类一般都需要有比较好的标注好的语料作为训练集，训练出模型，利用模型对未分类的文本进行分类。 对中文文本的处理一个无法避免的步骤就是分词，中文不像英文那样，词与词之间有空格作为分隔符，大部分的中文自然语言处理都无法避免这个步骤。分词算法简单的就是正逆向最大匹配（机械分词），n-gram、最大熵、隐马尔科夫等（统计分词）。现在现有的分词程序包中比较好的要算是中科院的ICTCLAS分词系统了，有兴趣的可以尝试，也可以自己实现简单的机械分词，在文本分类上来说，只要有一个比较好的词表（词表可以去搜狗词库上爬），用最大匹配和其他复杂的分词算法的效果的差距不是很大。 中文分词后文本就成了一个一个的词，这些词就是文本的特征，每篇文章通过含有不同的词语、不同的词语数目来进行区分。但是如果直接用分词后的词语集合进行建模的话，一来词语空间比较大，导致性能比较低；二来词语集合中有很多低频词、无意义词等噪音，也会降低分类的效果，实验证明，通过特征提取后的效果，比采用不提取的词语空间的性能和效果都要好。 特征提取是要提取出最能代表文本的特征和最能区分文本的特征。首先，先把词语空间中的停用词去掉，停用词主要是一些无意义的高频词，比如语气助词（吗、了、啊等），还有类似的、你我他等词语，这些词语几乎在所有文档中都出现过而且会出现很多次，而且跟文档要表达的主题几乎没有关系，因而需要去掉。 去除停用词之后，特征空间仍然很大，这个时候，就要采用统计学规律选取最能区分文本的特征，现有的方法主要有卡方统计、信息增益、互信息、几率比、交叉熵、类间信息差等方法。下面介绍其中的卡方统计，其他方法也能达到很好的效果，有心的同学可以查资料哈。 由于csdn对公式的支持度不高，因此切个图上来。 经过特征提取后，就有了一个特征集合了，文档可以依据这些特征集合来唯一确定一个向量，这就是VSM模型，中文名叫向量空间模型，这个模型假设特征与特征之间是相互独立的，这样虽然损失了很多信息，但从效果看，仍然很好，是现在统计文本分类中使用最多的模型。 这个模型咋用呢？举个栗子说一下，比如有两篇文档，文档1和文档2，经过分词和特征提取后，文档1词语集合（奥运会，体育，冠军，跳水），文档2词语集合（巴以冲突，资本主义，美国），那么特征词集合就是（奥运会，体育，冠军，跳水，巴以冲突，资本主义，美国），按照特征词集合中词语顺序把两个文档表示成向量，则文档1的向量就是（1,1,1,1,0,0,0)，文档二的向量就是(0,0,0,0,1,1,1)。其中0,1表示是某特征词在某文档中的权重，很显然，这里的权重指的是出现次数。权重的计算有一个经典的公式，TF-IDF，上个世纪六七十年代被人发明出来，效果很好，在很多方面都有应用。不过据说提出这个方法的人是误打误撞，自己也不知道个所以然。TT，貌似跑远了，继续正题。 经过向量化后，就要用分类算法进行建模，然后分类。对向量分类在机器学习中有很多种算法可以做到，比如朴素贝叶斯算法，KNN算法，支持向量机，神经网络算法等。朴素贝叶斯较为简单，我们主要介绍朴素贝叶斯方法。 朴素贝叶斯算法的基本介绍如下： 根据先验概率和条件概率估计方法以及P(X|C)计算方法的不同，朴素贝叶斯分为多项式模型、伯努利模型、泊松模型等，本文接下来主要介绍伯努利模型和多项式模型。 由上面的计算公式可知，伯努利模型是基于文档粒度的，多项式模型则是特征粒度的。 好了，到此文本分类暂时就介绍完毕了。","title":"文本分类综述"},{"content":"我经常在 TopLanguage 讨论组上推荐一些书籍，也经常问里面的牛人们搜罗一些有关的资料，人工智能、机器学习、自然语言处理、知识发现（特别地，数据挖掘）、信息检索 这些无疑是 CS 领域最好玩的分支了（也是互相紧密联系的），这里将最近有关机器学习和人工智能相关的一些学习资源归一个类： 首先是两个非常棒的 Wikipedia 条目，我也算是 wikipedia 的重度用户了，学习一门东西的时候常常发现是始于 wikipedia 中间经过若干次 google ，然后止于某一本或几本著作。  第一个是“人工智能的历史”（History of Artificial Intelligence），我在讨论组上写道： 而今天看到的这篇文章是我在 wikipedia 浏览至今觉得最好的。文章名为《人工智能的历史》，顺着 AI 发展时间线娓娓道来，中间穿插无数牛人故事，且一波三折大气磅礴，可谓\"事实比想象更令人惊讶\"。人工智能始于哲学思辨，中间经历了一个没有心理学（尤其是认知神经科学的）的帮助的阶段，仅通过牛人对人类思维的外在表现的归纳、内省，以及数学工具进行探索，其间最令人激动的是 Herbert Simon （决策理论之父，诺奖，跨领域牛人）写的一个自动证明机，证明了罗素的数学原理中的二十几个定理，其中有一个定理比原书中的还要优雅，Simon 的程序用的是启发式搜索，因为公理系统中的证明可以简化为从条件到结论的树状搜索（但由于组合爆炸，所以必须使用启发式剪枝）。后来 Simon 又写了 GPS （General Problem Solver），据说能解决一些能良好形式化的问题，如汉诺塔。但说到底 Simon 的研究毕竟只触及了人类思维的一个很小很小的方面 —— Formal Logic，甚至更狭义一点 Deductive Reasoning （即不包含 Inductive Reasoning , Transductive Reasoning (俗称 analogic thinking）。还有诸多比如 Common Sense、Vision、尤其是最为复杂的 Language 、Consciousness 都还谜团未解。还有一个比较有趣的就是有人认为 AI 问题必须要以一个物理的 Body 为支撑，一个能够感受这个世界的物理规则的身体本身就是一个强大的信息来源，基于这个信息来源，人类能够自身与时俱进地总结所谓的 Common-Sense Knowledge （这个就是所谓的 Emboddied  Mind 理论。 ），否则像一些老兄直接手动构建 Common-Sense Knowledge Base ，就很傻很天真了，须知人根据感知系统从自然界获取知识是一个动态的自动更新的系统，而手动构建常识库则无异于古老的 Expert System 的做法。当然，以上只总结了很小一部分我个人觉得比较有趣或新颖的，每个人看到的有趣的地方不一样，比如里面相当详细地介绍了神经网络理论的兴衰。所以我强烈建议你看自己一遍，别忘了里面链接到其他地方的链接。 顺便一说，徐宥同学打算找时间把这个条目翻译出来，这是一个相当长的条目，看不动 E 文的等着看翻译吧:) 第二个则是“人工智能”（Artificial Intelligence）。当然，还有机器学习等等。从这些条目出发能够找到许多非常有用和靠谱的深入参考资料。   然后是一些书籍 书籍： 1. 《Programming Collective Intelligence》，近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的:P 2. Peter Norvig 的《AI, Modern Approach 2nd》（无争议的领域经典）。 3. 《The Elements of Statistical Learning》，数学性比较强，可以做参考了。 4. 《Foundations of Statistical Natural Language Processing》，自然语言处理领域公认经典。 5. 《Data Mining, Concepts and Techniques》，华裔科学家写的书，相当深入浅出。 6. 《Managing Gigabytes》，信息检索好书。 7. 《Information Theory：Inference and Learning Algorithms》，参考书吧，比较深。 相关数学基础（参考书，不适合拿来通读）： 1. 线性代数：这个参考书就不列了，很多。 2. 矩阵数学：《矩阵分析》，Roger Horn。矩阵分析领域无争议的经典。 3. 概率论与统计：《概率论及其应用》，威廉·费勒。也是极牛的书，可数学味道太重，不适合做机器学习的。于是讨论组里的 Du Lei 同学推荐了《All Of Statistics》并说到 机器学习这个方向，统计学也一样非常重要。推荐All of statistics，这是CMU的一本很简洁的教科书，注重概念，简化计算，简化与Machine Learning无关的概念和统计内容，可以说是很好的快速入门材料。 4. 最优化方法：《Nonlinear Programming, 2nd》非线性规划的参考书。《Convex Optimization》凸优化的参考书。此外还有一些书可以参考 wikipedia 上的最优化方法条目。要深入理解机器学习方法的技术细节很多时候（如SVM）需要最优化方法作为铺垫。   王宁同学推荐了好几本书： 《Machine Learning, Tom Michell》, 1997.  老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能\"新\"到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。 《Modern Information Retrieval, Ricardo Baeza-Yates et al》. 1999  老书，牛人。貌似第一本完整讲述IR的书。可惜IR这些年进展迅猛，这本书略有些过时了。翻翻做参考还是不错的。另外，Ricardo同学现在是Yahoo Research for Europe and Latin Ameria的头头。 《Pattern Classification (2ed)》, Richard O. Duda, Peter E. Hart, David G. Stork  大约也是01年左右的大块头，有影印版，彩色。没读完，但如果想深入学习ML和IR，前三章（介绍，贝叶斯学习，线性分类器）必修。 还有些经典与我只有一面之缘，没有资格评价。另外还有两本小册子，论文集性质的，倒是讲到了了不少前沿和细节，诸如索引如何压缩之类。可惜忘了名字，又被我压在箱底，下次搬家前怕是难见天日了。 （呵呵，想起来一本：《Mining the Web – Discovering Knowledge from Hypertext Data》） 说一本名气很大的书：《Data Mining: Practical Machine Learning Tools and Techniques》。Weka 的作者写的。可惜内容一般。理论部分太单薄，而实践部分也很脱离实际。DM的入门书已经不少，这一本应该可以不看了。如果要学习了解 Weka ，看文档就好。第二版已经出了，没读过，不清楚。   信息检索方面，Du Lei 同学再次推荐： 信息检索方面的书现在建议看Stanford的那本《Introduction to Information Retrieval》，这书刚刚正式出版，内容当然up to date。另外信息检索第一大牛Croft老爷也正在写教科书，应该很快就要面世了。据说是非常pratical的一本书。 对信息检索有兴趣的同学，强烈推荐翟成祥博士在北大的暑期学校课程，这里有全slides和阅读材料：http://net.pku.edu.cn/~course/cs410/schedule.html maximzhao 同学推荐了一本机器学习： 加一本书：Bishop, 《Pattern Recognition and Machine Learning》. 没有影印的，但是网上能下到。经典中的经典。Pattern Classification 和这本书是两本必读之书。《Pattern Recognition and Machine Learning》是很新（07年），深入浅出，手不释卷。   最后，关于人工智能方面（特别地，决策与判断），再推荐两本有意思的书， 一本是《Simple Heuristics that Makes Us Smart》 另一本是《Bounded Rationality: The Adaptive Toolbox》 不同于计算机学界所采用的统计机器学习方法，这两本书更多地着眼于人类实际上所采用的认知方式，以下是我在讨论组上写的简介： 这两本都是德国ABC研究小组（一个由计算机科学家、认知科学家、神经科学家、经济学家、数学家、统计学家等组成的跨学科研究团体）集体写的，都是引起领域内广泛关注的书，尤其是前一本，後一本则是对 Herbert Simon （决策科学之父，诺奖获得者）提出的人类理性模型的扩充研究），可以说是把什么是真正的人类智能这个问题提上了台面。核心思想是，我们的大脑根本不能做大量的统计计算，使用fancy的数学手法去解释和预测这个世界，而是通过简单而鲁棒的启发法来面对不确定的世界（比如第一本书中提到的两个后来非常著名的启发法：再认启发法（cognition heuristics）和选择最佳（Take the Best）。当然，这两本书并没有排斥统计方法就是了，数据量大的时候统计优势就出来了，而数据量小的时候统计方法就变得非常糟糕；人类简单的启发法则充分利用生态环境中的规律性（regularities），都做到计算复杂性小且鲁棒。 关于第二本书的简介： 1. 谁是 Herbert Simon 2. 什么是 Bounded Rationality 3. 这本书讲啥的： 我一直觉得人类的决策与判断是一个非常迷人的问题。这本书简单地说可以看作是《决策与判断》的更全面更理论的版本。系统且理论化地介绍人类决策与判断过程中的各种启发式方法（heuristics）及其利弊 （为什么他们是最优化方法在信息不足情况下的快捷且鲁棒的逼近，以及为什么在一些情况下会带来糟糕的后果等，比如学过机器学习的都知道朴素贝叶斯方法在许多情况下往往并不比贝叶斯网络效果差，而且还速度快；比如多项式插值的维数越高越容易overfit，而基于低阶多项式的分段样条插值却被证明是一个非常鲁棒的方案）。 在此提一个书中提到的例子，非常有意思：两个团队被派去设计一个能够在场上接住抛过来的棒球的机器人。第一组做了详细的数学分析，建立了一个相当复杂的抛物线近似模型（因为还要考虑空气阻力之类的原因，所以并非严格抛物线），用于计算球的落点，以便正确地接到球。显然这个方案耗资巨大，而且实际运算也需要时间，大家都知道生物的神经网络中生物电流传输只有百米每秒之内，所以 computational complexity 对于生物来说是个宝贵资源，所以这个方案虽然可行，但不够好。第二组则采访了真正的运动员，听取他们总结自己到底是如何接球的感受，然后他们做了这样一个机器人：这个机器人在球抛出的一开始一半路程啥也不做，等到比较近了才开始跑动，并在跑动中一直保持眼睛于球之间的视角不变，后者就保证了机器人的跑动路线一定会和球的轨迹有交点；整个过程中这个机器人只做非常粗糙的轨迹估算。体会一下你接球的时候是不是眼睛一直都盯着球，然后根据视线角度来调整跑动方向？实际上人类就是这么干的，这就是 heuristics 的力量。 相对于偏向于心理学以及科普的《决策与判断》来说，这本书的理论性更强，引用文献也很多而经典，而且与人工智能和机器学习都有交叉，里面也有不少数学内容，全书由十几个章节构成，每个章节都是由不同的作者写的，类似于 paper 一样的，很严谨，也没啥废话，跟 《Psychology of Problem Solving》类似。比较适合 geeks 阅读哈。 另外，对理论的技术细节看不下去的也建议看看《决策与判断》这类书（以及像《别做正常的傻瓜》这样的傻瓜科普读本），对自己在生活中做决策有莫大的好处。人类决策与判断中使用了很多的 heuristics ，很不幸的是，其中许多都是在适应几十万年前的社会环境中建立起来的，并不适合于现代社会，所以了解这些思维中的缺点、盲点，对自己成为一个良好的决策者有很大的好处，而且这本身也是一个非常有趣的领域。 （完） P.S. 大家有什么好的资料请至讨论组上留言。 TAGS: 机器学习与人工智能, 计算机科学, 资源导引 11 Comments Reply 南瓜 Posted September 26, 2009 at 5:20 PM 很佩服你的这些思想，你使我改变了我以前的想法：在网络上写下这些东西是浪费时间，我想我也要从此开通博客了。顺便请教一下，我的学习领域在计算机视觉方面，也包含一些人工智能的问题。你能不能推荐几个注册博客的网站呢？还有就是你是在哪里开的户呢？谢谢，可能有些问题很可笑，但是正如你所说，我正在从不成熟走向成熟！再次感谢！ Reply feng Posted October 30, 2009 at 9:56 AM 因为不是你的group的成员, 所以无法加入group的讨论. 要说人工智能, 我想不能不提 Marvin Minsky 的 “Society of Mind”, 还有 Doug Hosftstadter 的所有的书, 包括 “Fluid concepts and creative analogies”, “Godel Escher, Bach”, “Metamagical Themas”. Hofstadter 曾在Stanford做过一个讲座, 叫 “Analogy as the core of cognition”, 对他自己的研究是很好的总结, 不妨google 一下找到其transcript 来读. (我不提供link 因为似乎提供了太多link, 你这个blog的系统就以为我是spam). 另外, 你在itunes u 上搜索hofstadter应该能够找到这个讲座的录音. 个人认为, 除了 Hofstadter 之外, 罕有从fundamental 来思考AI的. 对AI感兴趣, 还应该上google video 找到这个讲座 “winning the darpa grand challenge”. 这是stanford 的team 讲述如何赢了darpa grand challenge. 不仅对如vision, robotics 等具体的AI课题有所启发, 对于他们如何把这件事做成功了, 也是非常有启发. 算是 meta-启发吧. 做研究的的话, 一定要看看”How to do research at the MIT AI lab”. google一下就能找到. 另外可以看看的有: DeBono “the mechanism of mind”, kosslyn “ghosts in the mind’s machine”, wolpert: the triumph of the embryo (这本书并不是讲 AI,但其中所提到的想法远不是只局限一个学科, 因而有价值). Reply 热水器维修 Posted April 22, 2010 at 1:12 PM 在此提一个书中提到的例子，非常有意思 Reply 任晓祎 Posted July 7, 2010 at 12:18 PM 我把维基条目“人工智能的历史”翻译过来了, 希望有用, 也欢迎指正. 有兴趣的同学请移步: http://snarc.ia.ac.cn/ren/html/y2010/276.html","title":"机器学习与人工智能学习资源导引"},{"content":"条件随机场模型是由Lafferty在2001年提出的一种典型的判别式模型。它在观测序列的基础上对目标序列进行建模,重点解决序列化标注的问题条件随机场模型既具有判别式模型的优点,又具有产生式模型考虑到上下文标记间的转移概率,以序列化形式进行全局参数优化和解码的特点,解决了其他判别式模型(如最大熵马尔科夫模型)难以避免的标记偏置问题。         条件随机场理论（CRFs）可以用于序列标记、数据分割、组块分析等自然语言处理任务中。在中文分词、中文人名识别、歧义消解等汉语自然语言处理任务中都有应用，表现很好。   目前基于 CRFs 的主要系统实现有 CRF，FlexCRF，CRF++ 缺点：训练代价大、复杂度高 —预备知识 —产生式模型和判别式模型（Generative model vs. Discriminative model） —概率图模型 —隐马尔科夫模型 —最大熵模型   机器学习方法的两种分类：产生式模型和判别式模型 假定输入x, 类别标签y —产生式模型（生成模型）估计联合概率 P(x, y), 因可以根据联合概率来生成样本 —: HMMs —判别式模型（判别模型）估计条件概率 P(y|x), 因为没有x的知识，无法生成样本，只能判断分类: SVMs,CRF,MEM 一个举例： (1,0), (1,0), (2,0), (2, 1) 产生式模型： p(x, y)： P(1, 0) = 1/2, P(1, 1) = 0, P(2, 0) = 1/4, P(2, 1) = 1/4. 判别式模型： P(y|x)： P(0|1) = 1, P(1|1) = 0, P(0|2) = 1/2, P(1|2) = 1/2 —o和s分别代表观察序列和标记序列 —产生式模型 —     构建o和s的联合分布p(s,o) —判别式模型 —     构建o和s的条件分布p(s|o) —产生式模型中，观察序列作为模型的一部分； —判别式模型中，观察序列只作为条件，因此可以针对观察序列设计灵活的特征。 产生式模型：无穷样本==》概率密度模型 = 产生模型==》预测 判别式模型：有限样本==》判别函数 = 预测模型==》预测     一般认为判别型模型要好于生成型模型，因为它是直接根据数据对概率建模，而生成型模型还要先求两个难度相当的概率 概率图模型 —用图的形式表示概率分布 —基于概率论中贝叶斯规则建立起来的，解决不确定性问题，可以用于人工智能、 数据挖掘、 语言处理文本分类等领域     图模型是表示随机变量之间的关系的图，图中的节点表示随机变量，缺少边表示条件独立假设。因此可以对联合分布提供一种紧致表示 —根据边是否有方向，有两种主要的图模型 ◦无向图：亦称马尔科夫随机场(Markov Random Fields, MRF’s)或马尔科夫网络(Markov Networks) ◦有向图：亦称贝叶斯网络(Bayesian Networks)或信念网络(Belief Networks, BN’s). ◦还有混合图模型，有时称为链图(chain graphs) —我们不妨拿种地来打个比方。其中有两个概念：位置（site），相空间（phase space）。“位置”好比是一亩亩农田；“相空间”好比是种的各种庄稼。我们可以给不同的地种上不同的庄稼，这就好比给随机场的每个“位置”，赋予相空间里不同的值。所以，俗气点说，随机场就是在哪块地里种什么庄稼的事情。 —简单地讲，随机场可以看成是一组随机变量的集合（这组随机变量对应同一个样本空间）。当给每一个位置中按照某种分布随机赋予相空间的一个值之后，其全体就叫做随机场。 —当然，这些随机变量之间可能有依赖关系，一般来说，也只有当这些变量之间有依赖关系的时候，我们将其单独拿出来看成一个随机场才有实际意义。 —具有马尔科夫性质 —体现了一个思想：离当前因素比较遥远(这个遥远要根据具体情况自己定义）的因素对当前因素的性质影响不大。     条件随机场模型是一种无向图模型，它是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。即给定观察序列O,求最佳序列S。 本文来自CSDN博客，转载请标明出处：http://blog.csdn.net/wen718/archive/2010/10/23/5960820.aspx   CRF其实就是一种在生产模型基础上的判别模型？     条件随机场模型是由Lafferty在2001年提出的一种典型的判别式模型。它在观测序列的基础上对目标序列进行建模,重点解决序列化标注的问题条件随机场模型既具有判别式模型的优点,又具有产生式模型考虑到上下文标记间的转移概率,以序列化形式进行全局参数优化和解码的特点,解决了其他判别式模型(如最大熵马尔科夫模型)难以避免的标记偏置问题。     条件随机场理论（CRFs）可以用于序列标记、数据分割、组块分析等自然语言处理任务中。在中文分词、中文人名识别、歧义消解等汉语自然语言处理任务中都有应用，表现很好。 目前基于 CRFs 的主要系统实现有 CRF，FlexCRF，CRF++ 缺点：训练代价大、复杂度高 ———————————     条件随机场模型是一种无向图模型，它是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。即给定观察序列O,求最佳序列S。 与最大熵模型相似，条件随机场（Conditional random fields，CRFs）是一种机器学习模型，在自然语言处理的许多领域（如词性标注、中文分词、命名实体识别等）都有比较好的应用效果。条件随机场最早由John D. Lafferty提出，其也是Brown90的作者之一，和贾里尼克相似，在离开IBM后他去了卡耐基梅隆大学继续搞学术研究，2001年以第一作者的身份发表了CRF的经典论文 “Conditional random fields: Probabilistic models for segmenting and labeling sequence data”。     关于条件随机场的参考文献及其他资料，Hanna Wallach在05年整理和维护的这个页面“conditional random fields”非常不错，其中涵盖了自01年CRF提出以来的很多经典论文（不过似乎只到05年，之后并未更新）以及几个相关的工具包(不过也没有包括CRF++），但是仍然非常值得入门条件随机场的读者参考。 ——————————————–     一般序列分类模型常常采用隐马模型(HMM), 像基于类的中文分词, 但隐马模型中存在两个假设: 输出独立性假设和马尔可夫性假设. 其中, 输出独立性假设要求序列数据严格相互独立才能保证推导的正确性, 而事实上大多数序列数据不能 被表示成一系列独立事件. 而条件随机场则使用一种概率图模型, 具有表达长距离依赖性和交叠性特征的能力, 能够较好地解决标注(分类)偏置等问题的优点, 而且所有特征可以进行全局归一化, 能够求得全局的最优解.     条件随机场是一个无向图上概率分布的学习框架, 由Lafferty 等首先引入到自然语言处理的串标引学习任务中来. 最常用的一类CRF是线性链CRF, 适用于我们的分词学习. 记观测串为W=w1w2…wn, 标记串(状态)序列 Y=y1y2…yn, 线性链CRF对一个给定串的标注, 其概率定义为: 。。。 。。。     其中, Y是串的标注序列, W是待标记的字符, fk是特征函数, λk是对应的特征函数的权值, 而t是标记, Z(W)是归一化因子, 使得上式成为概率分布.     CRF模型的参数估计通常使用L-BFGS算法来完成. CRF的解码过程, 也就是求解未知串标注的过程, 需要搜索计算该串上的一个最大联合概率, 即: Y* = arg max(y)P(Y|W)     在线性链CRF上, 这个计算任务可以用一般的Viterbi算法来有效地完成. 目前我发现的关于CRF的实现有: * CRF++(http://crfpp.sourceforge.net/) * Pocket CRF(http://sourceforge.net/project/showfiles.php?group_id=201943)     关于CRF简介及代码实现的网址：http://www.inference.phy.cam.ac.uk/hmw26/crf/#software","title":"条件随机场 conditional random fields 及代码实现"},{"content":"命名实体识别： 指识别文本中 具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。在当今世界，随着计算机的普及以及互联网的迅猛发展，大量的信息以电子文档的形式呈现在人 们面前。为了应对信息爆炸带来的严重挑战，人们迫切需要一些自动化的工具帮助他们在海量的信息源中迅速找到真正重要的信息。于是信息抽取技术应运而生。而 命名实体识别是信息抽取技术中的重要组成部分，同时它还能应用于自动问答、机器翻译以及信息检索等自然语言处理领域，有助于它们的性能的提高。 网络热词 出现次数多，频率高的词","title":"ＩＴ知识普及"},{"content":"    学习Python有一段时间了，需要来一个阶段性总结了     个人看书的模式是，一边看书，一边敲代码，一遍记笔记（record for search），所以回过头发现，记得有些多了，需要花点时间整理下。     so，一篇篇整理，会陆续发出。 ------------------------------------------- update: 2012-12-30 是该有个了结了:)          目录:） 1. 简介&helloworld&开发环境配置 2.基本语法：标识符&变量&运算符 3.基本控制流 4.基本数据结构1-列表 5.基本数据结构2-元祖 6.基本数据结构3-字典 7.基本数据结构4-集合 5.数字处理  A.数字基础  B.相关扩展模块 6.字符串处理 A.字符串基础 B.格式化 C.正则表达式 D.编码相关 7.文件处理  A.文件基础  B.相关模块 8.异常处理 9.时间处理及相关模块 10.函数定义及参数传递 11.内建函数 12.模块和包 13.面相对象 A.基础 B.类 C魔术方法（方法运算符重载） 14.函数式编程： A.基础  B.迭代器 C.生成器D.装饰器 15.docstring 16.PyUnit及调试 17.垃圾回收 18.设计模式 19.元类 20.Pythonic最佳实践 21.性能最佳实践 22.命令行编程 23.多线程 24.Python相关要求 25.题目&学习路线&资源链接 PS:目测搞完后有些多，不会按照顺序发，写完一章会同步更新到目录。-20121005      有任何问题回复到这个页面吧，其他页估计没空去一个个看!     如果有兴趣学Python的话，可以看看。     以下主要是一些学习过程中的梳理，希望有帮助。 1. 书籍列表：（列了下看过和没看过的一些书的看法）   基本了解： <<A Byte of Python>> (Python简明教程http://sebug.net/paper/python/) 网上有资源，两小时了解基本 如果有其他语言功底，不需要那么久 入门： <<Python Tutorial>> 官方入门，英文好的可以通读一遍，不过貌似也有中文的 <<Python基础教程>> 我看到的第一本python书，大学图书馆借的，很基础，但感觉不够详细 作简单入门还是可以的 <<Head First Python>>(深入浅出Python) 买的影印版，没有程序基础的可以看看，有程序基础的可以当做英文阅读训练….. <<Learing Python>>(Python学习手册) 有Python3k的内容，内容很全面，很详细，就是有点厚，啃了一段时间才搞完，我读的第二本 <<Core Python Programming>>(Pyton核心编程) python2.x，社区翻译的，章节理得思路很清晰，比较符合中国人梳理知识的风格。2天翻完了，和上面那本内容基本相似。二选一 <<可爱的Python>> 没买，ibm dev里面有网络版，翻看了一些自己需要的文章 <<Programming Python> 没看，没有发言权…..   进阶& 再深入：   <<Dive into Python>> 很基础，也很不基础，应该算是入门后想再进一步要看的 <<Python 高级编程>> 买了，还没翻 <<Python网络编程>> 没买，没发言权，目前工作中没用到 打算买 <<Python源码剖析>> 正在看，对想更深入了解很有帮助，对写出更好的代码也很有帮助 其他： <<Python在Unix和Linux系统管理中的应用>> 买了，挑看了部分章节，如果经常在*nix下工作的童鞋，常常写脚本的同学，建议看看 <<Python灰帽子>> 没买，没看，没发言权 <<集体智慧编程>> 正在看，主要是机器学习一些基本算法的介绍，代码是python写的，很赞 <<Python自然语言处理>> 没买，没看，没发言权 <<The Django Book>>   迁移一个项目(http://mezzanine.sinaapp.com/)到sae时读完了所有章节，不过不深入。 网上有翻译版本的   <<Django web开发指南>> 买了，看完了，和上一本重复太多，二选一，建议网上看上面那本 大部头偶尔查询   <<Python standard Library>> 实体中文版貌似刚出，很厚一本，一百来大洋，还没下手。有电子版，偶尔查查东西用，不过貌似不大完整。 打算手里这些书过完了再入手 <<Python cookbook>>   python2.4的，很多奇淫巧计，可以买一本   2.学习路线（针对以上书籍，可以自我调整和评估下，以下只代表个人观点） #/usr/bin/env python#-*- coding=utf-8 -*-#@author: wklken@yeah.net#@version: 0.1#@date: 2012-08-25#@desc: python学习线路stepdef read(book=<<A Byte of Python>>) #  网上先过一遍    if 没兴趣:      return    else:        if 没编程基础:           <<Head First Python>>        if need py2.x:           <<Python 核心技术>>        elif py2.x and py3k:           <<Learning Python>>        if you want: #可选           <<Python Tutorial>>        if you have more time and energy:           <<可爱的Python>>           <<Programming Python>>        print \"Info: 基本入门了\"        if you want go farther:            if  True:#强烈建议                <<Dive Into Python>>                <<Python源码剖析>>                 <<Python高级编程>> #这个，没读过，自己判定吧            if  工作需要:                <<Python网络编程>>  #网络编程                <<Python在Unix和Linux系统管理中的应用>> #系统管理相关                <<The Django Book>> #web，用到django框架的                <<Django web开发指南>>  #同上                <<集体智慧编程>> #算法工程师，or 个人爱好                <<Python自然语言处理>>        if  you want to search for something useful:            <<Python standard Library>>            <<Python cookbook>> 3.选用IDE: 这个，自己感觉用得顺手就行，关于这个，不详细介绍，网上自个搜 晒下我用的 windows下:   实验：IDLE     工程：Eclipse+pydev  （目前转用sublime text2了，有兴趣的可以试试） Linux下:   实验: Ipython    工程: vim 4.学习方法： 感觉入门无他：   看书+敲代码实验 买书的话，基本就行，貌似lz买多了，有部分感觉其实重复了 进阶，个人认为：  思考+不停写代码&重构 菜鸟阶段，继续敲代码中 5.关于这堆笔记 目标：python基本入门及进阶 基于版本： py2.7 参考书籍： 上面那堆书籍 + 网络的一些博文 整理频率：不定期哈，这个没法保证，1是比较懒，2是工作比较忙(好吧，这都是借口><) 声明： 1.本人系菜鸟一枚，这些权做分享，水平有限，错误难免，欢迎指正 2.文中引用会尽量注明，由于太杂，遗漏的希望指出，会注明. 3.转载注明出处哈，谢绝一切商业用途 补充声明： 不要浪费时间讨论语言之争，好坏优劣，自己喜欢就行 Life is short, so i use python! The End, To Be Continue....... wklken 2012-08-25 个人Blog: http://wklken.sinaapp.com/ 迁移sae应用:  http://mezzanine.sinaapp.com/ （尚未完工）","title":"[置顶] [Python入门及进阶笔记00]-写在前面(目录/书籍/学习路线/其他)"},{"content":"中文分词一直都是中文自然语言处理领域的基础研究。目前，网络上流行的很多中文分词软件都可以在付出较少的代价的同时，具备较高的正确率。而且不少中文分词软件支持Lucene扩展。但不管实现如何，目前而言的分词系统绝大多数都是基于中文词典的匹配算法。   在这里我想介绍一下中文分词的一个最基础算法：最大匹配算法 (Maximum Matching，以下简称MM算法) 。MM算法有两种：一种正向最大匹配，一种逆向最大匹配。   ● 算法思想   正向最大匹配算法：从左到右将待分词文本中的几个连续字符与词表匹配，如果匹配上，则切分出一个词。但这里有一个问题：要做到最大匹配，并不是第一次匹配到就可以切分的 。我们来举个例子：            待分词文本：   content[]={\"中\"，\"华\"，\"民\"，\"族\"，\"从\"，\"此\"，\"站\"，\"起\"，\"来\"，\"了\"，\"。\"}            词表：   dict[]={\"中华\"， \"中华民族\" ， \"从此\"，\"站起来\"} (1) 从content[1]开始，当扫描到content[2]的时候，发现\"中华\"已经在词表dict[]中了。但还不能切分出来，因为我们不知道后面的词语能不能组成更长的词(最大匹配)。 (2) 继续扫描content[3]，发现\"中华民\"并不是dict[]中的词。但是我们还不能确定是否前面找到的\"中华\"已经是最大的词了。因为\"中华民\"是dict[2]的前缀。 (3) 扫描content[4]，发现\"中华民族\"是dict[]中的词。继续扫描下去： (4) 当扫描content[5]的时候，发现\"中华民族从\"并不是词表中的词，也不是词的前缀。因此可以切分出前面最大的词——\"中华民族\"。   由此可见，最大匹配出的词必须保证下一个扫描不是词表中的词或词的前缀才可以结束。     ● 算法实现   词表的内存表示： 很显然，匹配过程中是需要找词前缀的，因此我们不能将词表简单的存储为Hash结构。在这里我们考虑一种高效的字符串前缀处理结构——Trie树《Trie Tree 串集合查找 》。这种结构使得查找每一个词的时间复杂度为O(word.length)，而且可以很方便的判断是否匹配成功或匹配到了字符串的前缀。   下图是我们建立的Trie结构词典的部分，(词语例子：\"中华\"，\"中华名族\"，\"中间\"，\"感召\"，\"感召力\"，\"感受\")。             (1) 每个结点都是词语中的一个汉字。 (2) 结点中的指针指向了该汉字在某一个词中的下一个汉字。这些指针存放在以汉字为key的hash结构中。 (3) 结点中的\"#\"表示当前结点中的汉字是从根结点到该汉字结点所组成的词的最后一个字。   TrieNode源代码如下： Java代码   import java.util.HashMap;   /**   * 构建内存词典的Trie树结点   *    * @author single(宋乐)   * @version 1.01, 10/11/2009   */   public class TrieNode {          /**结点关键字，其值为中文词中的一个字*/       public char key=(char)0;       /**如果该字在词语的末尾，则bound=true*/       public boolean bound=false;       /**指向下一个结点的指针结构，用来存放当前字在词中的下一个字的位置*/       public HashMap<Character,TrieNode> childs=new HashMap<Character,TrieNode>();              public TrieNode(){       }              public TrieNode(char k){           this.key=k;       }   }     这套分词代码的优点是： (1) 分词效率高。纯内存分词速度大约240.6ms/M，算上IO读取时间平均1.6s/M。测试环境：Pentium(R)  4 CPU  3.06GHZ、1G内存。 (2) 传统的最大匹配算法需要实现确定一个切分的最大长度maxLen。如果maxLen过大，则大大影响分词效率。而且超过maxLen的词语将无法分出来。但本算法不需要设置maxLen。只要词表中有的词，不管多长，都能够切分。 (3) 对非汉字的未登录词具备一定的切分能力。比如英文单词[happy, steven]，产品型号[Nokia-7320]，网址[http://www.sina.com]等。   缺点也很明显： (1) 暂时无词性标注功能，对中文汉字的未登录词无法识别，比如某个人名。 (2) 内存占用稍大，目前词表为86725个词。如果继续扩展词表，很有可能内存Trie树将非常庞大。   代码的进一步优化方案： (1) 想在内存占用空间上降低代价。实际上Trie树主要的空间消耗在每个结点的指针HashMap上。我使用的是JDK中的HashMap，其加载因子为 loadFactor= 0.75，初始化空间大小为DEFAULT_INITIAL_CAPACITY= 16。每次存储数据量超过 loadFactor*DEFAULT_INITIAL_CAPACITY的时候，整个Map空间将翻倍。 因此会照成一定的空间浪费。         但目前还没有想到很好的办法，即能够随机定位到下一个结点的指针，又降低Hash结构的空间代价？ 转自：http://hxraid.iteye.com/blog/667134   【串和序列处理 3】Trie Tree 串集合查找 文章分类:综合技术 Trie 树， 又称字典树，单词查找树。它来源于retrieval(检索)中取中间四个字符构成(读音同try)。用于存储大量的字符串以便支持快速模式匹配。主要应用在信息检索领域。   Trie 有三种结构： 标准trie (standard trie)、压缩trie、后缀trie(suffix trie) 。 最后一种将在《字符串处理4：后缀树》中详细讲，这里只将前两种。     1. 标准Trie (standard trie)   标准 Trie树的结构 ： 所有含有公共前缀的字符串将挂在树中同一个结点下。实际上trie简明的存储了存在于串集合中的所有公共前缀。 假如有这样一个字符串集合X{bear,bell,bid,bull,buy,sell,stock,stop}。它的标准Trie树如下图：           上图（蓝色圆形结点为内部结点，红色方形结点为外部结点），我们可以很清楚的看到字符串集合X构造的Trie树结构。其中从根结点到红色方框叶子节点所经历的所有字符组成的串就是字符串集合X中的一个串。         注意这里有一个问题： 如果X集合中有一个串是另一个串的前缀呢？ 比如，X集合中加入串bi。那么上图的Trie树在绿色箭头所指的内部结点i 就应该也标记成红色方形结点。这样话，一棵树的枝干上将出现两个连续的叶子结点(这是不合常理的)。         也就是说字符串集合X中不存在一个串是另外一个串的前缀 。如何满足这个要求呢？我们可以在X中的每个串后面加入一个特殊字符$(这个字符将不会出现在字母表中)。这样，集合X{bear$、bell$、.... bi$、bid$}一定会满足这个要求。         总结：一个存储长度为n，来自大小为d的字母表中s个串的集合X的标准trie具有性质如下：       (1) 树中每个内部结点至多有d个子结点。       (2) 树有s个外部结点。       (3) 树的高度等于X中最长串的长度。       (4) 树中的结点数为O(n)。     标准 Trie树的查找        对于英文单词的查找，我们完全可以在内部结点中建立26个元素组成的指针数组。如果要查找a，只需要在内部节点的指针数组中找第0个指针即可(b=第1个指针，随机定位)。时间复杂度为O(1)。         查找过程：假如我们要在上面那棵Trie中查找字符串bull (b-u-l-l)。       (1) 在root结点中查找第('b'-'a'=1)号孩子指针，发现该指针不为空，则定位到第1号孩子结点处——b结点。       (2) 在b结点中查找第('u'-'a'=20)号孩子指针，发现该指针不为空，则定位到第20号孩子结点处——u结点。       (3) ... 一直查找到叶子结点出现特殊字符'$'位置，表示找到了bull字符串         如果在查找过程中终止于内部结点，则表示没有找到待查找字符串。         效率：对于有n个英文字母的串来说，在内部结点中定位指针所需要花费O(d)时间，d为字母表的大小，英文为26。由于在上面的算法中内部结点指针定位使用了数组随机存储方式，因此时间复杂度降为了O(1)。但是如果是中文字，下面在实际应用中会提到。因此我们在这里还是用O(d)。 查找成功的时候恰好走了一条从根结点到叶子结点的路径。因此时间复杂度为O(d*n)。       但是，当查找集合X中所有字符串两两都不共享前缀时，trie中出现最坏情况。除根之外，所有内部结点都自由一个子结点。此时的查找时间复杂度蜕化为O(d*(n^2))     标准 Trie树的Java代码实现：   Java代码   package net.hr.algorithm.stroper;      import java.util.ArrayList;      enum NodeKind{LN,BN};   /**   * Trie结点   */   class TrieNode{          char key;       TrieNode[] points=null;       NodeKind kind=null;   }   /**   * Trie叶子结点   */   class LeafNode extends TrieNode{              LeafNode(char k){           super.key=k;           super.kind=NodeKind.LN;       }   }   /**   * Trie内部结点   */   class BranchNode extends TrieNode{              BranchNode(char k){           super.key=k;           super.kind=NodeKind.BN;           super.points=new TrieNode[27];       }   }   /**   * Trie树   * @author heartraid   */   public class StandardTrie {          private TrieNode root=new BranchNode(' ');              /**       * 想Tire中插入字符串       */       public void insert(String word){                      //System.out.println(\"插入字符串：\"+word);           //从根结点出发           TrieNode curNode=root;           //为了满足字符串集合X中不存在一个串是另外一个串的前缀            word=word+\"$\";           //获取每个字符           char[] chars=word.toCharArray();           //插入           for(int i=0;i<chars.length;i++){               //System.out.println(\"   插入\"+chars[i]);                if(chars[i]=='$'){                   curNode.points[26]=new LeafNode('$');               //  System.out.println(\"   插入完毕,使当前结点\"+curNode.key+\"的第26孩子指针指向字符：$\");               }               else{                   int pSize=chars[i]-'a';                   if(curNode.points[pSize]==null){                       curNode.points[pSize]=new BranchNode(chars[i]);                   //  System.out.println(\"   使当前结点\"+curNode.key+\"的第\"+pSize+\"孩子指针指向字符: \"+chars[i]);                       curNode=curNode.points[pSize];                   }                   else{                   //  System.out.println(\"   不插入，找到当前结点\"+curNode.key+\"的第\"+pSize+\"孩子指针已经指向字符: \"+chars[i]);                       curNode=curNode.points[pSize];                   }               }           }       }       /**       * Trie的字符串全字匹配       */       public boolean fullMatch(String word){           //System.out.print(\"查找字符串：\"+word+\"/n查找路径：\");           //从根结点出发           TrieNode curNode=root;           //获取每个字符           char[] chars=word.toCharArray();                      for(int i=0;i<chars.length;i++){               if(curNode.key=='$'){                   System.out.println('&');               //  System.out.println(\" 【成功】\");                   return true;               }else{                   System.out.print(chars[i]+\" -> \");                      int pSize=chars[i]-'a';                   if(curNode.points[pSize]==null){                   //  System.out.println(\" 【失败】\");                       return false;                   }else{                       curNode=curNode.points[pSize];                   }               }           }       //  System.out.println(\"  【失败】\");           return false;       }                 /**       * 先根遍历Tire树       */       private void preRootTraverse(TrieNode curNode){                      if(curNode!=null){               System.out.print(curNode.key+\" \");               if(curNode.kind==NodeKind.BN)                   for(TrieNode childNode:curNode.points)                       preRootTraverse(childNode);           }                  }       /**       * 得到Trie根结点       */       public TrieNode getRoot(){           return this.root;       }       /**       * 测试       */       public static void main(String[] args) {                      StandardTrie trie=new StandardTrie();           trie.insert(\"bear\");           trie.insert(\"bell\");           trie.insert(\"bid\");           trie.insert(\"bull\");           trie.insert(\"buy\");           trie.insert(\"sell\");           trie.insert(\"stock\");           trie.insert(\"stop\");                      trie.preRootTraverse(trie.getRoot());                      trie.fullMatch(\"stoops\");       }   }     中文词语的 标准 Trie树       由于中文的字远比英文的26个字母多的多。因此对于trie树的内部结点，不可能用一个26的数组来存储指针。如果每个结点都开辟几万个中国字的指针空间。估计内存要爆了，就连磁盘也消耗很大。         一般我们采取这样种措施：      (1) 以词语中相同的第一个字为根组成一棵树。这样的话，一个中文词汇的集合就可以构成一片Trie森林。这篇森林都存储在磁盘上。森林的root中的字和root所在磁盘的位置都记录在一张以Unicode码值排序的有序字表中。字表可以存放在内存里。     (2) 内部结点的指针用可变长数组存储。        特点：由于中文词语很少操作4个字的，因此Trie树的高度不长。查找的时间主要耗费在内部结点指针的查找。因此将这项指向字的指针按照字的Unicode码值排序，然后加载进内存以后通过二分查找能够提高效率。     标准Trie树的应用和优缺点      (1) 全字匹配：确定待查字串是否与集合的一个单词完全匹配。如上代码fullMatch()。      (2) 前缀匹配：查找集合中与以s为前缀的所有串。        注意：Trie树的结构并不适合用来查找子串。这一点和前面提到的PAT Tree以及后面专门要提到的Suffix Tree的作用有很大不同。         优点： 查找效率比与集合中的每一个字符串做匹配的效率要高很多。在o(m)时间内搜索一个长度为m的字符串s是否在字典里。       缺点：标准Trie的空间利用率不高，可能存在大量结点中只有一个子结点，这样的结点绝对是一种浪费。正是这个原因，才迅速推动了下面所讲的压缩trie的开发。       2. 压缩Trie (compressed trie)         压缩Trie类似于标准Trie，但它能保证trie中的每个内部结点至少有两个子节点(根结点除外)。通过把单子结点链压缩进叶子节点来执行这个规则。   压缩Trie的定义         冗余结点(redundant node)：如果T的一个非根内部结点v只有一个子结点，那么我们称v是冗余的。       冗余链(redundant link)：如上标准Trie图中，内部结点e只有一个内部子结点l，而l也只有一个叶子结点。那么e-l-l就构成了一条冗余链。       压缩(compressed)：对于冗余链 v1- v2- v3- ... -vn，我们可以用单边v1-vn来替代。         对上面标准Trie的图压缩之后，形成了Compressed Trie的字符表示图如下： 压缩Trie的性质和优势：        与标准Trie比较，压缩Trie的结点数与串的个数成正比了，而不是与串的总长度成正比。一棵存储来自大小为d的字母表中的s个串的结合T的压缩trie具有如下性质：        (1) T中的每个内部结点至少有两个子结点，至多有d个子结点。      (2) T有s个外部结点。      (3) T中的结点数为O(s)      存储空间从标准Trie的O(n)降低到压缩后的O(s)，其中n为集合T中总字符串长度，s为T中的字符串个数。   压缩Trie的压缩表示        上面的图是压缩Trie的字符串表示。相比标准Trie而言，确实少了不少结点。但是细心的读者会发现，叶子结点中的字符数量增加了，比如结点ell，那么这种压缩空间的效率当然会打折扣了。那么有什么好办法呢，这里我们介绍一种压缩表示方法。即把所有结点中的字符串用三元组的形式表示如下图：              其中三元组(i，j，k)表示S[i]的从第j个位置到第k个位置间的子串。比如(5,1,3,)表示S[5][1...3]=\"ell\"。         这种压缩表示的一个巨大的优点就是：无论结点需要存储多长的字串，全部都可以用一个三元组表示，而且三元组所占的空间是固定有限的。但是为了做到这一点，必须有一张辅助索引结构（如上图右侧s0—s7所示）。     转自：http://hxraid.iteye.com/blog/618962","title":"中文分词：之Trie树"},{"content":"2006年4月17日 上午 08:01:00 发表者：吴军，Google 研究员 前言：隐含马尔可夫模型是一个数学模型，到目前为之，它一直被认为是实现快速精确的语音识别系统的最成功的方法。复杂的语音识别问题通过隐含马尔可夫模型能非常简单地被表述、解决，让我不由由衷地感叹数学模型之妙。 自然语言是人类交流信息的工具。很多自然语言处理问题都可以等同于通信系统中的解码问题 -- 一个人根据接收到的信息，去猜测发话人要表达的意思。这其实就象通信中，我们根据接收端收到的信号去分析、理解、还原发送端传送过来的信息。以下该图就表示了一个典型的通信系统： 其中 s1，s2，s3...表示信息源发出的信号。o1, o2, o3 ... 是接受器接收到的信号。通信中的解码就是根据接收到的信号 o1, o2, o3 ...还原出发送的信号 s1，s2，s3...。 其实我们平时在说话时，脑子就是一个信息源。我们的喉咙（声带），空气，就是如电线和光缆般的信道。听众耳朵的就是接收端，而听到的声音就是传送过来的信号。根据声学信号来推测说话者的意思，就是语音识别。这样说来，如果接收端是一台计算机而不是人的话，那么计算机要做的就是语音的自动识别。同样，在计算机中，如果我们要根据接收到的英语信息，推测说话者的汉语意思，就是机器翻译； 如果我们要根据带有拼写错误的语句推测说话者想表达的正确意思，那就是自动纠错。 那么怎么根据接收到的信息来推测说话者想表达的意思呢？我们可以利用叫做“隐含马尔可夫模型”（Hidden Markov Model）来解决这些问题。以语音识别为例，当我们观测到语音信号 o1,o2,o3 时，我们要根据这组信号推测出发送的句子 s1,s2,s3。显然，我们应该在所有可能的句子中找最有可能性的一个。用数学语言来描述，就是在已知 o1,o2,o3,...的情况下，求使得条件概率 P (s1,s2,s3,...|o1,o2,o3....) 达到最大值的那个句子 s1,s2,s3,... 当然，上面的概率不容易直接求出，于是我们可以间接地计算它。利用贝叶斯公式并且省掉一个常数项，可以把上述公式等价变换成 P(o1,o2,o3,...|s1,s2,s3....) * P(s1,s2,s3,...) 其中 P(o1,o2,o3,...|s1,s2,s3....) 表示某句话 s1,s2,s3...被读成 o1,o2,o3,...的可能性, 而 P(s1,s2,s3,...) 表示字串 s1,s2,s3,...本身能够成为一个合乎情理的句子的可能性，所以这个公式的意义是用发送信号为 s1,s2,s3...这个数列的可能性乘以 s1,s2,s3...本身可以一个句子的可能性，得出概率。 （读者读到这里也许会问，你现在是不是把问题变得更复杂了，因为公式越写越长了。别着急，我们现在就来简化这个问题。）我们在这里做两个假设： 第一，s1,s2,s3,... 是一个马尔可夫链，也就是说，si 只由 si-1 决定 (详见系列一)； 第二， 第 i 时刻的接收信号 oi 只由发送信号 si 决定（又称为独立输出假设, 即 P(o1,o2,o3,...|s1,s2,s3....) = P(o1|s1) * P(o2|s2)*P(o3|s3)...。 那么我们就可以很容易利用算法 Viterbi 找出上面式子的最大值，进而找出要识别的句子 s1,s2,s3,...。 满足上述两个假设的模型就叫隐含马尔可夫模型。我们之所以用“隐含”这个词，是因为状态 s1,s2,s3,...是无法直接观测到的。 隐含马尔可夫模型的应用远不只在语音识别中。在上面的公式中，如果我们把 s1,s2,s3,...当成中文，把 o1,o2,o3,...当成对应的英文，那么我们就能利用这个模型解决机器翻译问题； 如果我们把 o1,o2,o3,...当成扫描文字得到的图像特征，就能利用这个模型解决印刷体和手写体的识别。 P (o1,o2,o3,...|s1,s2,s3....) 根据应用的不同而又不同的名称，在语音识别中它被称为“声学模型” (Acoustic Model)， 在机器翻译中是“翻译模型” (Translation Model) 而在拼写校正中是“纠错模型” (Correction Model)。 而P (s1,s2,s3,...) 就是我们在系列一中提到的语言模型。 在利用隐含马尔可夫模型解决语言处理问题前，先要进行模型的训练。 常用的训练方法由伯姆（Baum）在60年代提出的，并以他的名字命名。隐含马尔可夫模型在处理语言问题早期的成功应用是语音识别。七十年代，当时 IBM 的 Fred Jelinek (贾里尼克) 和卡内基·梅隆大学的 Jim and Janet Baker (贝克夫妇，李开复的师兄师姐) 分别独立地提出用隐含马尔可夫模型来识别语音，语音识别的错误率相比人工智能和模式匹配等方法降低了三倍 (从 30% 到 10%)。 八十年代李开复博士坚持采用隐含马尔可夫模型的框架， 成功地开发了世界上第一个大词汇量连续语音识别系统 Sphinx。 我最早接触到隐含马尔可夫模型是几乎二十年前的事。那时在《随机过程》（清华“著名”的一门课）里学到这个模型，但当时实在想不出它有什么实际用途。几年后，我在清华跟随王作英教授学习、研究语音识别时，他给了我几十篇文献。 我印象最深的就是贾里尼克和李开复的文章，它们的核心思想就是隐含马尔可夫模型。复杂的语音识别问题居然能如此简单地被表述、解决，我由衷地感叹数学模型之妙。","title":"数学之美 系列三 -- 隐含马尔可夫模型在语言处理中的应用"},{"content":"发表者: 吴军, Google 研究员  前言 也许大家不相信，数学是解决信息检索和自然语言处理的最好工具。它能非常清晰地描述这些领域的实际问题并且给出漂亮的解决办法。每当人们应用数学工具解决一个语言问题时，总会感叹数学之美。我们希望利用 Google 中文黑板报这块园地，介绍一些数学工具，以及我们是如何利用这些工具来开发 Google 产品的。 系列一： 统计语言模型 (Statistical Language Models) Google 的使命是整合全球的信息，所以我们一直致力于研究如何让机器对信息、语言做最好的理解和处理。长期以来，人类一直梦想着能让机器代替人来翻译语言、识别语音、认识文字（不论是印刷体或手写体）和进行海量文献的自动检索，这就需要让机器理解语言。但是人类的语言可以说是信息里最复杂最动态的一部分。为了解决这个问题，人们容易想到的办法就是让机器模拟人类进行学习 - 学习人类的语法、分析语句等等。尤其是在乔姆斯基（Noam Chomsky 有史以来最伟大的语言学家）提出 “形式语言” 以后，人们更坚定了利用语法规则的办法进行文字处理的信念。遗憾的是，几十年过去了，在计算机处理语言领域，基于这个语法规则的方法几乎毫无突破。  其实早在几十年前，数学家兼信息论的祖师爷 香农 (Claude Shannon)就提出了用数学的办法处理自然语言的想法。遗憾的是当时的计算机条件根本无法满足大量信息处理的需要，所以他这个想法当时并没有被人们重视。七十年代初，有了大规模集成电路的快速计算机后，香农的梦想才得以实现。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师贾里尼克 (Fred Jelinek)。当时贾里尼克在 IBM 公司做学术休假 (Sabbatical Leave)，领导了一批杰出的科学家利用大型计算机来处理人类语言问题。统计语言模型就是在那个时候提出的。 给大家举个例子：在很多涉及到自然语言处理的领域，如机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询中，我们都需要知道一个文字序列是否能构成一个大家能理解的句子，显示给使用者。对这个问题，我们可以用一个简单的统计模型来解决这个问题。 如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为： P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1) 其中 P (w1) 表示第一个词w1 出现的概率；P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词wn，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。因此我们假定任意一个词wi的出现概率只同它前面的词 wi-1 有关(即马尔可夫假设），于是问题就变得很简单了。现在，S 出现的概率就变为： P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)… (当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。） 接下来的问题就是如何估计 P (wi|wi-1)。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。 也许很多人不相信用这么简单的数学模型能解决复杂的语音识别、机器翻译等问题。其实不光是常人，就连很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法都有效。比如在 Google 的中英文自动翻译中，用的最重要的就是这个统计语言模型。去年美国标准局(NIST) 对所有的机器翻译系统进行了评测，Google 的系统是不仅是全世界最好的，而且高出所有基于规则的系统很多。 现在，读者也许已经能感受到数学的美妙之处了，它把一些复杂的问题变得如此的简单。当然，真正实现一个好的统计语言模型还有许多细节问题需要解决。贾里尼克和他的同事的贡献在于提出了统计语言模型，而且很漂亮地解决了所有的细节问题。十几年后，李开复用统计语言模型把 997 词语音识别的问题简化成了一个 20 词的识别问题，实现了有史以来第一次大词汇量非特定人连续语音的识别。 我是一名科学研究人员 ，我在工作中经常惊叹于数学语言应用于解决实际问题上时的神奇。我也希望把这种神奇讲解给大家听。当然，归根结底，不管什莫样的科学方法、无论多莫奇妙的解决手段都是为人服务的。我希望 Google 多努力一分，用户就多一分搜索的喜悦。","title":"数学之美 系列一 -- 统计语言模型"},{"content":"     原文地址：互联网时代的社会语言学：基于SNS的文本数据挖掘http://www.matrix67.com/blog/archives/5044 今年上半年，我在人人网实习了一段时间，期间得到了很多宝贵的数据，并做了一些还算有意义的事情，在这里和大家一块儿分享。感谢人人网提供的数据与工作环境，感谢赵继承博士、詹卫东老师的支持和建议。在这项工作中，我得到了很多与众人交流的机会，特别感谢 OpenParty 、 TEDxBeijing 提供的平台。本文已发表在了《程序员》杂志，分上下两部分刊于 2012 年 7 月刊和 8 月刊，在此感谢卢鸫翔编辑的辛勤工作。由于众所周知的原因，《程序员》刊出的文章被和谐过（看到后面大家就自动地知道被和谐的内容是什么了），因而我决定把完整版发在 Blog 上，同时与更多的人一同分享。对此感兴趣的朋友可以给我发邮件继续交流。好了，开始说正文吧。 作为中文系应用语言学专业的学生以及一名数学 Geek ，我非常热衷于用计算的方法去分析汉语资料。汉语是一种独特而神奇的语言。对汉语资料进行自然语言处理时，我们会遇到很多其他语言不会有的困难，比如分词——汉语的词与词之间没有空格，那计算机怎么才知道，“已结婚的和尚未结婚的青年都要实行计划生育”究竟说的是“已／结婚／的／和／尚未／结婚／的／青年”，还是“已／结婚／的／和尚／未／结婚／的／青年”呢？这就是所谓的分词歧义难题。不过，现在很多语言模型已经能比较漂亮地解决这一问题了。但在中文分词领域里，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？更惨的则是机构名、品牌名、专业名词、缩略语、网络新词等等，它们的产生机制似乎完全无规律可寻。最近十年来，中文分词领域都在集中攻克这一难关。自动发现新词成为了关键的环节。 挖掘新词的传统方法是，先对文本进行分词，然后猜测未能成功匹配的剩余片段就是新词。这似乎陷入了一个怪圈：分词的准确性本身就依赖于词库的完整性，如果词库中根本没有新词，我们又怎么能信任分词结果呢？此时，一种大胆的想法是，首先不依赖于任何已有的词库，仅仅根据词的共同特征，将一段大规模语料中可能成词的文本片段全部提取出来，不管它是新词还是旧词。然后，再把所有抽出来的词和已有词库进行比较，不就能找出新词了吗？有了抽词算法后，我们还能以词为单位做更多有趣的数据挖掘工作。这里，我所选用的语料是人人网 2011 年 12 月前半个月部分用户的状态。非常感谢人人网提供这份极具价值的网络语料。 要想从一段文本中抽出词来，我们的第一个问题就是，怎样的文本片段才算一个词？大家想到的第一个标准或许是，看这个文本片段出现的次数是否足够多。我们可以把所有出现频数超过某个阈值的片段提取出来，作为该语料中的词汇输出。不过，光是出现频数高还不够，一个经常出现的文本片段有可能不是一个词，而是多个词构成的词组。在人人网用户状态中，“的电影”出现了 389 次，“电影院”只出现了 175 次，然而我们却更倾向于把“电影院”当作一个词，因为直觉上看，“电影”和“院”凝固得更紧一些。 为了证明“电影院”一词的内部凝固程度确实很高，我们可以计算一下，如果“电影”和“院”真的是各自独立地在文本中随机出现，它俩正好拼到一起的概率会有多小。在整个 2400 万字的数据中，“电影”一共出现了 2774 次，出现的概率约为 0.000113 。“院”字则出现了 4797 次，出现的概率约为 0.0001969 。如果两者之间真的毫无关系，它们恰好拼在了一起的概率就应该是 0.000113 × 0.0001969 ，约为 2.223 × 10-8 次方。但事实上，“电影院”在语料中一共出现了 175 次，出现概率约为 7.183 × 10-6 次方，是预测值的 300 多倍。类似地，统计可得“的”字的出现概率约为 0.0166 ，因而“的”和“电影”随机组合到了一起的理论概率值为 0.0166 × 0.000113 ，约为 1.875 × 10-6 ，这与“的电影”出现的真实概率很接近——真实概率约为 1.6 × 10-5 次方，是预测值的 8.5 倍。计算结果表明，“电影院”更可能是一个有意义的搭配，而“的电影”则更像是“的”和“电影”这两个成分偶然拼到一起的。 当然，作为一个无知识库的抽词程序，我们并不知道“电影院”是“电影”加“院”得来的，也并不知道“的电影”是“的”加上“电影”得来的。错误的切分方法会过高地估计该片段的凝合程度。如果我们把“电影院”看作是“电”加“影院”所得，由此得到的凝合程度会更高一些。因此，为了算出一个文本片段的凝合程度，我们需要枚举它的凝合方式——这个文本片段是由哪两部分组合而来的。令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义“电影院”的凝合程度就是 p(电影院) 与 p(电) · p(影院) 比值和 p(电影院) 与 p(电影) · p(院) 的比值中的较小值，“的电影”的凝合程度则是 p(的电影) 分别除以 p(的) · p(电影) 和 p(的电) · p(影) 所得的商的较小值。 可以想到，凝合程度最高的文本片段就是诸如“蝙蝠”、“蜘蛛”、“彷徨”、“忐忑”、“玫瑰”之类的词了，这些词里的每一个字几乎总是会和另一个字同时出现，从不在其他场合中使用。 光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、“进被子”、“好被子”、“这被子”等等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是“一辈子”、“这辈子”之类的整体。可见，文本片段的自由运用程度也是判断它是否成词的重要标准。如果一个文本片段能够算作一个词的话，它应该能够灵活地出现在各种不同的环境中，具有非常丰富的左邻字集合和右邻字集合。 “信息熵”是一个非常神奇的概念，它能够反映知道一个事件的结果后平均会给你带来多大的信息量。如果某个结果的发生概率为 p ，当你知道它确实发生了，你得到的信息量就被定义为 - log(p) 。 p 越小，你得到的信息量就越大。如果一颗骰子的六个面分别是 1 、 1 、 1 、 2 、 2 、 3 ，那么你知道了投掷的结果是 1 时可能并不会那么吃惊，它给你带来的信息量是 - log(1/2) ，约为 0.693 。知道投掷结果是 2 ，给你带来的信息量则是 - log(1/3) ≈ 1.0986 。知道投掷结果是 3 ，给你带来的信息量则有 - log(1/6) ≈ 1.79 。但是，你只有 1/2 的机会得到 0.693 的信息量，只有 1/3 的机会得到 1.0986 的信息量，只有 1/6 的机会得到 1.79 的信息量，因而平均情况下你会得到 0.693/2 + 1.0986/3 + 1.79/6 ≈ 1.0114 的信息量。这个 1.0114 就是那颗骰子的信息熵。现在，假如某颗骰子有 100 个面，其中 99 个面都是 1 ，只有一个面上写的 2 。知道骰子的抛掷结果是 2 会给你带来一个巨大无比的信息量，它等于 - log(1/100) ，约为 4.605 ；但你只有百分之一的概率获取到这么大的信息量，其他情况下你只能得到 - log(99/100) ≈ 0.01005 的信息量。平均情况下，你只能获得 0.056 的信息量，这就是这颗骰子的信息熵。再考虑一个最极端的情况：如果一颗骰子的六个面都是 1 ，投掷它不会给你带来任何信息，它的信息熵为 - log(1) = 0 。什么时候信息熵会更大呢？换句话说，发生了怎样的事件之后，你最想问一下它的结果如何？直觉上看，当然就是那些结果最不确定的事件。没错，信息熵直观地反映了一个事件的结果有多么的随机。 我们用信息熵来衡量一个文本片段的左邻字集合和右邻字集合有多随机。考虑这么一句话“吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮”，“葡萄”一词出现了四次，其中左邻字分别为 {吃, 吐, 吃, 吐} ，右邻字分别为 {不, 皮, 倒, 皮} 。根据公式，“葡萄”一词的左邻字的信息熵为 - (1/2) · log(1/2) - (1/2) · log(1/2) ≈ 0.693 ，它的右邻字的信息熵则为 - (1/2) · log(1/2) - (1/4) · log(1/4) - (1/4) · log(1/4) ≈ 1.04 。可见，在这个句子中，“葡萄”一词的右邻字更加丰富一些。 在人人网用户状态中，“被子”一词一共出现了 956 次，“辈子”一词一共出现了 2330 次，两者的右邻字集合的信息熵分别为 3.87404 和 4.11644 ，数值上非常接近。但“被子”的左邻字用例非常丰富：用得最多的是“晒被子”，它一共出现了 162 次；其次是“的被子”，出现了 85 次；接下来分别是“条被子”、“在被子”、“床被子”，分别出现了 69 次、 64 次和 52 次；当然，还有“叠被子”、“盖被子”、“加被子”、“新被子”、“掀被子”、“收被子”、“薄被子”、“踢被子”、“抢被子”等 100 多种不同的用法构成的长尾⋯⋯所有左邻字的信息熵为 3.67453 。但“辈子”的左邻字就很可怜了， 2330 个“辈子”中有 1276 个是“一辈子”，有 596 个“这辈子”，有 235 个“下辈子”，有 149 个“上辈子”，有 32 个“半辈子”，有 10 个“八辈子”，有 7 个“几辈子”，有 6 个“哪辈子”，以及“n 辈子”、“两辈子”等 13 种更罕见的用法。所有左邻字的信息熵仅为 1.25963 。因而，“辈子”能否成词，明显就有争议了。“下子”则是更典型的例子， 310 个“下子”的用例中有 294 个出自“一下子”， 5 个出自“两下子”， 5 个出自“这下子”，其余的都是只出现过一次的罕见用法。事实上，“下子”的左邻字信息熵仅为 0.294421 ，我们不应该把它看作一个能灵活运用的词。当然，一些文本片段的左邻字没啥问题，右邻字用例却非常贫乏，例如“交响”、“后遗”、“鹅卵”等，把它们看作单独的词似乎也不太合适。我们不妨就把一个文本片段的自由运用程度定义为它的左邻字信息熵和右邻字信息熵中的较小值。 在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、“睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。 我们把文本中出现过的所有长度不超过 d 的子串都当作潜在的词（即候选词，其中 d 为自己设定的候选词长度上限，我设定的值为 5 ），再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。为了提高效率，我们可以把语料全文视作一整个字符串，并对该字符串的所有后缀按字典序排序。下表就是对“四是四十是十十四是十四四十是四十”的所有后缀进行排序后的结果。实际上我们只需要在内存中存储这些后缀的前 d + 1 个字，或者更好地，只储存它们在语料中的起始位置。 十 十十四是十四四十是四十 十是十十四是十四四十是四十 十是四十 十四是十四四十是四十 十四四十是四十 是十十四是十四四十是四十 是十四四十是四十 是四十 是四十是十十四是十四四十是四十 四十 四十是十十四是十四四十是四十 四十是四十 四是十四四十是四十 四是四十是十十四是十四四十是四十 四四十是四十 这样的话，相同的候选词便都集中在了一起，从头到尾扫描一遍便能算出各个候选词的频数和右邻字信息熵。将整个语料逆序后重新排列所有的后缀，再扫描一遍后便能统计出每个候选词的左邻字信息熵。另外，有了频数信息后，凝固程度也都很好计算了。这样，我们便得到了一个无需任何知识库的抽词算法，输入一段充分长的文本，这个算法能以大致 O(n · logn) 的效率提取出可能的词来。 对不同的语料进行抽词，并且按这些词的频数从高到低排序。你会发现，不同文本的用词特征是非常明显的。下面是对《西游记》上册的抽词结果： 行者、师父、三藏、八戒、大圣、菩萨、悟空、怎么、和尚、唐僧、老孙、溃骸、什么、沙僧、太宗、徒弟、袈裟、妖精、玉帝、今日、兄弟、公主、玄奘、陛下、宝贝、性命、晓得、门外、妖魔、光蕊、观音、花果山、土地、木叉、东土、变化、变做、伯钦、判官、多少、真君、齐天大圣、蟠桃、丞相、魏征、扯住、溃骸澳、抬头、揭谛、言语、猪八戒、兵器、吩咐、安排、叩头、清风、哪吒、左右、美猴王、钉钯、孩儿、女婿、金箍棒、二郎、东西、许多、奈何、人参果、收拾、近前、太保、明月、南海、水帘洞、门首、弼马温、李天王⋯⋯ 《资本论》全文： 商品、形式、货币、我们、过程、自己、机器、社会、部分、表现、没有、流通、需要、增加、已经、交换、关系、先令、积累、必须、英国、条件、发展、麻布、儿童、进行、提高、消费、减少、任何、手段、职能、土地、特殊、实际、完全、平均、直接、随着、简单、规律、市场、增长、上衣、决定、什么、制度、最后、支付、许多、虽然、棉纱、形态、棉花、法律、绝对、提供、扩大、独立、世纪、性质、假定、每天、包含、物质、家庭、规模、考察、剥削、经济学、甚至、延长、财富、纺纱、购买、开始、代替、便士、怎样、降低、能够、原料、等价物⋯⋯ 《圣经》全文： 以色列、没有、自己、一切、面前、大卫、知道、什么、犹大、祭司、摩西、看见、百姓、吩咐、埃及、听见、弟兄、告诉、基督、已经、先知、扫罗、父亲、雅各、永远、攻击、智慧、荣耀、临到、洁净、离开、怎样、平安、律法、支派、许多、门徒、打发、好像、仇敌、原文作、名叫、巴比伦、今日、首领、旷野、所罗门、约瑟、两个、燔祭、法老、衣服、脱离、二十、公义、审判、十二、亚伯拉罕、石头、聚集、按着、祷告、罪孽、约书亚、事奉、指着、城邑、进入、彼此、建造、保罗、应当、摩押、圣灵、惧怕、应许、如今、帮助、牲畜⋯⋯ 《时间简史》全文： 黑洞、必须、非常、任何、膨胀、科学、预言、太阳、观察、定律、运动、事件、奇点、坍缩、问题、模型、方向、区域、知道、开始、辐射、部分、牛顿、产生、夸克、无限、轨道、解释、边界、甚至、自己、类似、描述、最终、旋转、爱因斯坦、绕着、什么、效应、表明、温度、研究、收缩、吸引、按照、完全、增加、开端、基本、计算、结构、上帝、进行、已经、发展、几乎、仍然、足够、影响、初始、科学家、事件视界、第二、改变、历史、世界、包含、准确、证明、导致、需要、应该、至少、刚好、提供、通过、似乎、继续、实验、复杂、伽利略⋯⋯ 哦，对了，还有我最喜欢的，《人民日报》 2000 年 4 月新闻版的抽词结果： 发展、我们、经济、主席、江泽民、领导、建设、关系、教育、干部、企业、问题、主义、政治、群众、改革、政府、思想、加强、台湾、地区、北京、总统、世界、记者、代表、民族、组织、历史、访问、原则、努力、管理、今天、技术、市场、世纪、坚持、社会主义、财政、江泽民主席、增长、积极、精神、同志、双方、自己、友好、领导干部、进一步、基础、提高、必须、不断、制度、政策、解决、取得、表示、活动、支持、通过、研究、没有、学习、稳定、举行、欢迎、农村、生活、促进、科技、投资、科学、环境、领域、公司、情况、充分⋯⋯ 当然，我也没有忘记对人人网用户状态进行分析——人人网用户状态中最常出现的词是： 哈哈、什么、今天、怎么、现在、可以、知道、喜欢、终于、这样、觉得、因为、如果、感觉、开始、回家、考试、老师、幸福、朋友、时间、发现、东西、快乐、为什么、睡觉、生活、已经、希望、最后、各种、状态、世界、突然、手机、其实、那些、同学、孩子、尼玛、木有、然后、以后、学校、所以、青年、晚安、原来、电话、加油、果然、学习、中国、最近、应该、需要、居然、事情、永远、特别、北京、他妈、伤不起、必须、呵呵、月亮、毕业、问题、谢谢、英语、生日快乐、工作、虽然、讨厌、给力、容易、上课、作业、今晚、继续、努力、有木有、记得⋯⋯ 事实上，程序从人人网的状态数据中一共抽出了大约 1200 个词，里面大多数词也确实都是标准的现代汉语词汇。不过别忘了，我们的目标是新词抽取。将所有抽出来的词与已有词库作对比，于是得到了人人网特有的词汇（同样按频数从高到低排序）： 尼玛、伤不起、给力、有木有、挂科、坑爹、神马、淡定、老爸、卧槽、牛逼、肿么、苦逼、无语、微博、六级、高数、选课、悲催、基友、蛋疼、很久、人人网、情何以堪、童鞋、哇咔咔、脑残、吐槽、猥琐、奶茶、我勒个去、刷屏、妹纸、胃疼、飘过、考研、弱爆了、太准了、搞基、忽悠、羡慕嫉妒恨、手贱、柯南、狗血、秒杀、装逼、真特么、碎觉、奥特曼、内牛满面、斗地主、腾讯、灰常、偶遇、拉拉、屌丝、九把刀、高富帅、阿内尔卡、魔兽世界、线代、三国杀、林俊杰、速速、臭美、花痴⋯⋯ 我还想到了更有意思的玩法。为什么不拿每一天状态里的词去和前一天的状态作对比，从而提取出这一天里特有的词呢？这样一来，我们就能从人人网的用户状态中提取出每日热点了！从手里的数据规模看，这是完全有可能的。我选了 12 个比较具有代表性的词，并列出了它们在 2011 年 12 月 13 日的用户状态中出现的频数（左列的数），以及 2011 年 12 月 14 日的用户状态中出现的频数（右列的数）：下雪 33 92 那些年 139 146 李宇春 1 4 看见 145 695 魔兽 23 20 高数 82 83 生日快乐 235 210 今天 1416 1562 北半球 2 18 脖子 23 69 悲伤 61 33 电磁炉 0 3 大家可以从直觉上迅速判断出，哪些词可以算作是 12 月 14 日的热词。比方说，“下雪”一词在 12 月 13 日只出现了 33 次，在 12 月 14 日却出现了 92 次，后者是前者的 2.8 倍，这不大可能是巧合，初步判断一定是 12 月 14 日真的有什么地方下雪了。“那些年”在 12 月 14 日的频数确实比 12 月 13 日更多，但相差并不大，我们没有理由认为它是当日的一个热词。 一个问题摆在了我们面前：我们如何去量化一个词的“当日热度”？第一想法当然是简单地看一看每个词的当日频数和昨日频数之间的倍数关系，不过细想一下你就发现问题了：它不能解决样本过少带来的偶然性。 12 月 14 日“李宇春”一词的出现频数是 12 月 13 日的 4 倍，这超过了“下雪”一词的 2.8 倍，但我们却更愿意相信“李宇春”的现象只是一个偶然。更麻烦的则是“电磁炉”一行， 12 月 14 日的频数是 12 月 13 日的无穷多倍，但显然我们也不能因此就认为“电磁炉”是 12 月 14 日最热的词。 忽略所有样本过少的词？这似乎也不太好，样本少的词也有可能真的是热词。比如“北半球”一词，虽然它在两天里的频数都很少，但这个 9 倍的关系确实不容忽视。事实上，人眼很容易看出哪些词真的是 12 月 14 日的热词：除了“下雪”以外，“看见”、“北半球”和“脖子”也应该是热词。你或许坚信后三个词异峰突起的背后一定有什么原因（并且迫切地想知道这个原因究竟是什么），但却会果断地把“李宇春”和“电磁炉”这两个“异常”归结为偶然原因。你的直觉是对的—— 2011 年 12 月 14 日发生了极其壮观的双子座流星雨，此乃北半球三大流星雨之一。白天网友们不断转发新闻，因而“北半球”一词热了起来；晚上网友们不断发消息说“看见了”、“又看见了”，“看见”一词的出现频数猛增；最后呢，仰望天空一晚上，脖子终于出毛病了，于是回家路上一个劲儿地发“脖子难受”。 让计算机也能聪明地排除偶然因素，这是我们在数据挖掘过程中经常遇到的问题。我们经常需要对样本过少的项目进行“平滑”操作，以避免分母过小带来的奇点。这里，我采用的是一个非常容易理解的方法：一个词的样本太少，就给这个词的热度打折扣。为了便于说明，我们选出四个词为例来分析。 下表截取了前四个词，右边四列分别表示各词在 12 月 13 日出现的频数，在 12 月 14 日出现的频数，在两天里一共出现的总频数，以及后一天的频数所占的比重。第三列数字是前两列数字之和，第四列数字则是第二列数字除以第三列数字的结果。最后一列应该是一个 0 到 1 之间的数，它表明对应的词有多大概率出现在了 12 月 14 日这一天。最后一列可以看作是各词的得分。可以看到，此时“下雪”的得分低于“李宇春”，这是我们不希望看到的结果。“李宇春”的样本太少，我们想以此为缘由把它的得分拖下去。下雪 33 92 125 0.736 那些年 139 146 285 0.512 李宇春 1 4 5 0.8 看见 145 695 840 0.827 （平均） 313.75 0.719 怎么做呢？我们把每个词的得分都和全局平均分取一个加权平均！首先计算出这四个词的平均总频数，为 313.75 ；再计算出这四个词的平均得分，为 0.719 。接下来，我们假设已经有 313.75 个人预先给每个词都打了 0.719 分，换句话说每个词都已经收到了 313.75 次评分，并且所有这 313.75 个评分都是 0.719 分。“下雪”这个词则还有额外的 125 个人评分，其中每个人都给了 0.736 分。因此，“下雪”一词的最终得分就是：下雪 (0.736 × 125 + 0.719 × 313.75) / (125 + 313.75) ≈ 0.724 类似地，其他几个词的得分依次为：那些年 (0.512 × 285 + 0.719 × 313.75) / (285 + 313.75) ≈ 0.62 李宇春 (0.8 × 5 + 0.719 × 313.75) / (5 + 313.75) ≈ 0.7202 看见 (0.827 × 840 + 0.719 × 313.75) / (840 + 313.75) ≈ 0.798 容易看出，此时样本越大的词，就越有能力把最终得分拉向自己本来的得分，样本太小的词，最终得分将会与全局平均分非常接近。经过这么一番调整，“下雪”一词的得分便高于了“李宇春”。实际运用中， 313.75 这个数也可以由你自己来定，定得越高就表明你越在意样本过少带来的负面影响。这种与全局平均取加权平均的思想叫做 Bayesian average ，从上面的若干式子里很容易看出，它实际上是最常见的平滑处理方法之一——分子分母都加上一个常数——的一种特殊形式。 利用之前的抽词程序抽取出人人网每一天内用户状态所含的词，把它们的频数都与前一天的作对比，再利用刚才的方法加以平滑，便能得出每一天的热词了。我手上的数据是人人网 2011 年 12 月上半月的数据，因此我可以得出从 12 月 2 日到 12 月 15 日的热词（选取每日前 5 名，按得分从高到低）。 2011-12-02：第一场雪、北京、金隅、周末、新疆 2011-12-03：荷兰、葡萄牙、死亡之组、欧洲杯、德国 2011-12-04：那些年、宣传、期末、男朋友、升旗 2011-12-05：教室、老师、视帝、体育课、质量 2011-12-06：乔尔、星期二、摄影、经济、音乐 2011-12-07：陈超、星巴克、优秀、童鞋、投票 2011-12-08：曼联、曼城、欧联杯、皇马、冻死 2011-12-09：保罗、月全食、交易、火箭、黄蜂 2011-12-10：变身、罗伊、穿越、皇马、巴萨 2011-12-11：皇马、巴萨、卡卡、梅西、下半场 2011-12-12：淘宝、阿内尔卡、双十二、申花、老师 2011-12-13：南京、南京大屠杀、勿忘国耻、默哀、警报 2011-12-14：流星雨、许愿、愿望、情人节、几颗 2011-12-15：快船、保罗、巴萨、昨晚、龙门飞甲 看来， 12 月 14 日果然有流星雨发生。 注意，由于我们仅仅对比了相邻两天的状态，因而产生了个别实际上是由工作日/休息日的区别造成的“热词”，比如“教室”、“老师”、“星期二”等。把这样的词当作热词可能并不太妥。结合上周同日的数据，或者干脆直接与之前整个一周的数据来对比，或许可以部分地解决这一问题。 事实上，有了上述工具，我们可以任意比较两段不同文本中的用词特点。更有趣的是，人人网状态的大多数发布者都填写了性别和年龄的个人信息，我们为何不把状态重新分成男性和女性两组，或者 80 后和 90 后两组，挖掘出不同属性的人都爱说什么？要知道，在过去，这样的问题需要进行大规模语言统计调查才能回答！然而，在互联网海量用户生成内容的支持下，我们可以轻而易举地挖掘出答案来。 我真的做了这个工作（基于另一段日期内的数据）。男性爱说的词有： 兄弟、篮球、男篮、米兰、曼联、足球、蛋疼、皇马、比赛、国足、超级杯、球迷、中国、老婆、政府、航母、踢球、赛季、股市、砸蛋、牛逼、铁道部、媳妇、国际、美国、连败、魔兽、斯内德、红十字、经济、腐败、程序、郭美美、英雄、民主、鸟巢、米兰德比、官员、内涵、历史、训练、评级、金融、体育、记者、事故、程序员、媒体、投资、事件、社会、项目、伊布、主义、决赛、操蛋、纳尼、领导、喝酒、民族、新闻、言论、和谐、农民、体制、城管⋯⋯ 下面则是女性爱说的词： 一起玩、蛋糕、加好友、老公、呜呜、姐姐、嘻嘻、老虎、讨厌、妈妈、呜呜呜、啦啦啦、便宜、减肥、男朋友、老娘、逛街、无限、帅哥、礼物、互相、奶茶、委屈、各种、高跟鞋、指甲、城市猎人、闺蜜、巧克力、第二、爸爸、宠物、箱子、吼吼、大黄蜂、狮子、胃疼、玫瑰、包包、裙子、游戏、遇见、嘿嘿、灰常、眼睛、各位、妈咪、化妆、玫瑰花、蓝精灵、幸福、陪我玩、任务、怨念、舍不得、害怕、狗狗、眼泪、温暖、面膜、收藏、李民浩、神经、土豆、零食、痘痘、戒指、巨蟹、晒黑⋯⋯ 下面是 90 后用户爱用的词： 加好友、作业、各种、乖乖、蛋糕、来访、卧槽、通知书、麻将、聚会、补课、欢乐、刷屏、录取、无限、互相、速度、一起玩、啦啦啦、晚安、求陪同、基友、美女、矮油、巨蟹、五月天、第二、唱歌、老虎、扣扣、啧啧、帅哥、哈哈哈、尼玛、便宜、苦逼、斯内普、写作业、劳资、孩纸、哎哟、炎亚纶、箱子、无聊、求来访、查分、上课、果断、处女、首映、屏蔽、混蛋、暑假、吓死、新东方、组队、下学期、陪我玩、打雷、妹纸、水瓶、射手、搞基、吐槽、同学聚会、出去玩、呜呜、白羊、表白、做作业、签名、姐姐、停机、伏地魔、对象、哈哈、主页、情侣、无压力、共同、摩羯、碎觉、肿么办⋯⋯ 下面则是 80 后用户爱用的词： 加班、培训、周末、工作、公司、各位、值班、砸蛋、上班、任务、公务员、工资、领导、包包、办公室、校内、郭美美、时尚、企业、股市、新号码、英国、常联系、实验室、论文、忙碌、项目、部门、祈福、邀请、招聘、顺利、朋友、红十字、男朋友、媒体、产品、标准、号码、存钱、牛仔裤、曼联、政府、简单、立秋、事故、伯明翰、博士、辞职、健康、销售、深圳、奶茶、搬家、实验、投资、节日快乐、坚持、规则、考验、生活、体制、客户、发工资、忽悠、提供、教育、处理、惠存、沟通、团购、缺乏、腐败、启程、红十字会、结婚、管理、环境、暴跌、服务、变形金刚、祝福、银行⋯⋯ 不仅如此，不少状态还带有地理位置信息，因而我们可以站在空间的维度对信息进行观察。这个地方的人都爱说些什么？爱说这个词的人都分布在哪里？借助这些包含地理位置的签到信息，我们也能挖掘出很多有意思的结果来。例如，对北京用户的签到信息进行抽词，然后对于每一个抽出来的词，筛选出所有包含该词的签到信息并按地理坐标的位置聚类，这样我们便能找出那些地理分布最集中的词。结果非常有趣：“考试”一词集中分布在海淀众高校区，“天津”一词集中出现在北京南站，“逛街”一词则全都在西单附近扎堆。北京首都国际机场也是一个非常特别的地点，“北京”、“登机”、“终于”、“再见”等词在这里出现的密度极高。 从全国范围来看，不同区域的人也有明显的用词区别。我们可以将全国地图划分成网格，统计出所有签到信息在各个小格内出现的频数，作为标准分布；然后对于每一个抽出来的词，统计出包含该词的签到信息在各个小格内出现的频数，并与标准分布进行对比（可以采用余弦距离等公式），从而找出那些分布最反常的词。程序运行后发现，这样的词还真不少。一些明显具有南北差异的词，分布就会与整个背景相差甚远。例如，在节假日的时候，“滑雪”一词主要在北方出现，“登山”一词则主要在南方出现。地方特色也是造成词语分布差异的一大原因，例如“三里屯”一词几乎只在北京出现，“热干面”一词集中出现在武汉地区，“地铁”一词明显只有个别城市有所涉及。这种由当地人的用词特征反映出来的真实的地方特色，很可能是许多旅游爱好者梦寐以求的信息。另外，方言也会导致用词分布差异，例如“咋这么”主要分布在北方地区，“搞不懂”主要分布在南方城市，“伐”则非常集中地出现在上海地区。当数据规模足够大时，或许我们能通过计算的方法，自动对中国的方言区进行划分。 其实，不仅仅是发布时间、用户年龄、用户性别、地理位置这四个维度，我们还可以对浏览器、用户职业、用户活跃度、用户行为偏好等各种各样的维度进行分析，甚至可以综合考虑以上维度，在某个特定范围内挖掘热点事件，或者根据语言习惯去寻找出某个特定的人群。或许这听上去太过理想化，不过我坚信，有了合适的算法，这些想法终究会被一一实现。","title":"互联网时代的社会语言学：基于SNS的文本数据挖掘"},{"content":"如何清晰地思考：近一年来业余阅读的关于思维方面的知识结构整理（附大幅思维导图） By 刘未鹏(pongba)  C++ 的罗浮宫(http://blog.csdn.net/pongba)  TopLanguage(https://groups.google.com/group/pongba)   一年前一个偶然的机会我遇到了一本书——《影响力》，看完这本书之后对我们如何思维产生了极大的兴趣，于是在一年的时间里面密集地阅读了以下一些方面的经典著作：社会心理学、认知科学、神经科学、进化心理学、行为经济学、机器学习、人工智能、自然语言处理、问题求解、辩论法（Argumentation Theory）、Critical Thinking、判断与决策。以及大量的 Wikipedia 条目。 这一年来，对以上这些领域的阅读和思考给我带来了极大的价值，我相信他们也会给你带来巨大的收益。 关于为什么我认为我们都需要学习这方面的知识，我曾在博客中写到： 另外还有一些我认为是 essential knowledge 的例子：分析问题解决问题的思维方法（这个东西很难读一两本书就掌握，需要很长时间的锻炼和反思）、判断与决策的方法（生活中需要进行判断与决策的地方远远多于我们的想象），波普尔曾经说过：All Life is Problem-Solving。而判断与决策又是其中最常见的一类Problem Solving。尽管生活中面临重大决策的时候并不多，但另一方面我们时时刻刻都在进行最重大的决策：如：决定自己的日常时间到底投入到什么地方去。如：你能想象有人宁可天天花时间剪报纸上的优惠券，却对于房价的1%的优惠无动于衷吗？（《别做正常的傻瓜》、《Predictably Irrational》）如：你知道为什么当手头股票的股价不可抑止地滑向深渊时我们却一边揪着头发一边愣是不肯撤出吗？（是的，我们适应远古时代的心理机制根本不适应金融市场。）糟糕的判断与决策令我们的生活变得糟糕，这还不是最关键的，最关键的是我们从来不会去质疑自己的判断，而是总是能“找到”其他为自己辩护的理由（《错不在我（Mistakes were made, but not by me）》）又，现在是一个信息泛滥的时代，于是另一个问题也出现：如何在海洋中有效筛选好的信息，以及避免被不好的信息左右我们的大脑（Critical Thinking）关于以上提到的几点我在豆瓣上有一个专门的豆列（“学会思考”），希望有一天我能够积累出足够多的认识对这个主题展开一些详细介绍。 人类的大脑和思维是目前已知最为复杂的系统，对这个系统的研究不仅自身是一件极其迷人的事情，对于像我们这样的芸芸众生来说即便不去做研究，学习一些这方面的科普知识，对于学会正确地思考有极大的益处。 你的大脑是你唯一的工具，要正确利用这个工具，唯一的途径就是去了解它。与很多人的直觉相反，实际上我们的思维有着各种各样的缺陷和陷阱（keyword: cognitive bias），我们解决日常问题的思维方式也并不总是最优的（keyword: bounded rationality），这里摘抄一段我在豆列上的导言： 我们的思维有很多很多的弱点，我一向认为，正确的思维方式，是一切高效学习的基础。比如参见如下2个例子，错误的思维方式得到的结论有大得多的可能性是谬误。 人总喜欢沿袭以往习得的经验，并通过类比来进行外推。我第一次在一个地铁终点站坐地铁的时候，看着从远方开来的地铁，我心生疑惑——“这车每节车厢都这么长，待会怎么调头呢（我心说没看到铁轨终点有一个大大的供调头的 U 形弯啊）？”，当车开始开的时候我终于意识到原来车是可以往两头方向开的。 人喜欢从关联当中寻找因果，有一次我我老婆去银行取款，到了 ATM 室的自动门口，我开玩笑地拿着手头的饭卡去刷了一下，然后——门居然开了。我顿时来了劲，立即得出一个结论：这个刷卡装置不安全，至少不是能够专门识别银联的卡的。我甚至飞快地泛化出了一个更具一般性的理论来解释这个现象：即可能所有带有磁性的卡都可以用来开门。老婆看我得意洋洋，就泼过来一盘冷水：不一定是你的卡刷开的啊，你不刷卡试试看。我不信，说怎么可能呢，心想我刷卡，门就开了，还有比这更明显的因果关系嘛。但出乎我意料的是，我走出门，这次没刷卡，门也开了——原来是感应门——原先这个 ATM 室的确是刷卡门，但后来改成了感应门，刷卡的那个装置只不过没拆掉残留在那里而已。 总的来说 人类的思维充满着各种各样的捷径，每一条捷径都是一把双刃剑。一方面，它降低了大脑的认知复杂性（笼统的看一个问题要比细致的分析简单得多），有助于迅速做出绝大部分时候都正确的判断；但另一方面，它也常常导致人们把大部分情况下成立的法则当成了放之四海而皆准的。可以说，有多少捷径，就有多少条谬误。 人类的情绪也在很大程度上影响着人的思考。比如，如果你憎恶一个人，你往往就会反对他的所有立场。反之亦成立。 人类大脑经过长时间的进化，先天就具备一些特定的“思维定势”，以使得人类能够在面对进化过程中经常出现的适应性问题时能够不假思索的做出迅速的反应。然而，在现代社会，这类思维定势已经不适应了。 人类不可避免的受着各种各样的偏见的影响，这些偏见有些是有一定适应价值的“思维定势”（如事后聪明式偏见），而有些则是大脑的认知机制的“缺陷”。 以上，构成了人类思维中的种种谬误。而学会思考，就是学会认识到这些谬误。 Critical-Thinking 在西方拥有悠久的历史，早到古希腊时代，亚里士多德就已经对人类语言中的各种各样的谬误有了一定的认识（譬如，“我们无法讨论不存在的东西，所以所有的事物都是真实的”），并对辩论之中存在的各种各样的谬误进行了归类。然而令人遗憾的是，在中国的文化里面，理性思维似乎是一直被抑制的，中国文人传统都是非理性思考者；所谓非理性思考，主要包括联想、比方等形式，这些思维方式作为人类天生具有的思维方式的一种，一方面当然有它的好处（比如在科研方面，联想往往能够启发新思路；类比也有助于用新颖的方式来解决既有问题），然而另一方面，这样的思维方式同样也充满了各种各样致命的谬误。在大众知识领域，自中国古代文人思维习惯流传下来的影响深刻地左右着人们的语言习惯，随处可见的不靠谱的类比和文字游戏就是证明（例如，严格来说，类比的一般形式是，A具有X、Y、Z三个属性，B具有X、Y属性（类似于A），所以B具有Z属性。这个类比要成立，必须要满足一个前提，即X、Y属性对于Z属性的有无必须是有关的。然而这个前提被根本忽视了，详见 False Analogy）。 这个豆列中的书，有一些是介绍人类思维工作的机制的，认识这些机制是正确思考的大前提；有许多是关于人类推理（Reasoning）过程中的形形色色的谬误的，因为唯有认识到 这些谬误，才能避免它们。唯有避免了思维的谬误，才能进行正确的思考。 注： 一个最完整的认知偏见（cognitive bias）列表见：http://en.wikipedia.org/wiki/List_of_cognitive_biases 一个完整的 Fallacies 列表见： http://en.wikipedia.org/wiki/Fallacies Wikipedia 上关于 Critical Thinking 的条目见：http://en.wikipedia.org/wiki/Critical_thinking 另： 人类在思考问题的过程中，自身的思维习惯、性格、知识积累无不都在悄悄地影响着思维的过程，所以，一些心理学的知识也非常有助于帮助正确的思考。更多心理学方面的推荐，参考：http://www.douban.com/doulist/46003/ 文章末尾将贴出的是我这一年来学习的知识结构总揽（用 XMind 画的思维导图）。注：这只是一个整体的知识结构，或者说“寻路图”，其中固然包含一些例子（用 “e.g.” 标出），但最重要的是从各个分支引申出去的延伸阅读，后者包含上百个很有价值的 wikipedia 条目，不下 50 本经典的著作（大部分我已经读过，小部分经过我的仔细考察，正在阅读中或者肯定是有价值的）。 如何获得这些延伸出去的阅读，有两个办法： 在总揽图中抽出关键字到 Wikipedia&Google 上查找，如：informal fallacy，cognitive biases, bounded rationality, critical thinking, argumentation theory, behavioral economics,problem solving 等等（以上这些关键字你都会在思维导图中看到）。注：阅读 Wikipedia 时要严重注意每个条目后面的 Reference ，一般来说这些参考资料本身也都非常经典，其价值不亚于 Wikipedia 条目本身。 查看我整理的四个豆列： 【只读经典】心理学改变生活 【只读经典】学会思考 【只读经典】判断与决策 机器学习与人工智能学习资源导引 以上四个豆列中整理的绝大多数都是我阅读过的，你也可以参考我的整个“思维”标签下的书。如何获得这些书（尤其是其中包含大量的无中文翻译版的英文书）请参考李笑来老师的笔记。 这个领域的新知识是如此的纷至沓来，以至于我只有时间不断地阅读和思考，以及不时在我的 Google Notebook 里做一些笔记，而完全没有时间一本书一本书，一个子领域一个子领域地写具体的 Introduction （目前具体的荐书只是在 TopLanguage 上零散的推荐了几本，还没有专题介绍）。既便如此，仍然还是在博客上写了很多相关的东西，它们就是这一年来的学习的收获的证明:-)，因此如果你想快速判断上面列出的一些书籍是否对你有价值，有多大的价值，不妨参考一下我写的这些文章，这些文章很大程度上是在这一年的学习过程当中的感悟或总结。注：第 3 部分（关于学习、记忆与思考）的文章基本上是领域无关的： 关于 Problem Solving 的 《跟波利亚学解题》 《知其所以然地学习（以算法学习为例）》 关于机器学习的（机器学习和人工智能领域对于理解我们的思维方式也提供了极好的参考） 《数学之美番外篇：平凡而又神奇的贝叶斯方法》 《机器学习与人工智能学习资源导引》 关于学习、记忆与思考的 《一直以来伴随我的一些学习习惯》（一，二，三，四） 《方法论、方法论——程序员的阿喀琉斯之踵》 《学习与记忆》 《阅读与思考》 《鱼是最后一个看到水的》 《我不想与我不能》 《学习密度与专注力》 好在我并不打算零星的一本一本推荐:D 所以我就花了点时间将整个的知识体系整理了一番，画了下面这张结构图，请按图索骥，如下（有三个版本，1. 至 xMind Share 的超链接，2. 内嵌在该页面中的幻灯片，如果无法载入请参考 1 。3. 图片版（注：图很大，请下载浏览或打印）） 我在前面写学习习惯的时候曾经提到： 8. 学习一项知识，必须问自己三个重要问题：1. 它的本质是什么。2. 它的第一原则是什么。3. 它的知识结构是怎样的。 有朋友问我具体的例子，好吧，那么这张思维导图便是第三点——知识结构——的一个很好的例子:) 1. 至 XMind Share 的超链接：http://share.xmind.net/pongba/how-to-think-straight-4/ 2. 嵌入的幻灯片（如加载失败请直接点击上面的 XMind Share 超链接至 XMind 浏览）： 3. 图片版（此为缩略版，完整版请至相册下载：google picasa 的 ，或 csdn 相册的）（最后提醒一下，别忘了这幅图只是大量书籍和 Wikipedia 条目的“藏宝图”，如何延伸阅读请参考前文所述的方法） From XMind ----------------------------------------------------------","title":"如何清晰地思考"},{"content":"原文来自：http://www.matrix67.com/blog/archives/5044 学习笔记： 内部凝固度：实际出现概率/max(最大预期概率），例如P(电影院)/max{P(电影)*P(院)，P(电)*P(影院)} 自由运用度：左邻字信息熵和右邻字信息熵中的较小值 贝叶斯平均： 原文： 今年上半年，我在人人网实习了一段时间，期间得到了很多宝贵的数据，并做了一些还算有意义的事情，在这里和大家一块儿分享。感谢人人网提供的数据与工作环境，感谢赵继承博士、詹卫东老师的支持和建议。在这项工作中，我得到了很多与众人交流的机会，特别感谢 OpenParty 、 TEDxBeijing 提供的平台。本文已发表在了《程序员》杂志，分上下两部分刊于 2012 年 7 月刊和 8 月刊，在此感谢卢鸫翔编辑的辛勤工作。由于众所周知的原因，《程序员》刊出的文章被和谐过（看到后面大家就自动地知道被和谐的内容是什么了），因而我决定把完整版发在 Blog 上，同时与更多的人一同分享。对此感兴趣的朋友可以给我发邮件继续交流。好了，开始说正文吧。 作为中文系应用语言学专业的学生以及一名数学 Geek ，我非常热衷于用计算的方法去分析汉语资料。汉语是一种独特而神奇的语言。对汉语资料进行自然语言处理时，我们会遇到很多其他语言不会有的困难，比如分词——汉语的词与词之间没有空格，那计算机怎么才知道，“已结婚的和尚未结婚的青年都要实行计划生育”究竟说的是“已／结婚／的／和／尚未／结婚／的／青年”，还是“已／结婚／的／和尚／未／结婚／的／青年”呢？这就是所谓的分词歧义难题。不过，现在很多语言模型已经能比较漂亮地解决这一问题了。但在中文分词领域里，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？更惨的则是机构名、品牌名、专业名词、缩略语、网络新词等等，它们的产生机制似乎完全无规律可寻。最近十年来，中文分词领域都在集中攻克这一难关。自动发现新词成为了关键的环节。 挖掘新词的传统方法是，先对文本进行分词，然后猜测未能成功匹配的剩余片段就是新词。这似乎陷入了一个怪圈：分词的准确性本身就依赖于词库的完整性，如果词库中根本没有新词，我们又怎么能信任分词结果呢？此时，一种大胆的想法是，首先不依赖于任何已有的词库，仅仅根据词的共同特征，将一段大规模语料中可能成词的文本片段全部提取出来，不管它是新词还是旧词。然后，再把所有抽出来的词和已有词库进行比较，不就能找出新词了吗？有了抽词算法后，我们还能以词为单位做更多有趣的数据挖掘工作。这里，我所选用的语料是人人网 2011 年 12 月前半个月部分用户的状态。非常感谢人人网提供这份极具价值的网络语料。 要想从一段文本中抽出词来，我们的第一个问题就是，怎样的文本片段才算一个词？大家想到的第一个标准或许是，看这个文本片段出现的次数是否足够多。我们可以把所有出现频数超过某个阈值的片段提取出来，作为该语料中的词汇输出。不过，光是出现频数高还不够，一个经常出现的文本片段有可能不是一个词，而是多个词构成的词组。在人人网用户状态中，“的电影”出现了 389 次，“电影院”只出现了 175 次，然而我们却更倾向于把“电影院”当作一个词，因为直觉上看，“电影”和“院”凝固得更紧一些。 为了证明“电影院”一词的内部凝固程度确实很高，我们可以计算一下，如果“电影”和“院”真的是各自独立地在文本中随机出现，它俩正好拼到一起的概率会有多小。在整个 2400 万字的数据中，“电影”一共出现了 2774 次，出现的概率约为 0.000113 。“院”字则出现了 4797 次，出现的概率约为 0.0001969 。如果两者之间真的毫无关系，它们恰好拼在了一起的概率就应该是 0.000113 × 0.0001969 ，约为 2.223 × 10-8次方。但事实上，“电影院”在语料中一共出现了 175 次，出现概率约为 7.183 × 10-6次方，是预测值的 300 多倍。类似地，统计可得“的”字的出现概率约为 0.0166 ，因而“的”和“电影”随机组合到了一起的理论概率值为 0.0166 × 0.000113 ，约为 1.875 × 10-6，这与“的电影”出现的真实概率很接近——真实概率约为 1.6 × 10-5次方，是预测值的 8.5 倍。计算结果表明，“电影院”更可能是一个有意义的搭配，而“的电影”则更像是“的”和“电影”这两个成分偶然拼到一起的。 当然，作为一个无知识库的抽词程序，我们并不知道“电影院”是“电影”加“院”得来的，也并不知道“的电影”是“的”加上“电影”得来的。错误的切分方法会过高地估计该片段的凝合程度。如果我们把“电影院”看作是“电”加“影院”所得，由此得到的凝合程度会更高一些。因此，为了算出一个文本片段的凝合程度，我们需要枚举它的凝合方式——这个文本片段是由哪两部分组合而来的。令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义“电影院”的凝合程度就是 p(电影院) 与 p(电) · p(影院) 比值和 p(电影院) 与 p(电影) · p(院) 的比值中的较小值，“的电影”的凝合程度则是 p(的电影) 分别除以 p(的) · p(电影) 和 p(的电) · p(影) 所得的商的较小值。 可以想到，凝合程度最高的文本片段就是诸如“蝙蝠”、“蜘蛛”、“彷徨”、“忐忑”、“玫瑰”之类的词了，这些词里的每一个字几乎总是会和另一个字同时出现，从不在其他场合中使用。 光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、“进被子”、“好被子”、“这被子”等等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是“一辈子”、“这辈子”之类的整体。可见，文本片段的自由运用程度也是判断它是否成词的重要标准。如果一个文本片段能够算作一个词的话，它应该能够灵活地出现在各种不同的环境中，具有非常丰富的左邻字集合和右邻字集合。 “信息熵”是一个非常神奇的概念，它能够反映知道一个事件的结果后平均会给你带来多大的信息量。如果某个结果的发生概率为 p ，当你知道它确实发生了，你得到的信息量就被定义为 - log(p) 。 p 越小，你得到的信息量就越大。如果一颗骰子的六个面分别是 1 、 1 、 1 、 2 、 2 、 3 ，那么你知道了投掷的结果是 1 时可能并不会那么吃惊，它给你带来的信息量是 - log(1/2) ，约为 0.693 。知道投掷结果是 2 ，给你带来的信息量则是 - log(1/3) ≈ 1.0986 。知道投掷结果是 3 ，给你带来的信息量则有 - log(1/6) ≈ 1.79 。但是，你只有 1/2 的机会得到 0.693 的信息量，只有 1/3 的机会得到 1.0986 的信息量，只有 1/6 的机会得到 1.79 的信息量，因而平均情况下你会得到 0.693/2 + 1.0986/3 + 1.79/6 ≈ 1.0114 的信息量。这个 1.0114 就是那颗骰子的信息熵。现在，假如某颗骰子有 100 个面，其中 99 个面都是 1 ，只有一个面上写的 2 。知道骰子的抛掷结果是 2 会给你带来一个巨大无比的信息量，它等于 - log(1/100) ，约为 4.605 ；但你只有百分之一的概率获取到这么大的信息量，其他情况下你只能得到 - log(99/100) ≈ 0.01005 的信息量。平均情况下，你只能获得 0.056 的信息量，这就是这颗骰子的信息熵。再考虑一个最极端的情况：如果一颗骰子的六个面都是 1 ，投掷它不会给你带来任何信息，它的信息熵为 - log(1) = 0 。什么时候信息熵会更大呢？换句话说，发生了怎样的事件之后，你最想问一下它的结果如何？直觉上看，当然就是那些结果最不确定的事件。没错，信息熵直观地反映了一个事件的结果有多么的随机。 我们用信息熵来衡量一个文本片段的左邻字集合和右邻字集合有多随机。考虑这么一句话“吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮”，“葡萄”一词出现了四次，其中左邻字分别为 {吃, 吐, 吃, 吐} ，右邻字分别为 {不, 皮, 倒, 皮} 。根据公式，“葡萄”一词的左邻字的信息熵为 - (1/2) · log(1/2) - (1/2) · log(1/2) ≈ 0.693 ，它的右邻字的信息熵则为 - (1/2) · log(1/2) - (1/4) · log(1/4) - (1/4) · log(1/4) ≈ 1.04 。可见，在这个句子中，“葡萄”一词的右邻字更加丰富一些。 在人人网用户状态中，“被子”一词一共出现了 956 次，“辈子”一词一共出现了 2330 次，两者的右邻字集合的信息熵分别为 3.87404 和 4.11644 ，数值上非常接近。但“被子”的左邻字用例非常丰富：用得最多的是“晒被子”，它一共出现了 162 次；其次是“的被子”，出现了 85 次；接下来分别是“条被子”、“在被子”、“床被子”，分别出现了 69 次、 64 次和 52 次；当然，还有“叠被子”、“盖被子”、“加被子”、“新被子”、“掀被子”、“收被子”、“薄被子”、“踢被子”、“抢被子”等 100 多种不同的用法构成的长尾⋯⋯所有左邻字的信息熵为 3.67453 。但“辈子”的左邻字就很可怜了， 2330 个“辈子”中有 1276 个是“一辈子”，有 596 个“这辈子”，有 235 个“下辈子”，有 149 个“上辈子”，有 32 个“半辈子”，有 10 个“八辈子”，有 7 个“几辈子”，有 6 个“哪辈子”，以及“n 辈子”、“两辈子”等 13 种更罕见的用法。所有左邻字的信息熵仅为 1.25963 。因而，“辈子”能否成词，明显就有争议了。“下子”则是更典型的例子， 310 个“下子”的用例中有 294 个出自“一下子”， 5 个出自“两下子”， 5 个出自“这下子”，其余的都是只出现过一次的罕见用法。事实上，“下子”的左邻字信息熵仅为 0.294421 ，我们不应该把它看作一个能灵活运用的词。当然，一些文本片段的左邻字没啥问题，右邻字用例却非常贫乏，例如“交响”、“后遗”、“鹅卵”等，把它们看作单独的词似乎也不太合适。我们不妨就把一个文本片段的自由运用程度定义为它的左邻字信息熵和右邻字信息熵中的较小值。 在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、“睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。 我们把文本中出现过的所有长度不超过 d 的子串都当作潜在的词（即候选词，其中 d 为自己设定的候选词长度上限，我设定的值为 5 ），再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。为了提高效率，我们可以把语料全文视作一整个字符串，并对该字符串的所有后缀按字典序排序。下表就是对“四是四十是十十四是十四四十是四十”的所有后缀进行排序后的结果。实际上我们只需要在内存中存储这些后缀的前 d + 1 个字，或者更好地，只储存它们在语料中的起始位置。 十 十十四是十四四十是四十 十是十十四是十四四十是四十 十是四十 十四是十四四十是四十 十四四十是四十 是十十四是十四四十是四十 是十四四十是四十 是四十 是四十是十十四是十四四十是四十 四十 四十是十十四是十四四十是四十 四十是四十 四是十四四十是四十 四是四十是十十四是十四四十是四十 四四十是四十 这样的话，相同的候选词便都集中在了一起，从头到尾扫描一遍便能算出各个候选词的频数和右邻字信息熵。将整个语料逆序后重新排列所有的后缀，再扫描一遍后便能统计出每个候选词的左邻字信息熵。另外，有了频数信息后，凝固程度也都很好计算了。这样，我们便得到了一个无需任何知识库的抽词算法，输入一段充分长的文本，这个算法能以大致 O(n · logn) 的效率提取出可能的词来。 对不同的语料进行抽词，并且按这些词的频数从高到低排序。你会发现，不同文本的用词特征是非常明显的。下面是对《西游记》上册的抽词结果： 行者、师父、三藏、八戒、大圣、菩萨、悟空、怎么、和尚、唐僧、老孙、溃骸、什么、沙僧、太宗、徒弟、袈裟、妖精、玉帝、今日、兄弟、公主、玄奘、陛下、宝贝、性命、晓得、门外、妖魔、光蕊、观音、花果山、土地、木叉、东土、变化、变做、伯钦、判官、多少、真君、齐天大圣、蟠桃、丞相、魏征、扯住、溃骸澳、抬头、揭谛、言语、猪八戒、兵器、吩咐、安排、叩头、清风、哪吒、左右、美猴王、钉钯、孩儿、女婿、金箍棒、二郎、东西、许多、奈何、人参果、收拾、近前、太保、明月、南海、水帘洞、门首、弼马温、李天王⋯⋯ 《资本论》全文： 商品、形式、货币、我们、过程、自己、机器、社会、部分、表现、没有、流通、需要、增加、已经、交换、关系、先令、积累、必须、英国、条件、发展、麻布、儿童、进行、提高、消费、减少、任何、手段、职能、土地、特殊、实际、完全、平均、直接、随着、简单、规律、市场、增长、上衣、决定、什么、制度、最后、支付、许多、虽然、棉纱、形态、棉花、法律、绝对、提供、扩大、独立、世纪、性质、假定、每天、包含、物质、家庭、规模、考察、剥削、经济学、甚至、延长、财富、纺纱、购买、开始、代替、便士、怎样、降低、能够、原料、等价物⋯⋯ 《圣经》全文： 以色列、没有、自己、一切、面前、大卫、知道、什么、犹大、祭司、摩西、看见、百姓、吩咐、埃及、听见、弟兄、告诉、基督、已经、先知、扫罗、父亲、雅各、永远、攻击、智慧、荣耀、临到、洁净、离开、怎样、平安、律法、支派、许多、门徒、打发、好像、仇敌、原文作、名叫、巴比伦、今日、首领、旷野、所罗门、约瑟、两个、燔祭、法老、衣服、脱离、二十、公义、审判、十二、亚伯拉罕、石头、聚集、按着、祷告、罪孽、约书亚、事奉、指着、城邑、进入、彼此、建造、保罗、应当、摩押、圣灵、惧怕、应许、如今、帮助、牲畜⋯⋯ 《时间简史》全文： 黑洞、必须、非常、任何、膨胀、科学、预言、太阳、观察、定律、运动、事件、奇点、坍缩、问题、模型、方向、区域、知道、开始、辐射、部分、牛顿、产生、夸克、无限、轨道、解释、边界、甚至、自己、类似、描述、最终、旋转、爱因斯坦、绕着、什么、效应、表明、温度、研究、收缩、吸引、按照、完全、增加、开端、基本、计算、结构、上帝、进行、已经、发展、几乎、仍然、足够、影响、初始、科学家、事件视界、第二、改变、历史、世界、包含、准确、证明、导致、需要、应该、至少、刚好、提供、通过、似乎、继续、实验、复杂、伽利略⋯⋯ 哦，对了，还有我最喜欢的，《人民日报》 2000 年 4 月新闻版的抽词结果： 发展、我们、经济、主席、江泽民、领导、建设、关系、教育、干部、企业、问题、主义、政治、群众、改革、政府、思想、加强、台湾、地区、北京、总统、世界、记者、代表、民族、组织、历史、访问、原则、努力、管理、今天、技术、市场、世纪、坚持、社会主义、财政、江泽民主席、增长、积极、精神、同志、双方、自己、友好、领导干部、进一步、基础、提高、必须、不断、制度、政策、解决、取得、表示、活动、支持、通过、研究、没有、学习、稳定、举行、欢迎、农村、生活、促进、科技、投资、科学、环境、领域、公司、情况、充分⋯⋯ 当然，我也没有忘记对人人网用户状态进行分析——人人网用户状态中最常出现的词是： 哈哈、什么、今天、怎么、现在、可以、知道、喜欢、终于、这样、觉得、因为、如果、感觉、开始、回家、考试、老师、幸福、朋友、时间、发现、东西、快乐、为什么、睡觉、生活、已经、希望、最后、各种、状态、世界、突然、手机、其实、那些、同学、孩子、尼玛、木有、然后、以后、学校、所以、青年、晚安、原来、电话、加油、果然、学习、中国、最近、应该、需要、居然、事情、永远、特别、北京、他妈、伤不起、必须、呵呵、月亮、毕业、问题、谢谢、英语、生日快乐、工作、虽然、讨厌、给力、容易、上课、作业、今晚、继续、努力、有木有、记得⋯⋯ 事实上，程序从人人网的状态数据中一共抽出了大约 1200 个词，里面大多数词也确实都是标准的现代汉语词汇。不过别忘了，我们的目标是新词抽取。将所有抽出来的词与已有词库作对比，于是得到了人人网特有的词汇（同样按频数从高到低排序）： 尼玛、伤不起、给力、有木有、挂科、坑爹、神马、淡定、老爸、卧槽、牛逼、肿么、苦逼、无语、微博、六级、高数、选课、悲催、基友、蛋疼、很久、人人网、情何以堪、童鞋、哇咔咔、脑残、吐槽、猥琐、奶茶、我勒个去、刷屏、妹纸、胃疼、飘过、考研、弱爆了、太准了、搞基、忽悠、羡慕嫉妒恨、手贱、柯南、狗血、秒杀、装逼、真特么、碎觉、奥特曼、内牛满面、斗地主、腾讯、灰常、偶遇、拉拉、屌丝、九把刀、高富帅、阿内尔卡、魔兽世界、线代、三国杀、林俊杰、速速、臭美、花痴⋯⋯ 我还想到了更有意思的玩法。为什么不拿每一天状态里的词去和前一天的状态作对比，从而提取出这一天里特有的词呢？这样一来，我们就能从人人网的用户状态中提取出每日热点了！从手里的数据规模看，这是完全有可能的。我选了 12 个比较具有代表性的词，并列出了它们在 2011 年 12 月 13 日的用户状态中出现的频数（左列的数），以及 2011 年 12 月 14 日的用户状态中出现的频数（右列的数）： 下雪 33 92 那些年 139 146 李宇春 1 4 看见 145 695 魔兽 23 20 高数 82 83 生日快乐 235 210 今天 1416 1562 北半球 2 18 脖子 23 69 悲伤 61 33 电磁炉 0 3 大家可以从直觉上迅速判断出，哪些词可以算作是 12 月 14 日的热词。比方说，“下雪”一词在 12 月 13 日只出现了 33 次，在 12 月 14 日却出现了 92 次，后者是前者的 2.8 倍，这不大可能是巧合，初步判断一定是 12 月 14 日真的有什么地方下雪了。“那些年”在 12 月 14 日的频数确实比 12 月 13 日更多，但相差并不大，我们没有理由认为它是当日的一个热词。 一个问题摆在了我们面前：我们如何去量化一个词的“当日热度”？第一想法当然是简单地看一看每个词的当日频数和昨日频数之间的倍数关系，不过细想一下你就发现问题了：它不能解决样本过少带来的偶然性。 12 月 14 日“李宇春”一词的出现频数是 12 月 13 日的 4 倍，这超过了“下雪”一词的 2.8 倍，但我们却更愿意相信“李宇春”的现象只是一个偶然。更麻烦的则是“电磁炉”一行， 12 月 14 日的频数是 12 月 13 日的无穷多倍，但显然我们也不能因此就认为“电磁炉”是 12 月 14 日最热的词。 忽略所有样本过少的词？这似乎也不太好，样本少的词也有可能真的是热词。比如“北半球”一词，虽然它在两天里的频数都很少，但这个 9 倍的关系确实不容忽视。事实上，人眼很容易看出哪些词真的是 12 月 14 日的热词：除了“下雪”以外，“看见”、“北半球”和“脖子”也应该是热词。你或许坚信后三个词异峰突起的背后一定有什么原因（并且迫切地想知道这个原因究竟是什么），但却会果断地把“李宇春”和“电磁炉”这两个“异常”归结为偶然原因。你的直觉是对的—— 2011 年 12 月 14 日发生了极其壮观的双子座流星雨，此乃北半球三大流星雨之一。白天网友们不断转发新闻，因而“北半球”一词热了起来；晚上网友们不断发消息说“看见了”、“又看见了”，“看见”一词的出现频数猛增；最后呢，仰望天空一晚上，脖子终于出毛病了，于是回家路上一个劲儿地发“脖子难受”。 让计算机也能聪明地排除偶然因素，这是我们在数据挖掘过程中经常遇到的问题。我们经常需要对样本过少的项目进行“平滑”操作，以避免分母过小带来的奇点。这里，我采用的是一个非常容易理解的方法：一个词的样本太少，就给这个词的热度打折扣。为了便于说明，我们选出四个词为例来分析。 下表截取了前四个词，右边四列分别表示各词在 12 月 13 日出现的频数，在 12 月 14 日出现的频数，在两天里一共出现的总频数，以及后一天的频数所占的比重。第三列数字是前两列数字之和，第四列数字则是第二列数字除以第三列数字的结果。最后一列应该是一个 0 到 1 之间的数，它表明对应的词有多大概率出现在了 12 月 14 日这一天。最后一列可以看作是各词的得分。可以看到，此时“下雪”的得分低于“李宇春”，这是我们不希望看到的结果。“李宇春”的样本太少，我们想以此为缘由把它的得分拖下去。 下雪 33 92 125 0.736 那些年 139 146 285 0.512 李宇春 1 4 5 0.8 看见 145 695 840 0.827 （平均）     313.75 0.719 怎么做呢？我们把每个词的得分都和全局平均分取一个加权平均！首先计算出这四个词的平均总频数，为 313.75 ；再计算出这四个词的平均得分，为 0.719 。接下来，我们假设已经有 313.75 个人预先给每个词都打了 0.719 分，换句话说每个词都已经收到了 313.75 次评分，并且所有这 313.75 个评分都是 0.719 分。“下雪”这个词则还有额外的 125 个人评分，其中每个人都给了 0.736 分。因此，“下雪”一词的最终得分就是： 下雪 (0.736 × 125 + 0.719 × 313.75) / (125 + 313.75) ≈ 0.724 类似地，其他几个词的得分依次为： 那些年 (0.512 × 285 + 0.719 × 313.75) / (285 + 313.75) ≈ 0.62 李宇春 (0.8 × 5 + 0.719 × 313.75) / (5 + 313.75) ≈ 0.7202 看见 (0.827 × 840 + 0.719 × 313.75) / (840 + 313.75) ≈ 0.798 容易看出，此时样本越大的词，就越有能力把最终得分拉向自己本来的得分，样本太小的词，最终得分将会与全局平均分非常接近。经过这么一番调整，“下雪”一词的得分便高于了“李宇春”。实际运用中， 313.75 这个数也可以由你自己来定，定得越高就表明你越在意样本过少带来的负面影响。这种与全局平均取加权平均的思想叫做 Bayesian average ，从上面的若干式子里很容易看出，它实际上是最常见的平滑处理方法之一——分子分母都加上一个常数——的一种特殊形式。 利用之前的抽词程序抽取出人人网每一天内用户状态所含的词，把它们的频数都与前一天的作对比，再利用刚才的方法加以平滑，便能得出每一天的热词了。我手上的数据是人人网 2011 年 12 月上半月的数据，因此我可以得出从 12 月 2 日到 12 月 15 日的热词（选取每日前 5 名，按得分从高到低）。 2011-12-02：第一场雪、北京、金隅、周末、新疆 2011-12-03：荷兰、葡萄牙、死亡之组、欧洲杯、德国 2011-12-04：那些年、宣传、期末、男朋友、升旗 2011-12-05：教室、老师、视帝、体育课、质量 2011-12-06：乔尔、星期二、摄影、经济、音乐 2011-12-07：陈超、星巴克、优秀、童鞋、投票 2011-12-08：曼联、曼城、欧联杯、皇马、冻死 2011-12-09：保罗、月全食、交易、火箭、黄蜂 2011-12-10：变身、罗伊、穿越、皇马、巴萨 2011-12-11：皇马、巴萨、卡卡、梅西、下半场 2011-12-12：淘宝、阿内尔卡、双十二、申花、老师 2011-12-13：南京、南京大屠杀、勿忘国耻、默哀、警报 2011-12-14：流星雨、许愿、愿望、情人节、几颗 2011-12-15：快船、保罗、巴萨、昨晚、龙门飞甲 看来， 12 月 14 日果然有流星雨发生。 注意，由于我们仅仅对比了相邻两天的状态，因而产生了个别实际上是由工作日/休息日的区别造成的“热词”，比如“教室”、“老师”、“星期二”等。把这样的词当作热词可能并不太妥。结合上周同日的数据，或者干脆直接与之前整个一周的数据来对比，或许可以部分地解决这一问题。 事实上，有了上述工具，我们可以任意比较两段不同文本中的用词特点。更有趣的是，人人网状态的大多数发布者都填写了性别和年龄的个人信息，我们为何不把状态重新分成男性和女性两组，或者 80 后和 90 后两组，挖掘出不同属性的人都爱说什么？要知道，在过去，这样的问题需要进行大规模语言统计调查才能回答！然而，在互联网海量用户生成内容的支持下，我们可以轻而易举地挖掘出答案来。 我真的做了这个工作（基于另一段日期内的数据）。男性爱说的词有： 兄弟、篮球、男篮、米兰、曼联、足球、蛋疼、皇马、比赛、国足、超级杯、球迷、中国、老婆、政府、航母、踢球、赛季、股市、砸蛋、牛逼、铁道部、媳妇、国际、美国、连败、魔兽、斯内德、红十字、经济、腐败、程序、郭美美、英雄、民主、鸟巢、米兰德比、官员、内涵、历史、训练、评级、金融、体育、记者、事故、程序员、媒体、投资、事件、社会、项目、伊布、主义、决赛、操蛋、纳尼、领导、喝酒、民族、新闻、言论、和谐、农民、体制、城管⋯⋯ 下面则是女性爱说的词： 一起玩、蛋糕、加好友、老公、呜呜、姐姐、嘻嘻、老虎、讨厌、妈妈、呜呜呜、啦啦啦、便宜、减肥、男朋友、老娘、逛街、无限、帅哥、礼物、互相、奶茶、委屈、各种、高跟鞋、指甲、城市猎人、闺蜜、巧克力、第二、爸爸、宠物、箱子、吼吼、大黄蜂、狮子、胃疼、玫瑰、包包、裙子、游戏、遇见、嘿嘿、灰常、眼睛、各位、妈咪、化妆、玫瑰花、蓝精灵、幸福、陪我玩、任务、怨念、舍不得、害怕、狗狗、眼泪、温暖、面膜、收藏、李民浩、神经、土豆、零食、痘痘、戒指、巨蟹、晒黑⋯⋯ 下面是 90 后用户爱用的词： 加好友、作业、各种、乖乖、蛋糕、来访、卧槽、通知书、麻将、聚会、补课、欢乐、刷屏、录取、无限、互相、速度、一起玩、啦啦啦、晚安、求陪同、基友、美女、矮油、巨蟹、五月天、第二、唱歌、老虎、扣扣、啧啧、帅哥、哈哈哈、尼玛、便宜、苦逼、斯内普、写作业、劳资、孩纸、哎哟、炎亚纶、箱子、无聊、求来访、查分、上课、果断、处女、首映、屏蔽、混蛋、暑假、吓死、新东方、组队、下学期、陪我玩、打雷、妹纸、水瓶、射手、搞基、吐槽、同学聚会、出去玩、呜呜、白羊、表白、做作业、签名、姐姐、停机、伏地魔、对象、哈哈、主页、情侣、无压力、共同、摩羯、碎觉、肿么办⋯⋯ 下面则是 80 后用户爱用的词： 加班、培训、周末、工作、公司、各位、值班、砸蛋、上班、任务、公务员、工资、领导、包包、办公室、校内、郭美美、时尚、企业、股市、新号码、英国、常联系、实验室、论文、忙碌、项目、部门、祈福、邀请、招聘、顺利、朋友、红十字、男朋友、媒体、产品、标准、号码、存钱、牛仔裤、曼联、政府、简单、立秋、事故、伯明翰、博士、辞职、健康、销售、深圳、奶茶、搬家、实验、投资、节日快乐、坚持、规则、考验、生活、体制、客户、发工资、忽悠、提供、教育、处理、惠存、沟通、团购、缺乏、腐败、启程、红十字会、结婚、管理、环境、暴跌、服务、变形金刚、祝福、银行⋯⋯ 不仅如此，不少状态还带有地理位置信息，因而我们可以站在空间的维度对信息进行观察。这个地方的人都爱说些什么？爱说这个词的人都分布在哪里？借助这些包含地理位置的签到信息，我们也能挖掘出很多有意思的结果来。例如，对北京用户的签到信息进行抽词，然后对于每一个抽出来的词，筛选出所有包含该词的签到信息并按地理坐标的位置聚类，这样我们便能找出那些地理分布最集中的词。结果非常有趣：“考试”一词集中分布在海淀众高校区，“天津”一词集中出现在北京南站，“逛街”一词则全都在西单附近扎堆。北京首都国际机场也是一个非常特别的地点，“北京”、“登机”、“终于”、“再见”等词在这里出现的密度极高。 从全国范围来看，不同区域的人也有明显的用词区别。我们可以将全国地图划分成网格，统计出所有签到信息在各个小格内出现的频数，作为标准分布；然后对于每一个抽出来的词，统计出包含该词的签到信息在各个小格内出现的频数，并与标准分布进行对比（可以采用余弦距离等公式），从而找出那些分布最反常的词。程序运行后发现，这样的词还真不少。一些明显具有南北差异的词，分布就会与整个背景相差甚远。例如，在节假日的时候，“滑雪”一词主要在北方出现，“登山”一词则主要在南方出现。地方特色也是造成词语分布差异的一大原因，例如“三里屯”一词几乎只在北京出现，“热干面”一词集中出现在武汉地区，“地铁”一词明显只有个别城市有所涉及。这种由当地人的用词特征反映出来的真实的地方特色，很可能是许多旅游爱好者梦寐以求的信息。另外，方言也会导致用词分布差异，例如“咋这么”主要分布在北方地区，“搞不懂”主要分布在南方城市，“伐”则非常集中地出现在上海地区。当数据规模足够大时，或许我们能通过计算的方法，自动对中国的方言区进行划分。 其实，不仅仅是发布时间、用户年龄、用户性别、地理位置这四个维度，我们还可以对浏览器、用户职业、用户活跃度、用户行为偏好等各种各样的维度进行分析，甚至可以综合考虑以上维度，在某个特定范围内挖掘热点事件，或者根据语言习惯去寻找出某个特定的人群。或许这听上去太过理想化，不过我坚信，有了合适的算法，这些想法终究会被一一实现。","title":"互联网时代的社会语言学：基于SNS的文本数据挖掘"},{"content":"  /* 版权声明：可以任意转载，转载时请务必标明文章原始出处和作者信息 .*/                      文本摘要技术调研                                                    CopyMiddle: 张俊林                          TimeStamp:2010 年9 月   一．文本摘要值得关注的几个方面    1.主题覆盖率         一篇新闻或者文摘往往会包含若干子主题，摘要应该能够覆盖所有这些子主题，至少应该包含主要的子主题；     2.冗余尽可能少         摘要因为是要利用较少的句子来尽可能体现文章主旨信息，所以摘要句子之间的信息冗余应该尽可能小，这样可以满足用尽可能少的信息表达尽可能丰富的文章主旨信息；    3.摘要流畅性强        句子之间往往因为会包含代词等指代信息，所以应该避免阅读起来不流畅的问题。     二．不同的摘要任务类型    1. 抽取式VS合成式        抽取式文摘：摘要的句子完全从文章正文中进行抽取而成 。基本思路是：按照一定因素给每个句子打分，然后根据句子得分排序，按比例输出得分高的句子作为摘要内容；常见做法是线性组合各种特征，各种特征的权值设定手工指定；        合成式文摘：不是纯粹从文章中抽取句子，而是对文中的句子片段进行改写，然后进行拼接生成句子集合作为文摘结果；       从目前研究看，绝大多数实际系统是抽取方式，合成方式目前还是不够成熟，只有少量研究型系统采取这种方法；   2. 单文档VS多文档[1,9]     多文档摘要指的是给定主题相关的K篇文档，通过摘要能够体现这K篇文档的主题信息；     多文档摘要与单文档摘要相比，有些需要特殊考虑之处，比如：            冗余度问题：单文档也有这个问题，但是由于K偏文档可能是非常相似的内容，所以这个问题尤其突出；            句子顺序问题；单文档一般在输出句子的时候，采取按照文章中出现顺序来进行输出；多文档摘要因为句子可能来自不同的文档，所以如何确定顺序是个比较重要的问题。            压缩率问题：单文档摘要只需按照用户指定的压缩率输出即可，多文档摘要要考虑各自从每个文章中抽取句子比例的问题；            指代消解问题：单文档也有类似问题，不过由于多文档的代词如果处理不当，可能会指代到另外一篇文章的命名实体，所以对于多文档摘要这个问题尤其突出；   3.查询相关VS查询无关[11]      所谓查询相关式文本摘要，即与一般的摘要不同，希望给定用户查询条件，然后抽取出的文摘摘要不仅要体现文章主旨，还要和用户查询密切相关。所以在对摘要句子重要性进行衡量时，要同时考虑主题性和查询相关性两方面的考虑因素。    三．抽取式摘要技术方法分类 （1）非监督方法      线性组合方法：利用手工构建的评分函数，采取若干重要特征并手工设定特征权重，以此来对句子重要性进行得分计算。         词汇链方法：通过文章中相邻句子的语义相似性来判断文章主题，引入Wordnet等语言资源中的同义词和近义词信息，分析文章中相邻句子的语义相似性。寻找若干最长的词汇链来确定文章包含主题，并依此来构建文摘句子集合；[6,7]        图模型方法：将文章中每个句子作为图中的节点，利用句子之间内容相似性构建图中节点之间的边。构建好文章图后，利用PageRank或者HITS算法来迭代计算图中节点的权值，按照权值大小作为句子重要性的评分依据来对文摘句子进行抽取。[3,4]       子主题分析方法：通过聚类或者语义块分析等手段，发现文章包含的子主题，并从不同的子主题中抽取句子来构造摘要句子集合。LSA，PLSA等方法属于这一类[8,10,12]。   （2）监督方法：    监督学习方法的基本思路如下：[2,5]       对于句中某个句子，利用分类器来进行二值分类，即0或者1，1代表这个句子可以作为摘要输出句子，0代表这个句子不能作为摘要输出的句子；系统输出被标注为1类型的句子作为文摘输出结果；       训练集往往通过手工生成的《文章，文摘》对来对分类器进行训练。通过事先定义好的特征集合，将句子映射为特征向量，之后对分类器进行训练生成分类模型。对于新的文章，则根据分类器对于句子的二值分类结果进行文摘输出；    常用的监督学习方法包括：      朴素贝叶斯方法(NB)：      决策树方法：      HMM方法：      CRF方法：      逻辑回归方法(LR)：      SVM方法      SVM-HMM方法:   四．自动文摘经常使用的特征  （1）位置因素：句子在文章中出现位置因素，判断句子是否出现在段落首句和尾句，一般段首和段尾是能够体现段落主旨的综合描述句子，尤其是段首句子，如果是则更可能是比较重要的句子； （2）统计特性：一般通过TF.IDF计算单词权值，统计因素的主旨是发现一些能够表达文章主旨的词汇列表，而那些包含较多这些词汇的句子被认为是能够比较充分体现文章主旨的句子； （3）文章标题：是否出现过标题中的内容词，标题作为文章的主旨，如果出现过标题中内容词则更可能体现文章主旨； （4）段落位置：对于新闻类文章而言，往往会在第一段交代很多文章主旨信息，所以距离文章开始位置越近，则一般认为这些句子越重要； （5）启发词汇：比如能够表达总结的句子，比如“总而言之，综上所述”等等，这种启发词汇列表需要归纳； （6）句子长度：以一定的长度作为标准，过长的或者过短的会增加惩罚因素；目前研究主要惩罚过短的句子，过长的也应该列入考虑；     （7）大写单词（英文）：一些大写的单词往往是比较重要的实体或者强调的内容，所以包含大写单词的句子较为重要；     （8）代词：包含代词的句子因为代词需要指明所指代的实体，需要解决指代消解问题，所以在不能有效解决指代消解问题的情况下，需要对于包含代词的句子进行减分；     （9）语义关系分析：有些工作是对句子之间的语义关系进行分析，抽取概述性句子，这个速度比较慢，效果也未必很好，但是可以借鉴的思路是：有些详述性的句子是有很明显特征出现的，对于详述性的句子，应该考虑降分；       (10)冗余的消除：在选择句子作为候选摘要句子时候，尽可能增加内容的信息含量，尽可能减少相同信息的句子重复出现；所以经常对冗余句子进行消除或者减分操作；     （11）语义块的切割：将文档切割成语义密切相关的语义段落，之后从语义段落中抽取句子；   五．目前方法的效果比较      目前有些研究工作[2,5]对目前的主流文摘方法效果进行了对比，综合这些结果，可以得出如下一些结论：       1.对于非监督方法来说，基于HITS的图模型方法明显优于其他方法，       2.对于监督方法来说，SVM-HMM和CRF方法效果最好，其中SVM-HMM方法在一般测试集合上稍微优于CRF，在难度高的测试集合上效果明显好于CRF方法。这两个方法优于HITS图模型方法，不过优势并非特别明显；       3.从测试结果来看，方法效果排序如下         SVM-HMM>CRF>HITS>HMM>SVM>LR>NB>LSA   六．可供选择的方法及其各自优缺点分析      （1）简单特征线性组合方法        即确定一些主要特征，然后设定特征权重后根据线性组合方式来进行句子打分和排序输出；       优点：           方法简单；           无需训练数据；           执行速度快；       缺点：           由于手工拟合评分函数，只能采取部分主要特征；           权重设定需要手工设置并不断调试；           效果一般；          （2）基于HITS的图模型方法    考虑到目前的研究表明，基于HITS的图模型方法是非监督方法中效果最好的，如果采取非监督方法，则优先考虑HITS的图模型方法；    优点:      无需训练集合；      基本与语言和领域无关；      效果好；    缺点：       由于存在任意句子相似性计算和迭代计算，所以运行速度相对比较慢；需要改进速度提出改进方法；       该方法没有考虑信息冗余的问题，可能需要有针对性的改进；   （3）基于CRF或者SVM-HMM的监督学习方法      目前研究表明，CRF和SVM-HMM在所有监督和非监督方法中是效果最好的，其中SVM-HMM效果略好于CRF，CRF略好于HITS图模型方法；       所以如果采取监督学习思路，可以考虑CRF或者SVM-HMM的方法；       优点：            效果好；        缺点：           需要训练数据；           效果依赖于训练数据质量和领域等方面的情况；           执行速度慢；尤其是融合HITS模型等复杂特征，需要首先计算复杂特征，所以速度应该是最慢的；          部分较重要参考文献：   [1] .Jie Tangy, Limin Yaoz, and Dewei Chen . Multi-topicbased Query-oriented Summarization. W.-T.Yih, J. Goodman, L. Vanderwende, and H. Suzuki. Multi-documentsummarization by maximizing informative content-words.In Proceedingsof IJCAI’07, 2007. [2]  Dou Shen1,Jian-Tao Sun.etc    DocumentSummarization using Conditional Random Fields.  InProceedingsof IJCAI’07, 2007.   [3] GunesErkan.  Dragomir R. Radev.  LexRank: Graph-based LexicalCentrality as Salience in   Text Summarization.  Journal of ArtificialIntelligence Research 22 (2004) 457-479 [4] Rada Mihalcea.  Language Independent Extractive Summarization. [5] LiangdaLi†, Ke Zhou†,Gui-Rong Xue etc  Enhancing Diversity, Coverage and Balance for  Summarization through Structure Learning.  WWW 2009. [6] GregorySilber and KathleenF. McCoy  EfficientText Summarization Using Lexical Chains. [7] Barzilay,Regina and Michael Elhadad. Using Lexical Chainsfor Text Summarization. in Proceedings of the IntelligentScalable Text Summarization Workshop(ISTS’97), 1997. [8] Shanmugasundaram Hariharan   Extraction Based Multi Document Summarization using Single Document  Summary Cluster   Int. J.Advance. Soft Comput. Appl., Vol. 2, No. 1, March 2010 [9] ShanmugasundaramHariharan, \"Merging Multi-Document Text Summaries-A Case Study\", Journal of Scienceand Technology, Vol.5, No.4,pp.63-74, December 2009. [10] JinZhang etc  AdaSum: An Adaptive Model for Summarization.  CIKM 2008. [11] Varadarajan and Hristidis. A System forQuery-Specific Document Summarization CIKM2006. [12] LeonhardHennig  Topic-based Multi-DocumentSummarization withProbabilistic Latent Semantic Analysis","title":"文本摘要技术调研"},{"content":"       最近把一些在网上见到的自然语言处理的资源整理了一下，包括论文列表、软件资源和一些实验室主页、个人主页等，希望能对NLP研究者有所帮助，由于个人视野有限，目前只整理了这些，以后会持续更新。在此也感谢这些资源的提供者和维护者。 论文、博客 1.     Google在研究博客中总结了他们2011年的精彩论文《Excellent Papers for 2011》，包括社会网络、机器学          习、人机交互、信息检索、自然语言处理、多媒体、系统等各个领域，很精彩的论文集锦。        http://googleresearch.blogspot.com/2012/03/excellent-papers-for-2011.html         或者zibuyu的BLOG http://blog.sina.com.cn/s/blog_574a437f0100y6zy.html 2.     Best paper awards for AAAI,ACL, CHI, CIKM, FOCS, ICML, IJCAI, KDD, OSDI, SIGIR, SIGMOD, SOSP,         STOC, UIST,VLDB, WWWhttp://jeffhuang.com/best_paper_awards.html 3.     IBM R&D Journal 刚发布了关于Watson的专刊《This is Watson》。总共17篇论文。        http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6177717&punumber=5288520 4.     Web Data Mining作者刘兵维护的一个专题资源：Opinion Mining,Sentiment Analysis, and Opinion Spam          Detection 。http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 5.     Statistical Machine Translationhttp://www.statmt.org/         Statistical Machine TranslationTutorial Readinghttp://cseweb.ucsd.edu/~dkauchak/mt-tutorial/         Philipp Koehn主页http://homepages.inf.ed.ac.uk/pkoehn/ 6.     Profile Hidden Markov ModelResourceshttp://webdocs.cs.ualberta.ca/~colinc/cmput606/         Hidden Markov Model (HMM) Toolbox forMatlabhttp://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html 7.     CRFhttp://www.inference.phy.cam.ac.uk/hmw26/crf/         Conditional Random Field (CRF)Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/CRF/crf.html         FlexCRFs: Flexible Conditional RandomFieldshttp://flexcrfs.sourceforge.net/ 8.     Transfer Learning 包含papers、talks、software等http://www.cse.ust.hk/TL/index.html 9.     Topic Model，Topic Modeling Bibliographyhttp://www.cs.princeton.edu/~mimno/topics.html         David M. Blei的主页http://www.cs.princeton.edu/~blei/publications.htmlMatlab Topic Modeling Toolbox           1.4http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm         LDA GIBBS Java源码http://arbylon.net/resources.html         GibbsLDA++: A C/C++ Implementation ofLatent Dirichlet Allocationhttp://gibbslda.sourceforge.net/ 10.   科学网—推荐系统的循序进阶读物（从入门到精通） - 张子柯的博文        http://blog.sciencenet.cn/home.php?mod=space&uid=210641&do=blog&id=508634 11.   SVM入门http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html 12.   斯坦福大学自然语言处理实验室整理的NLP资源http://www-nlp.stanford.edu/links/statnlp.html 13.   Stanford University InformationRetrieval Resourceshttp://nlp.stanford.edu/IR-book/information-retrieval.html 14.   Software Tools for NLP http://www-a2k.is.tokushima-u.ac.jp/member/kita/NLP/nlp_tools.html 实验室主页 1.       The Stanford NLP Group http://nlp.stanford.edu 2.       The Berkeley Natural LanguageProcessing Grouphttp://nlp.cs.berkeley.edu 3.       University of Tokyo TsujiiLaboratory http://www.nactem.ac.uk/tsujii/publications.cgi?lang=en 4.       Korea University NLP http://nlp.korea.ac.kr/ http://nlp.korea.ac.kr/new/ 5.       中国科学院计算技术研究所自然语言处理研究组http://nlp.ict.ac.cn/new/ 6.       清华大学自然语言处理组 http://nlp.csai.tsinghua.edu.cn/site2/ 7.       HIT-SCIR http://ir.hit.edu.cn/ 8.       武汉大学自然语言处理实验室http://nlp.whu.edu.cn/  9.       苏州大学自然语言处理实验室http://nlp.suda.edu.cn/ 个人主页 1.       David M. Blei， (Princeton) LDA，http://www.cs.princeton.edu/~blei/publications.html 2.       Noah Smith, (CMU),以自然语言处理、机器学习为基础研究computationalsocial science。          http://www.cs.cmu.edu/~nasmith/ 3.       Philipp Koehn (University ofEdinburgh)http://homepages.inf.ed.ac.uk/pkoehn/ 4.       Dekang Lin (University ofAlberta)http://webdocs.cs.ualberta.ca/~lindek/ 5.       Michael Collins(ColumbiaUniversity)http://www.cs.columbia.edu/~mcollins/ 6.       Dekai WU(HKUST) http://www.cs.ust.hk/~dekai/ 7.       Pascale Fung (HKUST) http://www.ee.ust.hk/~pascale/ 8.       Alessandro Moschitti (Universityof Trento)http://disi.unitn.it/moschitti/ 9.       Xiaojin (Jerry) Zhu (Universityof Wisconsin-Madison)http://pages.cs.wisc.edu/~jerryzhu/ 10.     Eugene Charniak (BrownUniversity)http://www.cs.brown.edu/~ec/","title":"自然语言处理网上资源整理"},{"content":"PLSA作为一种主题模型，提供了一种文本语义分析的手段，在自然语言处理中有很多应用，例如广告推荐、文本分类、改善搜索相关性等。关于PLSA的应用场景在下一篇博客中介绍，这里先对模型作一个简单的介绍，也算是对PLSA的推导过程做一个梳理。 PLSA：Probabilistic LatentSemantic Analysis，也即浅层概率语义分析，大体来讲就是通过概率手段计算潜在主题与word、document之间的关系。 传统的bag of words模型，通过word之间的匹配来计算文档之间的距离，对于汉语中的一词多义、同义词现象解决起来相对乏力。主题模型通过引入潜在主题维度，将文档投影到潜在主题上，将字面上不同的文档从语义上进行关联。 P(d)：在海量文档中选出文档d的概率 P(z|d)：文档d属于主题z的概率 p(w|z)：在主题z的中选中单词w的概率 因此可以得出以下等式： 在文档di中选出单词wj的概率： 根据条件概率可以得到： 文档集合被选中的概率： 对p求极大似然估计可得到： 极大似然估计也即要估计出的值，以使L最大，也即是使观测到的状态（文档集合）概率最大。 因此目标函数为： 在目标函数中含有对数加法，所以的似然解问题没有闭式解，但可以采用EM算法，不断迭代逼近最优解。 根据全概率公式可得到： 因此： 根据Jenson不等式可以得到： 可以同步不断的求F的最大值从而来逼近L的最大值。因此该问题转化为了一个约束条件下的最优化问题，约束条件为： 通过拉格朗日函数方法可以得到优化的目标函数为： 注意F’中的变量是，对他们分别求导可以得到： 通过简单的变形可以得到：      （EM算法中的E步） （EM算法中的M步）            （EM算法中的M步）   通过给定初始的，从而就可以计算出，然后又可以不同后者再计算出前者，以此不断迭代，从而逼近F的最优解。 至此，最后得到的也即PLSA模型的所求。","title":"PLSA模型简介"},{"content":"又是碰到了一些简单的基本概念，但是仔细想想发现自己没有理解透彻，Search一下，总结如下： 【摘要】 - 生成模型：无穷样本==》概率密度模型 = 产生模型==》预测 - 判别模型：有限样本==》判别函数 = 预测模型==》预测 【简介】 简单的说，假设o是观察值，q是模型。 如果对P(o|q)建模，就是Generative模型。其基本思想是首先建立样本的概率密度模型，再利用模型进行推理预测。要求已知样本无穷或尽可能的大限制。 这种方法一般建立在统计力学和bayes理论的基础之上。 如果对条件概率(后验概率) P(q|o)建模，就是Discrminative模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。代表性理论为统计学习理论。 这两种方法目前交叉较多。 【判别模型Discriminative Model】——inter-class probabilistic description 又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)， p(class|context)。 利用正负例和分类标签，focus在判别模型的边缘分布。目标函数直接对应于分类准确率。 - 主要特点： 寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。 - 优点: 分类边界更灵活，比使用纯概率方法或生产模型得到的更高级。 能清晰的分辨出多类或某一类与其他类之间的差异特征 在聚类、viewpoint changes, partial occlusion and scale variations中的效果较好 适用于较多类别的识别 判别模型的性能比生成模型要简单，比较容易学习 - 缺点： 不能反映训练数据本身的特性。能力有限，可以告诉你的是1还是2，但没有办法把整个场景描述出来。 Lack elegance of generative: Priors, 结构, 不确定性 Alternative notions of penalty functions, regularization, 核函数 黑盒操作: 变量间的关系不清楚，不可视 - 常见的主要有： logistic regression SVMs traditional neural networks Nearest neighbor Conditional random fields(CRF): 目前最新提出的热门模型，从NLP领域产生的，正在向ASR和CV上发展。 - 主要应用： Image and document classification Biosequence analysis Time series prediction 【生成模型Generative Model】——intra-class probabilistic description 又叫产生式模型。估计的是联合概率分布（joint probability distribution），p(class, context)=p(class|context)*p(context)。 用于随机生成的观察值建模，特别是在给定某些隐藏参数情况下。在机器学习中，或用于直接对数据建模（用概率密度函数对观察到的draw建模），或作为生成条件概率密度函数的中间步骤。通过使用贝叶斯rule可以从生成模型中得到条件分布。 如果观察到的数据是完全由生成模型所生成的，那么就可以fitting生成模型的参数，从而仅可能的增加数据相似度。但数据很少能由生成模型完全得到，所以比较准确的方式是直接对条件密度函数建模，即使用分类或回归分析。 与描述模型的不同是，描述模型中所有变量都是直接测量得到。 - 主要特点： 一般主要是对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。 只关注自己的inclass本身（即点左下角区域内的概率），不关心到底 decision boundary在哪。 - 优点: 实际上带的信息要比判别模型丰富， 研究单类问题比判别模型灵活性强 模型可以通过增量学习得到 能用于数据不完整（missing data）情况 modular construction of composed solutions to complex problems prior knowledge can be easily taken into account robust to partial occlusion and viewpoint changes can tolerate significant intra-class variation of object appearance - 缺点： tend to produce a significant number of false positives. This is particularly true for object classes which share a high visual similarity such as horses and cows 学习和计算过程比较复杂 - 常见的主要有： Gaussians, Naive Bayes, Mixtures of multinomials Mixtures of Gaussians, Mixtures of experts, HMMs Sigmoidal belief networks, Bayesian networks Markov random fields 所列举的Generative model也可以用disriminative方法来训练，比如GMM或HMM，训练的方法有EBW(Extended Baum Welch),或最近Fei Sha提出的Large         Margin方法。 - 主要应用： NLP: Traditional rule-based or Boolean logic systems (Dialog and Lexis-Nexis) are giving way to statistical approaches (Markov models and stochastic context grammars) Medical Diagnosis: QMR knowledge base, initially a heuristic expert systems for reasoning about diseases and symptoms been augmented with decision theoretic formulation Genomics and Bioinformatics Sequences represented as generative HMMs 【两者之间的关系】 由生成模型可以得到判别模型，但由判别模型得不到生成模型。 Can performance of SVMs be combined elegantly with flexible Bayesian statistics? Maximum Entropy Discrimination marries both methods: Solve over a distribution of parameters (a distribution over solutions) 【参考网址】 http://prfans.com/forum/viewthread.php?tid=80 http://hi.baidu.com/cat_ng/blog/item/5e59c3cea730270593457e1d.html http://en.wikipedia.org/wiki/Generative_model http://blog.csdn.net/yangleecool/archive/2009/04/05/4051029.aspx ================== 比较三种模型：HMMs and MRF and CRF http://blog.sina.com.cn/s/blog_4cdaefce010082rm.html HMMs(隐马尔科夫模型): 状态序列不能直接被观测到(hidden)； 每一个观测被认为是状态序列的随机函数； 状态转移矩阵是随机函数，根据转移概率矩阵来改变状态。 HMMs与MRF的区别是只包含标号场变量，不包括观测场变量。 MRF(马尔科夫随机场) 将图像模拟成一个随机变量组成的网格。 其中的每一个变量具有明确的对由其自身之外的随机变量组成的近邻的依赖性(马尔科夫性)。 CRF(条件随机场),又称为马尔可夫随机域 一种用于标注和切分有序数据的条件概率模型。 从形式上来说CRF可以看做是一种无向图模型，考察给定输入序列的标注序列的条件概率。 在视觉问题的应用： HMMs:图像去噪、图像纹理分割、模糊图像复原、纹理图像检索、自动目标识别等 MRF: 图像恢复、图像分割、边缘检测、纹理分析、目标匹配和识别等 CRF: 目标检测、识别、序列图像中的目标分割 P.S. 标号场为隐随机场，它描述像素的局部相关属性，采用的模型应根据人们对图像的结构与特征的认识程度，具有相当大的灵活性。 空域标号场的先验模型主要有非因果马尔可夫模型和因果马尔可夫模型。 参考自：link ================================================== 概率图模型之生成模型与判别模型 自然语言处理中，经常要处理序列标注问题（分词、词性标注、组快分析等），为给定的观察序列标注标记序列。 令o和s分别代表观察序列和标记序列，   根据贝叶斯公式， 1 生成模型和判别模型的定义 对o和s进行统计建模，通常有两种方式： (1)生成模型 构建o和s的联合分布p(s,o) (2)判别模型 构建o和s的条件分布p(s|o) 2 判别模型和生成模型的对比 （1）训练时，二者优化准则不同 生成模型优化训练数据的联合分布概率； 判别模型优化训练数据的条件分布概率，判别模型与序列标记问题有较好的对应性。 （2）对于观察序列的处理不同 生成模型中，观察序列作为模型的一部分； 判别模型中，观察序列只作为条件，因此可以针对观察序列设计灵活的特征。 （3）训练复杂度不同 判别模型训练复杂度较高。 （4）是否支持无指导训练 生成模型支持无指导训练。 From：link ==================================================== 一个通俗易懂的解释，摘录如下： Let’s say you have input data x and you want to classify the data into labels y. A generative model learns the joint probability distribution p(x,y) and a discriminative model learns the conditional probability distribution p(y|x) – which you should read as ‘the probability of y given x’. Here’s a really simple example. Suppose you have the following data in the form (x,y): (1,0), (1,0), (2,0), (2, 1) p(x,y) is y=0 y=1 ----------- x=1 | 1/2 0 x=2 | 1/4 1/4 p(y|x) is y=0 y=1 ----------- x=1 | 1 0 x=2 | 1/2 1/2 If you take a few minutes to stare at those two matrices, you will understand the difference between the two probability distributions. The distribution p(y|x) is the natural distribution for classifying a given example x into a class y, which is why algorithms that model this directly are called discriminative algorithms. Generative algorithms model p(x,y), which can be tranformed into p(y|x) by applying Bayes rule and then used for classification. However, the distribution p(x,y) can also be used for other purposes. For example you could use p(x,y) to generate likely (x,y) pairs. From the description above you might be thinking that generative models are more generally useful and therefore better, but it’s not as simple as that. This paper is a very popular reference on the subject of discriminative vs. generative classifiers, but it’s pretty heavy going. The overall gist is that discriminative models generally outperform generative models in classification tasks. 另一个解释，摘录如下： 判别模型Discriminative Model，又可以称为条件模型，或条件概率模型。估计的是条件概率分布(conditional distribution)， p(class|context)。 生成模型Generative Model，又叫产生式模型。估计的是联合概率分布（joint probability distribution），p(class, context)=p(class|context)*p(context)。 //////////////////////////////////////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 机器学习方法的两种分类：产生式模型和判别式模型 假定输入x, 类别标签y —产生式模型（生成模型）估计联合概率 P(x, y), 因可以根据联合概率来生成样本 —: HMMs —判别式模型（判别模型）估计条件概率 P(y|x), 因为没有x的知识，无法生成样本，只能判断分类: SVMs,CRF,MEM 一个举例： (1,0), (1,0), (2,0), (2, 1) 产生式模型： p(x, y)： P(1, 0) = 1/2, P(1, 1) = 0, P(2, 0) = 1/4, P(2, 1) = 1/4. 判别式模型： P(y|x)： P(0|1) = 1, P(1|1) = 0, P(0|2) = 1/2, P(1|2) = 1/2 —o和s分别代表观察序列和标记序列 —产生式模型 —     构建o和s的联合分布p(s,o) —判别式模型 —     构建o和s的条件分布p(s|o) —产生式模型中，观察序列作为模型的一部分； —判别式模型中，观察序列只作为条件，因此可以针对观察序列设计灵活的特征。 产生式模型：无穷样本==》概率密度模型 = 产生模型==》预测 判别式模型：有限样本==》判别函数 = 预测模型==》预测 一般认为判别型模型要好于生成型模型，因为它是直接根据数据对概率建模，而生成型模型还要先求两个难度相当的概率 —————————————– CRF其实就是一种在生产模型基础上的判别模型？ 条件随机场模型是由Lafferty在2001年提出的一种典型的判别式模型。它在观测序列的基础上对目标序列进行建模,重点解决序列化标注的问题条件随机场模型既具有判别式模型的优点,又具有产生式模型考虑到上下文标记间的转移概率,以序列化形式进行全局参数优化和解码的特点,解决了其他判别式模型(如最大熵马尔科夫模型)难以避免的标记偏置问题。 条件随机场理论（CRFs）可以用于序列标记、数据分割、组块分析等自然语言处理任务中。在中文分词、中文人名识别、歧义消解等汉语自然语言处理任务中都有应用，表现很好。 目前基于 CRFs 的主要系统实现有 CRF，FlexCRF，CRF++ 缺点：训练代价大、复杂度高 ——————————— 条件随机场模型是一种无向图模型，它是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。即给定观察序列O,求最佳序列S。 与最大熵模型相似，条件随机场（Conditional random fields，CRFs）是一种机器学习模型，在自然语言处理的许多领域（如词性标注、中文分词、命名实体识别等）都有比较好的应用效果。条件随机场最早 由John D. Lafferty提出，其也是Brown90的 作者之一，和贾里尼克相似，在离开IBM后他去了卡耐基梅隆大学继续搞学术研究，2001年以第一作者的身份发表了CRF的经典论文 “Conditional random fields: Probabilistic models for segmenting and labeling sequence data”。 关于条件随机场的参考文献及其他资料，Hanna Wallach在05年整理和维护的这个页面“conditional random fields”非常不错，其中涵盖了自01年CRF提出以来的很多经典论文（不过似乎只到05年，之后并未更新）以及几个相关的工具包(不过也没有包括CRF++），但是仍然非常值得入门条件随机场的读者参考。 ——————————————– 一 般序列分类模型常常采用隐马模型(HMM), 像基于类的中文分词, 但隐马模型中存在两个假设: 输出独立性假设和马尔可夫性假设. 其中, 输出独立性假设要求序列数据严格相互独立才能保证推导的正确性, 而事实上大多数序列数据不能 被表示成一系列独立事件. 而条件随机场则使用一种概率图模型, 具有表达长距离依赖性和交叠性特征的能力, 能够较好地解决标注(分类)偏置等问题的优点, 而且所有特征可以进行全局归一化, 能够求得全局的最优解. 条件随机场是一个无向图上概率分布的学习框架, 由Lafferty 等首先引入到自然语言处理的串标引学习任务中来. 最常用的一类CRF是线性链CRF, 适用于我们的分词学习. 记观测串为W=w1w2…wn, 标记串(状态)序列 Y=y1y2…yn, 线性链CRF对一个给定串的标注, 其概率定义为: 。。。 。。。 其中, Y是串的标注序列, W是待标记的字符, fk是特征函数, λk是对应的特征函数的权值, 而t是标记, Z(W)是归一化因子, 使得上式成为概率分布. CRF模型的参数估计通常使用L-BFGS算法来完成. CRF的解码过程, 也就是求解未知串标注的过程, 需要搜索计算该串上的一个最大联合概率, 即: Y* = arg max(y)P(Y|W) 在线性链CRF上, 这个计算任务可以用一般的Viterbi算法来有效地完成. 目前我发现的关于CRF的实现有: * CRF++(http://crfpp.sourceforge.net/) * Pocket CRF(http://sourceforge.net/project/showfiles.php?group_id=201943) Reference： [1] http://blog.csdn.net/wen718/archive/2010/10/23/5960820.aspx [2] http://hi.baidu.com/%D5%D4%B7%F6%B7%E7/blog/item/803dfccfedd38037f8dc617b.html","title":"判别模型 和 生成模型"},{"content":"最大熵工具包的使用     最大熵是自然语言处理中经常用到的一种统计方法。网上也有很多最大熵方面的工具包，目前大家用得最多的应该是张乐博士写的最大熵工具包了。该工具包既可以采用命令行形式运行，也可以直接调用接口函数，为大家的研究工作带来了很大的方便。但是，对于刚接触到该工具包的人来说，使用起来还是有些麻烦的。 一、命令行形式使用方法        1、利用命令行形式，首先要准备好特征文件。特征文件的格式在最大熵工具包的使用说明书（manual20041229.pdf）的P24。                    特征文件中，一行就是一个事件（event），其格式首先是该event对应的类别label，然后是特征feature，两者都是string类型的。如果feature后边有“:”，则后边是该feature的值。Label与feature之间，以及feature与feature之间以空格间隔。例如一个事件形式如下： Outdoor Sunny Sad Humid Outdoor     则，该event的label为Outdoor，特征有四个，包括Sunny，Sad，Humid，Outdoor。该事件中的特征没有给出特征值，如果没有显式给出，则其特征值默认为1。        准备好特征文件之后，可以采用命令进行训练：        maxent –m Modelname –i iteraterNum –v train.txt 例如：maxent –m Modelname –i 30 –v train.txt 则你将会得到一个名字为Modelname的最大熵模型。        测试过程，首先也是利用同样的特征模板抽取特征构成特征文件test.txt作为系统输入，然后测试的命令为：        maxent -p –m Modelname –o output.txt test.txt     将输出对每个事件的预测结果 或者采用命令： maxent -p –m Modelname –detail –o output.txt test.txt 将输出详细的概率信息   二、调用函数API n       使用函数接口 l       运行环境：VC7.1 l       加入头文件：#include <maxentmodel.hpp> l       类名：MaxentModel l       训练过程： 加入特征事件： n       begin_add_event(); n        add_event(const vector< string > &context, const outcome_type &outcome, size_t count=1) n       end_add_event() l       训练：train(size_t iter=15, const std::string &method=\"lbfgs\", double sigma=0.0, double tol=1E-05)            保存模型：save(const string &model, bool binary=false) ----------------------------------------------------------------------------------------------------------------- l       测试： n       加载模型：load (const string &model) n       加入特征，方法同训练 n       模型估计使用函数：eval_all（(const vector< string > &context, std::vector< pair< outcome_type, double > > &outcomes,   bool sort_result=true）   三、最大熵工具包下载        张乐博士的最大熵工具包 　 四、最大熵工具的一个介绍 下面的文档介绍了最大熵理论以及最大熵工具包的使用：   最大熵理论及其应用，下载   来源：http://ir.hit.edu.cn/~taozi/ME.htm","title":"最大熵工具包的使用"},{"content":"原文地址：http://blog.csdn.net/pongba/article/details/3549560 如何清晰地思考：近一年来业余阅读的关于思维方面的知识结构整理（附大幅思维导图） By 刘未鹏(pongba)  C++ 的罗浮宫(http://blog.csdn.net/pongba)  TopLanguage(https://groups.google.com/group/pongba)   一年前一个偶然的机会我遇到了一本书——《影响力》，看完这本书之后对我们如何思维产生了极大的兴趣，于是在一年的时间里面密集地阅读了以下一些方面的经典著作：社会心理学、认知科学、神经科学、进化心理学、行为经济学、机器学习、人工智能、自然语言处理、问题求解、辩论法（Argumentation Theory）、Critical Thinking、判断与决策。以及大量的 Wikipedia 条目。 这一年来，对以上这些领域的阅读和思考给我带来了极大的价值，我相信他们也会给你带来巨大的收益。 关于为什么我认为我们都需要学习这方面的知识，我曾在博客中写到： 另外还有一些我认为是 essential knowledge 的例子：分析问题解决问题的思维方法（这个东西很难读一两本书就掌握，需要很长时间的锻炼和反思）、判断与决策的方法（生活中需要进行判断与决策的地方远远多于我们的想象），波普尔曾经说过：All Life is Problem-Solving。而判断与决策又是其中最常见的一类Problem Solving。尽管生活中面临重大决策的时候并不多，但另一方面我们时时刻刻都在进行最重大的决策：如：决定自己的日常时间到底投入到什么地方去。如：你能想象有人宁可天天花时间剪报纸上的优惠券，却对于房价的1%的优惠无动于衷吗？（《别做正常的傻瓜》、《Predictably Irrational》）如：你知道为什么当手头股票的股价不可抑止地滑向深渊时我们却一边揪着头发一边愣是不肯撤出吗？（是的，我们适应远古时代的心理机制根本不适应金融市场。）糟糕的判断与决策令我们的生活变得糟糕，这还不是最关键的，最关键的是我们从来不会去质疑自己的判断，而是总是能“找到”其他为自己辩护的理由（《错不在我（Mistakes were made, but not by me）》）又，现在是一个信息泛滥的时代，于是另一个问题也出现：如何在海洋中有效筛选好的信息，以及避免被不好的信息左右我们的大脑（Critical Thinking）关于以上提到的几点我在豆瓣上有一个专门的豆列（“学会思考”），希望有一天我能够积累出足够多的认识对这个主题展开一些详细介绍。 人类的大脑和思维是目前已知最为复杂的系统，对这个系统的研究不仅自身是一件极其迷人的事情，对于像我们这样的芸芸众生来说即便不去做研究，学习一些这方面的科普知识，对于学会正确地思考有极大的益处。 你的大脑是你唯一的工具，要正确利用这个工具，唯一的途径就是去了解它。与很多人的直觉相反，实际上我们的思维有着各种各样的缺陷和陷阱（keyword: cognitive bias），我们解决日常问题的思维方式也并不总是最优的（keyword: bounded rationality），这里摘抄一段我在豆列上的导言： 我们的思维有很多很多的弱点，我一向认为，正确的思维方式，是一切高效学习的基础。比如参见如下2个例子，错误的思维方式得到的结论有大得多的可能性是谬误。 人总喜欢沿袭以往习得的经验，并通过类比来进行外推。我第一次在一个地铁终点站坐地铁的时候，看着从远方开来的地铁，我心生疑惑——“这车每节车厢都这么长，待会怎么调头呢（我心说没看到铁轨终点有一个大大的供调头的 U 形弯啊）？”，当车开始开的时候我终于意识到原来车是可以往两头方向开的。 人喜欢从关联当中寻找因果，有一次我我老婆去银行取款，到了 ATM 室的自动门口，我开玩笑地拿着手头的饭卡去刷了一下，然后——门居然开了。我顿时来了劲，立即得出一个结论：这个刷卡装置不安全，至少不是能够专门识别银联的卡的。我甚至飞快地泛化出了一个更具一般性的理论来解释这个现象：即可能所有带有磁性的卡都可以用来开门。老婆看我得意洋洋，就泼过来一盘冷水：不一定是你的卡刷开的啊，你不刷卡试试看。我不信，说怎么可能呢，心想我刷卡，门就开了，还有比这更明显的因果关系嘛。但出乎我意料的是，我走出门，这次没刷卡，门也开了——原来是感应门——原先这个 ATM 室的确是刷卡门，但后来改成了感应门，刷卡的那个装置只不过没拆掉残留在那里而已。 总的来说 人类的思维充满着各种各样的捷径，每一条捷径都是一把双刃剑。一方面，它降低了大脑的认知复杂性（笼统的看一个问题要比细致的分析简单得多），有助于迅速做出绝大部分时候都正确的判断；但另一方面，它也常常导致人们把大部分情况下成立的法则当成了放之四海而皆准的。可以说，有多少捷径，就有多少条谬误。 人类的情绪也在很大程度上影响着人的思考。比如，如果你憎恶一个人，你往往就会反对他的所有立场。反之亦成立。 人类大脑经过长时间的进化，先天就具备一些特定的“思维定势”，以使得人类能够在面对进化过程中经常出现的适应性问题时能够不假思索的做出迅速的反应。然而，在现代社会，这类思维定势已经不适应了。 人类不可避免的受着各种各样的偏见的影响，这些偏见有些是有一定适应价值的“思维定势”（如事后聪明式偏见），而有些则是大脑的认知机制的“缺陷”。 以上，构成了人类思维中的种种谬误。而学会思考，就是学会认识到这些谬误。 Critical-Thinking 在西方拥有悠久的历史，早到古希腊时代，亚里士多德就已经对人类语言中的各种各样的谬误有了一定的认识（譬如，“我们无法讨论不存在的东西，所以所有的事物都是真实的”），并对辩论之中存在的各种各样的谬误进行了归类。然而令人遗憾的是，在中国的文化里面，理性思维似乎是一直被抑制的，中国文人传统都是非理性思考者；所谓非理性思考，主要包括联想、比方等形式，这些思维方式作为人类天生具有的思维方式的一种，一方面当然有它的好处（比如在科研方面，联想往往能够启发新思路；类比也有助于用新颖的方式来解决既有问题），然而另一方面，这样的思维方式同样也充满了各种各样致命的谬误。在大众知识领域，自中国古代文人思维习惯流传下来的影响深刻地左右着人们的语言习惯，随处可见的不靠谱的类比和文字游戏就是证明（例如，严格来说，类比的一般形式是，A具有X、Y、Z三个属性，B具有X、Y属性（类似于A），所以B具有Z属性。这个类比要成立，必须要满足一个前提，即X、Y属性对于Z属性的有无必须是有关的。然而这个前提被根本忽视了，详见 False Analogy）。 这个豆列中的书，有一些是介绍人类思维工作的机制的，认识这些机制是正确思考的大前提；有许多是关于人类推理（Reasoning）过程中的形形色色的谬误的，因为唯有认识到 这些谬误，才能避免它们。唯有避免了思维的谬误，才能进行正确的思考。 注： 一个最完整的认知偏见（cognitive bias）列表见：http://en.wikipedia.org/wiki/List_of_cognitive_biases 一个完整的 Fallacies 列表见： http://en.wikipedia.org/wiki/Fallacies Wikipedia 上关于 Critical Thinking 的条目见：http://en.wikipedia.org/wiki/Critical_thinking 另： 人类在思考问题的过程中，自身的思维习惯、性格、知识积累无不都在悄悄地影响着思维的过程，所以，一些心理学的知识也非常有助于帮助正确的思考。更多心理学方面的推荐，参考：http://www.douban.com/doulist/46003/ 文章末尾将贴出的是我这一年来学习的知识结构总揽（用 XMind 画的思维导图）。注：这只是一个整体的知识结构，或者说“寻路图”，其中固然包含一些例子（用 “e.g.” 标出），但最重要的是从各个分支引申出去的延伸阅读，后者包含上百个很有价值的 wikipedia 条目，不下 50 本经典的著作（大部分我已经读过，小部分经过我的仔细考察，正在阅读中或者肯定是有价值的）。 如何获得这些延伸出去的阅读，有两个办法： 在总揽图中抽出关键字到 Wikipedia&Google 上查找，如：informal fallacy，cognitive biases, bounded rationality, critical thinking, argumentation theory, behavioral economics,problem solving 等等（以上这些关键字你都会在思维导图中看到）。注：阅读 Wikipedia 时要严重注意每个条目后面的 Reference ，一般来说这些参考资料本身也都非常经典，其价值不亚于 Wikipedia 条目本身。 查看我整理的四个豆列： 【只读经典】心理学改变生活 【只读经典】学会思考 【只读经典】判断与决策 机器学习与人工智能学习资源导引 以上四个豆列中整理的绝大多数都是我阅读过的，你也可以参考我的整个“思维”标签下的书。如何获得这些书（尤其是其中包含大量的无中文翻译版的英文书）请参考李笑来老师的笔记。 这个领域的新知识是如此的纷至沓来，以至于我只有时间不断地阅读和思考，以及不时在我的 Google Notebook 里做一些笔记，而完全没有时间一本书一本书，一个子领域一个子领域地写具体的 Introduction （目前具体的荐书只是在 TopLanguage 上零散的推荐了几本，还没有专题介绍）。既便如此，仍然还是在博客上写了很多相关的东西，它们就是这一年来的学习的收获的证明:-)，因此如果你想快速判断上面列出的一些书籍是否对你有价值，有多大的价值，不妨参考一下我写的这些文章，这些文章很大程度上是在这一年的学习过程当中的感悟或总结。注：第 3 部分（关于学习、记忆与思考）的文章基本上是领域无关的： 关于 Problem Solving 的 《跟波利亚学解题》 《知其所以然地学习（以算法学习为例）》 关于机器学习的（机器学习和人工智能领域对于理解我们的思维方式也提供了极好的参考） 《数学之美番外篇：平凡而又神奇的贝叶斯方法》 《机器学习与人工智能学习资源导引》 关于学习、记忆与思考的 《一直以来伴随我的一些学习习惯》（一，二，三，四） 《方法论、方法论——程序员的阿喀琉斯之踵》 《学习与记忆》 《阅读与思考》 《鱼是最后一个看到水的》 《我不想与我不能》 《学习密度与专注力》 好在我并不打算零星的一本一本推荐:D 所以我就花了点时间将整个的知识体系整理了一番，画了下面这张结构图，请按图索骥，如下（有三个版本，1. 至 xMind Share 的超链接，2. 内嵌在该页面中的幻灯片，如果无法载入请参考 1 。3. 图片版（注：图很大，请下载浏览或打印）） 我在前面写学习习惯的时候曾经提到： 8. 学习一项知识，必须问自己三个重要问题：1. 它的本质是什么。2. 它的第一原则是什么。3. 它的知识结构是怎样的。 有朋友问我具体的例子，好吧，那么这张思维导图便是第三点——知识结构——的一个很好的例子:) 1. 至 XMind Share 的超链接：http://share.xmind.net/pongba/how-to-think-straight-4/ 2. 嵌入的幻灯片（如加载失败请直接点击上面的 XMind Share 超链接至 XMind 浏览）： 3. 图片版（此为缩略版，完整版请至相册下载：google picasa 的 ，或 csdn 相册的）（最后提醒一下，别忘了这幅图只是大量书籍和 Wikipedia 条目的“藏宝图”，如何延伸阅读请参考前文所述的方法） From XMind ----------------------------------------------------------","title":"如何清晰地思考：近一年来业余阅读的关于思维方面的知识结构整理（附大幅思维导图）"},{"content":"ACL 求助编辑百科名片 访问控制列表（Access Control List，ACL） 是路由器和交换机接口的指令列表，用来控制端口进出的数据包。ACL适用于所有的被路由协议，如IP、IPX、AppleTalk等。这张表中包含了匹配关系、条件和查询语句，表只是一个框架结构，其目的是为了对某种访问进行控制。 ACL介绍 　　信息点间通信和内外网络的通信都是企业网络中必不可少的业务需求，但是为了保证内网的安全性，需要通过安全策略来保障非授权用户只能访问特定的网络资源，从而达到对访问进行控制的目的。简而言之，ACL可以过滤网络中的流量，控制访问的一种网络技术手段。 　　ACL的定义也是基于每一种协议的。如果路由器接口配置成为支持三种协议（IP、AppleTalk以及IPX）的情况，那么，用户必须定义三种ACL来分别控制这三种协议的数据包。 　　[1] 编辑本段ACL的作用 　　ACL可以限制网络流量、提高网络性能。例如，ACL可以根据数据包的协议，指定数据包的优先级。 　　ACL提供对通信流量的控制手段。例如，ACL可以限定或简化路由更新信息的长度，从而限制通过路由器某一网段的通信流量。 　　ACL是提供网络安全访问的基本手段。ACL允许主机A访问人力资源网络，而拒绝主机B访问。 　　ACL可以在路由器端口处决定哪种类型的通信流量被转发或被阻塞。例如，用户可以允许E-mail通信流量被路由，拒绝所有的Telnet通信流量。 　　例如：某部门要求只能使用 WWW 这个功能，就可以通过ACL实现； 又例如，为了某部门的保密性，不允许其访问外网，也不允许外网访问它，就可以通过ACL实现。 编辑本段ACL 3p原则 　　记住 3P 原则，您便记住了在路由器上应用 ACL 的一般规则。您可以为每种协议 (per protocol)、每个方向 (per direction)、每个接口 (per interface) 配置一个 ACL： 　　每种协议一个 ACL 要控制接口上的流量，必须为接口上启用的每种协议定义相应的 ACL。 　　每个方向一个 ACL 一个 ACL 只能控制接口上一个方向的流量。要控制入站流量和出站流量，必须分别定义两个 ACL。 　　每个接口一个 ACL 一个 ACL 只能控制一个接口（例如快速以太网 0/0）上的流量。 　　ACL 的编写可能相当复杂而且极具挑战性。每个接口上都可以针对多种协议和各个方向进行定义。示例中的路由器有两个接口配置了 IP、AppleTalk 和 IPX。该路由器可能需要 12 个不同的 ACL — 协议数 (3) 乘以方向数 (2)，再乘以端口数 (2)。 编辑本段ACL的执行过程 　　一个端口执行哪条ACL，这需要按照列表中的条件语句执行顺序来判断。如果一个数据包的报头跟表中某个条件判断语句相匹配，那么后面的语句就将被忽略，不再进行检查。 　　数据包只有在跟第一个判断条件不匹配时，它才被交给ACL中的下一个条件判断语句进行比较。如果匹配（假设为允许发送），则不管是第一条还是最后一条语句，数据都会立即发送到目的接口。如果所有的ACL判断语句都检测完毕，仍没有匹配的语句出口，则该数据包将视为被拒绝而被丢弃。这里要注意，ACL不能对本路由器产生的数据包进行控制。 编辑本段ACL的分类 　　目前有两种主要的ACL:标准ACL和扩展ACL。其他的还有标准MAC ACL、以太协议 ACL 、IPv6 ACL等。 　　标准的ACL使用 1 ~ 99 以及1300~1999之间的数字作为表号，扩展的ACL使用 100 ~ 199以及2000~2699之间的数字作为表号。 　　标准ACL可以阻止来自某一网络的所有通信流量，或者允许来自某一特定网络的所有通信流量，或者拒绝某一协议簇（比如IP）的所有通信流量。 　　扩展ACL比标准ACL提供了更广泛的控制范围。例如，网络管理员如果希望做到“允许外来的Web通信流量通过，拒绝外来的FTP和Telnet等通信流量”，那么，他可以使用扩展ACL来达到目的，标准ACL不能控制这么精确。 　　在标准与扩展访问控制列表中均要使用表号，而在命名访问控制列表中使用一个字母或数字组合的字符串来代替前面所使用的数字。使用命名访问控制列表可以用来删除某一条特定的控制条目，这样可以让我们在使用过程中方便地进行修改。 在使用命名访问控制列表时，要求路由器的IOS在11.2以上的版本，并且不能以同一名字命名多个ACL，不同类型的ACL也不能使用相同的名字。 　　随着网络的发展和用户要求的变化，从IOS 12.0开始，思科（CISCO）路由器新增加了一种基于时间的访问列表。通过它，可以根据一天中的不同时间，或者根据一星期中的不同日期，或二者相结合来控制网络数据包的转发。这种基于时间的访问列表，就是在原来的标准访问列表和扩展访问列表中，加入有效的时间范围来更合理有效地控制网络。首先定义一个时间范围，然后在原来的各种访问列表的基础上应用它。 　　基于时间访问列表的设计中，用time-range 命令来指定时间范围的名称，然后用absolute命令，或者一个或多个periodic命令来具体定义时间范围。[2] 编辑本段正确放置ACL 　　ACL通过过滤数据包并且丢弃不希望抵达目的地的数据包来控制通信流量。然而，网络能否有效地减少不必要的通信流量，这还要取决于网络管理员把ACL放置在哪个地方。 　　假设在的一个运行TCP/IP协议的网络环境中，网络只想拒绝从RouterA的T0接口连接的网络到RouterD的E1接口连接的网络的访问，即禁止从网络1到网络2的访问。 　　根据减少不必要通信流量的通行准则，网管员应该尽可能地把ACL放置在靠近被拒绝的通信流量的来源处，即RouterA上。如果网管员使用标准ACL来进行网络流量限制，因为标准ACL只能检查源IP地址，所以实际执行情况为：凡是检查到源IP地址和网络1匹配的数据包将会被丢掉，即网络1到网络2、网络3和网络4的访问都将被禁止。由此可见，这个ACL控制方法不能达到网管员的目的。同理，将ACL放在RouterB和RouterC上也存在同样的问题。只有将ACL放在连接目标网络的RouterD上（E0接口），网络才能准确实现网管员的目标。由此可以得出一个结论: 标准ACL要尽量靠近目的端。 　　网管员如果使用扩展ACL来进行上述控制，则完全可以把ACL放在RouterA上，因为扩展ACL能控制源地址（网络1），也能控制目的地址（网络2），这样从网络1到网络2访问的数据包在RouterA上就被丢弃，不会传到RouterB、RouterC和RouterD上，从而减少不必要的网络流量。因此，我们可以得出另一个结论：扩展ACL要尽量靠近源端。ACL的主要的命令　命令描述access-list 定义访问控制列表参数ip access-group 指派一个访问控制列表到一个接口ip access-list extended 定义一个扩展访问控制列表Remark 注释一个访问控制列表show ip access-list 显示已配置的访问控制列表 编辑本段定义ACL时所应遵循的规范 　　（1）ACL的列表号指出了是哪种协议的ACL。各种协议有自己的ACL，而每个协议的ACL又分为标准ACL和扩展ACL。这些ACL是通过ACL列表号区别的。如果在使用一种访问ACL时用错了列表号，那么就会出错误。 　　（2）一个ACL的配置是每协议、每接口、每方向的。路由器的一个接口上每一种协议可以配置进方向和出方向两个ACL。也就是说，如果路由器上启用了IP和IPX两种协议栈，那么路由器的一个接口上可以配置IP、IPX两种协议，每种协议进出两个方向，共四个ACL。 　　（3）ACL的语句顺序决定了对数据包的控制顺序。在ACL中各描述语句的放置顺序是很重要的。当路由器决定某一数据包是被转发还是被阻塞时，会按照各项描述语句在ACL中的顺序，根据各描述语句的判断条件，对数据报进行检查，一旦找到了某一匹配条件就结束比较过程，不再检查以后的其他条件判断语句。 　　（4）最有限制性的语句应该放在ACL语句的首行。把最有限制性的语句放在ACL语句的首行或者语句中靠近前面的位置上，把“全部允许”或者“全部拒绝”这样的语句放在末行或接近末行，可以防止出现诸如本该拒绝(放过)的数据包被放过(拒绝)的情况。 　　（5）新的表项只能被添加到ACL的末尾，这意味着不可能改变已有访问控制列表的功能。如果必须改变，只有先删除已存在的ACL，然后创建一个新ACL，将新ACL应用到相应的接口上。 　　（6）在将ACL应用到接口之前，一定要先建立ACL。首先在全局模式下建立ACL，然后把它应用在接口的出方向或进方向上。在接口上应用一个不存在的ACL是不可能的。 　　（7）ACL语句不能被逐条的删除，只能一次性删除整个ACL。 　　（8）在ACL的最后，有一条隐含的“全部拒绝”的命令，所以在 ACL里一定至少有一条“允许”的语句。 　　（9）ACL只能过滤穿过路由器的数据流量，不能过滤由本路由器上发出的数据包。 　　（10）在路由器选择进行以前，应用在接口进入方向的ACL起作用。 　　（11）在路由器选择决定以后，应用在接口离开方向的ACL起作用。　 　　ACL会议(Annual Meeting of the Association for Computational Linguistics) 　　ACL会议是自然语言处理与计算语言学领域最高级别的学术会议，由计算语言学协会主办，每年一届。涉及对话(Dialogue)篇章(Discourse)评测( Eval)信息抽取( IE) 信息检索( IR) 语言生成(LanguageGen) 语言资源(LanguageRes) 机器翻译(MT) 多模态(Multimodal) 音韵学/ 形态学( Phon/ Morph) 自动问答(QA) 语义(Semantics) 情感(Sentiment) 语音(Speech) 统计机器学习(Stat ML) 文摘(Summarisation) 句法(Syntax) 等多个方面。 编辑本段ACL 常见问题 　　1) “ACL 的最后一条语句都是隐式拒绝语句” 是什么意思？ 　　每个 ACL 的末尾都会自动插入一条隐含的 deny 语句，虽然ACL中看不到这条语句，它仍起作用。隐含的 deny 语句会阻止所有流量，以防不受欢迎的流量意外进入网络。 　　2) 配置ACL后为什么没有生效？ 　　在创建访问控制列表之后，必须将其应用到某个接口才可开始生效。ACL 控制的对象是进出接口的流量","title":"ACL 访问控制列表"},{"content":"  原文名称： An Efficient Digital Search Algorithm by Using a Double-Array Structure 作者： JUN-ICHI AOE 译文： 使用双数组结构的一个高效的Digital Search算法 摘要： 本文介绍了一种新的内部（内部排序的内部，也就是在内存里）数组结构的digital search算法，叫做双数组，结合了数组存取的快速和链式存储的压缩。Digital search树的每一条弧在双数组中都可以以O(1)的时间复杂度计算得到；也就是说，查找一个key值最坏的时间复杂度是O(k)，k是这个key值的长度。本文给出了同时具有速度和空间双重性能的双数组的查找，插入，删除算法。假设双数组的长度是n+cm，n是ds树中节点的数量，m是输入符号的数量，c是一个依赖于实现的常数；那么理论上可以证明插入和删除的最坏时间复杂度分别是cm2（插入要解决冲突，所以慢）和cm，与n没有关系。从实验的结果来看，建立双数组的时间随n增长，并且c是一个相当小的常数，从0.17到1.13。 关键词：数据库系统，数据结构，词典，digital search，动态内部存储，关键词查询算法。 --- 1.       导论 在很多信息检索算法中，很需要采用一种快速的digital search算法，或者叫做trie搜索，因为它一个字符(digital)一 个字符地查看输入。使用这种数据结构的例子有一种词法分析器，和一种编译器的本地代码优化器，一种图书搜索，拼写检查，常用单词过滤器，一种自然语言处理 中的形态学分析器等等。词典能够动态增长在自然语言处理中尤为重要，因为经常需要对词汇表添加新词（这其实是双数组的弱项- -）。本文展示的这一算法适合插入远远多于删除的情况，这样删除带来的空间浪费就可以由插入来填补。        关键词查询策略可以被大致分为两类，按照关键词集合是否可变可以将这些算法分为“动态方法”（允许查询表被修改）和“静态方法”（显然相反）两种。广为人知的“动态方法”有：hashing，二叉树，B+树，扩展hashing，和trie hashing。而“静态方法”有：完美hashing，稀疏表，以及压缩trie。 当使用静态方法的时候我们能专注于提高查询速度和压缩数据结构，而当使用动态方法的时候我们会使用额外的空间以达到更快的更新速度。本文提出的查询方法正 好介于这两者之间，所以我称之为“弱静态方法”。将静态方法扩展到弱静态方法，同时保持前者有用的特性是十分困难的。完美hashing的扩展已经有了，但不能确定插入的时间复杂度上限。本文的目标是建立一种digital search算法，它同时具有静态方法的速度和压缩特性，以及动态方法的快速更新的能力。        不同于基于key值的搜索方法，digital search采用一连串的字符(digit)来表示一个key。每个h层的DS树的节点表示所有以一定的h个字符开始的关键词；这个节点根据第(h+1)个字符定义它的分支。本文的基本观念是压缩trie树，使用两个一维数组base和check来表示trie树，成为双数组，并且给出更新（插入、删除）算法。Trie的每个节点使用指针指向下一个元素，每个索引元素是一个结束标志加上一个指向新节点的指针（或者null）。查询，插入，删除都非常快，但是它会占用很多空间，因为很多trie树节点是空的；也就是说，trie树是稀疏的。所以我们必须尝试映射节点r到check数组，这种映射关系由base[r]指定。        在接下来的章节，我们会详细描述我们的想法。在第二节，我们把DS树形式化为模式匹配机器并定义双数组使用O(1)时间计算一条弧。为了将双数组应用于大的关键词集合，双数组做出了一些修改。最主要的创新是仅将足以分辨不通关键词的前缀存储到双数组，将其他部分存储在单独的string里面。插入删除算法在第三章讨论。当插入一个新的非空位置r的时候遇到另一个节点k已经占用这个位置，插入算法通过重新调整base[r]或者base[k]来解决冲突。在本文中，为减少占用的时间和空间占用组少非空未知的k或r有优先权（就是移动占用少的，占用多的不变）。第四节讨论了每个算法的最坏时间复杂度的理论值，并且通过实验验证了。双数组的部分匹配和key-order查询也做出了讨论。最后第五节做出了结论总结。 双数组trie树的基本构造及简单优化 一、 基本构造 Trie树是搜索树的一种，来自英文单词\"Retrieval\"的简写，可以建立有效的数据检索组织结构，是中文匹配分词算法中词典的一种常见实现。它本质上是一个确定的有限状态自动机（DFA），每个节点代表自动机的一个状态。在词典中这此状态包括＂词前缀＂，＂已成词＂等。 双数组Trie（Double-Array Trie）是trie树的一个简单而有效的实现，由两个整数数组构成，一个是base[]，另一个是check[]。设数组下标为i ,如果base,check均为0,表示该位置为空。如果base为负值，表示该状态为词语。Check表示该状态的前一状态，t=base+a, check[t]=i 。 复制代码 下面举例(源自<<双数组Trie（Double-Array Trie）的数据结构与具体实现>>)来说明用双数组Trie（Double-Array Trie）构造分词算法词典的过程。假定词表中只有“啊，阿根廷，阿胶，阿拉伯，阿拉伯人，埃及”这几个词，用Trie树可以表示为： 我们首先对词表中所有出现的10个汉字进行编码：啊-1，阿-2，唉-3，根-4，胶-5，拉-6，及-7，廷-8，伯-9，人-10。。对于每一个汉字，需要确定一个base值，使得对于所有以该汉字开头的词，在双数组中都能放下。例如，现在要确定“阿”字的base值，假设以“阿”开头的词的第二个字序列码依次为a1，a2，a3……an，我们必须找到一个值i，使得base[i+a1]，check[i+a1]，base[i+a2]，check[i+a2]……base[i+an]，check[i+an]均为0。一旦找到了这个i，“阿”的base值就确定为i。用这种方法构建双数组Trie（Double-Array Trie），经过四次遍历，将所有的词语放入双数组中，然后还要遍历一遍词表，修改base值。因为我们用负的base值表示该位置为词语。如果状态i对应某一个词，而且Base=0，那么令Base=（-1）*i，如果Base的值不是0，那么令Base=（-1）*Base。得到双数组如下： 复制代码 下标 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Base -1 4 4 0 0 0 0 4 -9 4 -11 -12 -4 -14 Check 0 0 0 0 0 0 0 2 2 2 3 8 10 13 词缀 啊 阿 埃 阿根 阿胶 阿拉 埃及 阿根廷 阿拉伯 阿拉伯人 用上述方法生成的双数组，将“啊”，“阿”，“埃”，“阿根”，“阿拉”，“阿胶”，“埃及”，“阿拉伯”，“阿拉伯人”，“阿根廷”均视为状态。每个状态均对应于数组的一个下标。例如设“阿根”的下标为i=8，那么check的内容是“阿”的下标，而base是“阿根廷”的下标的基值。“廷”的序列码为x=8，那么“阿根廷”的下标为base+x=base[8]+8=12。 复制代码 二、 基本操作与存在问题 1， 查询 trie树的查询过程其实就是一个DFA的状态转移过程，在双数组中实现起来比较简单：只需按照状态标志进行状态转移即可．例如查询“阿根廷”，先根据“阿”的序列码b=2，找到状态“阿”的下标2，再根据“根”的序列码d=4找到“阿根”的下标base+d=8，同时根据check[base+d]=b，表明“阿根”是某个词的一部分，可以继续查询。然后再找到状态“阿根廷”。它的下标为y=12，此时base[y]<0，check[y]=base+d=8，表明“阿根廷”在词表中，查询完毕。 复制代码 查询过程中我们可以看到，对于一个词语的查询时间是只与它的长度相关的，也就是说它的时间复杂度为Ｏ(1)．在汉语中，词语以单字词，双字词居多，超过三字的词语少之又少．因此，用双数组构建的trie树词典查询是理论上中文机械分词中的最快实现。 2， 插入与删除 双数组的缺点在于：构造调整过程中，每个状态都依赖于其他状态，所以当在词典中插入或删除词语的时候，往往需要对双数组结构进行全局调整,灵活性能较差。 将一个词语插入原有的双数组trie树中，相当于对DFA增加一个状态。首先我们应根据查询方法找出该状态本应所处的位置，如果该位置为空，那好办，直接插入即可。如果该位置不为空。那么我们只好按照构造时一样的方法重新扫描得出该状态已存在的最大前缀状态的BASE值，并由此依次得出该状态后继结点的BASE值。在这其中还要注意CHECK值的相应变化。 例如说，如果＂阿拉根＂某一天也成为了一个词，我们要在trie树中插入这一状态。按计算它的位置应在８，但8是一个已成状态．所以我们得重新确定＂阿拉＂这一最大已成前缀状态的BASE值．重新扫描得出BASE[10]=11。这样状态１５为＂阿拉根＂，且BASE[15]为负（成词），CHECK[15]=10；状态２０为＂阿拉佰＂，且BASE[20]=-4，CHECK=10。 这样的处理其实是非常耗时间的，因为得依次对每一个可能BASE值进行扫描来进行确定最大已成前缀状态的BASE值。这个确定过程在构造时还是基本可以忍受的，毕竟你就算用上一，两天来构造也没有问题（只要你构造完后可以在效运行即可）。但在插入比较频繁时，如果每次都需要那么长的运行时间，那确实是无法忍受的。 双数组删除实现比较简单，只需要将删除词语的对应状态设为空即可――即BASE值，CHECK均为设0。但它存在存在一个空间效率的问题．例如，当我们在上面删除＂埃及＂这一词语时，状态11被设为空。而状态10则成了一个无用结点――它不成词，而且在插入新词时也不可重用。所以，随着删除的进行，空状态点和无用状态点不断增多，空间的利用率会不断的降低。 三、 简单优化 优化的基本思路是将双数组trie树构建为一种动态检索方法，从而解决插入和删除所存在的问题。 1， 插入优化 在插入需要确定新的BASE值时，我们是只需要遍历空状态的。非空状态的出现意味着某个BASE值尝试的打败，我们可以完全不必理会。所以，我们可以对所有的空状态构建一个序列，在确定BASE值时只需要扫描该序列即可。 对双数组中的空状态的递增结点r1,r2, …, rm，我们可以这样构建这一空序列： CHECK[ri]=−ri+1 (1 i m−1), CHECK[rm]=−(DA_SIZE+1) 其中r1= E_HEAD，为第一个空值状态对应的索引点。这样我们在确定BASE值时只需扫描这一序列即可。这样就省去了对非空状态的访问时间。 这种方法在空状态并不太多的情况下可以很大程度的提高插入速度。 2， 删除优化 1) 无用结点 对于删除叶结点时产生的无用结点，可以通过依次判断将它们置为空，使得可在插入新词时得以重用。例如，如果我们删除了上例中的＂阿根廷＂，可以看到＂阿根＂这一状态没有子状态，因此也可将它置为空。而＂阿＂这一状态不能置空，因为它还有两个子状态。 2) 数组长度的压缩 在删除了一个状态后，数组末尾可能出现的连续空状态我们是可以直接删除的。另外我们还可以重新为最大非空索引点的状态重新确定BASE值，因为它有可能已经由于删除的进行而变小。这们我们可能又得以删除一些空值状态。","title":"双数组TRIE树原理"},{"content":"SCI或SCIE收录的本学科刊物清单请登陆 http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=K 和http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=D  下面列一些CS的顶级会议和期刊    有些是网上查到的，有些是某些人用SCI的IF排序做出来的： Computer Vision Conf.:   Best:     ICCV, Inter. Conf. on Computer Vision     CVPR, Inter. Conf. on Computer Vision and Pattern Recognition   Good:     ECCV, Euro. Conf. on Comp. Vision     ICIP, Inter. Conf. on Image Processing     ICPR, Inter. Conf. on Pattern Recognition     ACCV, Asia Conf. on Comp. Vision Computer Vision  Jour.:   Best:     PAMI, IEEE Trans. on Patt. Analysis and Machine Intelligence     IJCV, Inter. Jour. on Comp. Vision   Good:     CVIU, Computer Vision and Image Understanding PR, Pattern Reco. Network Conf.:     ACM/SigCOMM     ACM Special Interest Group of Communication     ACM/SigMetric Info Com Globe Com Network Jour.:     ToN (ACM/IEEE Transaction on Network) A.I.Conf.:     AAAI: American Association for Artificial Intelligence     ACM/SigIR IJCAI: International Joint Conference on Artificial Intelligence     NIPS: Neural Information Processing Systems     ICML: International Conference on Machine Learning A.I.Jour.:     Machine Learning     NEURAL COMPUTATION     ARTIFICIAL INTELLIGENCE PAMI     IEEE TRANSACTIONS ON FUZZY SYSTEMS     IEEE TRANSACTIONS ON NEURAL NETWORKS AI MAGAZINE     NEURAL NETWORKS     PATTERN RECOGNITION     IMAGE AND VISION COMPUTING     IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING     APPLIED INTELLIGENCE OS,System Conf.:     SOSP: The ACM Symposium on Operating Systems Principles     OSDI: USENIX Symposium on Operating Systems Design and Implementation Database Conf.:     ACM SIGMOD     VLDB:International Conference on Very Large Data Bases     ICDE:International Conference on Data Engineering Security Conf.:     IEEE Security and Privacy     CCS: ACM Computer and Communications Security NDSS (Network and Distributed Systems Security) Web Conf.:     WWW(International World Wide Web Conference) Theory Conf.:     STOC FOCS  EDA Conf.:  Best:      DAC: IEEE/ACM Design Automation Conference     ICCAD: IEEE International Conference on Computer Aided Design Good:     ISCAS: IEEE International Symposium on Circuits And Systems     ISPD: IEEE International Symposium on Physical Design     ICCD: IEEE International Conference on Computer Design     ASP-DAC: European Design Automation Conference     E-DAC: Asia and South Pacific Design Automation Conference Graphics Conf.:   Best:     Siggraph: ACM SigGraph   Good:     Euro Graph Jour.: IEEE(ACM) Trans. on Graphics     IEEE Trans. on Visualization and Computer Graphics CAD    Jour.: CAD CAGD Softe Engineering: conf.:      ICSE The International Conference on Software Engineering     FSE The Foundations of Software Engineering Conferences     ICASE IEEE International Conference on Automated Software Engineering     COMPSAC International Computer Software and Applications Conferences     ESEC The European Software Engineering Conferences Jour.:     SEN ACM SIGSOFT Software Engineering Notes     TSE IEEE Transactions on Software Engineering     ASE Automated Software Engineering SPE Software-Practice and Experience   EI收录的中国期刊：     来自http://www.ei.org.cn/twice/coverage.jsp ISSN 期 刊 名 相关链接 0567-7718 Acta Mechanica Sinica 1006-7191 Acta Metallurgica Sinica (English Letters) 0253-4827 Applied Mathematics and Mechanics (English Edition) 0890-5487 China Ocean Engineering 1004-5341 China Welding 1004-9541 Chinese Journal of Chemical Engineering 1022-4653 Chinese Journal of Electronics 1000-9345 Chinese Journal of Mechanical Engineering (English Edition) 学报网站 1671-7694 Chinese Optics Letters 学报网站 1673-7350 Frontiers of Computer Science in China 期刊网址 1006-6748 High Technology Letters 1674-4799 International Journal of Minerals, Metallurgy and Materials 1004-0579 Journal of Beijing Institute of Technology (English Edition) 学报编辑部 1005-9784 Journal of Central South University of Technology 学报网站 1672-5220 Journal of Donghua University (English Edition) 1005-9113 Journal of Harbin Institute of Technology (New Series) 1001-6058 Journal of Hydrodynamics 1005-0302 Journal of Materials Science and Technology 1002-0721 Journal of Rare Earths 1674-4926 Journal of Semiconductors 学报编辑部 1007-1172 Journal of Shanghai Jiaotong University (Science) 1003-7985 Journal of Southeast University (English Edition) 1004-4132 Journal of Systems Engineering and Electronics 1009-6124 Journal of Systems Science and Complexity 1003-2169 Journal of Thermal Science 1000-2413 Journal of Wuhan University of Technology -Materials Science Edition 1673-565X Journal of Zhejiang University SCIENCE A 1674-5264 Mining Science and Technology 1001-0521 Rare Metals 1006-9291 Science in China, Series B: Chemistry 1672-1799 Science in China, Series G: Physics, Astronomy 1005-8885 The Journal of China Universities of Posts and Telecommunications 1005-1120 Transactions of Nanjing University of Aeronautics and Astronautics 1003-6326 Transactions of Nonferrous Metals Society of China 1006-4982 Transactions of Tianjin University 1007-0214 Tsinghua Science and Technology Editor Information 1001-1455 爆炸与冲击 0254-0037 北京工业大学学报 1001-5965 北京航空航天大学学报 学报编辑部 1001-053X 北京科技大学学报 学报编辑部 1001-0645 北京理工大学学报 学报编辑部 1007-5321 北京邮电大学学报 学报编辑部 1000-1093 兵工学报 1001-4381 材料工程 1005-0299 材料科学与工艺 1009-6264 材料热处理学报 学报网站 1005-3093 材料研究学报 1001-1595 测绘学报 学报编辑部 1007-7294 船舶力学 1000-8608 大连理工大学学报 1004-499X 弹道学报 1000-2383 地球科学 学报网站 1005-0388 电波科学学报 1000-6753 电工技术学报 1007-449X 电机与控制学报 1000-1026 电力系统自动化 学报网站 1006-6047 电力自动化设备 1001-0548 电子科技大学学报 0372-2112 电子学报 1009-5896 电子与信息学报 1005-3026 东北大学学报　(自然科学版) 1001-0505 东南大学学报 (自然科学版) 1000-3851 复合材料学报 1003-6520 高电压技术 1000-7555 高分子材料科学与工程 1002-0470 高技术通讯 1003-9015 高校化学工程学报 1000-5773 高压物理学报 1000-4750 工程力学 0253-231X 工程热物理学报 1001-9731 功能材料 学报网站 1006-2793 固体火箭技术 0254-7805 固体力学学报 1005-0086 光电子.激光 1000-0593 光谱学与光谱分析 1004-924X 光学精密工程 学报网站 0253-2239 光学学报 学报网站 0454-5648 硅酸盐学报 1001-2486 国防科技大学学报 1006-7043 哈尔滨工程大学学报 学报网站 0367-6234 哈尔滨工业大学学报 0253-360X 焊接学报 1005-5053 航空材料学报 1000-8055 航空动力学报 编辑部网站 1000-6893 航空学报 学报网站 0258-0926 核动力工程 1001-9014 红外与毫米波学报 1000-2472 湖南大学学报 (自然科学版) 1000-565X 华南理工大学学报(自然科学版) 编辑部网站 1671-4512 华中科技大学学报(自然科学版) 0438-1157 化工学报 1002-0446 机器人 学报网站 0577-6686 机械工程学报 学报网站 1671-5497 吉林大学学报(工学版) 学报编辑部 1003-9775 计算机辅助设计与图形学学报 1006-5911 计算机集成制造系统 编辑部网站 0254-4164 计算机学报 1000-1239 计算机研究与发展 学报网站 1007-4708 计算力学学报 1001-246X 计算物理 1007-9629 建筑材料学报 1000-6869 建筑结构学报 1671-7775 江苏大学学报（自然科学版） 1009-3443 解放军理工大学学报（自然科学版） 0412-1961 金属学报 0258-1825 空气动力学学报 1000-8152 控制理论与应用 学报网站 1001-0920 控制与决策 0459-1879 力学学报 学报网站 0253-9993 煤炭学报 学报网站 1003-6059 模式识别与人工智能 1004-0595 摩擦学学报 1672-6030 纳米技术与精密工程 1005-2615 南京航空航天大学学报 1005-9830 南京理工大学学报 (自然科学版) 1000-0925 内燃机工程 1000-0909 内燃机学报 1002-6819 农业工程学报 学报编辑部 1000-1298 农业机械学报 学报编辑部 1001-4322 强激光与粒子束 学报编辑部 1000-0054 清华大学学报 (自然科学版) 0253-2409 燃料化学学报 1006-8740 燃烧科学与技术 1000-985X 人工晶体学报 无机材料期刊网 1000-9825 软件学报 学报编辑部 1006-2467 上海交通大学学报 1000-2618 深圳大学学报（理工版） 0371-0025 声学学报 1000-7210 石油地球物理勘探 1000-0747 石油勘探与开发 0253-2697 石油学报 1001-8719 石油学报:石油加工 学报网站 1672-9897 实验流体力学 学报网站 1001-6791 水科学进展 0559-9350 水利学报 学报编辑部 1003-1243 水力发电学报 1009-3087 四川大学学报(工程科学版) 学报编辑部 0254-0096 太阳能学报 学报编辑部 0493-2137 天津大学学报 学报编辑部 1001-8360 铁道学报 1000-436X 通信学报 0253-374X 同济大学学报 (自然科学版) 1000-131X 土木工程学报 学报网站 1674-4764 土木建筑与环境工程 学报编辑部 1001-4055 推进技术 1000-324X 无机材料学报 1671-8860 武汉大学学报(信息科学版) 1001-2400 西安电子科技大学学报 学报网站 0253-987X 西安交通大学学报 1000-2758 西北工业大学学报 0258-2724 西南交通大学学报 学报网站 1002-185X 稀有金属材料与工程 1000-6788 系统工程理论与实践 1001-506X 系统工程与电子技术 1007-8827 新型炭材料 1000-6915 岩石力学与工程学报 学报网站 1000-4548 岩土工程学报 1000-7598 岩土力学 期刊编辑部 0254-3087 仪器仪表学报 1005-0930 应用基础与工程科学学报 学报网站 1000-6931 原子能科学技术 1008-973X 浙江大学学报 (工学版) 1672-7126 真空科学与技术学报 1004-6801 振动测试与诊断 1004-4523 振动工程学报 1000-3835 振动与冲击 学报网站 0258-8013 中国电机工程学报 1001-7372 中国公路学报 0258-7025 中国激光 学报网站 1000-1964 中国矿业大学学报 1673-5005 中国石油大学学报 (自然科学版) 1001-4632 中国铁道科学 1004-0609 中国有色金属学报 学报网站 1672-7207 中南大学学报（自然科学版） 学报网站 0254-4156 自动化学报 学报网站   中科院计算所推荐国际会议 序号 会议名称 会议介绍 代表领域 1 ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左 右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录 用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。< /p> 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。 GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千 人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会 议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左 右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一 次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一 次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu.  http://hpdc13.cs.ucsb.edu 高性能计算 42  NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/  高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications  高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括 technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing  该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems  由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing  This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下 半年排名。 高性能计算 48 ACM International Conference on Supercomputing  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing  IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 　 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57  FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶 尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59  SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60  IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62  IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等 概念。 自主计算 63  Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64  International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65  [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66  IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67  USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68  IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69  International Symposium on Computer Architecture（ISCA ） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很 难 系统结构 70  International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72  IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73  Annual ACM International Conference on Supercomputing（ICS）  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 74  Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其 困难 操作系统 75  ACM Symposium on Operating Systems Principles （SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要 中极其困难 操作系统 76  Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困 难 操作系统，程序语言 77  Workshop on Hot Topics in Operating Systems （HOTOS） 操作系统最好的会议之一 操作系统 78  Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79  Annual IEEE Conference on Local Computer Networks（LCN） 　 网络 80  International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影 响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收 率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会 议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影 响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收 率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。< /p> 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统 等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影 响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术 121 ECOOP - European Conference on Object-Oriented Programming 关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。 编译技术 122 ESOP - European Symposium on Programming ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。< /p> 编译技术 123 Euro-Par - European Conference on Parallel Computing 关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。 　 124 SAS - International Static Analysis Symposium 关注程序的静态分析的权威会议。 编译技术 125 CAV - Computer Aided Verification Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。 编译技术 126 FASE - Fundamental Approaches to Software Engineering ETAPS的member conference，主要关注Software Science，影响因子0.91。 编译技术 127 TACAS - Tools and Algorithms for the Construction and Analysis of Systems ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24 编译技术 128 VMCAI - Verification, Model Checking and Abstract Interpretation Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。 编译技术 129  ACL: The Association for Computational Linguistics 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。 人工智能 计算语言学 130 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索方面最好的会议, ACM 主办, 每年开。19％左右 信息检索技术 131 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining 数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右 　 132 WWW: The ACM International World Wide Web Conference 应用和媒体领域顶级国际会议 万维网 133 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库领域顶级国际 数据管理 134 CIKM: The ACM Conference on Information and Knowledge Management 数据库领域知名国际会议 数据管理 135 COLING: International Conference on Computational Linguistics 计算语言学知名国际会议 计算语言学 136 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 137 IEEE ICDM: International Conference on Data Mining 数据挖掘领域顶级国际会议 　 138 IJCAI: International Joint Conference on Artificial Intelligence 人工智能领域顶级国际会议，论文接受率18％左右 人工智能 139 VLDB: The ACM International Conference on Very Large Data Bases 数据库领域顶级国际 数据库 140 SIGGRAPH: ACM SIGGRAPH Conference 计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％ 计算机图形学 141 EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics 欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20% 计算机图形学 142 AAAI: American Association for Artificial Intelligence 美国人工智能学会AAAI的年会，使该领域的顶级会议 人工智能 143 ACM Conference　on　Computer and　Communications Security ACM通信和计算健全领域顶级学术会议 信息安全 144 ACM SIGCOMM: Special Interest Group on Data Communications 　 数据通信 145 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索领域的重要会议 信息检索 146 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。 通信与网络 147 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems ACM性能建模与评价领域顶级学术会议 通信与网络 148 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。 数据管理 149 ASPLOS: Architectural Support for Programming Languages and Operating Systems 编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。 体系结构 150 CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid 网格计算国际会议，网格平台、中间件 　 151 CIKM: The ACM Conference on Information and Knowledge Management 信息检索领域的会议，录用率为15% 信息检索 152 CLUSTER - IEEE International Conference on Cluster Computing 集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。 集群计算 153 CPM: Combinatorial Pattern Matching Symposium 组合模式匹配年会，是字符串匹配、模式匹配较好的会议。 模式匹配 154 FAST4: Third USENIX Conference on File and Storage Technologies, USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。 文件与存储 155 Grid : IEEE/ACM International Workshop on Grid Computing 网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。 网格计算 156 HPC: IEEE International Conference for High Performance Computing 　 　 157 HPDC: International Symposium on High Performance Distributed Computing 高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。< /p> 高性能计算 158 ICDCS: IEEE International Conference on Distributed Computing Systems IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS 举办了26届，录用率为15%。 分布式计算 159 ICML: International Conference on Machine Learning 机器学习领域中的顶级会议 机器学习 160 ICWS: IEEE International Conference on Web Services Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。 Web服务 161 IEEE CSB: Computer Society Bioinformatics 　 　 162 IEEE ICDM: International Conference on Data Mining 数据挖掘领域的著名会议，率用率为14%。 数据挖掘 163 IEEE ICNP: International Conference on Network Protocols IEEE 网络通信领域顶级学术会议，录用率在10%左右。 网络 164 IEEE ICON: IEEE International Conference on Networks 　 　 165 IEEE INFOCOM: conference on computer communications IEEE网络通信领域著名会议，领域广泛。 网络 166 IEEE IPCCC: International Performance Computing and Communications Conference IEEE性能领域著名学术会议，主要关注性能评价。 网络性能 167 IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval 字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。 字符串处理信息检索 168 IJCAI: International Joint Conference on AI 人工智能领域的顶级会议。 人工智能 169 IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。 170 International Symposium on High Performance Computer Architecture(HPCA) 　 服务计算 171 IPDPS: IEEE International Parallel and Distributed Processing Symposium IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%， 会议的优秀论文可能发表于JDPC杂志上。 并行计算分布式计算 172 ISMB: International conference on Intelligent Systems for Molecular Biology 　 无线 173 MobiCom: ACM/IEEE Conference on Mobile Computing and Networking 始于1995，无线、移动计算方面比较有历史和重要的会议。 安全 174 MobiSys: The International Conference on Mobile Systems, Applications, and Services 无线方面，2006年第4名。 无线 175 OSDI: USENIX Symposium on Operating Systems Design and Implementation USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。 操作系统 176 PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining 　 　 177 PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies 关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。 并行计算分布式计算 178 PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases 数据挖掘领域的重要会议，录用率为14%。 数据挖掘 179 SCC: IEEE International Conference on Services Computing 服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。< /p> 服务计算 180 SDM: SIAM International Conference on Data Mining 数据挖掘领域的重要会议，录用率为14% 数据挖据 181 SOSE: IEEE International Workshop on Service-Oriented System Engineering 　 　 182 USENIX Sec: USENIX Security Symposium USENIX安全领域重要会议，侧重安全技术。 安全 183 USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI) USENIX网络领域重要会议，设计网络涉及各方面内容。 网络 184 VLDB: The ACM International Conference on Very Large Data Bases 　 数据管理 185 WWW: The ACM International World Wide Web Conference ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。 Internet 186 RAID International Symposium on Recent Advances in Intrusion Detection 数据库顶级国际会议 　 187 IJCAI: International Joint Conference on Artificial Intelligence 人工智能顶级国际会议 人工智能 188 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据库 189 ICML: International Conference on Machine Learning 机器学习顶级国际会议 机器学习 190 PRICAI: Pacific Rim International Conference on Artificial Intelligence 亚太人工智能国际会议 人工智能 191 IFIP ICIIP: IFIP International Conference on Intelligent Information Processing IFIP智能信息处理国际会议 智能信息处理 192 NIPS: Neural Information Processing Systems 神经信息处理领域顶级国际会议 神经计算，机器学习 193 ISCA: International Symposium on Computer Architecture   体系结构领域的顶级会议 微处理器设计 194 International Symposium on Microarchitecture 体系结构领域的顶级会议 微处理器设计 195 HPCA：International Symposium on High Performance Computer Architecture 体系结构领域的顶级会议 微处理器设计 196 APCSAC: Asia-Pacific Computer Systems Architecture Conference  体系结构方面的重要会议 微处理器设计 197 ISLPED: International Symposium on Low Power Electronics and Design 低功耗设计的重要会议 微处理器设计 198 OSDI: Operation System Design & Implementation  操作系统方面的重要会议 微处理器设计 199 ASPLOS: Architecture Support for Programming Languages and Operation  体系结构方面的顶尖会议 微处理器设计 200 ICCD: IEEE International Conference on Computer Design 体系结构方面的顶尖会议 微处理器设计 201 DAC: Design Automation Conference  设计自动化领域的顶级会议 微处理器设计 202 IEEE/ACM International Conference on Computer Aided Design(ICCAD) 集成电路设计自动化方面的顶尖会议 微处理器设计 203 ASP-DAC: Asia and South Pacific Design Automation Conference  设计自动化领域的重要会议 微处理器设计 204 ISSCC: IEEE International Solid-State Circuits Conference 设计自动化领域的重要会议 微处理器设计 205 CICC: Custom Integrated Circuits Conference 集成电路设计方面的顶尖会议（公认排名第二） 微处理器设计 206 ESSCIRC: European Conference on Solid-State Circuits 集成电路设计方面的顶尖会议 微处理器设计 207 Symposium on VLSI Circuits 集成电路设计方面的顶尖会议 微处理器设计 208 IEEE International ASIC/SOC Conference 集成电路设计方面的重要会议 微处理器设计 209 Symposium on VLSI Technology 集成电路设计方面的重要会议 微处理器设计 210 ASSCC: Asian Conference on Solid-State Circuits 集成电路领域重要会议 微处理器设计 211 MWSCAS: Midwest Symposium on Circuits and Systems 集成电路领域重要会议 微处理器设计 212 ICECS: IEEE International Conference on Electronics, Circuits and Systems： 集成电路领域重要会议 微处理器设计 213 ISCAS: International Symposium Circuit and System  电路与系统方面的重要会议 微处理器设计 214 RFIC: IEEE Symposium on Radio Frequency Integrated Circuits  射频集成电路领域顶尖会议 微处理器设计 215 ACM RECOMB: Int. Conference on Research in Computational Molecular Biology RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在 20%左右。 计算生物学 216 IEEE CSB: Computer Society Bioinformatics CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办。 计算生物学 217 International Conference of the IEEE Engineering in Medicine and Biology Society （EMBC） 生物信息学与计算生物学国际研讨会 计算生物学 218 PSB: Pacific Symposium on Biocomputing  PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举 办。 计算生物学 219 WABI:Workshop on Algorithms in Bioinformatics WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方 面。 计算生物学 220 CSB: IEEE Computational Systems Bioinformatics Conference  CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办 计算生物学 221 ISMB：Annual International Conference on Intelligent Systems for Molecular Biology ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为 Bioinformatics杂志的专刊发表，近年来录取率为15%左右。 生物信息 222 ECCB：European Conference on Computational Biology ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为 Bioinformatics杂志的专刊发表，近年录取率在20%左右。 生物信息 223 APBC：Asia Pacific Bioinformatics Conference APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。< /p> 生物信息 224 COCOON：Annual International Computing and Combinatorics Conference COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左 右。 生物信息 225 CPM：Annual Symposium on Combinatorial Pattern Matching CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左 右。 生物信息 226 EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千 计。 生物信息 227 Geospatial Information and Technology Association（GITA） Annual Conference 地球空间信息与技术协会年会 遥感与空间信息处理 228 International Geoscience and Remote Sensing Symposium （IGRSS） 地球科学与遥感国际会议 遥感与空间信息处理 229 International Society for Photogrammetry and Remote Sensing （ISPRS）Technical Commission Symposium 国际摄影测量与遥感学会专业委员会会议 遥感与空间信息处理 230 International Conference on Geoinformatics 地球信息国际会议 遥感与空间信息处理 231 IEEE SKG (Semantics, Knowledge and Grid) 由计算所发起的IEEE国际会议，每年有100人参加。 知识网格 232 WWW: The ACM International World Wide Web Conference Internet领域顶级国际会议 Internet 233 International Semantic Web Conference Semantic Web领域顶级会议，录用率17% Semantic Web 234 ACM SIGMOD: ACM SIGMOD Conf on Management of Data ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。 数据管理 235 ACM PODS Conference ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。 数据管理 236 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据管理 237 IEEE ICDE - International Conference on Data Engineering 数据库顶级国际会议","title":"CS的顶级会议和期刊"},{"content":"SCI或SCIE收录的本学科刊物清单请登陆 http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=K 和http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=D  下面列一些CS的顶级会议和期刊    有些是网上查到的，有些是某些人用SCI的IF排序做出来的： Computer Vision Conf.:   Best:     ICCV, Inter. Conf. on Computer Vision     CVPR, Inter. Conf. on Computer Vision and Pattern Recognition   Good:     ECCV, Euro. Conf. on Comp. Vision     ICIP, Inter. Conf. on Image Processing     ICPR, Inter. Conf. on Pattern Recognition     ACCV, Asia Conf. on Comp. Vision Computer Vision  Jour.:   Best:     PAMI, IEEE Trans. on Patt. Analysis and Machine Intelligence     IJCV, Inter. Jour. on Comp. Vision   Good:     CVIU, Computer Vision and Image Understanding PR, Pattern Reco. Network Conf.:     ACM/SigCOMM     ACM Special Interest Group of Communication     ACM/SigMetric Info Com Globe Com Network Jour.:     ToN (ACM/IEEE Transaction on Network) A.I.Conf.:     AAAI: American Association for Artificial Intelligence     ACM/SigIR IJCAI: International Joint Conference on Artificial Intelligence     NIPS: Neural Information Processing Systems     ICML: International Conference on Machine Learning A.I.Jour.:     Machine Learning     NEURAL COMPUTATION     ARTIFICIAL INTELLIGENCE PAMI     IEEE TRANSACTIONS ON FUZZY SYSTEMS     IEEE TRANSACTIONS ON NEURAL NETWORKS AI MAGAZINE     NEURAL NETWORKS     PATTERN RECOGNITION     IMAGE AND VISION COMPUTING     IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING     APPLIED INTELLIGENCE OS,System Conf.:     SOSP: The ACM Symposium on Operating Systems Principles     OSDI: USENIX Symposium on Operating Systems Design and Implementation Database Conf.:     ACM SIGMOD     VLDB:International Conference on Very Large Data Bases     ICDE:International Conference on Data Engineering Security Conf.:     IEEE Security and Privacy     CCS: ACM Computer and Communications Security NDSS (Network and Distributed Systems Security) Web Conf.:     WWW(International World Wide Web Conference) Theory Conf.:     STOC FOCS  EDA  Conf.:  Best:      DAC: IEEE/ACM Design Automation Conference     ICCAD: IEEE International Conference on Computer Aided Design Good:     ISCAS: IEEE International Symposium on Circuits And Systems     ISPD: IEEE International Symposium on Physical Design     ICCD: IEEE International Conference on Computer Design     ASP-DAC: European Design Automation Conference     E-DAC: Asia and South Pacific Design Automation Conference Graphics  Conf.:   Best:     Siggraph: ACM SigGraph   Good:     Euro Graph Jour.: IEEE(ACM) Trans. on Graphics     IEEE Trans. on Visualization and Computer Graphics CAD    Jour.: CAD CAGD Softe Engineering: conf.:      ICSE The International Conference on Software Engineering     FSE The Foundations of Software Engineering Conferences     ICASE IEEE International Conference on Automated Software Engineering     COMPSAC International Computer Software and Applications Conferences     ESEC The European Software Engineering Conferences Jour.:     SEN ACM SIGSOFT Software Engineering Notes     TSE IEEE Transactions on Software Engineering     ASE Automated Software Engineering SPE Software-Practice and Experience   EI收录的中国期刊：     来自http://www.ei.org.cn/twice/coverage.jsp ISSN 期 刊 名 相关链接 0567-7718 Acta Mechanica Sinica 1006-7191 Acta Metallurgica Sinica (English Letters) 0253-4827 Applied Mathematics and Mechanics (English Edition) 0890-5487 China Ocean Engineering 1004-5341 China Welding 1004-9541 Chinese Journal of Chemical Engineering 1022-4653 Chinese Journal of Electronics 1000-9345 Chinese Journal of Mechanical Engineering (English Edition) 学报网站 1671-7694 Chinese Optics Letters 学报网站 1673-7350 Frontiers of Computer Science in China 期刊网址 1006-6748 High Technology Letters 1674-4799 International Journal of Minerals, Metallurgy and Materials 1004-0579 Journal of Beijing Institute of Technology (English Edition) 学报编辑部 1005-9784 Journal of Central South University of Technology 学报网站 1672-5220 Journal of Donghua University (English Edition) 1005-9113 Journal of Harbin Institute of Technology (New Series) 1001-6058 Journal of Hydrodynamics 1005-0302 Journal of Materials Science and Technology 1002-0721 Journal of Rare Earths 1674-4926 Journal of Semiconductors 学报编辑部 1007-1172 Journal of Shanghai Jiaotong University (Science) 1003-7985 Journal of Southeast University (English Edition) 1004-4132 Journal of Systems Engineering and Electronics 1009-6124 Journal of Systems Science and Complexity 1003-2169 Journal of Thermal Science 1000-2413 Journal of Wuhan University of Technology -Materials Science Edition 1673-565X Journal of Zhejiang University SCIENCE A 1674-5264 Mining Science and Technology 1001-0521 Rare Metals 1006-9291 Science in China, Series B: Chemistry 1672-1799 Science in China, Series G: Physics, Astronomy 1005-8885 The Journal of China Universities of Posts and Telecommunications 1005-1120 Transactions of Nanjing University of Aeronautics and Astronautics 1003-6326 Transactions of Nonferrous Metals Society of China 1006-4982 Transactions of Tianjin University 1007-0214 Tsinghua Science and Technology Editor Information 1001-1455 爆炸与冲击 0254-0037 北京工业大学学报 1001-5965 北京航空航天大学学报 学报编辑部 1001-053X 北京科技大学学报 学报编辑部 1001-0645 北京理工大学学报 学报编辑部 1007-5321 北京邮电大学学报 学报编辑部 1000-1093 兵工学报 1001-4381 材料工程 1005-0299 材料科学与工艺 1009-6264 材料热处理学报 学报网站 1005-3093 材料研究学报 1001-1595 测绘学报 学报编辑部 1007-7294 船舶力学 1000-8608 大连理工大学学报 1004-499X 弹道学报 1000-2383 地球科学 学报网站 1005-0388 电波科学学报 1000-6753 电工技术学报 1007-449X 电机与控制学报 1000-1026 电力系统自动化 学报网站 1006-6047 电力自动化设备 1001-0548 电子科技大学学报 0372-2112 电子学报 1009-5896 电子与信息学报 1005-3026 东北大学学报　(自然科学版) 1001-0505 东南大学学报 (自然科学版) 1000-3851 复合材料学报 1003-6520 高电压技术 1000-7555 高分子材料科学与工程 1002-0470 高技术通讯 1003-9015 高校化学工程学报 1000-5773 高压物理学报 1000-4750 工程力学 0253-231X 工程热物理学报 1001-9731 功能材料 学报网站 1006-2793 固体火箭技术 0254-7805 固体力学学报 1005-0086 光电子.激光 1000-0593 光谱学与光谱分析 1004-924X 光学精密工程 学报网站 0253-2239 光学学报 学报网站 0454-5648 硅酸盐学报 1001-2486 国防科技大学学报 1006-7043 哈尔滨工程大学学报 学报网站 0367-6234 哈尔滨工业大学学报 0253-360X 焊接学报 1005-5053 航空材料学报 1000-8055 航空动力学报 编辑部网站 1000-6893 航空学报 学报网站 0258-0926 核动力工程 1001-9014 红外与毫米波学报 1000-2472 湖南大学学报 (自然科学版) 1000-565X 华南理工大学学报(自然科学版) 编辑部网站 1671-4512 华中科技大学学报(自然科学版) 0438-1157 化工学报 1002-0446 机器人 学报网站 0577-6686 机械工程学报 学报网站 1671-5497 吉林大学学报(工学版) 学报编辑部 1003-9775 计算机辅助设计与图形学学报 1006-5911 计算机集成制造系统 编辑部网站 0254-4164 计算机学报 1000-1239 计算机研究与发展 学报网站 1007-4708 计算力学学报 1001-246X 计算物理 1007-9629 建筑材料学报 1000-6869 建筑结构学报 1671-7775 江苏大学学报（自然科学版） 1009-3443 解放军理工大学学报（自然科学版） 0412-1961 金属学报 0258-1825 空气动力学学报 1000-8152 控制理论与应用 学报网站 1001-0920 控制与决策 0459-1879 力学学报 学报网站 0253-9993 煤炭学报 学报网站 1003-6059 模式识别与人工智能 1004-0595 摩擦学学报 1672-6030 纳米技术与精密工程 1005-2615 南京航空航天大学学报 1005-9830 南京理工大学学报 (自然科学版) 1000-0925 内燃机工程 1000-0909 内燃机学报 1002-6819 农业工程学报 学报编辑部 1000-1298 农业机械学报 学报编辑部 1001-4322 强激光与粒子束 学报编辑部 1000-0054 清华大学学报 (自然科学版) 0253-2409 燃料化学学报 1006-8740 燃烧科学与技术 1000-985X 人工晶体学报 无机材料期刊网 1000-9825 软件学报 学报编辑部 1006-2467 上海交通大学学报 1000-2618 深圳大学学报（理工版） 0371-0025 声学学报 1000-7210 石油地球物理勘探 1000-0747 石油勘探与开发 0253-2697 石油学报 1001-8719 石油学报:石油加工 学报网站 1672-9897 实验流体力学 学报网站 1001-6791 水科学进展 0559-9350 水利学报 学报编辑部 1003-1243 水力发电学报 1009-3087 四川大学学报(工程科学版) 学报编辑部 0254-0096 太阳能学报 学报编辑部 0493-2137 天津大学学报 学报编辑部 1001-8360 铁道学报 1000-436X 通信学报 0253-374X 同济大学学报 (自然科学版) 1000-131X 土木工程学报 学报网站 1674-4764 土木建筑与环境工程 学报编辑部 1001-4055 推进技术 1000-324X 无机材料学报 1671-8860 武汉大学学报(信息科学版) 1001-2400 西安电子科技大学学报 学报网站 0253-987X 西安交通大学学报 1000-2758 西北工业大学学报 0258-2724 西南交通大学学报 学报网站 1002-185X 稀有金属材料与工程 1000-6788 系统工程理论与实践 1001-506X 系统工程与电子技术 1007-8827 新型炭材料 1000-6915 岩石力学与工程学报 学报网站 1000-4548 岩土工程学报 1000-7598 岩土力学 期刊编辑部 0254-3087 仪器仪表学报 1005-0930 应用基础与工程科学学报 学报网站 1000-6931 原子能科学技术 1008-973X 浙江大学学报 (工学版) 1672-7126 真空科学与技术学报 1004-6801 振动测试与诊断 1004-4523 振动工程学报 1000-3835 振动与冲击 学报网站 0258-8013 中国电机工程学报 1001-7372 中国公路学报 0258-7025 中国激光 学报网站 1000-1964 中国矿业大学学报 1673-5005 中国石油大学学报 (自然科学版) 1001-4632 中国铁道科学 1004-0609 中国有色金属学报 学报网站 1672-7207 中南大学学报（自然科学版） 学报网站 0254-4156 自动化学报 学报网站   中科院计算所推荐国际会议 序号 会议名称 会议介绍 代表领域 1 ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左 右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录 用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。< /p> 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。 GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千 人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会 议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左 右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一 次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一 次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu.  http://hpdc13.cs.ucsb.edu 高性能计算 42  NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/  高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications  高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括 technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing  该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems  由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing  This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下 半年排名。 高性能计算 48 ACM International Conference on Supercomputing  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing  IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 　 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57  FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶 尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59  SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60  IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62  IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等 概念。 自主计算 63  Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64  International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65  [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66  IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67  USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68  IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69  International Symposium on Computer Architecture（ISCA ） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很 难 系统结构 70  International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72  IEEE International Conference on High Performance Computing  IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73  Annual ACM International Conference on Supercomputing（ICS）  高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 74  Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其 困难 操作系统 75  ACM Symposium on Operating Systems Principles （SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要 中极其困难 操作系统 76  Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困 难 操作系统，程序语言 77  Workshop on Hot Topics in Operating Systems （HOTOS） 操作系统最好的会议之一 操作系统 78  Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79  Annual IEEE Conference on Local Computer Networks（LCN） 　 网络 80  International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影 响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收 率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会 议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影 响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收 率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。< /p> 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统 等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影 响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术 121 ECOOP - European Conference on Object-Oriented Programming 关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。 编译技术 122 ESOP - European Symposium on Programming ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。< /p> 编译技术 123 Euro-Par - European Conference on Parallel Computing 关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。 　 124 SAS - International Static Analysis Symposium 关注程序的静态分析的权威会议。 编译技术 125 CAV - Computer Aided Verification Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。 编译技术 126 FASE - Fundamental Approaches to Software Engineering ETAPS的member conference，主要关注Software Science，影响因子0.91。 编译技术 127 TACAS - Tools and Algorithms for the Construction and Analysis of Systems ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24 编译技术 128 VMCAI - Verification, Model Checking and Abstract Interpretation Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。 编译技术 129  ACL: The Association for Computational Linguistics 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。 人工智能 计算语言学 130 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索方面最好的会议, ACM 主办, 每年开。19％左右 信息检索技术 131 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining 数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右 　 132 WWW: The ACM International World Wide Web Conference 应用和媒体领域顶级国际会议 万维网 133 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库领域顶级国际 数据管理 134 CIKM: The ACM Conference on Information and Knowledge Management 数据库领域知名国际会议 数据管理 135 COLING: International Conference on Computational Linguistics 计算语言学知名国际会议 计算语言学 136 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 137 IEEE ICDM: International Conference on Data Mining 数据挖掘领域顶级国际会议 　 138 IJCAI: International Joint Conference on Artificial Intelligence 人工智能领域顶级国际会议，论文接受率18％左右 人工智能 139 VLDB: The ACM International Conference on Very Large Data Bases 数据库领域顶级国际 数据库 140 SIGGRAPH: ACM SIGGRAPH Conference 计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％ 计算机图形学 141 EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics 欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20% 计算机图形学 142 AAAI: American Association for Artificial Intelligence 美国人工智能学会AAAI的年会，使该领域的顶级会议 人工智能 143 ACM Conference　on　Computer and　Communications Security ACM通信和计算健全领域顶级学术会议 信息安全 144 ACM SIGCOMM: Special Interest Group on Data Communications 　 数据通信 145 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索领域的重要会议 信息检索 146 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。 通信与网络 147 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems ACM性能建模与评价领域顶级学术会议 通信与网络 148 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。 数据管理 149 ASPLOS: Architectural Support for Programming Languages and Operating Systems 编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。 体系结构 150 CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid 网格计算国际会议，网格平台、中间件 　 151 CIKM: The ACM Conference on Information and Knowledge Management 信息检索领域的会议，录用率为15% 信息检索 152 CLUSTER - IEEE International Conference on Cluster Computing 集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。 集群计算 153 CPM: Combinatorial Pattern Matching Symposium 组合模式匹配年会，是字符串匹配、模式匹配较好的会议。 模式匹配 154 FAST4: Third USENIX Conference on File and Storage Technologies, USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。 文件与存储 155 Grid : IEEE/ACM International Workshop on Grid Computing 网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。 网格计算 156 HPC: IEEE International Conference for High Performance Computing 　 　 157 HPDC: International Symposium on High Performance Distributed Computing 高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。< /p> 高性能计算 158 ICDCS: IEEE International Conference on Distributed Computing Systems IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS 举办了26届，录用率为15%。 分布式计算 159 ICML: International Conference on Machine Learning 机器学习领域中的顶级会议 机器学习 160 ICWS: IEEE International Conference on Web Services Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。 Web服务 161 IEEE CSB: Computer Society Bioinformatics 　 　 162 IEEE ICDM: International Conference on Data Mining 数据挖掘领域的著名会议，率用率为14%。 数据挖掘 163 IEEE ICNP: International Conference on Network Protocols IEEE 网络通信领域顶级学术会议，录用率在10%左右。 网络 164 IEEE ICON: IEEE International Conference on Networks 　 　 165 IEEE INFOCOM: conference on computer communications IEEE网络通信领域著名会议，领域广泛。 网络 166 IEEE IPCCC: International Performance Computing and Communications Conference IEEE性能领域著名学术会议，主要关注性能评价。 网络性能 167 IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval 字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。 字符串处理信息检索 168 IJCAI: International Joint Conference on AI 人工智能领域的顶级会议。 人工智能 169 IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。 170 International Symposium on High Performance Computer Architecture(HPCA) 　 服务计算 171 IPDPS: IEEE International Parallel and Distributed Processing Symposium IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%， 会议的优秀论文可能发表于JDPC杂志上。 并行计算分布式计算 172 ISMB: International conference on Intelligent Systems for Molecular Biology 　 无线 173 MobiCom: ACM/IEEE Conference on Mobile Computing and Networking 始于1995，无线、移动计算方面比较有历史和重要的会议。 安全 174 MobiSys: The International Conference on Mobile Systems, Applications, and Services 无线方面，2006年第4名。 无线 175 OSDI: USENIX Symposium on Operating Systems Design and Implementation USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。 操作系统 176 PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining 　 　 177 PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies 关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。 并行计算分布式计算 178 PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases 数据挖掘领域的重要会议，录用率为14%。 数据挖掘 179 SCC: IEEE International Conference on Services Computing 服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。< /p> 服务计算 180 SDM: SIAM International Conference on Data Mining 数据挖掘领域的重要会议，录用率为14% 数据挖据 181 SOSE: IEEE International Workshop on Service-Oriented System Engineering 　 　 182 USENIX Sec: USENIX Security Symposium USENIX安全领域重要会议，侧重安全技术。 安全 183 USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI) USENIX网络领域重要会议，设计网络涉及各方面内容。 网络 184 VLDB: The ACM International Conference on Very Large Data Bases 　 数据管理 185 WWW: The ACM International World Wide Web Conference ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。 Internet 186 RAID International Symposium on Recent Advances in Intrusion Detection 数据库顶级国际会议 　 187 IJCAI: International Joint Conference on Artificial Intelligence 人工智能顶级国际会议 人工智能 188 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据库 189 ICML: International Conference on Machine Learning 机器学习顶级国际会议 机器学习 190 PRICAI: Pacific Rim International Conference on Artificial Intelligence 亚太人工智能国际会议 人工智能 191 IFIP ICIIP: IFIP International Conference on Intelligent Information Processing IFIP智能信息处理国际会议 智能信息处理 192 NIPS: Neural Information Processing Systems 神经信息处理领域顶级国际会议 神经计算，机器学习 193 ISCA: International Symposium on Computer Architecture   体系结构领域的顶级会议 微处理器设计 194 International Symposium on Microarchitecture 体系结构领域的顶级会议 微处理器设计 195 HPCA：International Symposium on High Performance Computer Architecture 体系结构领域的顶级会议 微处理器设计 196 APCSAC: Asia-Pacific Computer Systems Architecture Conference  体系结构方面的重要会议 微处理器设计 197 ISLPED: International Symposium on Low Power Electronics and Design 低功耗设计的重要会议 微处理器设计 198 OSDI: Operation System Design & Implementation  操作系统方面的重要会议 微处理器设计 199 ASPLOS: Architecture Support for Programming Languages and Operation  体系结构方面的顶尖会议 微处理器设计 200 ICCD: IEEE International Conference on Computer Design 体系结构方面的顶尖会议 微处理器设计 201 DAC: Design Automation Conference  设计自动化领域的顶级会议 微处理器设计 202 IEEE/ACM International Conference on Computer Aided Design(ICCAD) 集成电路设计自动化方面的顶尖会议 微处理器设计 203 ASP-DAC: Asia and South Pacific Design Automation Conference  设计自动化领域的重要会议 微处理器设计 204 ISSCC: IEEE International Solid-State Circuits Conference 设计自动化领域的重要会议 微处理器设计 205 CICC: Custom Integrated Circuits Conference 集成电路设计方面的顶尖会议（公认排名第二） 微处理器设计 206 ESSCIRC: European Conference on Solid-State Circuits 集成电路设计方面的顶尖会议 微处理器设计 207 Symposium on VLSI Circuits 集成电路设计方面的顶尖会议 微处理器设计 208 IEEE International ASIC/SOC Conference 集成电路设计方面的重要会议 微处理器设计 209 Symposium on VLSI Technology 集成电路设计方面的重要会议 微处理器设计 210 ASSCC: Asian Conference on Solid-State Circuits 集成电路领域重要会议 微处理器设计 211 MWSCAS: Midwest Symposium on Circuits and Systems 集成电路领域重要会议 微处理器设计 212 ICECS: IEEE International Conference on Electronics, Circuits and Systems： 集成电路领域重要会议 微处理器设计 213 ISCAS: International Symposium Circuit and System  电路与系统方面的重要会议 微处理器设计 214 RFIC: IEEE Symposium on Radio Frequency Integrated Circuits  射频集成电路领域顶尖会议 微处理器设计 215 ACM RECOMB: Int. Conference on Research in Computational Molecular Biology RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在 20%左右。 计算生物学 216 IEEE CSB: Computer Society Bioinformatics CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办。 计算生物学 217 International Conference of the IEEE Engineering in Medicine and Biology Society （EMBC） 生物信息学与计算生物学国际研讨会 计算生物学 218 PSB: Pacific Symposium on Biocomputing  PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举 办。 计算生物学 219 WABI:Workshop on Algorithms in Bioinformatics WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方 面。 计算生物学 220 CSB: IEEE Computational Systems Bioinformatics Conference  CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办 计算生物学 221 ISMB：Annual International Conference on Intelligent Systems for Molecular Biology ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为 Bioinformatics杂志的专刊发表，近年来录取率为15%左右。 生物信息 222 ECCB：European Conference on Computational Biology ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为 Bioinformatics杂志的专刊发表，近年录取率在20%左右。 生物信息 223 APBC：Asia Pacific Bioinformatics Conference APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。< /p> 生物信息 224 COCOON：Annual International Computing and Combinatorics Conference COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左 右。 生物信息 225 CPM：Annual Symposium on Combinatorial Pattern Matching CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左 右。 生物信息 226 EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千 计。 生物信息 227 Geospatial Information and Technology Association（GITA） Annual Conference 地球空间信息与技术协会年会 遥感与空间信息处理 228 International Geoscience and Remote Sensing Symposium （IGRSS） 地球科学与遥感国际会议 遥感与空间信息处理 229 International Society for Photogrammetry and Remote Sensing （ISPRS）Technical Commission Symposium 国际摄影测量与遥感学会专业委员会会议 遥感与空间信息处理 230 International Conference on Geoinformatics 地球信息国际会议 遥感与空间信息处理        231 IEEE SKG (Semantics, Knowledge and Grid) 由计算所发起的IEEE国际会议，每年有100人参加。 知识网格        232 WWW: The ACM International World Wide Web Conference Internet领域顶级国际会议 Internet        233 International Semantic Web Conference Semantic Web领域顶级会议，录用率17% Semantic Web        234 ACM SIGMOD: ACM SIGMOD Conf on Management of Data ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。 数据管理        235 ACM PODS Conference ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。 数据管理        236 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据管理        237 IEEE ICDE - International Conference on Data Engineering 数据库顶级国际会议 数据管理","title":"CS的顶级会议和期刊"},{"content":"转载请注明出处：http://blog.csdn.net/wangxiaolong_china 序号 会议名称 会议介绍 代表领域 1 ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu. http://hpdc13.cs.ucsb.edu 高性能计算 42 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/ 高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications 高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing 该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下半年排名。 高性能计算 48 ACM International Conference on Supercomputing 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 　 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57 FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59 SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60 IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62 IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等概念。 自主计算 63 Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64 International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66 IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67 USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68 IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69 International Symposium on Computer Architecture（ISCA） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 70 International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72 IEEE International Conference on High Performance Computing IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73 Annual ACM International Conference on Supercomputing（ICS） 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。 高性能计算 74 Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难 操作系统 75 ACM Symposium on Operating Systems Principles（SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难 操作系统 76 Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems（ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困难 操作系统，程序语言 77 Workshop on Hot Topics in Operating Systems（HOTOS） 操作系统最好的会议之一 操作系统 78 Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79 Annual IEEE Conference on Local Computer Networks（LCN） 　 网络 80 International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools,新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术 121 ECOOP - European Conference on Object-Oriented Programming 关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。 编译技术 122 ESOP - European Symposium on Programming ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。 编译技术 123 Euro-Par - European Conference on Parallel Computing 关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。 　 124 SAS - International Static Analysis Symposium 关注程序的静态分析的权威会议。 编译技术 125 CAV - Computer Aided Verification Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。 编译技术 126 FASE - Fundamental Approaches to Software Engineering ETAPS的member conference，主要关注Software Science，影响因子0.91。 编译技术 127 TACAS - Tools and Algorithms for the Construction and Analysis of Systems ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24 编译技术 128 VMCAI - Verification, Model Checking and Abstract Interpretation Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。 编译技术 129 ACL: The Association for Computational Linguistics 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics)主办,每年开。 人工智能 计算语言学 130 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索方面最好的会议, ACM主办, 每年开。19％左右 信息检索技术 131 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining 数据挖掘方面最好的会议, ACM主办, 每年开。18％左右 　数据挖掘 132 WWW: The ACM International World Wide Web Conference 应用和媒体领域顶级国际会议 万维网 133 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库领域顶级国际 数据管理 134 CIKM: The ACM Conference on Information and Knowledge Management 数据库领域知名国际会议 数据管理 135 COLING: International Conference on Computational Linguistics 计算语言学知名国际会议 计算语言学 136 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 137 IEEE ICDM: International Conference on Data Mining 数据挖掘领域顶级国际会议 　数据挖掘 138 IJCAI: International Joint Conference on Artificial Intelligence 人工智能领域顶级国际会议，论文接受率18％左右 人工智能 139 VLDB: The ACM International Conference on Very Large Data Bases 数据库领域顶级国际 数据库 140 SIGGRAPH: ACM SIGGRAPH Conference 计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％ 计算机图形学 141 EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics 欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20% 计算机图形学 142 AAAI: American Association for Artificial Intelligence 美国人工智能学会AAAI的年会，使该领域的顶级会议 人工智能 143 ACM Conference　on　Computer and　Communications Security ACM通信和计算健全领域顶级学术会议 信息安全 144 ACM SIGCOMM: Special Interest Group on Data Communications 　 数据通信 145 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索领域的重要会议 信息检索 146 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。 通信与网络 147 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems ACM性能建模与评价领域顶级学术会议 通信与网络 148 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。 数据管理 149 ASPLOS: Architectural Support for Programming Languages and Operating Systems 编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。 体系结构 150 CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid 网格计算国际会议，网格平台、中间件 　 151 CIKM: The ACM Conference on Information and Knowledge Management 信息检索领域的会议，录用率为15% 信息检索 152 CLUSTER - IEEE International Conference on Cluster Computing 集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。 集群计算 153 CPM: Combinatorial Pattern Matching Symposium 组合模式匹配年会，是字符串匹配、模式匹配较好的会议。 模式匹配 154 FAST4: Third USENIX Conference on File and Storage Technologies, USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。 文件与存储 155 Grid : IEEE/ACM International Workshop on Grid Computing 网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。 网格计算 156 HPC: IEEE International Conference for High Performance Computing 　 　 157 HPDC: International Symposium on High Performance Distributed Computing 高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。 高性能计算 158 ICDCS: IEEE International Conference on Distributed Computing Systems IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS举办了26届，录用率为15%。 分布式计算 159 ICML: International Conference on Machine Learning 机器学习领域中的顶级会议 机器学习 160 ICWS: IEEE International Conference on Web Services Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。 Web服务 161 IEEE CSB: Computer Society Bioinformatics 　 　 162 IEEE ICDM: International Conference on Data Mining 数据挖掘领域的著名会议，率用率为14%。 数据挖掘 163 IEEE ICNP: International Conference on Network Protocols IEEE 网络通信领域顶级学术会议，录用率在10%左右。 网络 164 IEEE ICON: IEEE International Conference on Networks 　 　 165 IEEE INFOCOM: conference on computer communications IEEE网络通信领域著名会议，领域广泛。 网络 166 IEEE IPCCC: International Performance Computing and Communications Conference IEEE性能领域著名学术会议，主要关注性能评价。 网络性能 167 IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval 字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。 字符串处理信息检索 168 IJCAI: International Joint Conference on AI 人工智能领域的顶级会议。 人工智能 169 IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。 170 International Symposium on High Performance Computer Architecture(HPCA) 　 服务计算 171 IPDPS: IEEE International Parallel and Distributed Processing Symposium IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%，会议的优秀论文可能发表于JDPC杂志上。 并行计算分布式计算 172 ISMB: International conference on Intelligent Systems for Molecular Biology 　 无线 173 MobiCom: ACM/IEEE Conference on Mobile Computing and Networking 始于1995，无线、移动计算方面比较有历史和重要的会议。 安全 174 MobiSys: The International Conference on Mobile Systems, Applications, and Services 无线方面，2006年第4名。 无线 175 OSDI: USENIX Symposium on Operating Systems Design and Implementation USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。 操作系统 176 PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining 　 　 177 PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies 关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。 并行计算分布式计算 178 PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases 数据挖掘领域的重要会议，录用率为14%。 数据挖掘 179 SCC: IEEE International Conference on Services Computing 服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。 服务计算 180 SDM: SIAM International Conference on Data Mining 数据挖掘领域的重要会议，录用率为14% 数据挖据 181 SOSE: IEEE International Workshop on Service-Oriented System Engineering 　 　 182 USENIX Sec: USENIX Security Symposium USENIX安全领域重要会议，侧重安全技术。 安全 183 USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI) USENIX网络领域重要会议，设计网络涉及各方面内容。 网络 184 VLDB: The ACM International Conference on Very Large Data Bases 　 数据管理 185 WWW: The ACM International World Wide Web Conference ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。 Internet 186 RAID International Symposium on Recent Advances in Intrusion Detection 数据库顶级国际会议 　 187 IJCAI: International Joint Conference on Artificial Intelligence 人工智能顶级国际会议 人工智能 188 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据库 189 ICML: International Conference on Machine Learning 机器学习顶级国际会议 机器学习 190 PRICAI: Pacific Rim International Conference on Artificial Intelligence 亚太人工智能国际会议 人工智能 191 IFIP ICIIP: IFIP International Conference on Intelligent Information Processing IFIP智能信息处理国际会议 智能信息处理 192 NIPS: Neural Information Processing Systems 神经信息处理领域顶级国际会议 神经计算，机器学习 193 ISCA: International Symposium on Computer Architecture 体系结构领域的顶级会议 微处理器设计 194 International Symposium on Microarchitecture 体系结构领域的顶级会议 微处理器设计 195 HPCA：International Symposium on High Performance Computer Architecture 体系结构领域的顶级会议 微处理器设计 196 APCSAC: Asia-Pacific Computer Systems Architecture Conference 体系结构方面的重要会议 微处理器设计 197 ISLPED: International Symposium on Low Power Electronics and Design 低功耗设计的重要会议 微处理器设计 198 OSDI: Operation System Design & Implementation 操作系统方面的重要会议 微处理器设计 199 ASPLOS: Architecture Support for Programming Languages and Operation 体系结构方面的顶尖会议 微处理器设计 200 ICCD: IEEE International Conference on Computer Design 体系结构方面的顶尖会议 微处理器设计 201 DAC: Design Automation Conference 设计自动化领域的顶级会议 微处理器设计 202 IEEE/ACM International Conference on Computer Aided Design(ICCAD) 集成电路设计自动化方面的顶尖会议 微处理器设计 203 ASP-DAC: Asia and South Pacific Design Automation Conference 设计自动化领域的重要会议 微处理器设计 204 ISSCC: IEEE International Solid-State Circuits Conference 设计自动化领域的重要会议 微处理器设计 205 CICC: Custom Integrated Circuits Conference 集成电路设计方面的顶尖会议（公认排名第二） 微处理器设计 206 ESSCIRC: European Conference on Solid-State Circuits 集成电路设计方面的顶尖会议 微处理器设计 207 Symposium on VLSI Circuits 集成电路设计方面的顶尖会议 微处理器设计 208 IEEE International ASIC/SOC Conference 集成电路设计方面的重要会议 微处理器设计 209 Symposium on VLSI Technology 集成电路设计方面的重要会议 微处理器设计 210 ASSCC: Asian Conference on Solid-State Circuits 集成电路领域重要会议 微处理器设计 211 MWSCAS: Midwest Symposium on Circuits and Systems 集成电路领域重要会议 微处理器设计 212 ICECS: IEEE International Conference on Electronics, Circuits and Systems： 集成电路领域重要会议 微处理器设计 213 ISCAS: International Symposium Circuit and System 电路与系统方面的重要会议 微处理器设计 214 RFIC: IEEE Symposium on Radio Frequency Integrated Circuits 射频集成电路领域顶尖会议 微处理器设计 215 ACM RECOMB: Int. Conference on Research in Computational Molecular Biology RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在20%左右。 计算生物学 216 IEEE CSB: Computer Society Bioinformatics CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主办。 计算生物学 217 International Conference of the IEEE Engineering in Medicine and Biology Society（EMBC） 生物信息学与计算生物学国际研讨会 计算生物学 218 PSB: Pacific Symposium on Biocomputing PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举办。 计算生物学 219 WABI:Workshop on Algorithms in Bioinformatics WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方面。 计算生物学 220 CSB: IEEE Computational Systems Bioinformatics Conference CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主办 计算生物学 221 ISMB：Annual International Conference on Intelligent Systems for Molecular Biology ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为Bioinformatics杂志的专刊发表，近年来录取率为15%左右。 生物信息 222 ECCB：European Conference on Computational Biology ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为Bioinformatics杂志的专刊发表，近年录取率在20%左右。 生物信息 223 APBC：Asia Pacific Bioinformatics Conference APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。 生物信息 224 COCOON：Annual International Computing and Combinatorics Conference COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左右。 生物信息 225 CPM：Annual Symposium on Combinatorial Pattern Matching CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左右。 生物信息 226 EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千计。 生物信息 227 Geospatial Information and Technology Association（GITA） Annual Conference 地球空间信息与技术协会年会 遥感与空间信息处理 228 International Geoscience and Remote Sensing Symposium（IGRSS） 地球科学与遥感国际会议 遥感与空间信息处理 229 International Society for Photogrammetry and Remote Sensing（ISPRS）Technical Commission Symposium 国际摄影测量与遥感学会专业委员会会议 遥感与空间信息处理 230 International Conference on Geoinformatics 地球信息国际会议 遥感与空间信息处理 231 IEEE SKG (Semantics, Knowledge and Grid) 由计算所发起的IEEE国际会议，每年有100人参加。 知识网格 232 WWW: The ACM International World Wide Web Conference Internet领域顶级国际会议 Internet 233 International Semantic Web Conference Semantic Web领域顶级会议，录用率17% Semantic Web 234 ACM SIGMOD: ACM SIGMOD Conf on Management of Data ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。 数据管理 235 ACM PODS Conference ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。 数据管理 236 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据管理 237 IEEE ICDE - International Conference on Data Engineering 数据库顶级国际会议 数据管理","title":"计算机学科国际会议排名"},{"content":"第九章 分布式人工智能与Agent 9.1 分布式人工智能 分布式人工智能（distributed artificial intelligence） 分布式问题求解（distributed problem solving） 多agent系统（multi-agent system） 9.2 Agent及其要素 9.2.1 Agent的定义和译法 9.2.2 真体的要素和特性 9.3 真体的结构 9.3.1 真体的抽象结构和结构特点 9.3.2 真体结构的分类 9.4 真体通信 9.4.1 通信的过程 9.4.2 真体通信的类型和方式 9.4.3 交谈的规划和实现 9.4.4 真体的通信语言 9.5 移动真体和多真体系统 9.5.1 移动真体的定义和系统构成 9.5.2 多真体系统的特征和关键技术 9.5.3 多真体系统的模型和结构 9.5.4 多真体的协作、协商和协调 9.5.5 多真体的学习和规划 9.6 小结 第十章 自然语言理解 自然语言处理（natural language processing） 10.1 自然语言理解概述 10.1.1 语言与语言理解  10.1.2 自然语言处理的概念和定义 10.1.3 自然语言处理的研究领域和意义 10.1.4 自然语言理解研究的基本方法和进展 10.1.5 自然语言理解过程的层次 10.2 词法分析 词法分析（lexical analysis） 10.3 句法分析 10.3.1 短语结构语法 10.3.2 乔姆斯基形式语法 10.3.3 转移网络 转移网络（transition network） 10.3.4 扩充转移网络 10.3.5 词汇功能语法 词汇功能语法（lexical function grammar） 10.4 语义分析 10.5 句子的自动理解 10.5.1 简单句的理解方法 10.5.2 复合句的理解方法 10.6 语料库语言学 10.7 文本的自动翻译——机器翻译 10.8 自然语言理解系统的主要模型 10.9 自然语言理解系统应用举例 10.10 小结","title":"《人工智能及其应用》整理（5）"},{"content":"        众所周知人们在搜索引擎上搜索自己所要的信息都是通过输入关键词触发行为才得到相关结果的,那么在这个相关的数据结果少则上万多则上几十亿相关信息,在这种海量的数据中要想让站长的相关信息出现在搜索引擎较前的位置起航视觉SEO工作室认为是需要站门必学课程。其实道理很简单科学运用中文分词处理技术能让站长一劳永逸。        中文分词到底对搜索引擎有多大影响？对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。对站长或企业主来说他的意义也是非常重大的，关键词运用得当轻则带来大量流量，也能帮助企业主做好网络公关与转换率。        那么费话就不多说了，进入主题，接下来起航视觉SEO工作室为大家分享如何科学掌握中文分词处理技术。 一、什么是分词：         分词就是将连续的字（词）序列按照一定的规范重新组合成词序列的过程。《信息处理用现代汉语分词规范》中对分词的定义是：从信息处理需要出发，按照特定的规范，对汉语按分词单位进行划分的过程。对于英文分词，只要简单地以空格为分界符就能很好地把句子分析出来。这是由于英文是以词为单位的。不同于英文，计算机对中文分词时，由于中文句子中词与词之间是没有空格的，而且，两个字组合起来看似是一个词在句子中未必是一个词，所以计算机想要识别出中文句子中的词，就必须采用不同于英文分词的新技术。 二、分词出现的必要性：        1)人与计算机的沟通的基础。        由于中文文本的字与字之间的连续性，即汉语文本中词与词之间却没有明确的分隔标记，计算机无法识别出中文文本中哪些汉字串组合成词，导致处理中文信息无法直接理解中文的意义。所以，中文信息处理就必须比西文信息处理多了中文分词这一基本的步骤。汉语的中文信息处理就是要“用计算机对汉语的音、形、义进行处理”。而“词是最小的能够独立活动的有意义的语言成分”。        2）中文信息处理的基础性工作。        互联网的出现，彻底改变了人们对世界的认识;获得信息的成本越来越低，时间越来越短，信息量也越来越大。在信息贫泛与信息爆炸同时存在的时候，伴着信息几何级增长，如何对海量数据的处理，快速的定位到资源，是信息化时代不可缺少的部分。由此发展而来的，信息检索技术，文本挖掘，都是依赖于分词技术，分词技术还广泛应用于文本校对、机器翻译、语音识别等领域。 三、分词处理技术：         目前对汉语分词方法的研究主要有三个方面：基于规则的分词方法、基于统计的分词方法和基于理解的分词方法。         1）基于规则的分词方法         这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功(识别出一个词) 。常用的方法：最小匹配算法(Minimum Matching)，正向（逆向）最大匹配法(Maximum Matching)，逐字匹配算法,神经网络法、联想一回溯法，基于N-最短路径分词算法，以及可以相互组合，例如,可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法等。目前机械式分词占主流地位的是正向最大匹配法和逆向最大匹配法。         在所有的分词算法中，最早研究的是最小匹配算法(Minimum Matching)，该算法从待比较字符串左边开始比较，先取前两个字符组成的字段与词典中的词进行比较，如果词典中有该词，则分出此词，继续从第三个字符开始取两个字符组成的字段进行比较，如果没有匹配到，则取前3个字符串组成的字段进行比较，依次类推，直到取的字符串的长度等于预先设定的阈值，如果还没有匹配成功，则从待处理字串的第二个字符开始比较，如此循环。例如，“如果还没有匹配成功”，取出左边两个字组成的字段与词典进行比较，分出“如果”；再从“还”开始，取“还没”，字典中没有此词，继续取“还没有”，依次取到字段“还没有匹配”(假设阈值为5)，然后从“没”开始，取“没有”，如此循环直到字符串末尾为止。这种方法的优点是速度快，但是准确率却不是很高，比如待处理字符串为“中华人民共和国”，此匹配算法分出的结果为：中华、人民、共和国，因此该方法基本上已经不被采用 。        基于字符串的最大匹配，这种方法现在仍比较常用，最大匹配(Maximum Matching)分为正向和逆向两种最大匹配，正向匹配的基本思想是：假设词典中最大词条所含的汉字个数为n个，取待处理字符串的前n个字作为匹配字段，查找分词词典。若词典中含有该词，则匹配成功，分出该词，然后从被比较字符串的n+1处开始再取n个字组成的字段重新在词典中匹配；如果没有匹配成功，则将这n个字组成的字段的最后一位剔除，用剩下的n一1个字组成的字段在词典中进行匹配，如此进行下去，直到切分成功为止。例如，待处理字符串为“汉字多为表意文字”，取字符串“汉语多为表”(假设比较的步长为5，本文步长step都取5)与词典进行比较，没有与之对应的词，去除“表”字，用字段“汉语多为”进行匹配，直至匹配到“汉语”为至，再取字符串“多为表意”，循环到切分出“文字”一词。目前，正向最大匹配方法作为一种基本的方法已被肯定下来，但是由于错误比较大，一般不单独使用。如字符串“处理机器发生的故障”，在正向最大匹配方法中会出现歧义切分，该字符串被分为：处理机、发生、故障，但是使用逆向匹配就能得到有效的切分。        逆向最大匹配RMM(Reverse Directional Maximum Matching Method)的分词原理和过程与正向最大匹配相似，区别在于前者从文章或者句子(字串)的末尾开始切分，若不成功则减去最前面的一个字。比如对于字符串“处理机器发生的故障”，第一步，从字串的右边取长度以步长为单位的字段“发生的故障”在词典中进行匹配，匹配不成功，再取字段“生的故障”进行匹配，依次匹配，直到分出“故障”一词，最终使用RMM方法切分的结果为：故障、发生、机器、处理。该方法要求配备逆序词典。        一般来说根据汉语词汇构成的特点，从理论上说明了逆向匹配的精确度高于正向匹配，汉语语句的特点一般中心语偏后。有研究数据，单纯使用正向最大匹配的错误率为1/ 169，单纯使用逆向最大匹配的错误率为1/245。实际应用中可以从下面几方面改进，同时采取几种分词算法，来提高正确率；改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点,可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率等。        逐字匹配算法，基于TRIE索引树的逐字匹配算法，是建立在树型词典机制上，匹配的过程是从索引树的根结点依次同步匹配待查词中的每个字，可以看成是对树某一分枝的遍历。因此，采用该算法的分词速度较快，但树的构造和维护比较复杂。一种改进的算法是和最大匹配算法相结合，吸取最大匹配算法词典结构简单、TRIE索引树算法查询速度快的优点。因此词典结构和最大匹配词典构造机制相似，区别在于词典正文前增加了多级索引。匹配过程类似TRIE索引树进行逐字匹配，在性能上和TRIE索引树相近。        神经网络分词算法，尹峰等提出了以神经网络理论(BP模型)为基础的汉语分词模型，为汉语分词研究开辟了新途径。在实用中，BP算法存在收敛速度慢、易陷入局部最小等缺点，严重妨碍了分词速度。一种改进算法采用Levenbery2Marquart 算法来加速收敛速度,加快了收敛速度利用神经网络的基本原理进行分词。        联想—回溯法(Association－Backtracking Method，简称AB 法)。这种方法要求建立三个知识库——特 征词词库、实词词库和规则库。首先将待切分的汉字字符串序列按特征词词库分割为若干子串，子串可以是词，也可以是由几个词组合而成的词群；然后，再利用实词词库和规则库将词群再细分为词。切词时，要利用一定的语法知识，建立联想机制和回溯机制。联想机制由联想网络和联想推理构成，联想网络描述每个虚词的构词能力，联想推理利用相应的联想网络来判定所描述的虚词究竟是单独成词还是作为其他词中的构词成分。回溯机制主要用于处理歧义句子的切分。联想—回溯法虽然增加了算法的时间复杂度和空间复杂度，但这种方法的切词正确率较高，是一种行之有效的方法。        基于N-最短路径分词算法，其基本思想是根据词典，找出字串中所有可能的词，构造词语切分有向无环图。每个词对应图中的一条有向边，并赋给相应的边长(权值)。然后针对该切分图，在起点到终点的所有路径中，求出长度值按严格升序排列(任何两个不同位置上的值一定不等，下同)依次为第1，第2，…，第i，…，第N的路径集合作为相应的粗分结果集。如果两条或两条以上路径长度相等，那么他们的长度并列第i，都要列入粗分结果集，而且不影响其他路径的排列序号，最后的粗分结果集合大小大于或等于N。N一最短路径方法实际上是最短路径方法和全切分的有机结合。该方法的出发点是尽量减少切分出来的词数，这和最短路径分词方法是完全一致的；同时又要尽可能的包含最终结果，这和全切分的思想是共通的。通过这种综合，一方面避免了最短路径分词方法大量舍弃正确结果的可能，另一方面又大大解决了全切分搜索空间过大，运行效率差的弊端。N一最短路径方法相对的不足就是粗分结果不唯一，后续过程需要处理多个粗分结果。但是，对于预处理过程来讲，粗分结果的高召回率至关重要。因为低召回率就意味着没有办法再作后续的补救措施。预处理一旦出错，后续处理只能是一错再错，基本上得不到正确的最终结果。而少量的粗分结果对后续过程的运行效率影响不会太大，后续处理可以进一步优选排 错，如词性标注、句法分析等。        除上面之外，还有基于词频统计的切词法，基于期望的切词法，有穷多级列举法等。        2）基于统计的分词方法         基于统计的方法是基于(两个或多个) 汉字同时出现的概率,通过对语料库(经过处理的大量领域文本的集合)中的文本进行有监督或无监督的学习。可以获取该类文本的某些整体特征或规律。如果能够充分地利用这些统计现象、规律，就可以构造基于语料库的统计学信息抽取算法统计的分析方法多种多样。近来研究的热点主要集中于由随机过程发展而来的理论和方法，其中最重要的是应用隐马尔科夫模型(HMM)进行自然语言处理的方法。隐马尔科夫模型，在语音识别领域已经取得了很好的成效,在信息抽取领域的应用也正在不断的尝试和推广中。        3）基于理解分词         又称之为知识分词。知识分词是一种理想的分词方法,但这类分词方案的算法复杂度高,其有效性与可行性尚需在实际工作中得到进一步的验证。知识分词利用有关词、句子等的句法和语义信息或者从大量语料中找出汉字组词的结合特点来进行评价以期找到最贴近于原句语义的分词结果。        起航视觉SEO专家测试小结：目标位置百度，我们提交一个查询“毛泽东北京华烟云”，又一个不知所云的查询，尽管不知所云但是自有它的道理，我想看看百度的分词是如何消歧以及是否有词典未登录词的识别的功能，如果是正向最大匹配算法的话，那么输出应该是：“毛泽东/北京/华/烟云”，如果是反向最大匹配算法的话，那么输出应该是：“毛/泽/东北/京华烟云”，我们看看百度的分词结果：“毛泽东/北/京华烟云”，一个很奇怪的输出，跟我们的期望相差较多，但是从中我们可以获得如下信息：百度分词可以识别人名，也可以识别“京华烟云”，这说明有词典未登录词的识别的功能，我们可以假设分词过程分为两个阶段：第一阶段，先查找一个特殊词典，这个词典包含一些人名，部分地名以及一些普通词典没有的新词，这样首先将“毛泽东”解析出来，剩下了字符串“北京华烟云”，而“北/京华烟云”，可以看作是反向最大匹配的分词结果。这样基本说得通。为了证明这一点，我们提交查询“发毛泽东北”，我们期望两种分词结果，一个是正向最大匹配，一个是上述假设的结果，事实上百度输出是第二种情况，这样基本能确定百度分词采取了至少两个词典，一个是普通词典，一个是专用词典（人名等）。而且是专用词典先切分，然后将剩余的片断交由普通词典来切分。        继续测验，提交查询“古巴比伦理”，如果是正向最大匹配，那么结果应该是，如果是反向最大匹配，那么结果应该是，事实上百度的分词结果是，从这个例子看，好像用了正向最大匹配算法；此外还有一些例子表明好像是使用正向最大匹配的；但是且慢，我们看这个查询“北京华烟云”，正向最大匹配期望的结果是，而反向最大匹配期望的结果是，事实上百度输出的是后者，这说明可能采用的反向最大匹配；从这点我们可以猜测百度采用的是双向最大匹配分词算法，如果正向和反向匹配分词结果一致当然好办，直接输出即可；但是如果两者不一致，正向匹配一种结果，反向匹配一种结果，此时该如何是好呢？ 从上面两个例子看，在这种情况下，百度采取最短路径方法，也就是切分的片断越少越好，比如和相比选择后者，和相比选择后者。还有类似的一些例子，这样基本可以解释这些输出结果。        但是仍然遗留的问题是：如果正向反向分词不一致，而且最短路径也相同，那怎么办？输出正向的还是反向的结果？         我们再来看一个例子。提交查询“遥远古古巴比伦”，这个查询被百度切分为，说明词典里面有“巴比伦”，但是是否有“古巴比伦”这个词汇不确定，此时看不出是正向切分还是反向切分得出的结果，换查询为“遥远古巴比伦”，此时被切分为“遥远/古巴比伦”，这说明词典里面有“古巴比伦”这个词汇，这说明了“遥远古古巴比伦”是正向最大匹配的结果。那为什么“遥远古古巴比伦”不会被反向切分为“遥/远古/古巴比伦”呢，百度的可能选择是这种情况下选择单字少的那组切分结果。        当然还可以继续追问：如果切分后单字也一样多，那怎么办？最后看一个例子，查询“王强大小”，百度将其切分为“王/强大/小”，是正向切分的结果，如果是反向的会被切分为“王/强/大小”，这说明有歧义而且单字也相同则选择正向切分结果。        OK，看到这里可能头已经有些晕了，最后总结一下百度的分词算法，当然里面还是有猜测的成分，算法如下：         首先查询专用词典（人名，部分地名等），将专有名称切出，剩下的部分采取双向分词策略，如果两者切分结果相同，说明没有歧义，直接输出分词结果。如果不一致，则输出最短路径的那个结果，如果长度相同，则选择单字词少的那一组切分结果。如果单字也相同，则选择正向分词结果。        百度一直宣传自己在中文处理方面的优势，从上面看，分词算法并无特殊之处，消歧效果并不理想，即使百度采取比上述分词算法复杂些的算法也难以说成是优势，如果说百度有优势的话，唯一的优势就是那个很大的专用词典，这个专用词典登录了人名（比如大长今），称谓（比如老太太），部分地名（比如阿联酋等），估计百度采用学术界公布的比较新的命名实体识别算法从语料库里面不断识别出词典未登录词，逐渐扩充这个专门词典。如果这就是优势的话，那么这个优势能够保持多久就是个很明显的问题。       文章出自起航视觉SEO工作室（http://www.kxfdj.com/） 来源： http://www.kxfdj.com/newsshow.asp?id=3573 http://wenku.baidu.com/view/6323783e0912a21614792942.html http://groups.google.com/group/seoing/browse_thread/thread/8c85fdd90cbf271e/5cb22513a9a570eb?show_docid=5cb22513a9a570eb 转载请注明出处。","title":"科学运用中文分词处理技术"},{"content":"本文转自：http://www.ebigear.com/news-26-20865.html  abbreviation 缩写 [省略语] ablative 夺格(的) abrupt 突发音 accent 口音/{Phonetics}重音 accusative 受格（的） acoustic phonetics 声学语音学 acquisition 习得 action verb 动作动词 active 主动语态 active chart parser 活动图句法剖析程序 active knowledge 主动知识 active verb 主动动词 actor-action-goal 施事(者)-动作-目标  actualization 实现(化) acute 锐音 address 位址{资讯科学}/称呼（语）{语言学} adequacy 妥善性 adjacency pair 邻对 adjective 形容词 adjunct 附加语 [附加修饰语] adjunction 加接 adverb 副词  adverbial idiom 副词片语  affective 影响的 affirmative 肯定（的；式） affix 词缀 affixation 加缀 affricate 塞擦音 agent 施事 agentive-action verb 施事动作动词 agglutinative 胶着（性） agreement 对谐 AI (artificial intelligence) 人工智慧 [人工智能] AI language 人工智慧语言 [人工智能语言] Algebraic Linguistics 代数语言学 algorithm 演算法 [算法] alienable 可分割的 alignment 对照 [多国语言文章词；词组；句子翻译的] allo- 同位- allomorph 同位语素 allophone 同位音位 alpha notation alpha 标记 alphabetic writing 拼音文字 alternation 交替 alveolar 齿龈音 ambiguity 歧义 ambiguity resolution 歧义消解 ambiguous 歧义 American structuralism 美国结构主义 analogy 类推 analyzable 可分析的  anaphor 照应语 [前方照应词] animate 有生的 A-not-A question 正反问句 antecedent 先行词 anterior 舌前音 anticipation 预期 (音变) antonym 反义词 antonymy 反义 A-over-A A-上-A 原则 apposition 同位语 appositive construction 同位结构 appropriate 恰当的 approximant 无擦通音 approximate match 近似匹配 arbitrariness 任意性 archiphoneme 大音位 argument 论元 [变元] argument structure 论元结构 [变元结构] arrangement 配列 array 阵列 articulatory configuration 发音结构 articulatory phonetics 发音语音学 artificial intelligence (AI) 人工智慧 [人工智能] artificial language 人工语言 ASCII 美国标准资讯交换码 aspect 态 [体] aspirant 气音 aspiration 送气 assign 指派 assimilation 同化 association 关联 associative phrase 联想词组 asterisk 标星号 ATN (augmented transition network) 扩充转移网路 attested 经证实的 attribute 属性 attributive 属性 auditory phonetics 听觉语音学 augmented transition network 扩充转移网路 automatic document classification 自动文件分类 automatic indexing 自动索引 automatic segmentation 自动切分 automatic training 自动训练 automatic word segmentation 自动分词 automaton 自动机 autonomous 自主的 auxiliary 助动词 axiom 公理 baby-talk 儿语 back-formation 逆生构词(法) backtrack 回溯 Backus-Naur form 巴科斯诺尔形式 [巴科斯诺尔范式] backward deletion 逆向删略 ba-construction 把─字句 balanced corpus 平衡语料库 base 词基 Bayesian learning 贝式学习 Bayesian statistics 贝式统计 behaviorism 行为主义 belief system 信念系统 benefactive 受益（格；的） best first parser 最佳优先句法剖析器 bidirectional linked list 双向串列 bigram 双连词 bilabial 双唇音 bilateral 双边的 bilingual concordancer 双语关键词前后文排序程式 binary feature 双向特征[二分征性] binding 约束 bit 位元 [二进位制;比特] biuniqueness 双向唯一性 blade 舌叶 blend 省并词 block 封阻[封杀] Bloomfieldian 布隆菲尔德(学派)的 body language 肢体语言 Boolean lattice 布林网格 [布尔网格] borrow 借移 Bottom-up 由下而上 bottom-up parsing 由下而上剖析 bound 附着（的） bound morpheme 附着语素 [粘着语素] boundary marker 界线标记 boundary symbol 界线符号 bracketing 方括弧法 branching 分枝法 breadth-first search 广度优先搜寻 [宽度优先搜索] breath group 换气单位 breathy 气息音的 buffer 缓冲区 byte 位元组 CAI (Computer Assisted Instruction) 电脑辅助教学 CALL (computer assisted language learning) 电脑辅助语言学习 canonical 典范的 capacity 能力 cardinal 基数的 cardinal vowels 基本元音 case 格位 case frame 格位框架 Case Grammar 格位语法 case marking 格位标志 CAT (computer assisted translation) 电脑辅助翻译 cataphora 下指 Categorial Grammar 范畴语法 Categorial Unification Grammar 范畴连并语法 [范畴合一语法] causative 使动 causative verb 使役动词 causativity 使役性 centralization 央元音化 chain 炼 chart parsing 表式剖析 [图表句法分析] checked 受阻的 checking 验证 Chinese character code 中文编码 [汉字代码] Chinese character code for information interchange 中文资讯交换码 [汉字交换码] Chinese character coding input method 中文输入法 [汉字编码输入] choice 选择 Chomsky hierarchy 杭士基阶层 [Chomsky 层次结构] citation form 基本形式 CKY algorithm (Cocke-Kasami-Younger) CKY 演算法 classifier 类别词 cleft sentence 分裂句 click 啧音 clitic 附着词 closed world assumption 封闭世界假说 cluster 音群 Cocke-Kasami-Younger algorithm CKY 演算法 coda 音节尾 code conversion 代码变换 cognate 同源（的； 词） Cognitive Linguistics 认知语言学 coherence 一致性 cohesion 凝结性 [粘着性;结合力] collapse 合并 collective 集合的 collocation 连用语 [同现;搭配] combinatorial construction 合并结构  combinatorial insertion 合并中插 combinatorial word 合并词  Combinatory Categorial Grammar 组合范畴语法 comment 评论 commissive 许诺[语行] common sense semantics 常识语意学  Communication Theory 通讯理论 [通讯论;信息论] Comparative Linguistics 比较语言学 comparison 比较 competence 语言知能 compiler 编译器 complement 补语 complementary 互补 complementary distribution 互补分布 complementizer 补语标记 complex predicate 复杂谓语 complex stative construction 复杂状态结构 complex symbol 复杂符号 complexity 复杂度 component 成分 compositionality 语意合成性 [合成性] compound word 复合词  Computational Lexical Semantics 计算词汇语意学 Computational Lexicography 计算词典编纂学 Computational Linguistics 计算语言学 Computational Phonetics 计算语音学 Computational Phonology 计算声韵学 Computational Pragmatics 计算语用学 Computational Semantics 计算语意学 Computational Syntax 计算句法学 computer language 计算机语言 computer-aided translation 电脑辅助翻译 [计算机辅助翻译] computer-assisted instruction (CAI) 电脑辅助教学 computer-assisted language learning 电脑辅助语言学习[计算机辅助语言学习] concatenation 串联 concept classification 概念分类  concept dependency 概念依存  conceptual hierarchy 概念阶层 concord 谐和 concordance 关键词 (前后文) 排序 concordancer 关键词 (前后文) 排序的程式 concurrent parsing 并行句法剖析 conditional decision 条件决定 [条件决策] conjoin 连接 conjunction 连接词 (合取;逻辑积;\"与\";连词) conjunctive 连接的 connected speech 连续语言 Connectionist model 类神经网路模型 Connectionist model for natural language 自然语言类神经网路模型 [自然语言连接模型] connotation 隐涵意义 consonant 子音 [辅音] constituent 成分 constituent structure tree 词组结构树 constraint 限制 constraint propagation 限制条件的传递 [限定因素增殖] constraint-based grammar formalism 限制为本的语法形式 Construct Grammar 句构语法 content word 实词 context 语境 context-free language 语境自由语言 [上下文无关语言] context-sensitive language 语境限定语言 [上下文有关语言;上下文敏感语言] continuant 连续音 continuous speech recognition 连续语音识别 contraction 缩约 control agreement principle 控制一致原理 control structure 控制结构 control theory 控制论  convention 约定俗成[规约] convergence 收敛[趋同现象] conversational implicature 会话含义  converse 相反（词;的） cooccurrence relation 共现关系 [同现关系] co-operative principle 合作原则 coordination 对称连接词 [同等;并列连接] copula 系词 co-reference 同指涉 [互指] co-referential 同指涉 coronal 前舌音 corpora 语料库 corpus 语料库 Corpus Linguistics 语料库语言学 corpus-based learning 语料库为本的学习 correlation 相关性 counter-intuitive 违反语感的 courseware 课程软体 [课件] coverb 动介词 C-structure 成分结构 data compression 资料压缩 [数据压缩] data driven analysis 资料驱动型分析 [数据驱动型分析] data structure 资料结构 [数据结构] database 资料库 [数据库] database knowledge representation 资料库知识表示 [数据库知识表示] data-driven 资料驱动 [数据驱动] dative 与格 declarative knowledge 陈述性知识 decomposition 分解 deductive database 演译资料库 [演译数据库] default 预设值 [默认;缺省] definite 定指 Definite Clause Grammar 确定子句语法 definite state automaton 有限状态自动机  Definite State Grammar 有限状态语法 definiteness 定指 degree adverb 程度副词  degree of freedom 自由度 deixis 指示 delimiter 定界符号 [定界符] denotation 外延 denotic logic 符号逻辑  dependency 依存关系 Dependency Grammar 依存关系语法  dependency relation 依存关系 depth-first search 深度优先搜寻 derivation 派生 derivational bound morpheme 派生性附着语素 Descriptive Grammar 描述型语法 [描写语法] Descriptive Linguistics 描述语言学 [描写语言学] desiderative 意愿的 determiner 限定词  deterministic algorithm 决定型演算法 [确定性算法] deterministic finite state automaton 决定型有限状态机  deterministic parser 决定型语法剖析器 [确定性句法剖析程序] developmental psychology 发展心理学 Diachronic Linguistics 历时语言学 diacritic 附加符号 dialectology 方言学 dictionary database 辞典资料库 [词点数据库] dictionary entry 辞典条目 digital processing 数位处理 [数值处理] diglossia 双言 digraph 二合字母 diminutive 指小词 diphone 双连音 directed acyclic graph 有向非循环图 disambiguation 消除歧义 [歧义消除] discourse 篇章 discourse analysis 篇章分析 [言谈分析] discourse planning 篇章规划 Discourse Representation Theory 篇章表征理论 [言谈表示理论] discourse strategy 言谈策略 discourse structure 言谈结构 discrete 离散的 disjunction 选言 dissimilation 异化 distributed 分散式的 distributed cooperative reasoning 分布协调型推理 distributed text parsing 分布式文本剖析 disyllabic 双音节的 ditransitive verb 双宾动词 [双宾语动词;双及物动词] divergence 扩散[分化] D-M (Determiner-Measure) construction 定量结构 D-N (determiner-noun) construction 定名结构  document retrieval system 文件检索系统 [文献检索系统] domain dependency 领域依存性 [领域依存关系] double insertion 交互中插 double-base 双基 downgrading 降级 dummy 虚位 duration 音长{语音学}/时段{语法学/语意学} dynamic programming 动态规划 Earley algorithm Earley 演算法 echo 回声句 egressive 呼气音 ejective 紧喉音 electronic dictionary 电子词典 elementary string 基本字串 [基本单词串]  ellipsis 省略 EM algorithm EM演算法 embedding 入 emic 功能关系的 empiricism 经验论 Empty Category Principle 虚范畴原则 [空范畴原理] empty word 虚词 enclitics 后接成份 end user 终端用户 [最终用户] endocentric 同心的 endophora 语境照应  entailment 蕴涵 entity 实体  entropy 熵 entry 条目 episodic memory 情节性记忆 epistemological network 认识论网路  ergative verb 作格动词 ergativity 作格性 Esperando 世界语 etic 无功能关系 etymology 词源学 event 事件 event driven control 事件驱动型控制 example-based machine translation 以例句为本的机器翻译 exclamation 感叹 exclusive disjunction 排它性逻辑 “或” experiencer case 经验者格 expert system 专家系统 extension 外延 external argument 域外论元 extraposition 移外变形 [外置转换] facility value 易度值 feature 特征 feature bundle 特征束 feature co-occurrence restriction 特征同现限制 [特性同现限制] feature instantiation 特征体现 feature structure 特征结构 [特性结构] feature unification 特征连并 [特性合一] feedback 回馈 felicity condition 妥适条件 file structure 档案结构 finite automaton 有限状态机 [有限自动机] finite state 有限状态 Finite State Morphology 有限状态构词法 [有限状态词法] finite-state automata 有限状态自动机 finite-state language 有限状态语言 finite-state machine 有限状态机 finite-state transducer 有限状态置换器 flap 闪音 flat 降音 foreground information 前景讯息 [前景信息] formal Language Theory 形式语言理论 formal Linguistics 形式语言学 formal Semantics 形式语意学 forward inference 前向推理 [向前推理] forward-backward algorithm 前前后后演算法 frame 框架 frame based knowledge representation 框架型知识表示 Frame Theory 框架理论 free morpheme 自由语素 Fregean principle Fregean 原则  fricative 擦音 F-structure 功能结构 full text searching 全文检索  function word 功能词 Functional Grammar 功能语法 functional programming 函数型程式设计 [函数型程序设计] functional sentence perspective 功能句子观 functional structure 功能结构 functional unification 功能连并 [功能合一] functor 功能符 fundamental frequency 基频 garden path sentence 花园路径句 GB (Government and Binding) 管辖约束 geminate 重迭音 gender 性 Generalized Phrase Structure Grammar 概化词组结构语法 [广义短语结构语法] Generative Grammar 衍生语法  Generative Linguistics 衍生语言学 [生成语言学] generic 泛指 genetic epistemology 发生认识论  genetive marker 属格标记 genitive 属格 gerund 动名词 Government and Binding Theory 管辖约束理论  GPSG (Generalized Phrase Structure Grammar) 概化词组结构语法 [广义短语结构语法] gradability 可分级性 grammar checker 文法检查器 grammatical affix 语法词缀 grammatical category 语法范畴 grammatical function 语法功能 grammatical inference 文法推论 grammatical relation 语法关系 grapheme 字素 haplology 类音删略 head 中心语 head driven phrase structure 中心语驱动词组结构 [中心词驱动词组结构] head feature convention 中心语特征继承原理 [中心词特性继承原理] Head-Driven Phrase Structure Grammar 中心语驱动词组结构律 heteronym 同形 heuristic parsing 经验式句法剖析 Heuristics 经验知识 hidden Markov model 隐式马可夫模型 hierarchical structure 阶层结构 [层次结构] holophrase 单词句 homograph 同形异义词 homonym 同音异义词 homophone 同音词 homophony 同音异义 homorganic 同部位音的 Horn clause Horn 子句 HPSG (Head-Driven Phrase Structure Grammar) 中心语驱动词组结构语法 human-machine interface 人机界面 hypernym 上位词 hypertext 超文件 [超文本] hyponym 下位词 hypotactic 主从结构的 IC (immediate constituent) 直接成份 ICG (Information-based Case Grammar) 讯息为本的格位语法 idiom 成语 [熟语]  idiosyncrasy 特异性 illocutionary 施为性 immediate constituent 直接成份 imperative 祈使句 implicative predicate 蕴含谓词  implicature 含意 indexical 标引的 indirect object 间接宾语 indirect speech act 间接言谈行动 [间接言语行为]  Indo-European language 印欧语言 inductional inference 归纳推理 inference machine 推理机器 infinitive 不定词 [to 不定式] infix 中缀 inflection/inflexion 屈折变化 inflectional affix 屈折词缀 information extraction 资讯撷取 information processing 资讯处理 [信息处理] information retrieval 资讯检索 Information Science 资讯科学 [信息科学; 情报科学] Information Theory 资讯理论 [信息论] inherent feature 固有特征 inherit 继承 inheritance 继承 inheritance hierarchy 继承阶层 [继承层次] inheritance of attribute 属性继承 innateness position 语法天生假说 insertion 中插 inside-outside algorithm 里里外外演算法 instantiation 体现 instrumental (case) 工具格 integrated parser 集成句法剖析程式 integrated theory of discourse analysis 篇章分析综合理论 [言谈分析综合理论]  intelligence intensive production 知识密集型生产  intensifier 加强成分 intensional logic 内含逻辑 Intensional Semantics 内涵语意学 intensional type 内含类型 interjection/exclamation 感叹词 inter-level 中间成分 interlingua 中介语言 interlingual 中介语(的） interlocutor 对话者 internalise 内化 International Phonetic Association (IPA) 国际语音学会 internet 网际网路 Interpretive Semantics 诠释性语意学 intonation 语调  intonation unit (IU) 语调单位 IPA (International Phonetic Association) 国际语音学会 IR (information retrieval) 资讯检索 IS-A relation IS-A 关系 isomorphism 同形现象 IU (intonation unit) 语调单位 junction 连接 keyword in context 上下文中关键词[上下文内关键字] kinesics 体势学 knowledge acquisition 知识习得 knowledge base 知识库 knowledge based machine translation 知识为本之机器翻译 knowledge extraction 知识撷取 [知识题取] knowledge representation 知识表示 KWIC (keyword in context) 关键词前后文 [上下文内关键字] label 标签 labial 唇音 labio-dental 唇齿音 labio-velar 软颚唇音 LAD (language acquisition device) 语言习得装置 lag 发声延迟 language acquisition 语言习得 language acquisition device 语言习得装置 language engineering 语言工程 language generation 语言生成 language intuition 语感 language model 语言模型 language technology 语言科技 left-corner parsing 左角落剖析 [左角句法剖析] lemma 词元 lenis 弱辅音 letter-to-phone 字转音 lexeme 词汇单位 lexical ambiguity 词汇歧义 lexical category 词类 lexical conceptual structure 词汇概念结构 lexical entry 词项 lexical entry selection standard 选词标准 lexical integrity 词语完整性 Lexical Semantics 词汇语意学 Lexical-Functional Grammar 词汇功能语法 Lexicography 词典学 Lexicology 词汇学 lexicon 词汇库 [词典;词库] lexis 词汇层 LF (logical form) 逻辑形式 LFG (Lexical-Functional Grammar) 词汇功能语法 liaison 连音 linear bounded automaton 线性有限自主机 linear precedence 线性次序 lingua franca 共通语 linguistic decoding 语言解码 linguistic unit 语言单位 linked list 串列 loan 外来语 local 局部的 localism 方位主义 localizer 方位词 locus model 轨迹模型 locution 惯用语 logic 逻辑 logic array network 逻辑阵列网路 logic programming 逻辑程式设计 [逻辑程序设计] logical form 逻辑形式 logical operator 逻辑算子 [逻辑算符] Logic-Based Grammar 逻辑为本语法 [基于逻辑的语法] long term memory 长期记忆 longest match principle 最长匹配原则 [最长一致法] LR (left-right) parsing LR 剖析   machine dictionary 机器词典 machine language 机器语言  machine learning 机器学习 machine translation 机器翻译 machine-readable dictionary (MRD) 机读辞典 Macrolinguistics 宏观语言学 Markov chart 马可夫图  Mathematical Linguistics 数理语言学  maximum entropy 最大熵 M-D (modifier-head) construction 偏正结构  mean length of utterance (MLU) 语句平均长度  measure of information 讯习测度 [信息测度] memory based 根据记忆的 mental lexicon 心理词汇库 mental model 心理模型  mental process 心理过程 [智力过程;智力处理] metalanguage 超语言 metaphor 隐喻 metaphorical extension 隐喻扩展 metarule 律上律 [元规则] metathesis 语音易位 Microlinguistics 微观语言学 middle structure 中间式结构 minimal pair 最小对 Minimalist Program 微言主义 MLU (mean length of utterance) 语句平均长度 modal 情态词 modal auxiliary 情态助动词 modal logic 情态逻辑 modifier 修饰语 Modular Logic Grammar 模组化逻辑语法  modular parsing system 模组化句法剖析系统  modularity 模组性(理论) module 模组 monophthong 单元音 monotonic 单调 monotonicity 单调性 Montague Grammar 蒙泰究语法 [蒙塔格语法] mood 语气 morpheme 词素 morphological affix 构词词缀 morphological decomposition 语素分解 morphological pattern 词型 morphological processing 词素处理 morphological rule 构词律 [词法规则] morphological segmentation 语素切分 Morphology 构词学 Morphophonemics 词音学 [形态音位学;语素音位学] morphophonological rule 形态音位规则 Morphosyntax 词句法 Motor Theory 肌动理论 movement 移位 MRD (machine-readable dictionary) 机读辞典 MT (machine translation) 机器翻译 multilingual processing system 多语讯息处理系统 multilingual translation 多语翻译 multimedia 多媒体 multi-media communication 多媒体通讯 multiple inheritance 多重继承 multistate logic 多态逻辑 mutation 语音转换 mutual exclusion 互斥 mutual information 相互讯息 nativist position 语法天生假说 natural language 自然语言 natural language processing (NLP) 自然语言处理 natural language understanding 自然语言理解 negation 否定 negative sentence 否定句 neologism 新词语 nested structure 套结构 network 网路 neural network 类神经网路 Neurolinguistics 神经语言学 neutralization 中立化 n-gram n-连词 n-gram modeling n-连词模型 NLP (natural language processing) 自然语言处理 node 节点 nominalization 名物化 nonce 暂用的 non-finite 非限定 non-finite clause 非限定式子句 non-monotonic reasoning 非单调推理 normal distribution 常态分布 noun 名词 noun phrase 名词组 NP (noun phrase) completeness 名词组完全性 object 宾语{语言学}/物件{资讯科学} object oriented programming 物件导向程式设计 [面向对向的程序设计]  official language 官方语言 one-place predicate 一元述语 on-line dictionary 线上查询词典 [联机词点] onomatopoeia 拟声词  onset 节首音 ontogeny 个体发生 Ontology 本体论 open set 开放集 operand 运算元 [操作对象] optimization 最佳化 [最优化] overgeneralization 过度概化 overgeneration 过度衍生 paradigmatic relation 聚合关系 paralanguage 附语言 parallel construction 并列结构 Parallel Corpus 平行语料库 parallel distributed processing (PDP) 平行分布处理 paraphrase 转述 [释意;意译;同意互训] parole 言语 parser 剖析器 [句法剖析程序] parsing 剖析 part of speech (POS) 词类 particle 语助词 PART-OF relation PART-OF 关系 part-of-speech tagging 词类标注 pattern recognition 型样识别 P-C (predicate-complement) insertion 述补中插 PDP (parallel distributed processing) 平行分布处理 perception 知觉 perceptron 感觉器 [感知器] perceptual strategy 感知策略 performative 行为句 periphrasis 用独立词表达 perlocutionary 语效性的 permutation 移位 Petri Net Grammar Petri 网语法  philology 语文学 phone 语音 phoneme 音素 phonemic analysis 因素分析 phonemic stratum 音素层 Phonetics 语音学 phonogram 音标 Phonology 声韵学 [音位学;广义语音学] Phonotactics 音位排列理论 phrasal verb 词组动词 [短语动词] phrase 词组 [短语] phrase marker 词组标记 [短语标记] pitch 音调 pitch contour 调形变化 Pivot Grammar 枢轴语法 pivotal construction 承轴结构 plausibility function 可能性函数 PM (phrase marker) 词组标记 [短语标记] polysemy 多义性 POS-tagging 词类标记 postposition 方位词 PP (preposition phrase) attachment 介词依附 Pragmatics 语用学 Precedence Grammar 优先顺序语法 precision 精确度 predicate 述词 predicate calculus 述词计算 predicate logic 述词逻辑 [谓词逻辑] predicate-argument structure 述词论元结构 prefix 前缀 premodification 前置修饰 preposition 介词 Prescriptive Linguistics 规定语言学 [规范语言学] presentative sentence 引介句 presupposition 前提 Principle of Compositionality 语意合成性原理 privative 二元对立的 probabilistic parser 概率句法剖析程式 problem solving 解决问题 program 程式 programming language 程式设计语言 [程序设计语言] proofreading system 校对系统 proper name 专有名词 prosody 节律 prototype 原型 pseudo-cleft sentence 准分裂句 Psycholinguistics 心理语言学 punctuation 标点符号 pushdown automata 下推自动机 pushdown transducer 下推转换器 qualification 后置修饰 quantification 量化 quantifier 范域词 Quantitative Linguistics 计量语言学 question answering system 问答系统 queue 伫列 radical 字根 [词干;词根;部首;偏旁] radix of tuple 元组数基 random access 随机存取 rationalism 理性论 rationalist (position) 理性论立场 [唯理论观点] reading laboratory 阅读实验室 real time 即时 real time control 即时控制 [实时控制] recursive transition network 递回转移网路 reduplication 重迭词 [重复] reference 指涉 referent 指称对象 referential indices 指标 referring expression 指涉词 [指示短语] register 暂存器 [寄存器]{资讯科学}/调高{语音学}/语言的场合层级{社会语言学} regular language 正规语言 [正则语言] relational database 关联式资料库 [关系数据库] relative clause 关系子句 relaxation method 松弛法 relevance 相关性 Restricted Logic Grammar 受限逻辑语法 resumptive pronouns 复指代词 retroactive inhibition 逆抑制 rewriting rule 重写规则 rheme 述位 rhetorical structure 修辞结构 rhetorics 修辞学 robust 强健性 robust processing 强健性处理 robustness 强健性 schema 基朴 school grammar 教学语法 scope 范域 [作用域;范围] script 脚本 search mechanism 检索机制 search space 检索空间 searching route 检索路径 [搜索路径] second order predicate 二阶述词 segmentation 分词 segmentation marker 分段标志 selectional restriction 选择限制 semantic field 语意场 semantic frame 语意架构 semantic network 语意网路 semantic representation 语意表征 [语义表示] semantic representation language 语意表征语言 semantic restriction 语意限制 semantic structure 语意结构 Semantics 语意学 sememe 意素 Semiotics 符号学 sender 发送者 sensorimotor stage 感觉运动期  sensory information 感官讯息 [感觉信息] sentence 句子 sentence generator 句子产生器 [句子生成程序] sentence pattern 句型 separation of homonyms 同音词区分 sequence 序列 serial order learning 顺序学习 serial verb construction 连动结构 set oriented semantic network 集合导向型语意网路 [面向集合型语意网路] SGML (Standard Generalized Markup Language) 结构化通用标记语言 shift-reduce parsing 替换简化式剖析 short term memory 短程记忆 sign 信号 signal processing technology 信号处理技术 simple word 单纯词 situation 情境 Situation Semantics 情境语意学 situational type 情境类型 social context 社会环境 sociolinguistics 社会语言学 software engineering 软体工程 [软件工程] sort 排序 speaker-independent speech recognition 非特定语者语音识别 spectrum 频谱 speech 口语 speech act assignment 言语行为指定 speech continuum 言语连续体 speech disorder 语言失序 [言语缺失] speech recognition 语音辨识 speech retrieval 语音检索 speech situation 言谈情境 [言语情境] speech synthesis 语音合成 speech translation system 语音翻译系统 speech understanding system 语音理解系统 spreading activation model 扩散激发模型 standard deviation 标准差 Standard Generalized Markup Language 标准通用标示语言 start-bound complement 接头词 state of affairs algebra 事态代数 state transition diagram 状态转移图 statement kernel 句核 static attribute list 静态属性表 statistical analysis 统计分析 Statistical Linguistics 统计语言学 statistical significance 统计意义 stem 词干 stimulus-response theory 刺激反应理论  stochastic approach to parsing 概率式句法剖析 [句法剖析的随机方法] stop 爆破音 Stratificational Grammar 阶层语法 [层级语法] string 字串[串；字符串] string manipulation language 字串操作语言  string matching 字串匹配 [字符串] structural ambiguity 结构歧义 Structural Linguistics 结构语言学 structural relation 结构关系 structural transfer 结构转换 structuralism 结构主义 structure 结构 structure sharing representation 结构共享表征 subcategorization 次类划分 [下位范畴化] subjunctive 假设的 sublanguage 子语言 subordinate 从属关系 subordinate clause 从属子句 [从句;子句] subordination 从属 substitution rule 代换规则 [置换规则] substrate 底层语言 suffix 后缀 superordinate 上位的 superstratum 上层语言 suppletion 异型[不规则词型变化] suprasegmental 超音段的 syllabification 音节划分 syllable 音节 syllable structure constraint 音节结构限制 symbolization and verbalization 符号化与字句化 synchronic 同步的 synonym 同义词 syntactic category 句法类别  syntactic constituent 句法成分 syntactic rule 语法规律 [句法规则] Syntactic Semantics 句法语意学  syntagm 句段  syntagmatic 组合关系 [结构段的;组合的] Syntax 句法 Systemic Grammar 系统语法 tag 标记 target language 目的语言 [目标语言] task sharing 课题分享 [任务共享] tautology 套套逻辑 [恒真式;重言式;同义反复] taxonomical hierarchy 分类阶层 [分类层次] telescopic compound 套装合并 template 模板 temporal inference 循序推理 [时序推理] temporal logic 时间逻辑 [时序逻辑] temporal marker 时貌标记 tense 时态 terminology 术语 text 文本 text analyzing 文本分析 text coherence 文本一致性 text generation 文本生成 [篇章生成] Text Linguistics 文本语言学 text planning 文本规划 text proofreading 文本校对 text retrieval 文本检索 text structure 文本结构 [篇章结构] text summarization 文本自动摘要 [篇章摘要] text understanding 文本理解 text-to-speech 文本转语音 thematic role 题旨角色 thematic structure 题旨结构 theorem 定理 thesaurus 同义词辞典 theta role 题旨角色 theta-grid 题旨网格 token 实类 [标记项] tone 音调 tone language 音调语言 tone sandhi 连调变换 top-down 由上而下 [自顶向下] topic 主题 topicalization 主题化 [话题化] trace 痕迹 Trace Theory 痕迹理论  training 训练 transaction 异动 [处理单位] transcription 转写 [抄写;速记翻译] transducer 转换器 transfer 转移 transfer approach 转换方法 transfer framework 转换框架 transformation 变形 [转换] Transformational Grammar 变形语法 [转换语法] transitional state term set 转移状态项集合 transitivity 及物性 translation 翻译 translation equivalence 翻译等值性 translation memory 翻译记忆 transparency 透明性 tree 树状结构 [树] Tree Adjoining Grammar 树形加接语法 [树连接语法] treebank 树图资料库[语法关系树库] trigram 三连词 t-score t-数 turing machine 杜林机 [图灵机] turing test 杜林测试 [图灵试验] type 类型 type/token node 标记类型/实类节点 type-feature structure 类型特征结构 typology 类型学 ultimate constituent 终端成分 unbounded dependency 无界限依存 underlying form 基底型式  underlying structure 基底结构 unification 连并 [合一] Unification-based Grammar 连并为本的语法 [基于合一的语法] Universal Grammar 普遍性语法 universal instantiation 普遍例式 universal quantifier 全称范域词 unknown word 未知词 [未定义词] unrestricted grammar 非限制型语法 usage flag 使用旗标 user interface 使用者界面 [用户界面] Valence Grammar 结合价语法 Valence Theory 结合价理论 valency 结合价 variance 变异数 [方差] verb 动词 verb phrase 动词组 [动词短语] verb resultative compound 动补复合词 verbal association 词语联想 verbal phrase 动词组 verbal production 言语生成 vernacular 本地话 V-O construction (verb-object) 动宾结构 vocabulary 字汇 vocabulary entry 词条 vocal track 声道 vocative 呼格 voice recognition 声音辨识 [语音识别] vowel 母音 vowel harmony 母音和谐 [元音和谐] waveform 波形 weak verb 弱化动词 Whorfian hypothesis Whorfian 假说 word 词 word frequency 词频 word frequency distribution 词频分布 word order 词序 word segmentation 分词 word segmentation standard for Chinese 中文分词规范  word segmentation unit 分词单位 [切词单位] word set 词集 working memory 工作记忆 [工作存储区] world knowledge 世界知识 writing system 书写系统 X-Bar Theory X标杠理论 [\"x\"阶理论] Zipf's Law 利夫规律 [齐普夫定律]","title":"自然语言处理相关词汇"},{"content":"也许大家不相信，数学是解决信息检索和自然语言处理的最好工具。它能非常清晰地描述这些领域的实际问题并且给出漂亮的解决办法。每当人们应用数学工具解决一个语言问题时，总会感叹数学之美。我们希望利用 Google 中文黑板报这块园地，介绍一些数学工具，以及我们是如何利用这些工具来开发 Google 产品的。 系列一： 统计语言模型 (Statistical Language Models) Google 的使命是整合全球的信息，所以我们一直致力于研究如何让机器对信息、语言做最好的理解和处理。长期以来，人类一直梦想着能让机器代替人来翻译语言、识别语音、认识文字（不论是印刷体或手写体）和进行海量文献的自动检索，这就需要让机器理解语言。但是人类的语言可以说是信息里最复杂最动态的一部分。为了解决这个问题，人们容易想到的办法就是让机器模拟人类进行学习 - 学习人类的语法、分析语句等等。尤其是在乔姆斯基（Noam Chomsky 有史以来最伟大的语言学家）提出 “形式语言” 以后，人们更坚定了利用语法规则的办法进行文字处理的信念。遗憾的是，几十年过去了，在计算机处理语言领域，基于这个语法规则的方法几乎毫无突破。 其实早在几十年前，数学家兼信息论的祖师爷 香农 (Claude Shannon)就提出了用数学的办法处理自然语言的想法。遗憾的是当时的计算机条件根本无法满足大量信息处理的需要，所以他这个想法当时并没有被人们重视。七十年代初，有了大规模集成电路的快速计算机后，香农的梦想才得以实现。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师贾里尼克 (Fred Jelinek)。当时贾里尼克在 IBM 公司做学术休假 (Sabbatical Leave)，领导了一批杰出的科学家利用大型计算机来处理人类语言问题。统计语言模型就是在那个时候提出的。 给大家举个例子：在很多涉及到自然语言处理的领域，如机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询中，我们都需要知道一个文字序列是否能构成一个大家能理解的句子，显示给使用者。对这个问题，我们可以用一个简单的统计模型来解决这个问题。 如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为： P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1) 其中 P (w1) 表示第一个词w1 出现的概率；P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词wn，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。因此我们假定任意一个词wi的出现概率只同它前面的词 wi-1 有关(即马尔可夫假设），于是问题就变得很简单了。现在，S 出现的概率就变为： P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)… (当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。） 接下来的问题就是如何估计 P (wi|wi-1)。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。 也许很多人不相信用这么简单的数学模型能解决复杂的语音识别、机器翻译等问题。其实不光是常人，就连很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法都有效。比如在 Google 的中英文自动翻译中，用的最重要的就是这个统计语言模型。去年美国标准局(NIST) 对所有的机器翻译系统进行了评测，Google 的系统是不仅是全世界最好的，而且高出所有基于规则的系统很多。 现在，读者也许已经能感受到数学的美妙之处了，它把一些复杂的问题变得如此的简单。当然，真正实现一个好的统计语言模型还有许多细节问题需要解决。贾里尼克和他的同事的贡献在于提出了统计语言模型，而且很漂亮地解决了所有的细节问题。十几年后，李开复用统计语言模型把 997 词语音识别的问题简化成了一个 20 词的识别问题，实现了有史以来第一次大词汇量非特定人连续语音的识别。 我是一名科学研究人员 ，我在工作中经常惊叹于数学语言应用于解决实际问题上时的神奇。我也希望把这种神奇讲解给大家听。当然，归根结底，不管什莫样的科学方法、无论多莫奇妙的解决手段都是为人服务的。我希望 Google 多努力一分，用户就多一分搜索的喜悦。","title":"统计语言模型"},{"content":"记得第一次了解中文分词算法是在 Google 黑板报 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进行研究，期间诞生了很多有意思的理论。     中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。     有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。     最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来／应聘”。     维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。     还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如“民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民”并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／服务”。       不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。     当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词数法上，我们就有了一种简明而强大的算法：     对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。     这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分：       他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ）       他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ）       他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ）     正确答案胜出。     需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。     算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。“鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概率，每个词出现的频率也是不同的。     何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。     以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10-9 ，但“有意／见／分歧”的得分只有 1.0×10-11 ，正确方案完胜。     这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话：       这／事／的确／定／不／下来     但是概率算法却会把这个句子分成：       这／事／的／确定／不／下来     原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。     其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。     于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w1 、 w2 ，统计在语料库中词语 w1 后面恰好是 w2 的概率 P(w1, w2) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w1) · P(w1, w2) · … · P(wn-1, wn) ，其中 w1, w2, …, wn 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。     至此，中文自动分词算是有了一个漂亮而实用的算法。         但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。     在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。     可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、“说”、“报道”、“参加”、“访问”、“表示”等动作词。     但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了；要是听说了有人居然敢叫“令计划”，估计直接就崩溃了。     还有那些恰好与上下文组合成词的人名，例如：      费孝通向人大常委会提交书面报告      邓颖超生前使用过的物品     这就是最考验分词算法的句子了。     相比之下，中国地名的用字就分散得多了，重庆就有一个叫做“犀牛屙屎”的地方。不过，中国地名委员会编写了《中华人民共和国地名录》，收录了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。外文人名和地名的用字非常集中，识别的正确率要高出许多。     真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。     最难识别的未登录词就是缩略语了。“高数”、“抵京”、“女单”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。     汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。     说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。 原文 ：http://www.matrix67.com/blog/archives/4212","title":"漫话中文分词算法(转)"},{"content":"510　电子、通信与自动控制技术                                                                                                                                                                                                                                                　510.10　电子技术 　　510.1010　电子电路 　　510.1015　天线电波传播 　　510.1020　无线电技术 　　510.1025　微波技术 　　510.1030　敏感电子学 　　510.1035　微电子学 　　510.1040　仿真技术 　　510.1045　超导电子技术 　　510.1050　电子元件与器件技术 　　510.1055　电子束、离子束技术 　　510.1060　红外与夜视技术 　　510.1099　电子技术其他学科 　510.20　光电子学与激光技术 　510.30　半导体技术 　　510.3010　半导体测试技术 　　510.3020　半导体材料 　　510.3030　半导体器件与技术 　　510.3040　集成电路技术 　　510.3050　半导体加工技术 　　510.3099　半导体技术其他学科 　510.40　信息处理技术 　　510.4010　信号检测 　　510.4020　参数估计 　　510.4030　数据处理 　　510.4040　语音处理 　　510.4050　图象处理 　　510.4099　信息处理技术其他学科 　510.50　通信技术 　　510.5010　有线通信技术 　　510.5015　无线通信技术(包括微波通信、卫星通信等) 　　510.5020　光纤通信技术 　　510.5025　通信传输技术 　　510.5030　通信网络技术 　　510.5035　通信终端技术 　　510.5040　电信 　　510.5045　邮政 　　510.5050　邮电通信管理工程 　　510.5099　通信技术其他学科 　510.60　广播与电视工程技术 　510.70　雷达工程 　510.80　自动控制技术 　　510.8010　自动控制理论(包括线性、非线性、随机控制，最优控制、自适应控制系统、分布式控制系统、柔性控制系统等) 　　510.8020　控制系统仿真技术 　　510.8030　机电一体化技术 　　510.8040　自动化仪器仪表与装置 　　510.8050　机器人控制 　　510.8060　自动化技术应用 　　510.8099　自动控制技术其他学科 　510.99　电子、通信与自动控制技术其他学科 520　计算机科学技术                                                                                                                                                                                                                                                　520.10　计算机科学技术基础学科 　　520.1010　自动机理论 　　520.1020　可计算性理论 　　520.1030　计算机可靠性理论 　　520.1040　算法理论 　　520.1050　数据结构 　　520.1060　数据安全与计算机安全 　　520.1099　计算机科学技术基础学科其他学科 　520.20　人工智能 　　520.2010　人工智能理论 　　520.2020　自然语言处理 　　520.2030　机器翻译 　　520.2040　模式识别 　　520.2050　计算机感知 　　520.2060　计算机神经网络 　　520.2070　知识工程(包括专家系统) 　　520.2099　人工智能其他学科 　520.30　计算机系统结构 　　520.3010　计算机系统设计 　　520.3020　并行处理 　　520.3030　分布式处理系统 　　520.3040　计算机网络 　　520.3050　计算机运行测试与性能评价 　　520.3099　计算机系统结构其他学科 　520.40　计算机软件 　　520.4010　软件理论 　　520.4020　操作系统与操作环境 　　520.4030　程序设计及其语言 　　520.4040　编译系统 　　520.4050　数据库 　　520.4060　软件开发环境与开发技术 　　520.4070　软件工程 　　520.4099　计算机软件其他学科 　520.50　计算机工程 　　520.5010　计算机元器件 　　520.5020　计算机处理器技术 　　520.5030　计算机存储技术 　　520.5040　计算机外围设备 　　520.5050　计算机制造与检测 　　520.5060　计算机高密度组装技术 　　520.5099　计算机工程其他学科 　520.60　计算机应用 　　520.6010　中国语言文字信息处理(包括汉字信息处理) 　　520.6020　计算机仿真 　　520.6030　计算机图形学 　　520.6040　计算机图象处理 　　520.6050　计算机辅助设计 　　520.6060　计算机过程控制 　　520.6070　计算机信息管理系统 　　520.6080　计算机决策支持系统 　　520.6099　计算机应用其他学科 　520.99　计算机科学技术其他学科 　","title":"中华人民共和国学科分类与代码国家标准(GB/T 13745-2009)_ 510 520"},{"content":"进过几天的努力，Chindle的排版已经能够达到清华自然语言处理组推出的微博关键词的排版水平了。原来的目标就是达到相似水平即可，但走到这一步就觉得实在不够，还有太多地方可以改进。下面真正的努力方向是tagxedo这个水平，人家玩儿的不是技术，是艺术。 参考： http://ued.ctrip.com/blog/?p=2471  携程UED的一点解释，很不错。","title":"Chindle自动排版设计【2】"},{"content":"【串和序列处理 1】PAT Tree 子串匹配结构 Patricia Tree  简称PAT tree。 它是 trie 结构的一种特殊形式。是目前信息检索领域应用十分成功的索引方法，它是1992年由Connel根据《PATRICIA——Patrical Algorithm to Retrieve Information Coded in Alphanumeric》算法发展起来的。   PAT tree 在字符串子串匹配 上有这非常优异的表现，这使得它经常成为一种高效的全文检索算法，在自然语言处理领域也有广泛的应用。其算法中最突出的特点就是采用半无限长字串(semi-infinite string 简称 sistring) 作为字符串的查找结构。   采用半无限长字串(sistring): 一种特殊的子串信息存储方式。 比如一个字符串CUHK。它的子串有C、CU、CUH、CUHK、U、UH、UHK、H、HK、K十种。如果有n个字符的串，就会有n(n+1)/2种子串，其中最长的子串长度为n。因此我们不得不开辟 n(n+1)/2个长度为n的数组来存储它们，那么存储的空间复杂度将达到惊人的O(n^3)级别。   但是我们发现这样一个特点：             CUHK ——  完全可以表示 C、CU、CUH、CUHK             UHK   ——  完全可以表示 U、UH、UHK             HK     ——  完全可以表示 H、HK、             K       ——  完全可以表示 K 这样我们就得到了4个sistring: CUHK、UHK、HK和K。   PAT tree的存储结构 如果直接用单个字符作为存储结点，势必构造出一棵多叉树(如果是中文字符的话，那就完蛋了)。检索起来将会相当不便。事实上，PAT tree是一棵压缩存储的二叉树结构。现在我们用“CUHK”来构造出这样一棵PAT tree 。   开始先介绍一下PAT tree的结点结构(看了后面的过程就再来理解这些概念) * 内部结点：用椭圆形表示，用来存储不同的bit位在整个完整bit sequence中的位置。 * 外部节点(叶子结点)： 用方形表示，用来记录sistring的首字符在完整sistring中的开始位置(字符索引)和sistring出现的频次。 * 左指针：如果 待存储的sistring在 内部结点所存储的bit位置上的数据 是0，则将这个sistring存储在该结点的左子树中。 * 右指针：若数据是1，则存储在右子树中。   (1) 将所有sistring的字符转化成1 bytes的ASCII码值，用二进制位来表示。形成一个bit sequence pattern(没有的空字符我们用0来填充)。                            sistring                           bit sequence  完整sistring  ->   CUHK        01000011   01010101   01001000   01001011   <- 完整bit sequence                           UHK0         01010101   01001000   01001011   00000000                                                       HK00         01001000   01001011   00000000   00000000                             K000         01001011   00000000   00000000   00000000  (2) 从第一个bit开始我们发现所有sistring的前3个bit位都相同010，那么相同的这些0/1串对于匹配来说就毫无意义了，因此我们接下来发现第4个bit开始有所不同了。UHK 的第4个bit是1，而CUHK、HK、K的第4个bit是0。则先构造一个内部结点iNode.bitSize=4（第4个bit），然后将UHK的字符索引 cIndex=2(UHK的开始字符U在完整的CUHK的第2位置上)构造成叶子结点插入到iNode的左孩子上，而CUHK、HK、K放在iNode右子树中。(如下图2)   (3) 递归执行第2步，将CUHK、HK、K进一步插入到PAT tree中。流程如下图所示。所有sistring都插入以后结束。 注意：既然PAT tree 是二叉查找树，那么一定要满足二叉查找树的特点。所以，内部结点中的bit 位就需要满足，左孩子的bit位< 结点bit 位< 右孩子的bit 位。       PAT tree的检索过程   利用PAT tree可以实现对语料的快速检索，检索过程就是根据查询字串在PAT tree中从根结点寻找路径的过程。当比较完查询字串所有位置后，搜索路径达到PAT tree的某一结点。         若该结点为叶子结点，则判断查询字串是否为叶子结点所指的半无限长字串的前缀，如果判断为真，则查询字串在语料中出现的频次即为叶子结点中记录的频次；否则，该查询字串在语料中不存在。         若该结点为内部结点，则判断查询字串是否为该结点所辖子树中任一叶子结点所指的半无限长字串的前缀。如果判断为真，该子树中所有叶子结点记录的频次之和即为查询字串的出现频次。否则，查询字串在语料中不存在。         这样，通过PAT tree可以检索原文中任意长度的字串及其出现频次，所以，PAT tree也是可变长统计语言模型优良的检索结构。     例如：要查找string= “CU ”(bit sequence=010 00 0 1 1 01010101) 是不是在CUHK 中。 (1)   根据“CUHK ”的PAT tree 结构( 如上图) ，根结点r 的bit position=4 ，那么查找bit sequence 的第4 个bit=0 。然后查找R 的左孩子rc 。 (2)    rc 的bit position=5 ，在bit sequence 的第5 个bit=0 。则查找rc 的左孩子rcc 。 (3)   rcc= ” CUHK ” 已经是叶子结点了，则确定一下CU 是不是CUHK 的前缀即可。   PAT tree 的效率         特点：PAT tree查找的时间复杂度和树的深度有关，由于树的构造取决于不同bit位上0,1的分布。因此PAT tree有点像二叉查找树 ，最坏情况下是单支树(如上图例子)，此时的时间复杂度是O(n-1)，n为字符串的长度。最好情况下是平衡二叉树 结构，时间复杂度是O(log2(N))。另外，作为压缩的二叉查找树，其存储的空间代价大大减少了。   PAT tree的实际应用          PAT tree在子串匹配上有很好的效率，这一点和Suffix Tree(后缀树)，KMP算法的优点相同。因此PAT tree在信息检索和自然语言处理领域是非常常用的工具。比如：关键字提取，新词发现等NLP领域经常使用这种结构。     【串和序列处理 2】字符串编辑距离算法 http://hxraid.iteye.com/blog/615469 我们来看一个实际应用。现代搜索技术的发展很多以提供优质、高效的服务作为目标。比如说：baidu、google、sousou等知名全文搜索系统。当我们输入一个错误的query=\"Jave\" 的时候，返回中有大量包含正确的拼写 \"Java\"的网页。当然这里面用到的技术绝对不会是我们今天讲的怎么简单。但我想说的是：字符串的相似度计算也是做到这一点的方法之一。   字符串编辑距离： 是一种字符串之间相似度计算的方法。给定两个字符串S、T，将S转换成T所需要的删除，插入，替换操作的数量就叫做S到T的编辑路径。而最短的编辑路径就叫做字符串S和T的编辑距离。   举个例子：S=“eeba”   T=\"abac\"   我们可以按照这样的步骤转变：(1) 将S中的第一个e变成a;(2) 删除S中的第二个e;(3)在S中最后添加一个c; 那么S到T的编辑路径就等于3。当然，这种变换并不是唯一的，但如果3是所有变换中最小值的话。那么我们就可以说S和T的编辑距离等于3了。   动态规划解决编辑距离 动态规划(dynamic programming)是一种解决复杂问题最优解的策略。它的基本思路就是：将一个复杂的最优解问题分解成一系列较为简单的最优解问题，再将较为简单的的最优解问题进一步分解，直到可以一眼看出最优解为止。   动态规划算法是解决复杂问题最优解的重要算法。其算法的难度并不在于算法本身的递归难以实现，而主要是编程者对问题本身的认识是否符合动态规划的思想。现在我们就来看看动态规划是如何解决编辑距离的。   还是这个例子：S=“eeba”   T=\"abac\" 。我们发现当S只有一个字符e、T只有一个字符a的时候，我们马上就能得到S和T的编辑距离edit(0,0)=1(将e替换成a)。那么如果S中有1个字符e、T中有两个字符ab的时候，我们是不是可以这样分解：edit(0,1)=edit(0,0)+1(将e替换成a后，在添加一个b)。如果S中有两个字符ee，T中有两个字符ab的时候，我们是不是可以分解成：edit(1,1)=min(edit(0,1)+1, edit(1,0)+1, edit(0,0)+f(1,1)). 这样我们可以得到这样一些动态规划公式：               如果i=0且j=0        edit(0, 0)=1         如果i=0且j>0        edit(0, j )=edit(0, j-1)+1         如果i>0且j=0        edit( i, 0 )=edit(i-1, 0)+1         如果i>0且j>0        edit(i, j)=min(edit(i-1, j)+1, edit(i,j-1)+1, edit(i-1,j-1)+f(i , j) )   小注：edit(i,j)表示S中[0.... i]的子串 si 到T中[0....j]的子串t1的编辑距离。f(i,j)表示S中第i个字符s(i)转换到T中第j个字符s(j)所需要的操作次数，如果s(i)==s(j)，则不需要任何操作f(i, j)=0； 否则，需要替换操作，f(i, j)=1 。   这就是将长字符串间的编辑距离问题一步一步转换成短字符串间的编辑距离问题，直至只有1个字符的串间编辑距离为1。   编辑距离的实际应用        在信息检索领域的应用我们在文章开始的时候就提到了。另外，编辑距离在自然语言文本处理领域(NLP)中是计算字符串相似度的重要方法。一般而言，对于中文语句的相似度处理，我们很多时候都是将词作为一个基本操作单位，而不是字(字符)。   【串和序列处理 3】Trie Tree 串集合查找 http://hxraid.iteye.com/blog/618962 Trie 树， 又称字典树，单词查找树。它来源于retrieval(检索)中取中间四个字符构成(读音同try)。用于存储大量的字符串以便支持快速模式匹配。主要应用在信息检索领域。   Trie 有三种结构： 标准trie (standard trie)、压缩trie、后缀trie(suffix trie) 。 最后一种将在《字符串处理4：后缀树》中详细讲，这里只将前两种。   1. 标准Trie (standard trie) 标准 Trie树的结构 ： 所有含有公共前缀的字符串将挂在树中同一个结点下。实际上trie简明的存储了存在于串集合中的所有公共前缀。 假如有这样一个字符串集合X{bear,bell,bid,bull,buy,sell,stock,stop}。它的标准Trie树如下图：           上图（蓝色圆形结点为内部结点，红色方形结点为外部结点），我们可以很清楚的看到字符串集合X构造的Trie树结构。其中从根结点到红色方框叶子节点所经历的所有字符组成的串就是字符串集合X中的一个串。         注意这里有一个问题： 如果X集合中有一个串是另一个串的前缀呢？ 比如，X集合中加入串bi。那么上图的Trie树在绿色箭头所指的内部结点i 就应该也标记成红色方形结点。这样话，一棵树的枝干上将出现两个连续的叶子结点(这是不合常理的)。         也就是说字符串集合X中不存在一个串是另外一个串的前缀 。如何满足这个要求呢？我们可以在X中的每个串后面加入一个特殊字符$(这个字符将不会出现在字母表中)。这样，集合X{bear$、bell$、.... bi$、bid$}一定会满足这个要求。         总结：一个存储长度为n，来自大小为d的字母表中s个串的集合X的标准trie具有性质如下：       (1) 树中每个内部结点至多有d个子结点。       (2) 树有s个外部结点。       (3) 树的高度等于X中最长串的长度。       (4) 树中的结点数为O(n)。   标准 Trie树的查找        对于英文单词的查找，我们完全可以在内部结点中建立26个元素组成的指针数组。如果要查找a，只需要在内部节点的指针数组中找第0个指针即可(b=第1个指针，随机定位)。时间复杂度为O(1)。         查找过程：假如我们要在上面那棵Trie中查找字符串bull (b-u-l-l)。       (1) 在root结点中查找第('b'-'a'=1)号孩子指针，发现该指针不为空，则定位到第1号孩子结点处——b结点。       (2) 在b结点中查找第('u'-'a'=20)号孩子指针，发现该指针不为空，则定位到第20号孩子结点处——u结点。       (3) ... 一直查找到叶子结点出现特殊字符'$'位置，表示找到了bull字符串       如果在查找过程中终止于内部结点，则表示没有找到待查找字符串。         效率：对于有n个英文字母的串来说，在内部结点中定位指针所需要花费O(d)时间，d为字母表的大小，英文为26。由于在上面的算法中内部结点指针定位使用了数组随机存储方式，因此时间复杂度降为了O(1)。但是如果是中文字，下面在实际应用中会提到。因此我们在这里还是用O(d)。 查找成功的时候恰好走了一条从根结点到叶子结点的路径。因此时间复杂度为O(d*n)。       但是，当查找集合X中所有字符串两两都不共享前缀时，trie中出现最坏情况。除根之外，所有内部结点都自由一个子结点。此时的查找时间复杂度蜕化为O(d*(n^2))   中文词语的 标准 Trie树       由于中文的字远比英文的26个字母多的多。因此对于trie树的内部结点，不可能用一个26的数组来存储指针。如果每个结点都开辟几万个中国字的指针空间。估计内存要爆了，就连磁盘也消耗很大。         一般我们采取这样种措施：      (1) 以词语中相同的第一个字为根组成一棵树。这样的话，一个中文词汇的集合就可以构成一片Trie森林。这篇森林都存储在磁盘上。森林的root中的字和root所在磁盘的位置都记录在一张以Unicode码值排序的有序字表中。字表可以存放在内存里。     (2) 内部结点的指针用可变长数组存储。        特点：由于中文词语很少操作4个字的，因此Trie树的高度不长。查找的时间主要耗费在内部结点指针的查找。因此将这项指向字的指针按照字的Unicode码值排序，然后加载进内存以后通过二分查找能够提高效率。   标准Trie树的应用和优缺点      (1) 全字匹配：确定待查字串是否与集合的一个单词完全匹配。如上代码fullMatch()。      (2) 前缀匹配：查找集合中与以s为前缀的所有串。        注意：Trie树的结构并不适合用来查找子串。这一点和前面提到的PAT Tree以及后面专门要提到的Suffix Tree的作用有很大不同。         优点： 查找效率比与集合中的每一个字符串做匹配的效率要高很多。在o(m)时间内搜索一个长度为m的字符串s是否在字典里。       缺点：标准Trie的空间利用率不高，可能存在大量结点中只有一个子结点，这样的结点绝对是一种浪费。正是这个原因，才迅速推动了下面所讲的压缩trie的开发。   2. 压缩Trie (compressed trie)       压缩Trie类似于标准Trie，但它能保证trie中的每个内部结点至少有两个子节点(根结点除外)。通过把单子结点链压缩进叶子节点来执行这个规则。   压缩Trie的定义       冗余结点(redundant node)：如果T的一个非根内部结点v只有一个子结点，那么我们称v是冗余的。       冗余链(redundant link)：如上标准Trie图中，内部结点e只有一个内部子结点l，而l也只有一个叶子结点。那么e-l-l就构成了一条冗余链。       压缩(compressed)：对于冗余链 v1- v2- v3- ... -vn，我们可以用单边v1-vn来替代。         对上面标准Trie的图压缩之后，形成了Compressed Trie的字符表示图如下： 压缩Trie的性质和优势：      与标准Trie比较，压缩Trie的结点数与串的个数成正比了，而不是与串的总长度成正比。一棵存储来自大小为d的字母表中的s个串的结合T的压缩trie具有如下性质：        (1) T中的每个内部结点至少有两个子结点，至多有d个子结点。      (2) T有s个外部结点。      (3) T中的结点数为O(s)        存储空间从标准Trie的O(n)降低到压缩后的O(s)，其中n为集合T中总字符串长度，s为T中的字符串个数。   压缩Trie的压缩表示      上面的图是压缩Trie的字符串表示。相比标准Trie而言，确实少了不少结点。但是细心的读者会发现，叶子结点中的字符数量增加了，比如结点ell，那么这种压缩空间的效率当然会打折扣了。那么有什么好办法呢，这里我们介绍一种压缩表示方法。即把所有结点中的字符串用三元组的形式表示如下图：         其中三元组(i，j，k)表示S[i]的从第j个位置到第k个位置间的子串。比如(5,1,3,)表示S[5][1...3]=\"ell\"。         这种压缩表示的一个巨大的优点就是：无论结点需要存储多长的字串，全部都可以用一个三元组表示，而且三元组所占的空间是固定有限的。但是为了做到这一点，必须有一张辅助索引结构（如上图右侧s0—s7所示）。   【串和序列处理 4】Suffix Trie 子串匹配结构 Suffix Trie ： 又称后缀Trie或后缀树。它与Trie树的最大不同在于，后缀Trie的字符串集合是由指定字符串的后缀子串构成的。比如、完整字符串\"minimize\"的后缀子串组成的集合S分别如下：            s1=minimize          s2=inimize          s3=nimize          s4=imize          s5=mize          s6=ize          s7=ze          s8=e         然后把这些子串的公共前缀作为内部结点构成一棵\"minimize\"的后缀树，如图所示，其中上图是Trie树的字符表示，下图是压缩表示(详细见《Trie树 》)。可见Suffic Trie是一种很适合操作字符串子串的数据结构。 它和PAT tree在这一点上类似。     Suffix Trie的创建           标准Tire树的每一个内部结点只有一个字符，也就是说公共前缀每一次只找一个。而Suffix Trie的公共前缀可以是多个字符，因此在创建Suffix Trie的时候，每插入一个后缀子串，就可能对内部结点造成一次分类。下面我们我们看一种后缀树构造算法。以\"minimize\"为例：         当插入子串时，发现叶子结点中的关键字与子串有公共前缀，则需要将该叶子结点分裂。如上图第3到4步。否则，重新创建一个叶子结点来存放后缀，如上图第1到2步   Suffix Trie的子串查询        如果在后缀树T中查找子串P，我们需要这样的过程：      (1) 从根结点root出发，遍历所有的根的孩子结点：N1,N2,N3....      (2) 如果所有孩子结点中的关键字的第一个字符都和P的第一个字符不匹配，则没有这个子串，查找结束。      (3) 假如N3结点的关键字K3第一个字符与P的相同，则匹配K3和P。           若 K3.length>=P.length  并且K3.subString(0,P.length-1)=P，则匹配成功，否则匹配失败。           若 K3.length<=P.length  并且K3=P.subString(0, K3.length-1)，则将子串P1=P.subString(K3.length, P.length); 即取出P中排除K3之后的子串。然后P1以N3为根结点继续重复(1)~(3)的步骤。直到匹配完P1的所有字符，则匹配成功。否则匹配失败。         查询效率：很显然，在上面的算法中。匹配成功正好比较了P.length次字符。而定位结点的孩子指针，和Trie情况类似，假如字母表数量为d。则查询效率为O(d*m)，实际上，d是固定常数，如果使用Hash表直接定位，则d=1.       因此，后缀树查询子串P的时间复杂度为O(m)，其中m为P的长度。   Suffix Trie的应用         标准Trie树只适合前缀匹配和全字匹配，并不适合后缀和子串匹配。而后缀树在这方面则非常合适。         另外后缀树也可以进行前缀匹配。 如果模式串P是字符串S的前缀的话，那么从根结点出发遍历后缀树，一定能够寻找到一条路径完全匹配完P。比如上图： 模式串P=“mini”，主串S=\"minimize\"。P从根节点出发，首先匹配到结点mi，然后再匹配孩子结点nimize。直到P中所有的字符都找到为止。所以P是S的前缀。   【串和序列处理 5】KMP子串匹配算法 http://hxraid.iteye.com/category/83702 模式匹配： 在字符串S中，子串P的定位操作通常称做串的模式匹配。说白了，就是在一个字符串中寻找子串。在Suffix Trie和PAT tree中我们已经讨论过匹配子串的方法了。这里我们讨论一种线性匹配算法来寻找子串。   例： 我们要在S=\"ababcabcacbab\"中查找子串P=\"abcac\"。下图左侧是一种很普通的模式匹配算法 这种普通的模式匹配算法很简单，但时间复杂度是O(n*m)。其中n=S.length，m=T.length.  代价很高。难道真的要像第三趟到第四趟那样：好不容易匹配到S中的第7个位置，但由于不相同，则有回溯到第4个位置重新开始。   其实，每一次匹配过后，主串S中被匹配过的位置其实不用再匹配了。也就是上一趟匹配结果对下一趟有指导作用。我们用一个证明来说明这一点(假如主串S=s1 s2 .... sn ，模式串P=p1 p2 ... pm  )。   证明：当一趟匹配完成之后，我们发现si !=pj 。那么至少说明了模式串p1... p(j-1)是匹配成功的。也就是得到了： p1  p2  ...  p(j-1) = s(i-j+1)   s(i-j+2) .....  s(i-1) 。 比如第三趟 s7!=p5 => s2...s6=p1...p4 = \"abca\".          如果像上图左侧算法那样，从第三趟i=7回溯到第四趟i=4是有必要的话，那么也说第四趟可能完全匹配到了模式串P。好，我们现在就假设si != sj之后，i 回溯主串(i-j+1)的下一个位置上可以匹配成功模式串P，那么我们可以得到 p1 p2 ... p(j-1) =s(i-j+2)   s(i-j+3) ... s(i)。          合并下标蓝色的两个式子，我们可以得到：                    s(i-j+1)   s(i-j+2) .....  s(i-1)= s(i-j+2)   s(i-j+3) ...  s(i)          也就是：  s(i-j+1)= s(i-j+2)= s(i-j+3)= .... =s(i-1)=s(i)          我靠，主串S中全部的字符都一样，这种情况下才必须每一次都回到主串的下一个位置上重新开始。而只要有字符不同，就完全没有这个必要了。好了，基于这个证明成果，我们开始介绍KMP算法。     KMP 模式匹配 —— D.E.Knuth   V.R.Pratt和J.H.Morris同时发现的。因此人们称它为克努特-莫里斯-莫拉特操作（简称KMP算法）。KMP的优势在于当每一趟匹配结果中出现了不等情况时，主串并不需要回溯位置i (上面已经证明)，而只要 回溯模式串P即可 。也就是说，只需要在主串S的位置i 处重新与模式串的位置k进行比较(如上图右侧过程)。 那么这个 重新 需要定位的模式串k位置有什么要求呢，或者说我们怎么确定这个k呢。我们再用一个小小的证明来揭示( 还是假如主串S=s1 s2 .... sn ，模式串P=p1 p2 ... pm  )：   证明：当一趟匹配完成之后，我们发现si !=pj 。此时首先可以肯定的是k< j，因为模式串j 必须回溯。         如果 s1 与 pk 有重新比较的必要，那么模式串P前k-1个字符必须满足下列关系式：                             p1  p2  ...  p(k-1)=s(i-k+1)  s(i-k+2) ...  s(i-1)         此外，由于经过额一趟的匹配之后，已经可以得到“部分匹配”结果，主串S中i 位置的前k个字符一定等于模式串P中j 位置上的前k个字符：                            p(j-k+1)  p(j-k+2)  ...  p(j-1) = s(i-k+1)   s(i-k+2) .....  s(i-1) 。         合并两个蓝色的式子：                            p1  p2  ...  p(k-1) = p(j-k+1)   p(j-k+2)   ...   p(j-1)         我们发现了，位置k的值取决于模式串P自己必须满足上面这个红色的式子。     失效函数： 当在模式串P的第j 个位置上发生匹配不成功时，需要将模式串回溯到位置 k处的这样一个f(j)=k 的函数，就叫做失效函数。其中j 和k 的值必须满足p1  p2  ...  p(k-1) = p(j-k+1)   p(j-k+2)   ...   p(j-1)。 也就是说 pj 的前k-1个字符必须等于pk 的前k-1 个字符。因此，失效函数f(j)的定义如下：                                                    0       当j=1时                                    f(j) =      Max{k|1<k<j 且 p1...p(k-1)=p(j-k+1)...p(j-1) }                                                  1       不满足上面的情况   比如模式串P=“abcac”的每一个位置的失效函数如下：                                             j         1   2   3   4   5                                            P         a   b   c   a    c                                          k=f(j)     0   1   1   1   2   失效函数的算法      假如f(j)=k，则表明 p1...p(k-1) = p(j-k+1)...p(j-1)。这说明 pj 的前k-1个字符必须等于pk 的前k-1 个字符。也就是说p1...pk一定是p1...pj的一个后缀子串。因此我们可以把模式串P与自身做KMP算法的匹配来求解这个K值。算法如下：      (1) 若 pk=pj, 则p1...p(k-1)pk= p(j-k+1)...p(j-1) pj 。 表明f(j+1)=k+1      (2) 若 pk!=pj , 则可以把求f(j+1)看成以P为主串和匹配串的模式匹配问题。即 pk!=pj 则比较pj与p(f(k))，如果pj==p(f(k))，则f(j+1)=f(k)+1。否则继续比较下去直到f(k)=0为止。        失效函数算法的运行时间是o(m).   KMP算法效率         对于长度为n的文本串和长度为m的模式串进行模式匹配的运行时间为O(n+m) . 很显然，因为文本串在KMP算法中并不需要回溯，因此与模式串的比较次数为O(n)。但模式串要建立失效函数，所付出的代价是O(m)。因此总体的时间复杂度是O(m+n)。实际中，m要远小于n，因此近似可以认为KMP效率为O(n)。       但是KMP算法有种最坏的情况，当模式串P=\"aaaaa\"时，即每一个字符都一样的时候。则失效函数为：                                    j     1 2 3 4 5                                    P     a a a a a                                  f(j)    0 1 2 3 4       此时如果主串中的s[i]!=p[j]的时候，根据模式串P回溯j=f(j)的原则。s[i]需要从模式串P的最后一个字符一步一步回溯到第一个字符，每次都要比较一遍。这时的时间复杂度为O(m)。那么对于n个字符的S串而言，最差的时间复杂度就是O(n*m)了，退化成了蛮力匹配。        KMP和后缀树都可以用来匹配子串。因此我们这里与后缀树做一个比较，虽然后缀树在查找的过程中只需要大概O(m)的时间复杂度。对长度n的文本串建立后缀树最好的算法需要O(n)时间复杂度，因此后缀树大致也需要O(n+m)。   【串和序列处理 6】LCS最长公共子序列 http://blog.csdn.net/yysdsyl/article/details/4226630 http://hxraid.iteye.com/blog/622462 LCS：又称 最长公共子序列。 其中子序列(subsequence)的概念不同于串的子串。它是一个不一定连续但按顺序取自字符串X中的字符序列。 例如：串\"AAAG\"就是串“CGATAATTGAGA”的一个子序列。   字符串的相似性问题可以通过求解两个串间的最长公共子序列(LCS)来得到。 当然如果使用穷举算法列出串的所有子序列，一共有2^n种，而每个子序列是否是另外一个串的子序列又需要O(m)的时间复杂度，因此这个穷举的方法时间复杂度是O(m*(2^n))指数级别，效率相当的可怕。我们采用动态规划算法来解决这个问题。   动态规划算法解决最长公共子序列   假如我们有两个字符串：X=[0,1,2....n]  Y=[0,1,2...m]。我们定义L(i, j)为X[0...i]与Y[0...j]之间的最长公共子序列的长度。通过动态规划思想(复杂问题的最优解是子问题的最优解和子问题的重叠性质决定的)。我们考虑这样两种情况：   (1)  当X[i]=Y[j]时， L(i, j)=L(i-1, j-1)+1 。证明很简单。 (2)  当X[i]!=Y[j]时， 说明此事X[0...i]和Y[0...j]的最长公共子序列中绝对不可能同时含有X[i]和Y[j]。那么公共子序列可能以X[i]结尾，可能以Y[j]结尾，可以末尾都不含有X[i]或Y[j]。因此                                L(i, j)= MAX{L(i-1 , j), L(i, j-1)} 引进一个二维数组c[][]，用c[i][j]记录X[i]与Y[j] 的LCS 的长度，b[i][j]记录c[i][j]是通过哪一个子问题的值求得的，以决定搜索的方向。 我们是自底向上进行递推计算，那么在计算c[i,j]之前，c[i-1][j-1]，c[i-1][j]与c[i][j-1]均已计算出来。此时我们根据X[i] = Y[j]还是X[i] != Y[j]，就可以计算出c[i][j]。 问题的递归式写成：  回溯输出最长公共子序列过程： 为什么呢？因为通过表的回溯过程，从后向前重构了一个最长公共子序列。对于任何位置lcs[i][j]，确定是否X[i]=Y[j]。如果是，那么X[i]必是最长公共子序列的一个字符。如果否，那么移动到lcs[i,j-1]和lcs[i-1, j]之间的较大者。   动态规划方法LCS效率：   动态规划方法构造最长公共子序列需要O(m*n)的代价，另外，如果想要得到最长公共子序列，又需要O(m+n)的时间来读取csl[][]数组。尽管如此，其时间复杂度仍然比蛮力穷举的指数级别要强的多。   问题拓展：设A,B,C是三个长为n的字符串，它们取自同一常数大小的字母表。设计一个找出三个串的最长公共子串的O(n^3)的时间算法。 (来自《Algorithm Design》(中文版：算法分析与设计) - Chapter9 - 文本处理 - 创新题C-9.18）   【串和序列处理 7】LIS 最长递增子序列 http://hxraid.iteye.com/blog/624858 LIS： 给定一个字符串序列S={x0,x1,x2,...,x(n-1)}，找出其中的最长子序列，而且这个序列必须递增存在。   下面给出解决这个问题的几种方法：   (1) 转化为LCS问题         思想： 将原序列S递增排序成序列T，然后利用动态规划算法取得S与T的公共最长子序列。具体算法详见《LCS最长公共子序列 》。         效率： 这个方法排序最好的是时间复杂度是O(n*logn)，动态规划解决LCS的时间复杂度是O(n^2)。因此总体时间复杂度是O(n*logn)+O(n^2)=O(n^2) 级别。   (2) 分治策略         思想： 假设f(i)表示S中 x0 ... xi 子串的最长递增子序列的长度。则有如下递归：找到所有在xi之前，且值小于xi 的元素xj，即j<i 且 xj<xi。如果这样的元素存在，那么所有的xj 都有一个x0  ... xj 子串的最长递增子序列，其长度为f(j)。把其中最大的f(j)选出来，则                                         f(i)=Max(f(j))+1.  其中{j | j<i 且xj<xi} 如果这样的j不存在，则xi自身构成一个长度为1的递增子序列。 效率： 算法时间复杂度为O(n^2)级别。   (3) 动态规划算法         实际上这是一道很典型的动态规划问题。我们假设a[0]....a[i-1] 有一个最长递增子序列，其长度f(i-1)<=i, 且该最长递增子序列的最后一个元素为b。       那么对于a[0].... a[i] 而言，如果b<a[i]，那么f(i)=f(i-1)+1，且最长递增子序列的最后一个元素变成了a[i]。如果b>=a[i]，那么f(i)=f(i-1)。       上面的过程有一个难点：如果a[0]....a[i-1] 有多个最大长度为f(i-1)的递增子序列怎么办？需不需要所有长度等于f(i-1)的递增子序列的最后一个元素b0...bi全部存储起来，再一一和a[i]比较大小呢？如果是这样，那么整个算法与上面的分治策略将没有什么不同了？       事实上，并不需要怎么做。我们举个例子： a[]={1、2、5、3、7}       a[0] ... a[3] 的最大递增子序列有两个{1,2,5}和{1,2,3}，当增加a[4]的时候，如果a[4]>5，则两个子序列都需要增加a[4]；如果a[4]>3，则{1,2,3}+a[4]将必定成为新的最大子序列，而{1,2,5}不确定。因此我们看出，只要保存所有最大序列的最小的末尾元素即可。         因此我们设计一个如下的算法：其中b[k]用来表示最大子序列长度为k时的最小末尾元素。    该算法的时间复杂为O(N*logN)。       【串和序列处理 8】最长平台问题 http://hxraid.iteye.com/blog/655389 1、经典最长平台算法   已知一个已经从小到大排序的数组，这个数组中的一个平台(Plateau)就是连续的一串值相同的元素 ，并且这一串元素不能再延伸。例如，在 1,2,2,3,3,3,4,5,5,6中[1]、[2,2]、[3,3,3]、[4]、[5,5]、[6]都是平台。是编写一个程序，接受一个数组，把这个数组中最长的平台找出 来。在上面的例子中3,3,3就是该数组中最长的平台。 【说明】 这个程序十分简单，但是要编写好却不容易，因此在编写程序时应该考虑下面 几点： (1) 使用的变量越少越好； (2) 把数组的元素每一个都只查一次就得到结果； (3) 程序语句也要越少越好。 这个问题曾经困扰过David Gries 这位知名的计算机科学家。本题与解答取自David Gries 编写的有关程序设计的专著。 这是一个时间复杂度为O(n) 的经典算法，其代码十分简练。   另外，我自己也写了一个时间复杂度为O(n)的算法，原理就是找出所有平台分界位置，后一个位置减前一个位置(平台长度)的最大值。   2、改进的最长平台算法   上面O(n)的时间复杂度级别已经很不错了，但是如果n值特别大，那么仍然要比较n次才可以出结果，我们能不能降低比较次数呢？ 显然，这个问题是可以优化的。   我们再来回顾一下David Gries的经典算法(代码1的line: 9)，不管当前最长平台的长度为多少，每一次比较都是i++。难道每一次比较都是必须的吗？ 比如下面这个平台串：                                               pArr[]:    1  1  1  2  2  2  2   2  3  3    4   4    5    5                                               index:     0  1  2  3  4  5  6  7  8   9  10  11  12  13 分析： 当pArr[2]==pArr[0]的时候，最长平台长度已经增到了3。此时继续比较pArr[3]==pArr[0]发现不相等。那么说明pArr[3]已经开始了一个新的平台。依据经典算法，我们还要继续比较pArr[4]==pArr[1]，pArr[5]==pArr[2]。显然，这两个比较是不必要的，因为pArr[3]开始了新的平台，位置3之前的所有数据都不会和3之后的所有数据相等了。   根据上面的分析，我们很容易的想到可以跳跃一定的次数进行比较，跳跃多少呢？最简单的想法就是跳跃一个当前的longest(最长平台长度)。因为如果当前pArr[index]==pArr[index+longest]的话，说明当前平台长度比上一次的longest还要长，如果pArr[index]！=pArr[index+longest]的话，那么目前的平台长度绝对不会超过longest，也就没有必要再去比较小于longest的平台长度是多少了。   问题并没有想象的那么简单，跳跃longest长度之后，为了下一次还能够跳跃longest长度，有的时候是需要回溯一段距离的。 我们来看看下面的详细算法分析。   还是上面的例子，我们来一步一步的研究这个改进的算法。 (1)  首先计算第一个平台 \"1  1  1\" 得到了当前最长平台长度为longest=3，当前串位置index=3。 (2)  这时我们比较pArr[index]==pArr[index+longest](即比较pArr[3]<->pArr[6])。显然相等，那么longest++(即longest=4)。然后继续循环比较pArr[index]==pArr[index+longest]，直到不相等为止。此时index=8, longest=5. (3)  这一步非常重要，当前的index=8已近开始了一个新的平台, 而当前的longest=5。继续比较pArr[index]==pArr[index+longest](即比较pArr[8]<->pArr[13])，发现不相等。此时我们能不能继续从pArr[13]开始向后跳跃longest=5的长度呢。显然不对，因为pArr[13]并不是平台5的开始位置，pArr[12]=5。如果跳跃longest长度，后面的计算结果将全部错误。 此时，我们必须从13开始回头遍历，直到找到平台的其实位置pArr[12]，然后从12位置开始跳跃longest=5的长度才可以。   算法分析：时间复杂度仍然是O(n)级别 (注意：不要看到双重循环就认为是O(n^2)级别)。随然有的时候需要回溯到平台的起始位置，但改进之后的算法仍然降低了比较次数。 因为跳跃longest后最多需要回溯longest-1次(此时共比较longest次)。也就是最差情况下位置index每次跳跃之后都会回溯到index+1的位置上，因此最差情况下改进算法会蜕化成经典算法的比较次数n。   我们列举出一个最差情况的平台串：  1  1  2  2  3  3  4  4  5  5  6  6  7  7 ....   平均而言，1000个长度的随机初始化平台串，改进算法的比较次数在149次左右。而经典算法必须比较1000次。    ","title":"面试-字符串的处理总结"},{"content":"古希腊著名的数学家毕达哥拉斯（Pythagoras）曾给后人留下过这样一个观点：“万物皆数也”。如果他的观点是正确的，那么史上一切的存在——生命、艺术、商业……都是按照数学方式设计而成的。 “数”概念的形成与火的使用一样，是各个时期人类文明的标志。在追求效益的商业社会里，有人说数学的研究已经走到了尽头，也有人说数字太枯燥乏味，还有人说高等数学研究太抽象，与实际生活相去甚远，“数学专业无用论”，曾一度盛行高校。 时光流转，当公元纪年指向2010时，我们发现，数学这门最古老而又前卫的学科已经渗透到人们日常生活的方方面面。数学作为理学的三大组成部分之一，伴随着科技事业的发展，其内涵和外延越来越丰富。就像马克思所说：“一种科学只有在成功地运用数学时，才算达到了真正完善的地步”，各学科、行业都需要高层次数学人才来将其完善。在传统的数学中，许多人感到数学总是一大堆数字、符号、理论、法则，数学内容是枯燥乏味、抽象难懂的。因此，只有真正理解数学的内在价值、本质含义，才能体会到数学的魅力无穷，从而享受到数学学习的快乐。现在，各国政府警觉地意识到了“一个国家只有在成功地运用高等数学人才时，才算达到了真正完善的地步”，纷纷制定各种优惠政策培养引进高水平的数学硕士、博士。今天，数学专业研究生的入学门槛前，聚集了越来越多的学子。 著名数学史家克莱因说：“数学是一种精神，一种理性的精神”。是的，数学是一门系统性和实践性很强的学科，它是研究现实世界的空间形式和数量关系的一门科学。虽然作为学科课程的数学经过生活化、人为化，但亦不能离开科学的准则和结构。任何事物都有一定的量，因此，关于显示生活中的各种与形状和数量有关的问题，都可以作为数学的研究对象，但是，数学并不完全等同于知识的简单汇聚，数学知识是由许多具体的特殊的数学现象，用科学的思维方式进行不断分析和探索而得到的。它是一种对于人类理性思维的养成和发展有着特别意义的科学活动。 不久之前，我开始考虑数学。你也知道，到目前为止，我编写软件也有几年了。老实说，在我的工作当中，还没发现需要用到数学的。我要学习和掌握许多新东西，包括语言、框架、工具、流程、沟通技巧和没完没了、形形色色的各种库，对这些东西，数学真没啥用处。当然了，这不足为奇，我所做的工作，大部分都是某种类型的CRUD（编注：CRUD是Create、Read、Update和Delete的首字母缩写，这里指数据库相关的编程）。在互联网时代，我们多数开发人员所做的大部分工作都是如此。你是独立顾问？你恐怕主要就是在做网站。你在大公司上班？你主要还是在做网站。你是自由职业者？你主要还是在做网站。 终于有一天你对此有些厌倦了，就像我一样。别误会我，这可以是项有趣并有挑战性的工作，有机会解决问题，并和有趣的人一起互动，在工作时间做这个，我高兴。但在我个人时间中搭建更多的网站，就没太大意思了，于是你开始寻找一些更加有趣/酷/好玩的事情，像我一样。（所以，）有些人转移到前端和图形化技术——能有直观的反馈是非常诱人的。但我并不是其中一员（虽然我和别人一样都喜爱前端，但它真的不能让我兴奋。）这就是遇到一些搜索相关的问题后，我为什么决定深入挖掘的原因了。这把我带回到故事的一开始，因为一旦我抓到第一把充满搜索的铁铲，一旦我“撞到”数学时，我才真正意识到，我的技能恶化的程度。数学并不像骑自行车，长期不用就会忘记。   拓展视野 多对搜索的一些了解，让我接触到各种有趣的软件和计算机科学相关的事情和问题（包括机器学习、自然语言处理、算法分析等）。现在，在我接触的各方面，我都看到了数学，所以我更加强烈地感觉到自己技能缺乏。我已经意识到，如果你想利用计算机做又酷、又有趣的事，你需要达到一个像样的数学能力水平。除了上面说的三个，还有一些，如：密码学、游戏人工智能、压缩算法、遗传算法、3D图形算法等。在理解之后，如果你想要编写我们正讨论的那些库和工具，而不是仅仅使用它们（即：做一个“消费者”，而不是“生产者”），那你需要数学（知识）来理解这些领域背后的你能应用的理论。即便如果你不想编写任何库，当你真正理解事情的原理，你在构建软件时，它能给带来更多的成就感，绝非仅仅把它们连起来，就希望它们去做任何它们应该能做的。  虽然大多数开发人员会告诉你，他们在工作中从来不需要数学(就像我前面说的)，但是经过一番沉思后，我有了想法（突发灵感）：就是反马斯洛的锤子理论。你知道这个吧，当你有一把锤子，你会把一切看成是钉子。这是一个隐喻，也就是说人们乐于使用自己钟爱的工具，即便这并不是手中工作的最好工具。数学就是我们的一个相反的锤子。我们知道有这个锤子，但并不太子的如何使用。所以，当我们遇到问题，我们的锤子是解决问题的最佳工具时，我们却从未认真考虑过它。对我祖父而言，螺丝刀够用了；对我父亲来说，也很好；对我来说，同样如此。谁还需要锤子？数学的技巧在于，人们惧怕它，甚至大多数程序员，你认为我们不会怕，但我们确实怕。所以，我们把自己的话转变为可以自我实现的预言。这并不是我在工作中不需要数学，这只是我真的不知道，即便我知道，我也不知道如何使用它。所以我并没有使用它，当缺少某些东西时，如果你长期将就，不久后你甚至不会察觉它的缺失，所以对其需要更少了，这自我实现的预言。  针对思索接近我们内心世界，这里有一些的“粮食”——学习新技术。作为一名协作世界的开发人员，你努力成为一名通才型的专才（如果你不知道我在说什么，可以看看《我编程，我快乐》这本书）。你尽力在多数事情上做的体面，并在有些事情上做的优秀。但是你擅长什么？一般来说，人们会选择一两个框架或一门语言，然后与之相伴，这样是不错。但是要看到，框架和较小范围内的语言都有保质期。如果你要做一名Hibernate、Rails或Struts专家（使用 Struts的朋友现在真的应该担忧一下了），当新框架取代当前的框架时，你在几年内将不得不重新洗牌。所以，这也许是你真正的最好投资，但也可能不是。另一方面，数学是不会很快消逝的。在我们领域中所做的一切，都是建立在稳固的数学原理之上（算法和数据结构正是这样的例证），所以用在数学上的时间绝不是浪费，这不可辩论。再重复一次，总结起来就是：要真正理解东西，而不是死记硬背地使用。当涉及到计算机时，数学能有助你更深入地理解你所做的。事实上，正如Steve Yeager（著名技术博客，Google程序员）所言，作为程序员我们所做的事很像数学，只是我们甚至都没有意识到这一点。   什么/谁造就了与众不同？ 你不相信我？那请你想想：在我们的领域中，几乎人人普遍尊敬的卓越程序员同样也是大数学家。我是说像 Donald Knuth（图灵奖得主，计算机科学大师），Edsger W. Dijkstra（图灵奖得主，计算机科学大师），Noam Chomsky（著名语言学家），Peter Norvig（Google研究院总监）这一类人。但是这些家伙并非真正的开发人员，他们是计算机科学家，这能真正算数么？我再一次觉得，在我们写出的纯代码行数能达到这些人所写的十分之一之前，也许我们不应该再去讨论这些问题了。当然，不当科学家，你也能获得成功和名誉，大家都听过Gavin King（Hibernate创始人）或DHH（Ruby on Rails创始人）不是。这当然没错，但是“听说过”和普遍尊敬是不同的，这种差别就如同创建一个框架，和在你的领域中为人类知识所做出的全部重大推动两者之间的差别。（不要误会我，我尊重Gavin和David，他们所做的事，远远超过我，但是这不能影响我所说的事实）。所有的这些重要么？我不知道，可能不重要。但我认为还是应该说出来，因为我们都爱反思不是。  如今的世界正充满着数据，每日都增加更多的数据。而在以前，我们在相对少量的数据下享受工作。我们今日编写的软件必须高效处理海量数据。甚至在协作世界，这也是愈加明显的事实。这也就是说，你越来越不可能只是让程序跑起来，再来考虑如何运作，因为你要处理的数据量将困住你，除非你非常了解它。我的预测是：算法分析将对于普通程序员越来越重要了，并不是说以前不重要，但未来将越来越重要。如果想成为靠谱的算法设计专家，需要什么？你猜到了，是一些数学技能。  所以，我该怎么办呢？嗯，我已决定一点一点地建立或恢复我的数学技能，虽然还有大量的书要看，大量的代码要写，但我会尽力抽时间放在数学上，这就像锻炼，时不常的锻炼胜于无（再次引用Steve Yegge的话）。说到数学，我袖中当然还藏有一张王牌，它对我有利，但很幸运，有这个博客，我们都会受益的。   你在5年内的规划如何？ 那么，数学对所有事都有利么？这事先很难说，我对我现在的处境十分满意，或许你也如此，但这都和潜能有关系。如果你是协作世界的一名开发人员，你真的不需要数学。如果你乐于你的整个职业生涯是这样的：在工作时间中做企业CRUD应用，或在闲暇时间滑翔跳伞或极限水上滑板（或其他各种时髦的极客运动），也分配较多时间在Spring、Hibernate、Visual Studio或其它东西上。那些特殊的职位并没有真正限制你的潜力，你能变得极具价值，甚至可深入追求。但是如果你想为多样化的职业生涯而奋斗，想要有能力尝试几乎所有涉及代码的事，从信息检索到Linux内核。总之，如果你想成为一个开发人员、程序员和计算机科学家的完美组合，你必须确保你的数学技能达到标准。长话短说，如果你在数学方面有一定天赋，那在软件开发领域中所有的大门都是向你敞开的，如果没有，那你就安安心心地做CRUD型工作吧！","title":"数学是卓越开发者的必备技能！"},{"content":" 记得第一次了解中文分词算法是在 Google 黑板报 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进行研究，期间诞生了很多有意思的理论。     中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。     有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。     最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来／应聘”。     维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。     还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如“民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民”并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／服务”。       不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。     当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词数法上，我们就有了一种简明而强大的算法：     对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。     这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分：       他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ）       他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ）       他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ）     正确答案胜出。     需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。     算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。“鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概率，每个词出现的频率也是不同的。     何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。     以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10-9 ，但“有意／见／分歧”的得分只有 1.0×10-11 ，正确方案完胜。     这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话：       这／事／的确／定／不／下来     但是概率算法却会把这个句子分成：       这／事／的／确定／不／下来     原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。     其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。     于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w1 、 w2 ，统计在语料库中词语 w1 后面恰好是 w2 的概率 P(w1, w2) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w1) · P(w1, w2) · … · P(wn-1, wn) ，其中 w1, w2, …, wn 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。     至此，中文自动分词算是有了一个漂亮而实用的算法。         但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。     在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。     可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、“说”、“报道”、“参加”、“访问”、“表示”等动作词。     但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了；要是听说了有人居然敢叫“令计划”，估计直接就崩溃了。     还有那些恰好与上下文组合成词的人名，例如：      费孝通向人大常委会提交书面报告      邓颖超生前使用过的物品     这就是最考验分词算法的句子了。     相比之下，中国地名的用字就分散得多了，重庆就有一个叫做“犀牛屙屎”的地方。不过，中国地名委员会编写了《中华人民共和国地名录》，收录了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。外文人名和地名的用字非常集中，识别的正确率要高出许多。     真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。     最难识别的未登录词就是缩略语了。“高数”、“抵京”、“女单”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。     汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。     说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。","title":"漫话中文分词算法"},{"content":"           国庆前第三天，被叫到办公室。我觉得应该把“文言文翻译”这件事情做好。时间紧迫，11月份就要结题了，阳光明媚、悠闲自在的十一长假肯定是没了。            国庆前一天下午就没课了，我就开始准备如何入手。本来想按照和老师讨论出得方案做下去的，但是已知的词库太小了；分词问题我们是想了一种“偷懒”的方法；现在关键是解决一词多义问题；查找了好久的资料，机器翻译共有两种方法：基于规则的和基于统计的。而且解决一词多义问题的最好方法是统计模型，但是统计翻译模型必须掌握比较大的语料库，和样本才行。在search的过程中，偶尔发现有一个网站居然有很多古文的全文人工翻译，还有一部文言文词典。我马上用软件把整个网站下载下来，开始研究如何把网页中的资料导入到数据库中，这非常重要。因为只要有了数据库，做其实验来就轻松多了。             到了第二天，我开始写程序建数据库。一写就是7天，还没有成功得把网页中的资料导入到数据库中。这个和5月份的“智能体”大赛是同一种情况——写着写着就没法继续往下写了，因为代码太乱了，条件判断语句太多了，整个下来只有一个函数。并且不是所有web页面的数据都很规则，有些就是不同、出来捣乱。第到第4天的时候，最好的结果是：能把前50个页面的数据读进去，剩下的就得自己要么去改web页面，要么该程序。开始的时候我是该web页面，后来发现改的频率太高了，况且总共有3890个htm文件，根本就没法一个一个去改，改程序的话，我怕越改越乱。没办法，我知道我犯了一个同样地错误，必须改正，必须让自己写代码这件事是可持续的。以前做数据结构，算法题总感觉C语言比较好，c++太麻烦，现在感觉做稍微大一点的程序，如果不面向对象，代码将把你这个人淹没。              我开始反思，并寻找出路。我刚开始认为是我对网页、文本信息的处理技术不了解，我也深刻的知道到致我写不下去的原因和上次“智能体”比赛时发生的情况差不多。但我更希望做些“感觉高级”的事情，于是我先研究自然语言处理、网页信息抽取、人工智能等知识。现在我发现我一直在犯同一个错误：好高骛远、舍本求末。找得结果是我什么都没找到。网上的资料太过于简略，而且一般是：只讲思想，从来就没有代码，没有人知道这个到底能不能实现。现在，我不得不会回到起点，去探索怎样才能让写代码可持续。              我找了一个研究AI实现的小例子，发现简单的AI其实可以很简单，但是关键是要把情况分清楚。当初我搞“智能体”的时候，一上来就想吃成胖子，没有全盘可持续编程的细想，只看重眼前利益，结果是作茧自缚。我又找了一个决策树的示例代码下载下来研究，还有，把当初Eric的AI代码找出来一起研究，到底如何实现那些逻辑性很强，需要大量判断，有很多条件的智能，想看看他们是如何实现的。主要研究“决策树代码”和Eric的AI代码。                 研究了一天了。说实话，没看懂。因为数据流比较长，可能也是没耐心。但是看他们的代码让我感觉很舒服，很清楚，层次分明，整个代码设计得很周到。一般一个函数（方法）不会超过15行。而我为了“省事”、为了快速看到效果，有时候我的一个函数就是200行，自然，这种代码是没法发展下去的，光你去维护以前的堆砌的东西就够你受的了——难道这就是所谓的作茧自缚？                 显然，这些代码是经过设计的，没有整体框架的设计能力，是没法一写就能写2千行代码的。不要急，不要怕写子函数，也许一时没用，但是随着代码的增多，认识的加深，情况的复杂、逻辑量的增加，这些原来的子函数就可以为你分担负担，写代码就不会那么累。                 当然，要想让你的代码可持续发展，最重要的是先整体分类，把一些东西抽象出来，先占个坑，以后有东西就是这些已有东西的细化，那时自顶向下开发，代码自然就可持续了。遇到解决方案，最急的不是写代码、看效果，而是设计代码编写策略、实现结构、问题抽像、只有从整体上有把握，才能让代码编写可持续，才能最终写出好的代码，实现相应的功能。","title":"2011国庆总结——代码不是写出来的，是设计出来的"},{"content":"  1 互联网搜索 其实搜索的这个核心就是分词与PageRank算法，择日和大家讨论具体的实现，依靠PR算法，sogou 3.0的搜索结果相当不错，如果融入人工智能、数据挖掘自然语言理解等最新信息技术成果的搜索引擎，将会给用户带来了一种方便、易用、灵活的检索方式，为用户提供的是详实、准确、直接的信息。 这部分讨论几种特色搜索，概念检索是以概念为核心，这种方式一改以关键词核心的搜索模式，借助概念词典，通过与用户交互而获得用户要搜索的目的核心的一种搜索方式。提问式检索是以自然语言形式的问答式检索。引导式分类检索和聚类检索是对结果显示方式的重新布置方法，使方便用户的一种搜索方式。个性化搜索是利用个性化提取技术对不同的用户获得不同的结果的一种方法。 1.1 概念检索 检索并不是一次完成的，而是通过概念树进行交互过程中实现的，这种技术的前提是用户搜索并不是想查某个关键词，而是想查一个概念，或一件事。比如：用户输入绿茶，它是想查绿茶电影，通过概念树的描述结构获得，绿茶的不同树结点，相应有趣标的节点信息提示给用户，以便用户电击，用户的电击过程其实是与系统交互的过程，通过交互，获得用户所需要的信息。这既是所谓的语义检索。 语义检索是把信息检索与人工智能技术、自然语言处理技术相结合的检索，能够较好地解决传统全文检索中关键词词间关系模糊、检准率低的问题。传统的全文检索系统在网络信息检索中最大的问题就是检索模式单一，表面化，仅用单一的词或词的组合来对网络式结构的知识进行检索，缺乏对知识的理解和处理，其结果是返回的匹配网页数目过多，起不到真正的信息检索的作用。语义检索立足于对原文信息进行语义层次上的分析和理解，提取各种概念信息，并由此形成一个知识库，然后根据对用户提问的理解来检索知识库中相关的信息以提供直接的问答。它提供的不是知识的标识，而是知识的内容。语义检索与全文检索的根本差别在于具有一个巨大的知识库，而知识库本身就是一个概念的语义空间网络。建立知识库首先要构造语义网络，即利用人工智能、计算语言学等技术从大量网页中自动提炼概念、计算概念之间联系，包括确立概念等级体系、概念之间的语义映射关系及语义关系的推理原则等。检索系统可利用语义网络对网页进行语义标注，并形成索引库，智能型检索接口利用语义网络进行语义推理，主动分析用户提出的检索需求，向用户提出既符合用户检索需求又符合索引系统规范的检索关键词，实现语义检索和其他智能处理。 因此，建立在语义空间网络基础上的语义检索具有分析和理解自然语言的能力、记忆能力、智能人机接口，可以实现同义词扩展检索、语义蕴涵和外延扩展检索、语义相关扩展检索，突破了关键词检索单一模式的固有缺陷，实现概念层次上的词义扩展，能够在检索和获取信息过程中有针对性地提供有关解释、说明、范例、辅导、纠错等动态服务，对用户检索实现智能导航，充分保证用户的实际检索效率。但是，如何在一个分布式的、异构的信息环境下实现完全意义上的语义检索是目前所面临的最大挑战。 这项技术提供对用户输入词做一定的运算，获得关键词的中心意义，使得搜索变聪明了。用户只需在问题框里输入关键词，而不用在关键词之间加上一大堆诸如空格、加号、减号之类的东西，搜索引擎就会搞清楚想问的是什么。 1.2 直接提问式搜索 这是对传统检索放出的补充，是中文问答系统的一下应用。 搜索引擎允许用户可以按照平时说话的方式，通过问话框直接提问。把想问的问题输入问话框并提交，用户将会马上看到用户需要的内容。这是一种最直接、最快捷的获取答案的方式。比如，用户想知道北京今天的天气情况，就可以在问话框中直接输入“北京今天天气怎样”或“北京今天的天气好吗”等问法，然后按回车键或点击“提交”按钮，用户会看到有关天气的内容出现在窗口中，里面就是用户要了解的北京的天气情况。还可以询问诸如“《红楼梦》的作者是谁?”，“《大话西游》的导演是谁?”之类的问题。有的时候，用户在问题框中输入问题后得到的是一些相关问题而不是直接的网站，这些相关问题都是搜索引擎精心提炼、归纳的，用户可以从这些相关问题中找到他想确切表述的问题，还可以发现他感兴趣的其它问题；同时，相关问题还可以引导用户快速适应搜索引擎所提倡的提问方式，即用简捷的、有目的性的问句(即有意向的问题)来提问，以便其更快速的找到想要的内容。 1.3 引导式分类浏览 分类浏览是一种更为传统的搜索方法，但某些搜索引擎的分类浏览有别于传统的分类浏览方式，因为在浏览智能搜索引擎的分类时，用户看到的不是传统的网页内容，而是一个个相关问题，也就是说，如果用户不想输入问题，可以浏览相关类目，在问题列表中找出用户想问的问题。用户只要点击每个问题前的按钮，就能看到相应的内容了。（聚类方法见下节） 1.4 聚类搜索 搜索引擎的站点类聚方式和时限的选择等。站点类聚方式的选择，用户在搜索时经常遇到以下情况：一种是一个词频繁一个网站的许多网页中出现，如果搜索这个词的话，就会出现大量的属于同一网站的许多不同网页，如果只关心这个网站，就可以使用“站点类聚”方式，这样在结果中所有同一个网站的网页就会合并成一个键接；另一种情况是同样的内容在不同的网站频繁出现，那么可以选择“内容类聚”方式，这样在搜索结果中就会只显示出与具体内容相关的一个网页。也可以选择“站点类聚+内容类聚”方式，可同时达到上述两个目标。时限选择即选择网页生成时间，提供“任何时间的网页”、“‘三个月以内的网页”、“半年以内的网页”与“一年以内的网页”四个选择。 1.5 个性化搜索 个性化搜索的目的是做到：利用用户在搜索得10分钟，留住用户20分钟，如何做到这一点呢，首先，要在用户的搜索行为发生后对其个性进行分析，获得其感兴趣的信息，同时，在下一个电击行为开始时推送适合这个用户的感兴趣信息。获得用户行为的方法在个性化部分论述，推送方案在以后论述。这项技术同样可以影响结果排序。 1.5.1 个性化广告投放 见智能广告投放技术（2.5） 1.5.1.1 个性化查找服务 用户自动分类：目的是用户在聊天的过程中，自动被识别出自己兴趣爱好相似的同类别，这更增加了同类用户的亲和力。 这部分是面向又交友为目的人，开发的聊天服务功能产品，目前的聊天查找都是以系统内注册的信息获得的，可以通过聊天者的聊天内容的知识表示，获得用户的经历、历史等信息，最终达到聊天交友查找的目的。这种方式显然比直接查找来的更加准确。 2 个性化检索服务 如何面向用户需求，以最最方便的方式让用户获得所需的信息正是智能应用的任务。对于用户来说，没有输入任何信息而获得到自己所需要的这当然是最合适的方案。这部分应用即为个性化检索。 个性化服务的根本原因就是用户的情况千差万别，针对不同的用户可以进行如下分类：地域、性别、文化程度、行业、职业、上网时间等等。针对不同需求，比如上网目的：获取信息、学习、学术研究、休闲娱乐、情感需要、交友、获得各种免费资源、对外通讯、联络、炒股、网上购物、商务活动、追崇时尚、赶时髦、好奇等等也可以作不同的处理 2.1 个性化服务内容 个性化问题已经在研究领域里做了很多工作，这里将用大量的篇幅论述个性化问题的应用方法。 个性化是使事物具有个性,或者使其个性凸显。这里包含了两层含义，其一，个性是需要经过培养而逐步形成的。这个过程可以称之为使个体个性化的一个过程;其二，个体总是具有一定的个性的，让这种个性得到别人的了解、认可，并在一定的空间得以体现、展示，是每个个体都拥有的潜在需求，这个过程也称之为个性化的过程。那么什么是个性化信息，这个概念可从两个角度来分析。其一，个性化信息是指反映人类个性特性的一切信息，这些信息包括了这个个体的各种属性的描述;其二，个性化信息是指由人类个体特性所决定的其对信息的需求的一种信息组合，也就是由人类个性对信息需求的决定关系而产生的一系列对个体有用的信息。个性化信息服务既是一种个性化服务，又是一种信息服务。因此，可根据前面阐述的两种个性化过程和两类个性化信息，找到个性化信息服务的定位。个性化信息服务首先应该是能够满足用户的个体信息需求的一种服务，即根据用户提出的明确要求提供信息服务，或通过对用户个性、使用习惯的分析而主动地向用户提供其可能需要的信息服务。其次，个性化信息服务应能够根据用户的知识结构、心理倾向、信息需求和行为方式等来充分激励用户需求，促进用户有效检索和获取信息，促进用户对信息的有效利用并在此基础上进行知识创新。 2.2 个性化信息服务机制 如前所述，个性化信息服务就是将用户感兴趣的信息主动提供给用户，要实现这项服务，服务系统必须具备两方面的能力:一是构建个性化信息模型，即将个性化信息从全局信息空间中分离出来;二是构建用户信息模型，即跟踪用户行为，学习、记忆用户兴趣，通过描述用户的兴趣来建立个性化用户模型。此外，还需要有功能强大的网络信息搜索能力和友好的用户界面，它们即是构建个性化信息模型和用户模型的基础，也是实现个性化信息服务的保障。 2.3 个性化获取 目前，Internet网上站点从不同角度出发，采用了不同的用户个体特性采集和个性化信息传递方法，主要有以下四种方法。 2.3.1 界面定制法 界面定制个性化信息服务是根据用户需求对用户个体所需的系统界面、资源集合、检索工具与技术、检索利用服务过程、检索结果等进行定制。在理想情况下，系统应提供以下定制功能：1方便实用的定制工具；2用户能对系统提供的定制内容进行选择、引入或自定义定制内容、组合定制内容、调整定制结构；3查看定制效果；4对定制信息进行修改、存储和管理；5根据用户使用选择倾向和历史统计自动修改定制信息。从个性化程度讲，定制应能充分支持和展示个人的特色，允许用户积极参与界面的定制，通过填写表格，用户指示出自己的需求和选择，并依次决定所需的内容和对话界面的外观。一般系统为用户提供一个基础模板，用户根据需要从中选择或添加相关内容。用户定制的数据存放在服务器端数据库里，在用户登录时系统确定用户身份，调用相关定制信息，并利用定制信息匹配系统数据或过程，动态生成个性化的系统形态和系统行为。界面定制至少包括界面结构的定制和界面内容的定制。界面结构指对话界面的总体模块类别和布局形式，例如页面将包括哪些模块或服务，各模块的布局方式(上下或左右或层次)，有关图像、菜单等的位置设置，界面色彩设计等。而界面内容定制主要是对各个信息或服务模块的具体内容进行定制。这种形式定制的个性化信息服务效率依赖于用户定制的能力和动机，如用户不愿花费力气建立复杂、准确的个性化特性，这项服务就不会发挥作用。而且所定制的界而是静止不动的，不会随用户的需求而变化，除非用户能及时调整和更新。 2.3.2 点击流分析法(Click Stream Analysis) 点击流分析法是采集用户在站点上运动情况的方法，可用于跟踪记录访问过的链结点，包括用户的来源地点、浏览站点的路线和最终到达的目标，链结分析包括对点击过的链结的观察、它们在屏幕上的相关位置、用户在网页上停留的时间以及点击过的链接间的关系和最终结果(象用户是否发生了电子交易等)。通过对这些数据的有效分析，不但能够对网站的建设起到指导作用，增强网站的黏着度，而且也能够反映出企业在市场、销售、服务和财务等各个方面的状况。总之，对这些数据深层次分析能够使网站改善客户关系、培养顾客忠诚、增加网上销售和提高服务质量。在电子商务网站环境中，点击流分析的已经远远超出点击流的范围，而成为企业了解经营状况、了解客户行为的有效工具。点击流分析以WEB上的点击流数据为基础，利用OLAP、数据挖掘等技术满足电子商务企业的所有人员(市场、销售、工程与管理)的需求，不同的部门有不同的需求，通过对点击流不同数据的分析来达到不同的目的。 (1)网站点击分析。网站点击分析是点击流分析的一个重要部分，网站点击主要回答了市场开拓部门所关心的以下问题：1网站的哪个部分或产品吸引了最多的访问者；2网站的哪个部分导致的直接购买行为最多；3网站的哪个部分是多余的或者很少有访问的；4哪个部分是会话结束最多；5哪个部分进入的购买会话或其他类型的会话最多；通过对这些问题的充分了解，市场开拓部门在进行网上广告宣传时就会有比较好的倾向性，也利于有倾向的发展广告客户。 (2)点击状态分析。访问者的每次有效点击都是对网站服务器的一个资源请求，因此点击状态等同于请求状态。请求状态是指对于一个访问请求，服务器返回的结果类型。其中，对网站影响最大的请求结果是：资源错误，请求失败。作为网站来讲，实时监测请求的错误情况，找出故障原因并及时排除是至关重要的，点击状态分析提供实时的网站质量报告，给维护人员详细的故障信息做指导，评测故障恢复时间和影响范围。 (3)客户关系管理(CRM)。客户关系管理也是点击流分析的一个重要内容。点击流分析中的客户关系管理通常回答了以下几个方面的问题：1一个新用户的会话模式(Click Profile)是什么样的；2退出客户的会话模式通常是什么样的；3给网站带来利润的客户会话模式是什么样的；4取消服务的客户会话模式是什么样子的；5抱怨和投诉客户的会话模式是什么样的；6怎样可以吸引一个访问者成为网站的注册用户；通过对这些会话模式的分析，为市场、销售等部的CRM提供了数据分析基础。对客户群体进行划分，找到网站所关心的客户，如潜在客户、有价值客户和保持客户等。 2.3.3 协调过滤法(Collaborative Filtering) 协调过滤法把一个用户的偏好与其他用户的偏好进行比较，建立同偏好群体的描述。然后假定这个特定用户与这个同偏好群体的需求相同，对其需求内容进行推荐。协调过滤法的基本机制是：1登记团体人群的偏好；2用相似测度法，挑择偏好类似的子群组；3对子群组的偏好加权平均；4利用由此导出的偏好函数(preference function)为用户作出推荐。如相似测度法确实挑选了具有相似偏好的群体，那么，根据该群体确定的选项满足用户个体的几率就大。协调过滤法较典型的应用是推荐图书、光盘或电影。也可用于文献、服务或产品的挑选。 目前协调过滤法存在的主要瓶颈是用户偏好的收集。为增加可靠性，系统需要大量的人群(数千)在数量相对大(数十)的选项中做出选择。这需要众多人群的共同努力。避免这个问题的方法是采集隐含在用户行为中的偏好。例如，从网上书店订购图书的人们，在他们订购的过程中含蓄地表达着他们对图书的偏好。已经购买了与他们相同图书的顾客则有可能具有与他们相似的图书偏好。这种方法由Amazon网上书店采用，Amazon书店为每本书提供了相似人群购买的有关图书书目。 2.3.4 Cookies方法 Cookie不再是Internet上的新名词，但对个性化信息服务仍然有用。Cookie是由站点发送的小数据包并存储在浏览器一侧，因此作为用户的唯一标识可以在服务器一方(发送Cookie的站点)，重复使用。Cookie提供了追踪用户的方法。它给用户加标识，更确切他说是给用户的浏览器文件加标识，当再次访问发送Cookie的站点时，浏览器被当作唯一可以鉴别的实体。在浏览器中用作Cookie存储的用户信息在以后的访问中能够被发布站点调用，也能够在重复地访问中更新。它构成了到存储在服务器(提供方)方的指定文档信息的连接。Cookie可用于存储用户的其它信息——用户自己提供(填表格)的资料、最后一次访问的时间和其它的对话信息。 2.4 用户分类 用户分类是实现网站个性化的一项重要工作，我们可以根据需要进行多种分类，可以根据访问内容分出用户的各类兴趣爱好：如喜好足球的、喜好电脑技术的、喜好休闲娱乐的、喜好交友的：根据一般的上网时刻、访问量、上网的总时间、上网总次数等把用户分为一般网友、中级网友、高级网友等：甚至还可以根据访问内容确定用户大致所在的阶层，如白领、蓝领等。从以往相同喜好的用户的访问内容、访问顺序中进行学习，经过综合、筛选后将其推荐给当前用户。这些推荐信息与用户兴趣间的相关度很高，能很大程度上满足用户的需求。 2.4.1 通过日志获取兴趣 客户浏览信息被Web服务器自动收集，并保存在访问日志、引用日志和代理日志中有效地对这些Web日志进行定量分析，揭示用户兴趣路径等，不但可以为优化Web站点的拓扑结构提供参考，而且还可以为企业制定更有效的市场营销策略提供依据，使其及时改进决策，获得更大的竞争优势 目前，Web日志的挖掘研究主要集中在用户浏览模式的获取上，算法有最大向前序列法、参考长度法和树形拓扑结构法等它们先将日志中的用户浏览历史记录转换成一个浏览子序列集：最大向前序列法根据用户折返的特性形成若干浏览子序列；参考长度法根据用户在网页上停留的时间形成若干个浏览子序列；树形拓扑结构法则把整个日志当作浏览子序列然后利用关联规则法对浏览子序列进行挖掘找出频繁访问路径以上算法单纯地考虑了浏览频度，简单地认为用户的浏览频度就反应了用户的访问兴趣，这很不精确网页浏览频度的影响因素有很多，其中的页面放置位置和其它页面对该页面的链接都起着非常重要的作用所以有必要提出一种可正确挖掘用户浏览兴趣路径的算法本文就是从提出的支持 偏爱度的概念出发，给出了一种Web站点访问的矩阵表示模型，在此基础上挖掘用户浏览偏爱路径，然后进行了实验，提出需要进一步研究的问题。 2.4.2 个性化聚类 作为一种重要的知识发现方法，数据聚类主要用于发现属性间有用的模式和（或）关联（统称为知识），对于大规模数据集的探测性分析有着重要的作用。由于操作的对象是海量数据，所以其效率也就显得特别的重要。为此，近年来除了对聚类算法本身寻求改进以外，还对算法的并行化进行了大量的工作，以充分利用了当今计算机的综合计算能力，缩短聚类过程所需的时间。而且数据聚类的相应技术已经在图象处理，模式识别，信息融合等各个领域里都有重要的应用。发现知识的效率固然重要，但是发现后所得到的知识的“质量”也同样不可忽视。一般来说，一个知识发现系统是面向多用户的（或者说是面向多应用的）。如果一次聚类所得到的知识多于、或少于、甚至根本不是当前用户所需要的知识，那么这些知识对当前用户而言就是存在所谓的质量问题。例如，对于一群学生，教学工作者可能需要把他们分为一年级学生、二年级学生等；而对于同一群学生，医务工作者则可能把他们分为甲肝患者、乙肝患者等。显然，如果一个教学工作者去操作知识发现系统的时候，系统按患病情况进行聚类时，其结果是不能接受的。所以理想的情况是，不但要高效地产生所需要的知识，而且产生的知识要能够满足用户的实际需要，不存在与用户需要无关和多余的知识。当然，这些知识是在反映属性间内在客观联系的前提下满足用户需要的一种知识。该文通过聚类的方法获得仅满足用户需要的知识（不存在无关的知识）的过程，称为个性化聚类，相应的知识就称为个性化知识。 在当今信息的海洋中，研究个性化聚类，发现个性化知识，对于减少用户的工作量、提高工作效率和正确率、以及进一步推动信息处理系统向智能化和实用化方向发展有着极为重要的现实意义。 2.5 信息过滤 信息过滤是个大的概念，这里专门来讨论这个问题。 信息过滤技术基本分为两类：一种是基于内容的过滤(Content-based Filtering)；另一种是合作过滤(Collaborative Filtering)。在基于内容过滤模式中，每个用户假定是相互独立操作的。因此，过滤的结果只取决于用户信息需求模型(即用户模板Profiles)与信息源的匹配程度。在相关反馈的基础上，系统辅助维护用户模板。基于内容过滤的系统如Personal Web Personalizer等，它们利用资源与用户兴趣的相似性来过滤信息。它的优点是简单、有效，缺点是难以区分资源内容的品质和风格，且不能为用户发现新的感兴趣的信息。合作过滤的出发点在于任何人的兴趣不是孤立的，而是处于某个群体中。根据相同或相近兴趣的用户对相应信息做出的评价，向其它用户进行推荐。由于不依赖于内容，这种模式不仅适用于文本格式，也可以广泛应用于非文本介质的资源，如视频、音频等。协作过滤系统如：Web Watcher，Grou-pLens，Firefly，SELECT，LileMinds和Citeseer等，它们利用用户之间的相似性来过滤信息。基于合作过滤系统的优点是能为用户发现新的感兴趣的信息。但是，它也存在两个致命的缺点：其一是稀疏性问题，即在系统使用初期，由于系统资源还未获得足够多的评价，系统很难利用这些评价来发现相似的用户。另一缺点是系统可扩展性，即随着系统用户和信息资源的增多，系统的性能会下降。为了综合基本内容和合作过滤两种方式的优点，本文拟在数字图书馆中采用基于混合模式的信息过滤(Hybrid Filtering)模型。它建立面向个人的用户模板和面向合作的公共模板，抽取信息特征，作为可能的特征项，便于用户动态地修改模板；利用其它用户对文档的评价以及用户模板与文档的相似度来预测用户的接受程度，另外还考虑到推荐者的权威性和与用户兴趣的一致性。结合这两种过滤技术可以克服各自的一些缺点，从而提高信息过滤的性能。 3 热门新词提取 这是利用自然语言未登录词识别技术，获取网上最新的动态，同时，将获得的新词进行分类标注的新技术。这项功能有许多应用，可以很好的有用户的兴趣，结合用户的习惯及个性化，甚至可以改变用户的上网模式，使得用户具有相应的依赖性。   未登录词(unlisted/unknown words)是指词表未收因而机器不认识的词。词表应当有一定规模(一般是几万词)；极而言之，如果词表为空，文本中的每一个词都成了未登录词。另一方面，未登录词本质上是不可穷尽登录的：人名、地名几乎可以看成是无限的，新词也在不断地产生。 缺乏识别未登录词的能力，计算机就难以自动处理大规模语料。目前处理百万词级的语料时，通常的做法是先用机器分词，然后进行人工校对。但是人工校对费时费力，缺乏一致性，即使校对多次也难以保证没有错误。要处理几千万、几亿词语料，用这种办法是肯定行不通的，因此亟需开发一种免校对的自动分词系统。所谓“免校对”，不是说正确率要达到百分之百，而是指切分错误极少，至少能跟人工校对后的正确率相当。要做到这一点，就必须比较彻底地解决未登录词问题。 未登录词可以分为专名和非专名两大类。专名包括人名、地名等，非专名包括新词、简称、方言词语、文言词语、行业用词、港台用词等。目前关于未登录词识别的研究，集中在专名上，非专名的未登录词识别问题尚未引起足够的重视。如前面所举的例子，在真实文本中，非专名的未登录词占相当大的比例。词组式专名中含普通词语，如“蒙古人民共和国”“北京工业大学”。在自动分词中，能把其中的专名未登录词“蒙古”“北京”识别出来就可以了；至于组合处理，有时可能有困难。如，“美国大学”不是一个词组式专名，而“韩国大学”是一个词组式专名(例子引自张小衡，1997)。品牌名常常用普通词语，如“长虹牌彩电”“联想电脑”。对于这些品牌名，按照上面的建议(不作组合处理)，就不存在未登录词识别问题。但“康佳彩电”“富士牌彩卷”之类的品牌名，应该把其中的专名识别出来。如果把这些词组都整个儿看成未登录词，显然会加重自动分词的困难。 3.1 新词的自动获取 第一，先用最大概率法进行第一趟分词，识别已登录的多字词；第二，在“分词碎片”中寻找未登录词：分词碎片中任意字串皆为候选未登录词，利用局部统计和单字概率来计算其概率；但碎片中任意单字亦为候选单字词，故应同时计算每个单字的成词概率，与候选未登录词形成竞争，依概率来决定每个单字究竟是词还是未登录词的一部分，对分词碎片进行第二趟分词。最大概率法分词给出了最大概率法分词的算法，其基本思路是从各种可能的词串中，找出各词概率乘积最大的词串。其实，是自动分词的统计模型。 计算单字概率为了在分词碎片中识别未登录词，计算单字的以下几种概率：(1)单字概率Pz(c)，即每个单字在语料中的出现概率。Pz(c)应从极大规模语料库中统计得到，因为规模不够大时，许多非常用字难得出现，而未登录词(特别是专名)中往往包含非常用字。(2)单字词概率Pw(c)。语料规模当然也是越大越好，但实际上很难得到极大规模的、校对精确的分词语料。关键是如何计算Pw(c)。用单字词出现次数除以分词语料的总词次，得到的是该单字词的出现概率；用单字词的出现次数除以该单字的出现次数，得到的是该单字的相对成词概率。8孤立地考虑某单字是否成词的时候，应该用它的相对成词概率；但如果在上下文中考虑它是否成词时，应该用它的单词出现概率与相对成词概率的乘积。(3)单字非词概率Pf(c)=Pz(c)-Pw(c)分词碎片中的候选单字词连续出现时，用其转移概率：λ1Pw(ci)+λ2Pw(ci-1ci)，其中λ1+λ2=1，具体值可通过试验得到。这是强调慎重识别单字词。最大匹配法和最大概率法为了保证任意汉字串能在有限步骤内切分完毕，都是把词表中查不到的单字权且当作一个词。这是不能发现和识别未登录词的根本原因。未登录词是无限的，而单字是有限的，单字词更少。把单字和单字词作为识别未登录词的关键，就是想以有限来驾驭无限。当然，也不能说，只要把已登录的多字词和单字词都识别出来了，其余的连续单字都当作未登录词。事情不是这么简单，因为在分词碎片中，一个单字是不是词，也有赖于它周围的单字是不是待识别的未登录词的一部分。 3.2 新词的推送技术 通过为登陆处的识别，可以定期获得新词，这样就构成了新词词库。 新词仍让有个性化的问题，也就是说，对以某些用户来说，成为新词的，对于另一些用户来说，可能不是新词，这样。新词的概念有所变化，即个性化新词，新词，对于用户来说是新的兴趣点，对于新词的个性化推送不仅可以获得新的点击效益，对于用户来说，该功能也具有了新的市场，用户在使用过程中获益后，会对此能能产生依赖性的认可。 4 智能广告投放技术 4.1 智能广告投放的内涵 自从万维网WWW出现以来，Internet已经成为人们获取信息重要媒介，据统计中国网民人数已经达到上千万，这么多网民意味着巨大的潜在的广告市场许多商家将目光从传统的广告模式——电视、电台、报纸转向网络广告网站如何设置、播放什么样式的网络广告才能吸引网民而不是造成网民的反感已成为网站设计者需要考虑的问题传统网站常采用两愿营销的网络广告模式，这种模式在实际中存在安全性问题及随着时间的变化，使用者的兴趣与偏好可能会随着改变的问题基于上述的原因，为了能更准确地掌握使用者信息以提供使用者个性化的广告，本文提出一种新的网络广告模式根据用户的浏览行为和所浏览网页的内容，分析出用户的爱好与兴趣有针对地对个人播放个性化广告也既是每个用户看到的广告是根据其爱好不同而动态的改变，并不是现在常见的网站所呈现的固定不变的广告在此模式中，主要使用近来兴起的网页挖掘(WEB挖掘)技术获取用户爱好和兴趣网站使用者在网络上浏览行为存成日志文件，可利用网页挖掘中的网页使用挖掘方法，对于使用者浏览网页的行为找出其浏览网页的偏好，而后利用所得之结果，获取与使用者浏览网页特性有关联的网页其次，未来的网页将具有扩展标记语言中可自我描述与可自行定义标记的特性，对于这些网页内容可利用扩展标记语言挖掘方法进行挖掘，找出使用者对网页内容浏览的偏好与特性如此既能动态而且随时追踪使用者特性，结合广告数据库，建立一个个性化(个人化)的广告播放机制。 4.2 以网页挖掘为基础的个性化网络广告模式 从营销使用者必须主动提供信息，并且维护自己的偏好信息，使得当时间一长或使用者兴趣改变的时候，可能再也无法确切的掌握使用者的偏好，因此，可以利用网页挖掘，且能随时动态地更新使用者资料的个性化网络广告模式：(1)日志文件：此日志文件为一浏览的日志文件，它记录的内容包括记录使用者的浏览路径、时间、使用者ＩＰ地址、浏览器等使用者相关讯息(2)使用者信息数据库：记录使用者的基本资料与经过挖掘之后的使用者特性信息(3)广告数据库：储存广告主托播的各式广告，并包括广告的类别等信息(4)经预处理后的资料：将日志文件透过预处理的程序，进行过滤与筛选，以去掉不必要或错误的信息，得到想要的资料(5)协商代理程序：为一全自动化的代理程序，负责自动与广告主依广告的价格、时段、版面配置、类型、营销目标进行协调广告播放、并负责传送广告效果回报给广告主(6)广告媒合系统：获取使用者数据库并对广告数据库进行媒合，而后呈现经媒合之后的个性化广告(7)网页使用挖掘系统：读取浏览日志文件，进行网页使用挖掘，藉以了解使用者浏览的习惯来找出其浏览的特性，例如：网页浏览的频率高低，亦即使用者对各个网页的浏览频率，分析之后，并加以记录，以作为网页内容挖掘的基础(8)网页内容挖掘系统：在这个系统之中最主要的工作就是以网页挖掘的技术，对XML文件中的标记来做挖掘，并将挖掘之后的所得的资料存入使用者的数据库，以作为将来播放广告的依据 5 自然语言处理的基础资源 这里简单介绍一下实现以上技术所需要的自然语言处理的基础资源-综合型语言知识库。 语言知识库的规模和质量决定了自然语言处理系统的成败。经过18年的努力，北京大学计算语言学研究所已经积累了一系列颇具规模、质量上乘的语言数据资源：现代汉语语法信息词典，大规模基本标注语料库，现代汉语语义词典，中文概念词典，不同单位对齐的双语语料库，多个专业领域的术语库，现代汉语短语结构规则库，中国古代诗词语料库等等。本项研究将把这些语言数据资源集成为一个综合型的语言知识库。集成不同的语言数据资源时，必须克服它们之间的\"缝隙\"。规划中的综合型语言知识库除了有统一的友好的使用界面和方便的应用程序接口外，还将提供支持知识挖掘的工具软件，促使现有的语言数据资源从初级产品形式向深加工产品形式不断发展；提供多种形式的知识传播和信息服务机制，让综合型语言知识库为语言信息处理研究、语言学本体研究和语言教学提供全方位的、多层次的支持。综合型语言知识库将在应用中扩充规模，更新内容，提高质量，永葆活力。","title":"【转】想开发搜索的人必读的文章(www.lucene.com.cn)"},{"content":"针对社区网站的作弊行为大致可以分为两种，一种是个人在社区中发布的所谓的不和谐内容；另一种是利用社区的信息传播机制向社区内的其他用户进行的广告推广活动，社区进行SEO，信息诈骗，垃圾灌水等行为也属于此类。前者在西方国家算不算是作弊还不好说，但是广告贴，垃圾贴，却是任何社区都要认真对待的问题。单位时间内，垃圾信息的发布量要远大于正常用户，如果一个社区没有基本的反作弊系统，那么很可能出现正常信息被垃圾信息淹没的情况，这对于社区的危害是相当严重的。 不过在国内，这两种作弊究竟哪个更严重，还要根据具体情况而定。在某些敏感时期，如果不对第一类作弊行为严肃处理，很可能就要面对关站的风险，好多网站都曾上演过两会期间紧急删贴的好戏。所以需要一个强大的反作弊系统对这两种行为加以控制，将其对社区的影响减小到可以接受的范围。下面，笔者将之前工作中对于社区反作弊的一点心得体会写出来，与大家分享。我暂且管他们叫做社区反作弊的“七武器”。 武器一，多模式匹配算法 毋庸置疑，多模式匹配算法是反作弊工作最核心也是最基本的工具，几乎所有基于内容的反作弊系统都需要多模式匹配算法的支持。在我以前的文章中，也介绍过AC，WM这两个经典的多模式匹配算法。对于中小网站而言，一个好用的多模式匹配程序，往往就能够解决他们所遇到的绝大多数作弊问题。而在成熟的大型社区网站，无论是同步过滤，还是异步召回，没有多模式匹配算法的支持，基本的反作弊工作也将难以开展。 优秀的多模式匹配算法还可以大幅提高用户提交文本的分析效率，对于每天百万提交的网站而言，分析效率的提升所带来的收益是相当可观的。如果一个社区没有在多模式匹配算法方面做过投入，那么他们基本也就等同于没有搞过反作弊。但是遗憾的是，国内的好多社区网站在如此重要的环节，做的也不理想，好多网站目前还在用Trie来执行多模式匹配运算。 武器二，文本预处理 对于所谓的不和谐内容而言，其发布者往往认为自己是“人间正道”，一般不会对自己所发内容做太大的修饰。但是对于广告推广和水军作弊来说，为了要达到推广效果，内容中往往会夹带手机，QQ号，Email，URL，超链接这样的能够直接指向受益者的信息。作弊者往往会绞尽脑汁，修饰这些内容，利用程序和人的差异性，逃避反作弊系统的检测。比较典型的例子就是在数字书写中，用小写字母'l'代替数字1，用大写字母'O'代替数字0。这样的转义修饰五花八门，往往会让基于内容的反作弊系统一筹莫展。此时就必须要有足够强劲的文本预处理系统来降低修饰对于内容分析的影响。基本的预处理操作包括空格压缩，简繁体转换，全半角转换，特殊字符替换，火星文转换等等。不过即便是进行了上述处理，也只能解决一部分的内容修饰问题。从根本上说，预处理与内容修饰是“道高一尺，魔高一丈”的关系，作弊用户总会通过自己孜孜不倦的尝试，找到程序无法理解而又不影响人类阅读的书写方式（比如说经典的文本竖排）。但是这并不意味着我们就可以忽视文本预处理在反作弊工作中的重要性。有一点我们要牢记，推广类作弊是由利益链条支撑的，当作弊用户将自己的内容改的面目全非之时，同时也意味着其推广作用的下降。因为这既增加了其他用户阅读难度，又使得社区的其他系统难以识别。 武器三，文本相似度算法 对于广告推广类作弊，无论是机器发帖还是人肉发帖，其内容都会围绕一个原始模板。所以文本相似度计算在基于内容的反作弊中，作用不可估量。举个最简单的例子，一个人在自己的主页发布一些广告贴，单从一篇内容来分析，可能找不到什么明显的作弊迹象，但是很可能他一天之内所发的若干篇帖子的标题都是相似的。所以文本相似度算法适合于使用在对某一维度下的内容的相似度分析之中。广告推广由于其利益要求，内容相似的比例很高，或多或少都会被文本相似算法抓到，而正常用户所发布的内容，除非是转帖，否则各文章之间的相似性可以忽略不计。 从技术上讲，文本相似度分为两个层面，第一个层面，是基于编辑距离的文本相似度计算，这种算法是根据一段文字如何经过添加，删除，移动转化为另一段文字的操作步数，来计算两段文字的相似程度，运算的时间和空间复杂度都很高，对于长文本不太适用，而且没有考虑文本中意群的重要性，但是对于评论，标题这样的短文本往往能获得不错的效果。第二个层面涉及到自然语言处理的相关知识，需要在原始文本中切分出有意义的Term，然后对于两篇文章的Term集合，运算得出文本的相似程度。从复杂性上要高于前者，但是在处理长文本的方面有优势，而且更有可能从意义的角度识别出相似的文本族。 武器四，机器学习 从某种意义上说，基于机器学习的反作弊系统是内容反作弊的最高境界。机器学习理论用于反作弊，最早始于反垃圾邮件，国外学者在这方面做了大量的研究，Jonathan Zdziarski的《Ending Spam》一书是这方面的集大成者。机器学习反作弊的核心思路在于，对于用户文本的所有分词，有一些出现在作弊文本中的可能性要明显大于其他，我们对于一篇文本中，出现在作弊文本中可能性最大的一批分词，使用公式运算，来判断文本是否作弊。通俗的讲“减肥”，“发票”这样的词条出现在作弊内容中的可能性要远远大于“吃饭”，“喝水”，但是出现了“减肥”的内容又未见得真是一篇作弊文章，也许用户真的要征求好的减肥意见，而不是推广某某减肥药，但是真正要推广减肥药的帖子，其内容中肯定不止包含“减肥”这么一个高危词，同时肯定还会伴随其他的高危词条，甚至还可能有电话，QQ等联系方式，通过综合分析文本内容，就可以给出正确度较高的判断。 但是某一个分词是否是作弊高危词是动态变化的，某些之前的高危词由于不断打击，可能现在变成了普通词，而另一些却由普通词变成了高危词，这时就需要引入机器学习的概念来动态调整系统的知识库。机器学习系统利用一个训练集，由人先来告诉他，那些词条出现在文章中作弊的可能性较大，通过训练集规模的不断增长，不断修正自己的数据库中，各词条的作弊倾向性，实现类似于智能的反馈互动，动态应对文本的反作弊任务。 反作弊中有一种比较棘手的情况，就是对于突发的新词条作弊（比如某个新注册的品牌）往往响应不及时，机器学习系统在这方面具有一定的适应性，只要及时的用一个包含新词的训练集强化系统，机器学习系统就可以获得对新词的反作弊能力。当然社区反作弊相比于反垃圾邮件，要复杂的多，机器学习也不是解决社区作弊的万灵药，例如对于评论这样的短文本，机器学习系统的作弊识别效果就要逊于长文本。 武器五，正则表达式 既然是基于内容的反作弊，那就不得不提一下文本处理的利器——正则表达式，正则表达式是我们日常工作中处理文本的得力助手，但是在反作弊工作中的作用比较有限。之所如此，是因为现在的作弊群体，往往也拥有一定的技术背景，他们也知道正则表达式是一种基本的文本处理工具，所以会煞费苦心的对发布内容进行修饰，逃避正则表达式的检测。而如果将正则表达式的识别范围改的过于宽泛，又很可能发生误识别，将不是作弊特征的内容，识别为作弊，得不偿失。所以在实际工作中，正则表达式除了能识别一些富文本中的超链接之外，比较乏力。 前面所罗列的五种武器，都是基于内容的反作弊工具。不过，基于内容的反作弊只是整个反作弊系统的一个方面，这类工具也存在着一定的瓶颈。举个简单的例子，如果用户将要推广的内容写在一张jpg图片中，然后将这张图片发布到相册里来进行推广作弊，那么前面所提到的所有工具都会一筹莫展。也许你会说，还可以利用OCR工具识别图片中的文字信息，然后再使用上面的工具来处理。那么作弊者还可以对图片扭曲，错位，变色，直到你的OCR工具也望洋兴叹为止。以目前的技术水平，机器还达不到人的识别能力。不过实践中，除非迫不得已，一般很少会大批量的用图片作为作弊媒介。道理很简单，图片虽然可以愚弄社区的作弊系统，但是同样也会愚弄检索系统和推荐系统，目前还没有哪个搜索引擎为图片中的文字做索引，所以使用图片来进行广告推广，同时也意味着丧失了利用社区中的其他系统，扩大推广效果的机会。还是那句话，作弊者也是拥有技术背景的，他们也会考虑作弊的投入产出比问题，所以除非是恶意破坏，否则用图片来进行作弊推广实乃下策。下面我来介绍两个基于行为的反作弊武器。 武器六，频度控制 基本的频度控制，是防止灌水，维护社区秩序与安全的重要基石。同时，频度控制也是社区反作弊的重要手段。在实践中，频度控制之于反作弊系统的价值，不在于防止作弊内容的产生，而是提高作弊成本。举个例子，对于某社区网站，可能限制单个账户每天发布十个帖子，对于正常用户来说，很少情况会达到控制的上限。而对于作弊用户，为了达到推广的效果，必须要有量的支撑，所以如果一个作弊用户有一万个帐号储备的话，他可以在每天某个时刻，用自动化发帖工具，让这一万个帐号各发10篇广告贴，那么他今天10万贴的灌水推广任务就完成了。但是如果我们细化发帖的频度控制粒度，在每天10篇的基础上，同时限定每小时最多3篇，那么这个作弊用户就不得不分3次完成一天的作弊任务，这就变相提高了他的作弊成本。 当然，如此简单的情况他也可以想办法自动化完成，但是有一点需要注意，除非他在这个网站有内线，否则一个网站具体的频度控制策略如何，作弊用户一般是不知道的，他需要花费一定的精力去了解某个社区网站的频度控制策略。如果我们将社区的频度控制做的灵活多变，层次分明，就会让作弊者摸不着头脑，不得不放弃对网站频度控制策略的探查，转而使用暴力的人工作弊方式，这样作弊者的人工成本就会大大增加，人工成本的增加往往也意味着金钱投入的增加，当作弊者觉得他的投入和收益不成比例的时候，甚至会放弃对该社区的作弊活动转投他处。 武器七，行为挖掘 对于作弊用户来说，最难模仿的不是文章，图片，分享或者评论的内容——这些都可以通过伪装，在一定程度上骗过内容反作弊系统——而是正常用户的行为。在我所列的反作弊七武器之中，我将用户的行为挖掘列在首位，行为挖掘毫无争议，是社区反作弊最有效，最犀利的工具。 对于作弊用户来说，由于是利益驱动，所以一般不会花大精力维护少数帐号，而是会聚敛相当数量的帐号作为帐号池，在有作弊需求的时候，从其中取出一部分未被列入黑名单的帐号进行集中的作弊活动，所以作弊帐号在社区中的行为往往呈现一种突发性。比如，某帐号自申请之日两个月内都没有任何活动，在第三月却突然发帖500篇。 对于作弊者手头的成百上千个帐号，要让这些帐号每周都登陆几次，浏览一下别人的文章，图片，发几篇评论，分享几个热点视频，规律性还不能太强，对于他们来说将是极大的负担，不会有人愿意出这份力气。即使真有人把自己手头的几百个帐号都整得像模像样，那么此时这些帐号对于他们的价值也将大大提升，他们将不会再愿意冒帐号被封禁的风险，进行大规模的作弊活动。相较于内容反作弊费尽心思在用户发送的一两篇帖子中寻找蛛丝马迹，行为挖掘显然更为痛快。发帖时间间隔过短，发帖的时间过于规律，只发帖从不浏览别人的帖子，只浏览别人的帖子从不自己发帖，从不装扮自己的主页这样行为，往往可以和作弊帐号关联起来，如果某一个行为不能够说明问题，那么综合考虑一个帐号在几个维度的行为特征，识别其中的作弊者简直易如反掌。待识别出作弊帐号之后，再根据该帐号所发布内容，召回其他帐号的作弊贴，这才是社区反作弊的人间正道。 而且行为挖掘不但可以有效的反作弊，更可以实现对帐号的分类，挖掘潜在的优质用户和风险用户。对于风险帐号，我们严密监控，一旦发现其越雷池半步，马上进行严格控制，甚至封禁帐号，这使得我们的反作弊工作能够有所侧重；而对于优质帐号，我们可以放宽限制，提供更高的自由度，鼓励他们在社区做多动，培养潜在的活跃用户。 行为挖掘说起来如此美好，但是也是整个反作弊体系中最难以达到的境界。用户的行为涉及到一个社区的方方面面，一个用户在几个子系统中活动的综合体才能作为行为挖掘的原料。行为挖掘能不能搞，是检验一个社区网站架构的试金石，对于那些体系结构羸弱不堪的社区，很可能连最起码的数据，你都得不到，更遑论挖掘。国外成熟的社区网站我不是很清楚，但是国内的几乎所有社区网站，在这方面都没有什么建树。 备选武器 下面列出几个在社区反作弊中也很常用备选武器，这些武器的重要性不及前面七种，但是有的仍不可或缺。 验证码，验证码对于防止机器灌水，发挥着重要的作用。但是随着目前国内网站作弊的泛滥，验证码的地位变得比较尴尬。一方面，对于机器自动作弊，一定强度的验证码系统完全可以应付；但是对于人肉作弊，再复杂验证码系统也无济于事。现在国内的社区网站，在反作弊整体规划方面往往惜墨如金，但是对于验证码系统却下得起血本。各大网站的验证码系统，花样翻新，层出不穷，好多做到连人也很难识别，更有甚者搞出了中文验证码，问答验证码等等神奇。我想如果哪个作弊团伙能够开发一个智能程序来解决问答验证码，那么他就解决了图灵的人机问答问题。但是为什么各大网站的灌水现象还是十分严重呢。两方面原因，一方面，国内互联网公司的人员流动频繁，很可能在此过程中，泄露了验证码的核心机密，如果这个东西泄露了，那别说是问答验证码，就是数学题验证码也解决不了灌水的问题。另一方面，是作弊组织的分工细化，据说有人专门负责填写验证码，每天可以人肉填写几千到上万个验证码，正确率可以达到9成，填写一个可以有几分钱的收益。 帐号绑定，反作弊最为头痛的就是，作弊者往往拥有数量庞大的帐号，即使封禁其中的一部分，作弊者也觉得不痛不痒，正所谓“帐号在手，天下我有”。帐号绑定正是在这方面增加作弊用户帐号持有成本的一种手段。通过将社区帐号，与邮箱，手机等实体的绑定，限制用户的帐号拥有量，甚至有的帐号还需要定期激活。但是帐号绑定是一把双刃剑，他既增加了作弊者的帐号持有成本，也增加了正常用户的持有成本，对于刚创出点名堂的小社区，如果帐号的持有成本较高，很可能会为此失去很多的潜在用户，对于社区的快速成长是相当不利的。另一方面，如果作弊者在你的社区可以获得足够多的利益，那么即使你将帐号与身份证绑定，也同样无法阻止他们。 提示系统，在国内的社区网站活动，如果无意中输入了某些被屏蔽内容，就会得到一条提示——“你所发的信息中包含非法内容”，但是却不告诉我们到底是那些内容非法了。这样做的本意是防止作弊用户通过不断调整内容，规避反作弊系统。但是对于正常用户，这实际上是一种伤害，因为我们往往不知道，自己所发的内容中到底是那些出了问题，正常用户与作弊用户是有区别的，他们没有与反作弊系统长期斗争的经验，很难通过简单的调整使自己的内容得到通过，而对于作弊用户而言，这却是易如反掌的事情，如此提示极大的损害了正常用户的使用体验。同时，这也反映了一个更深层面的问题，那就是国内社区产品在反作弊方面的无能。这句提示语相当于告诉我们，如果你设法绕过了这一关，后面就没有进一步的过滤系统了，你的数据就可以驰骋社区，畅通无阻了，你作弊推广的目的达到了。实际上，对于一个拥有完备反作弊系统的社区，我们大可以把用户数据究竟是哪部分非法了告诉他，对于正常用户，这是一种善意的提示，而对于心怀叵测的用户，这也是一种自信的表现。“躲得过初一，躲不了十五”，绕过了这一关，总有一关你是绕不过去的。 人工审核，人工审核是反作弊工作的最后一道防线，虽然反作弊的目标是用机器尽量代替人的劳动，节约人力成本，但是必要的人工介入还是不可缺少的，特别是像帐号封禁这样的用户数据攸关的情境，必须要人来把关，在控制作弊的前提下，尽一切可能保证用户的帐号和数据安全。对于用户数据，我们要抱有足够的敬畏，宁可放过一千，决不能错杀一个，不到万不得已，慎言一个“删”字。 在百度一年的反作弊工作，留下更多的是无奈与遗憾，上面我写了这么多，读者可能会惊讶，这一年我所负责的虽然是反作弊工作，但是与前面的那些内容几乎毫无关系。对于一个成熟社区，反作弊可以起到锦上添花的效果，但是对于那些连基本系统都不完善的网站，你又怎么能指望让反作弊系统雪中送炭呢。如果不能卖钱，对于一个衣不遮体的人来说，裤衩背心肯定比项链更重要。在此即将离开之际，写下此文，来纪念我在百度的那段岁月，今后不管是不是还会继续从事反作弊工作，这段时光都将是我珍贵的回忆。","title":"社区反作弊工作的一点体会"},{"content":"中文分词技术 　　中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。 　　现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 　　1、基于字符串匹配的分词方法 　　这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下： 　　1）正向最大匹配法（由左到右的方向）； 　　2）逆向最大匹配法（由右到左的方向）； 　　3）最少切分（使每一句中切出的词数最小）。 　　还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。 　　一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。 　　对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。 　　2、基于理解的分词方法 　　这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。 　　3、基于统计的分词方法 　　从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 　　到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。笔者了解，海量科技的分词算法就采用“复方分词法”，所谓复方，相当于用中药中的复方概念，即用不同的药才综合起来去医治疾病，同样，对于中文词的识别，需要多种算法来处理不同的问题。 　　分词中的难题 　　有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。 　　1、歧义识别 　　歧义是指同样的一句话，可能有两种或者更多的切分方法。例如：表面的，因为“表面”和“面的”都是词，那么这个短语就可以分成“表面 的”和“表 面的”。这种称为交叉歧义。像这种交叉歧义十分常见，前面举的“和服”的例子，其实就是因为交叉歧义引起的错误。“化妆和服装”可以分成“化妆 和 服装”或者“化妆 和服 装”。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。 　　交叉歧义相对组合歧义来说是还算比较容易处理，组合歧义就必需根据整个句子来判断了。例如，在句子“这个门把手坏了”中，“把手”是个词，但在句子“请把手拿开”中，“把手”就不是一个词；在句子“将军任命了一名中将”中，“中将”是个词，但在句子“产量三年中将增长两倍”中，“中将”就不再是词。这些词计算机又如何去识别? 　　如果交叉歧义和组合歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：“乒乓球拍卖完了”，可以切分成“乒乓 球拍 卖 完 了”、也可切分成“乒乓球 拍卖 完 了”，如果没有上下文其他的句子，恐怕谁也不知道“拍卖”在这里算不算一个词。 　　2、新词识别 　　新词，专业术语称为未登录词。也就是那些在字典中都没有收录过，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解句子“王军虎去广州了”中，“王军虎”是个词，因为是一个人的名字，但要是让计算机去识别就困难了。如果把“王军虎”做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子“王军虎头虎脑的”中，“王军虎”还能不能算词？ 　　新词中除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。 　　中文分词的应用 　　目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。 　　分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有了。科研院校研究的技术，大部分不能很快产品化，而一个专业公司的力量毕竟有限，看来中文分词技术要想更好的服务于更多的产品，还有很长一段路。 备注：转载于 http://www.williamlong.info/archives/333.html","title":"中文分词技术"},{"content":"  IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI    实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个    领域就没几篇了，象machine learning、computer vision这么大的领域每次大概也    就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内    行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会    议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在    complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年    国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了    减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司    的\"IJCAI Inc.\"主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要    发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer    & Thoughts Award,   前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的    是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的    青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外,    IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member    去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约    这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找    3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可    以给到1+, 也可以给到1-或者2+, 总的来说我给它\"1\". 这是因为它的开法完全受    IJCAI制约: 每年开, 但如果这一年的IJCAI在北美举行, 那么就停开. 所以, 偶数年    里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些,    特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱    一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比    IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协    调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章    可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI    那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上    可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算    机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: \"一小群数    学家在开会\". 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便    提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出    论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的    会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题    目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识    别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. IEEE一直有个倾向, 要把    会办成\"盛会\", 历史上已经有些会被它从quality很好的会办成\"盛会\"了. CVPR搞不好    也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信    说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减    少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的    介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会    每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会,    会开完后第2年才出论文集, 也就是说, NIPS'05的论文集是06年出. 会议的名字是    \"Advances in Neural Information Processing Systems\", 所以, 与ICML\\ECML这样    的\"标准的\"机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有    一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以    不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael    Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很    强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给\"外    人\"的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说,    ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有    些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,    但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选    理事, 有资格提名的人包括近三年在ICML\\ECML\\COLT发过文章的人, NIPS则被排除在    外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of    Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI)    最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来    越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至    有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,    毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列    在tier-1里面, 一方面是名声远不及其他的top conference响亮, 另一方面是相对容易    被录用. 但现在它被列在tier-1应该是毫无疑问的事情了.    另: 参见sir和lucky的介绍. UAI (1-): 名字叫\"人工智能中的不确定性\", 涉及表示\\推理\\学习等很多方面, AUAI    (Association of UAI) 主办, 每年开.","title":"几个模式识别领域的期刊和会议"},{"content":"1.  软件工程 /软件测试 /软件设计 /软件管理   282 (13.96%) 2.  数据库 /数据挖掘 /搜索引擎技术   220 (10.89%) 3.  网络应用 /网络协议 / 网络理论 /网站设计 /无线网络 /移动通讯   322 (15.94%) 4.  计算机安全 /网络安全   194 (9.60%) 5.  电子商务   56 (2.77%) 6.  分布式计算 /并行计算   77 (3.81%) 7.  操作系统   94 (4.65%) 8.  计算机语言设计   41 (2.03%) 9.  人工智能 /自然语言处理 /知识工程   251 (12.43%) 10.  计算机游戏设计 /图形学 /人机交互 /计算机动画 / 多媒体   256 (12.67%) 11.  算法分析 /计算理论   82 (4.06%) 12.  计算机硬件 /体系结构 /嵌入式设计   145 (7.18%)","title":"大家喜欢计算机的哪些方向呢？"},{"content":"             使用信息熵来度量信息，单位bit。 信息量度量的理解： 例子：马上要举行世界杯赛了，大家都很关心谁会是冠军。假如我错过了看世界杯，赛后我问一个知道比赛结果的观众“哪支球队是冠军”？ 他不愿意直接告诉我， 而要让我猜，并且我每猜一次，他要收一元钱才肯告诉我是否猜对了，那么我需要付给他多少钱才能知道谁是冠军呢? 我可以把球队编上号，从 1 到 32， 然后提问：“冠军的球队在 1-16 号中吗?” 假如他告诉我猜对了， 我会接着问： “冠军在 1-8 号中吗?”假如他告诉我猜错了， 我自然知道冠军队在 9-16 中。 这样只需要五次， 我就能知道哪支球队是冠军。所以，谁是世界杯冠军这条消息的信息量只值五块钱。对于信息量的度量，香农不是用钱，而是用 “比特”（bit）。 假设有32个足球队，每个足球队夺冠的概率是pi，那么“哪个球队是世界冠军”的信息量是： 对于一个随机变量，熵的定义是： 互信息：对两个随机事件相关性的量化度量。 在机器翻译中，可以使用互信息来解决具有二义性的词的翻译。例如：bush既可以翻译为bush总统，又可以翻译为灌木丛。在不同的语境中如何翻译：在大量文本中，找到和总统bush一起出现的互信息最大的词：美国、国会、总统等，再找到和灌木丛一起出现的互信息最大的词：土壤、植物等。在翻译时，看上下文哪类的词较多，就翻译为哪个。 相对熵（交叉熵）： 衡量2个正函数是否相似，在自然语言处理中，可以使用交叉熵衡量2个常用词是否同义，两篇文章的内容是否相近。    ","title":"信息论的几个概念—读数学之美"},{"content":"    过去和现在流行的做法就是美国人开辟的自然语言数字化的第一条路径，其特点是数字计算机人机交互的过程在本质上是就低（机器语言）不就高（日常语言），其理由就是因为后者存在多义性而计算机不能直接处理这种多义造成的歧义性难题；而本人即中国人开辟的自然语言数字化的第二条路径，其特点是不在低（机器语言）、中（各种各样的程序语言）、高（日常语言）三者现有的关系上直接走自然语言数字化的第一条路径，而间接走自然语言数字化的第二条路径，其理由之一是因为计算机的数字化技术已经非常成熟，故对自然语言处理和理解的关键是不能从就低（机器语言）不就高（日常语言）人机交互过程中得到解决的，因此，务必在低端（机器语言）和高端（日常语言）的中间地带另觅它途——也就是在现有的中间过渡过程（各种各样的程序语言）这一看似直接其实更为曲折的第一条路径之外另寻新途——这就是一方面在低端（机器语言）建立{二进制数}和{十进制数}自动转换机制，另一方面又在高端（日常语言）建立{单音节字（即：言）}与{双音节乃至多音节的字组（即：语）}人机互助优化机制进而再与前述低端（机器语言）建立的{二进制数}和{十进制数}自动转换机制之间通过寻求一一对应的函数关系而建立双向的“同义并列、对应转换”关系，并以此实现自然语言数字化的第二条路径。这样一来，只有少数熟悉某程序语言的人们所走的第一条路径就恰似一条羊肠小道；而可以满足绝大多数至少精通一门日常语言的人们所走的第二条路径就恰似一条宽广大道。换一句话说，一般的标准化程序和初级的个性化程序几乎均可通过广义的双语信息处理的方式帮助用户理解计算机处理自然语言的进程。这样一来，不仅作为计算机这类第二脑智的特征对用户透明起来了，而且，由此也可通过人机协作过程的透明化来帮助人们较好地理解第一脑智、第二脑智、第三脑智三者之间的区别和联系，这也就可以帮助人们逐步揭开脑智的奥秘，至少可以在最基本的层次理解：第一脑智基于神经生理及心理{兴奋与抑制}和逻辑思维{真与假}、第二脑智基于机电物理{开与关}及数理{0和1}、第三脑智同时基于上述四个方面即生理及心理{兴奋与抑制}和逻辑思维{真与假}和物理{开与关}及数理{0和1}。而余下的进一步的理解就要从《间接计算模型和间接形式化方法》（《软件》2011第5期专家论坛的第一篇论文）图2所示的这一最简单的通用模型来获得顿悟，进而贯通图1-图6可获得较为全面的理解或进一步的顿悟。如果说这主要是从信息学基础研究获得的突破，那么，另一方面的理解则要从《语言的取值与置信》（被“语言与价值国际学术会议”正式录取并在会上宣读的英文论文——其中文稿先期公开发表之后‏排在《信息科学,经管科教》第15期首发论文第一篇）则主要是从语言学基础研究获得的突破（其中图1是从语言学研究对象做的纯形式研究，图2是从语言哲学对语言学研究对象做的纯形式研究的取值方法和置信态度所做的分析研究，其中揭示了一个涉及语言学科学研究对象的进一步形式化细分的问题，即：在索绪尔区分语言和言语之后已被学界公认了近百年的研究成果基础之上邹晓辉基于汉语的特点并从它可能与其他语种之间存在某种共性的角度进一步区分了言和语）。两篇文章相交的那一部分内容（尤其是那几幅示意图）正好说明语言与信息的交叉特征。   《软件》2011第5期：间接计算模型和间接形式化方法 http://www.ccomsoft.com/index.asp http://bbs.sciencenet.cn/home.php?mod=space&uid=94143&do=blog&id=482065 《信息科学,经管科教》2011第15期：语言的取值与置信 http://bbs.sciencenet.cn/home.php?mod=space&uid=94143&do=blog&quickforward=1&id=485268 http://blog.sina.com.cn/s/blog_65197d930100tvz6.html 《  计算机科学技术》20070817 ：探索汉语理论建设及中文信息处理的新路 http://www.paper.edu.cn/index.php/default/releasepaper/content/200708-255","title":"揭示自然语言数字化的两条路径（前者是美国人开辟的；后者是本人中国人开辟的）"},{"content":"  信息的飞速增长，使搜索引擎成为人们查找信息的首选工具，Google、百度、中国搜索等大型搜索引擎一直是人们讨论的话题。随着搜索市场价值的不断增加，越来越多的公司开发出自己的搜索引擎，阿里巴巴的商机搜索、8848的购物搜索等也陆续面世，自然，搜索引擎技术也成为技术人员关注的热点。 　　搜索引擎技术的研究，国外比中国要早近十年，从最早的Archie，到后来的Excite，以及altvista、overture、google等搜索引擎面世，搜索引擎发展至今，已经有十几年的历史，而国内开始研究搜索引擎是在上世纪末本世纪初。在许多领域，都是国外的产品和技术一统天下，特别是当某种技术在国外研究多年而国内才开始的情况下。例如操作系统、字处理软件、浏览器等等，但搜索引擎却是个例外。虽然在国外搜索引擎技术早就开始研究，但在国内还是陆续涌现出优秀的搜索引擎，像百度（http://www.baidu.com）等。目前在中文搜索引擎领域，国内的搜索引擎已经和国外的搜索引擎效果上相差不远。之所以能形成这样的局面，有一个重要的原因就在于中文和英文两种语言自身的书写方式不同，这其中对于计算机涉及的技术就是中文分词。 　　什么是中文分词 　　众所周知，英文是以词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。例如，英文句子I am a student，用中文则为：“我是一个学生”。计算机可以很简单通过空格知道student是一个单词，但是不能很容易明白“学”、“生”两个字合起来才表示一个词。把中文的汉字序列切分成有意义的词，就是中文分词，有些人也称为切词。我是一个学生，分词的结果是：我 是 一个 学生。 　　中文分词和搜索引擎 　　中文分词到底对搜索引擎有多大影响？对于搜索引擎来说，最重要的并不是找到所有结果，因为在上百亿的网页中找到所有结果没有太多的意义，没有人能看得完，最重要的是把最相关的结果排在最前面，这也称为相关度排序。中文分词的准确与否，常常直接影响到对搜索结果的相关度排序。笔者最近替朋友找一些关于日本和服的资料，在搜索引擎上输入“和服”，得到的结果就发现了很多问题。下面就以这个例子来说明分词对搜索结果的影响，在现有三个中文搜索引擎上做测试，测试方法是直接在Google（http://www.google.com）、百度（http://www.baidu.com）上以“和服”为关键词进行搜索： 　　在Google上输入“和服”搜索所有中文简体网页，总共结果507,000条，前20条结果中有14条与和服一点关系都没有。 　　在百度上输入“和服”搜索网页，总共结果为287,000条，前20条结果中有6条与和服一点关系都没有。 　　在中搜上输入“和服”搜索网页，总共结果为26,917条，前20条结果都是与和服相关的网页。 　　这次搜索引擎结果中的错误，就是由于分词的不准确所造成的。通过笔者的了解，Google的中文分词技术采用的是美国一家名叫Basis Technology（http://www.basistech.com）的公司提供的中文分词技术，百度使用的是自己公司开发的分词技术，中搜使用的是国内海量科技（http://www.hylanda.com）提供的分词技术。由此可见，中文分词的准确度，对搜索引擎结果相关性和准确性有相当大的关系。 　　中文分词技术 　　中文分词技术属于自然语言处理技术范畴，对于一句话，人可以通过自己的知识来明白哪些是词，哪些不是词，但如何让计算机也能理解？其处理过程就是分词算法。 　　现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 　　1、基于字符串匹配的分词方法 　　这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下： 　　1）正向最大匹配法（由左到右的方向）； 　　2）逆向最大匹配法（由右到左的方向）； 　　3）最少切分（使每一句中切出的词数最小）。 　　还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。 　　一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。 　　对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。 　　2、基于理解的分词方法 　　这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。 　　3、基于统计的分词方法 　　从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 　　到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。笔者了解，海量科技的分词算法就采用“复方分词法”，所谓复方，相当于用中药中的复方概念，即用不同的药才综合起来去医治疾病，同样，对于中文词的识别，需要多种算法来处理不同的问题。 　　分词中的难题 　　有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。 　　1、歧义识别 　　歧义是指同样的一句话，可能有两种或者更多的切分方法。例如：表面的，因为“表面”和“面的”都是词，那么这个短语就可以分成“表面 的”和“表 面的”。这种称为交叉歧义。像这种交叉歧义十分常见，前面举的“和服”的例子，其实就是因为交叉歧义引起的错误。“化妆和服装”可以分成“化妆 和 服装”或者“化妆 和服 装”。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。 　　交叉歧义相对组合歧义来说是还算比较容易处理，组合歧义就必需根据整个句子来判断了。例如，在句子“这个门把手坏了”中，“把手”是个词，但在句子“请把手拿开”中，“把手”就不是一个词；在句子“将军任命了一名中将”中，“中将”是个词，但在句子“产量三年中将增长两倍”中，“中将”就不再是词。这些词计算机又如何去识别? 　　如果交叉歧义和组合歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：“乒乓球拍卖完了”，可以切分成“乒乓 球拍 卖 完 了”、也可切分成“乒乓球 拍卖 完 了”，如果没有上下文其他的句子，恐怕谁也不知道“拍卖”在这里算不算一个词。 　　2、新词识别 　　新词，专业术语称为未登录词。也就是那些在字典中都没有收录过，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解句子“王军虎去广州了”中，“王军虎”是个词，因为是一个人的名字，但要是让计算机去识别就困难了。如果把“王军虎”做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子“王军虎头虎脑的”中，“王军虎”还能不能算词？ 　　新词中除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。 　　中文分词的应用 　　目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。 　　分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有了。科研院校研究的技术，大部分不能很快产品化，而一个专业公司的力量毕竟有限，看来中文分词技术要想更好的服务于更多的产品，还有很长一段路。","title":"搜索引擎技术揭密：中文分词技术"},{"content":"当初保研成功，一时激情澎湃，思绪万缕，记得当时冲动的跑去图书馆打开notepad就开始敲这篇帖子，真的是非常的上心了。现在看看这篇帖子，反响还不错，转发到这里，作为一个存档吧，主要是给自己看吧(呵呵，也很少人看我的博客-_-)。 这都12月份了，现在才发offer贴并且开些写一些保研的经验是因为我一直是EE小学生，我发不了帖子… 在这里记下一些东西，一方面是写给自己看的，毕竟保研这个事情真的是一波三折，自己也感慨万千。另一方面，当然希望自己的经历和体验能对其他人能有所帮助。 我本来觉得自己是没机会保研的。我的成绩不是非常好，仅能拿到报送本校的名额的。但是由于我觉得我目前所在学校的优势学科并非自己所学的学科，同时考虑自己以后的发展方向，不太适合留在本校读研。于是，八月份，我开始了我的考研之路。每天早晨六点多起床做一个小时的公交地铁到北化去上政治考研班，对保研没有抱过什么奢望。 但是大约八月中旬的时候却意外的得到消息，说自己以前所发的专利以及论文是可以尝试一下所谓的学术专长保研路线的。但我依然保持半信半疑的态度，在被各种不确定因素包围的情况下，我开始一边准备材料一边继续复习考研。 八月底到九月底大约一个月的时间是我现在回想起来都觉得不可思议的日子。在八月底九月初，我给各种学校的各大计算机方向专业的老师们发出了我的保研意向请求，但是回复我的少之又少。同时，由于我在那个时候根本就没有确定自己能否拿到那个飘渺不定的保研资格，自己对所做的一切也底气不足，尝尝觉得自己毫无胜算，前途黯淡。 不过生活就是这样，转折真的就在一瞬间。 9.19号晚上，我接到老师的电话告诉我拿到了那个学术专长保研的资格。 9.20号，这是清华北大递交保研材料的最后一天，在大部分人都觉得我没机会准备材料的时候，我花了一上午的时间把当初提交给北航的材料变成了提交给北大的材料，并在下午递交过去了。从早晨8点到下午3点，我跑了N个地方，盖了N个章，让N个领导给我签了字，我觉得那天是我脸皮最后的一天… 9.21号，由于我提交北大的推荐信还没凑够，他们给了我一个机会让我在21号补交，在去北大补交材料的路上，有人给我打电话，说他们是北大计算机智能系做自然语言处理方面的，说我填报的志愿(信息挖掘)和他们的研究方向是非常相似的，如果愿意的话来面试，不然就刷掉我等别的方向的老师与我联系。我觉得这种问题让我别无选择啊T_T，难道我会让到手的鸭子飞了等下一只飞过来么。 9.24号，机考，考得那个悲惨… 9.25号，面试，在那么惨的机考的前提下，我觉得我没戏了…可我还是硬着头皮去了，可这就是那最戏剧性的一天。我的面试过程很顺利，大三我做了很多乌七八糟的项目，有论文有专利，老师们问我我一些项目中的细节，他们对我的回答似乎很满意，然后就开始介绍他们的系所，聊了一会儿就告诉我在外面等一会儿，马上就出结果。而最终的结果就像我在面试刚刚结束后的预期是一样的，我被录取了。 ———————————————————————————————————— 说了那么多自己的经历，还是说点真正对大家有用的自己的看法吧。 这里要郑重说明一下，我是一个理工科的学生，我本科的专业是计算机科学，只能说我的感受是拘泥于理工科方面的，甚至是拘泥于单纯的信息类方向的，至于文科方面、经济学方面是怎样的情况我不敢妄加推断。 首先，最主要的，你的材料一定要反映出你的能力，材料的形式不需要太拘泥于形式，个人的自述也不用非要找一个很规范的模版。但是事先你肯定是不太懂应该怎么去写的，借鉴别人的内容当然必不可少。 从申请学校的正规材料中可以明显的表现出他们对一个优秀学生的定义是什么：你的成绩怎么样，你本科期间得过什么荣誉，你有什么样的科研经历，你的在科研方面取得了什么成果。你能把那个表格中的内容都填满么？如果不能，就快快去做些帮你填满它们事情吧~ 面试什么的并不可怕，别的不知道，起码我所经历的面试并不是非常残酷，真正残酷的应该在于材料的海选，如果你有幸得到面试的机会那么你的胜算就非常大了。而且，从另外几个我认识的报送到清华北大的同学那里了解到他们的情况和我是类似的。 不要用别人的情况来判定自己，每个人的情况都不一样，他有的你没有，可你有的他也为必有。不要觉得看了别人的经历发现自己不具备他的条件而沮丧甚至放弃。 永远不要“自杀”，脸皮要厚起来，不要觉得自己不行就不申请了，不要觉得北大不会要你什么的，对自己一定要有一个客观理性的评判。 充分的学习与积累永远都是必要条件。在青年文摘上看到一篇文章，笔者的经历告诉他，其实那些目的性极强的行为往往得不到预期的结果，因为对于所有人来说，未来永远都是个未知数。但我还是觉得你现在所做的事情可能在具体的内容上不能完全决定你的未来，但是它一定会给你带来一个趋势、一个方向。凡是预则立，不预则废。而准备并只是说在申请的时候要好好的写写材料准备准备面试而已。什么成就都不是三五天就搞出来的。论文不是一天就写出来的，专利不是一天就做出来的，好成绩也不是一天就考出来的，甚至不论你到底最后要不要去保研，只要时间没有荒废，只要每天都有在学习、在做一些事情，那你的结果就一定不差的，天道酬勤，永远要相信这一点。 还有的就是做事情要主动，要多打听消息，很多机遇错过是因为你根本就不知道它曾经有过。多联络老师，不要因为一次被拒就沮丧，其实你碰到合适的老师的同时也需要老师碰到你这个合适的学生。 最后真心感谢那些曾经帮助过我的人，保研这个事情真的让我是大动干戈了，能骚扰的人我都骚扰了一遍了，谢谢你们的帮助 这里又有对这篇文章的新的感受。 最重要的一点，做什么事情一定要凭自己的爱好，有一份热情，有一份冲动。很多事情其实没有一个明确的终了的，一般都是”日臻完善”。你只要肯投入就一定有收获，很多事情并不是我们水平不够而是不上心，没有投入更多罢了。只有你自己非常喜欢的事情你才能天天去对着它，天天想办法让它更好。不然，事情变成了负担，开始磨洋工那就没有什么发展了。写这篇文章的时候就是很冲动的想去写并反复的审查修改才能很好的表达自己的观点，才能让自己回过头来觉得还不错~","title":"我在eeban的保研经历贴"},{"content":"推荐FudanNLP，这是一个复旦大学计算机学院开发的开源中文自然语言处理（NLP）工具包 Fudan NLP里包含中文分词、关键词抽取、命名实体识别、词性标注、时间词抽取、语法分析等功能，对搜索引擎、文本分析等极为有价值。 开源项目地址为： http://code.google.com/p/fudannlp/   DEMO地址为： http://jkx.fudan.edu.cn/nlp   svn： svn checkout http://fudannlp.googlecode.com/svn/trunk/fudannlp-read-only 软件包下载地址： http://code.google.com/p/fudannlp/downloads/list 系统截图如下，分词和抽取的效果很不错","title":"开源中文分词FudanNLP"},{"content":"     转载自刘挺老师的博客：http://blog.sina.com.cn/s/blog_4cbec5e90100hd79.html     近来，互联网领域概念纷飞，云计算(Cloud Computing)、服务计算(Service Computing)、物联网(Internet of Things)、CPS(Cyber-Physical System)、社会计算(Social Computing)、智慧地球(Smart Earth)等等，看的人眼花缭乱。     我不认为上述概念是企业界的炒作，无风不起浪，这些新名词的频繁出现预示着一次新的互联网革命的到来。作为信息检索和自然语言处理领域的研究者，我像一只焦躁不安的海鸟，预感到大潮的来临，迫切地想知道这次狂潮会给我脚下的这片岛屿带来什么。     经过多日的苦思冥想，我确认自己在恍恍惚惚中悟出了一点儿道理，搜肠刮肚，找不到一个合适的词语来表达，且借时下有点儿时髦，却又不甚火爆的“人本计算”来阐述我的想法吧。         所为人本计算，英文可以翻译为Human-based(centered) Computing，简写为HC，含义是“基于人、以人为中心的计算”，一看便知包括两个方面：“基于人的计算（以人为本源）”(Human-Based Computation，HBC)和“以人为中心的计算（以人为本位）”(Human-Centered Computing，HCC)。我们先说基于人的计算。         知识获取是一切智能系统的关键，让我们从这个角度回顾一下语言技术的发展。第一代语言技术是专家系统式的，其知识获取形式是语言学家的语言学专业知识加上计算机专家的算法知识，这些知识是封闭的，有限的，以封闭而有限的知识面对开放而无限的语言现象，当然很快就败下阵来。第二代语言技术是基于和面向大规模真实文本的，广泛采用统计方法，语言学家插不上手，知识获取的来源是大规模原始语料和小规模经过标注人员手工标注的深加工语料。真实文本蕴含着用户提供的知识，但不经标注的原始纯文本只能提供很浅的不精确的知识，而组织一些学生对语料进行标注的工作每每令研究者备尝艰苦，几万句的树库搞搞研究尚可，面向不同领域的实际应用就差得远了。第三代语言技术就是人本计算，即基于人的计算，有大规模用户参与的计算，下面开始阐述。     当我们把互联网理解为一群机器的网络时，我们把用户排斥在外了，而Web 2.0时代最大的特点就是用户的参与。我们搞自然语言处理的人，最喜欢纯文本，处理网页时第一个动作就是提取正文，费好大的气力把我们最亲切的纯文本挖出来，各种tag标记全都去掉。这就好比日本兵拼刺刀，一定要先推掉子弹，这样的做法是自己难为自己，或者说是蒙上眼睛不去看新形势下一切对我有利的条件，仍按老办法行事。当我们换一个观念，把互联网上每台电脑背后的用户也算作互联网的一部分时，我们惊讶地发现，陈旧的方法损失掉了太多有价值的信息。     这些有价值的信息都是人提供的，包括：格式、链接、网络结构、用户行为、社会化标签、发帖时间、网址等。网页格式的解析最让人感觉枯燥乏味，但其实格式是信息发出者向信息接收者主动透漏的元数据，比如专名加黑，专名是超链接，这可以有效地帮助我们识别命名实体。链接上的文字是对其指向的那个页面的描述。社会化标签就更有用了，比如自然语言处理领域非常棘手的“别名、简称”问题，以前几乎是一筹莫展，现在有了新的解决方案，可以从不同用户对同一篇博文或同一张照片给出的不同标签词中发现别名和简称。至于用户的行为则向我们展示出更多的蛛丝马迹，用户输入一个查询，继而点击了搜索结果中的某个网页，则我们可以认为这个查询与这个网页具有相关性。用户查询连续发出的一组相关查询称为Session，我们看到当前搜索引擎提供的相关查询服务正是利用Session实现的，而不是人工编制的同义词词典。通过发现兴趣相似的用户，当用户浏览新闻时，可以看到其他相似用户看过的新闻。PageRank，浏览数、回帖数等等都是页面价值和热点话题计算的重要依据。总之，除了文本内容以外，互联网上围绕内容透露着非常丰富的信息，这为语言技术提供了新的知识源泉。     上面描述的还只是用户在不经意间提供的信息描述，除此之外，互联网上已经涌现出更多人人协同工作的模式。最典型的是维基（百科）、知道，这些互动产品，用户的动力在于自我表现，他们在自我表现的过程中挖掘出丰富的常识知识和行业知识。人工智能最缺少的就是常识知识，从Wiki这种半结构化的文本中获取常识知识，比直接由人工编写常识知识库要省力得多。此外，搜狗拼音输入法是一个非常好的例子，借助网友贡献的词汇，使输入的速度和便捷性迈上了一个新的台阶。     要让人更多地贡献知识，需要对他们进行激励，除了激发用户的自我表现欲外，游戏是非常重要的一种激励手段。CMU的Luis Von Ahn发明了“人计算”(Human Computation)，这种思想的核心是变“人让机器算”为“机器让人算”，通过设计各种游戏，让用户在娱乐中为机器贡献标注数据，比如给两个互不认识的网友同时看一张图片，如果两人给出了相同的关键词，就都可以得分，而这个关键词可以作为该图片的正确标注保存起来。Luis关于人计算的论文在Science上发表，这让我们更认清了到底什么叫“创新”，创新不是一定要把简单的问题复杂化，只要是巧思独运，切实解决问题，就是创新。     从用户那里获取知识，还有一个重要的特点，那就是Lifelong的学习。以往我们要在训练阶段把机器的学习任务完成，在使用阶段机器不再进化。而在基于人的计算中，用户一边用系统，一边教系统，以个性化新闻推送为例，用户用得越多，系统就越了解用户，变得越聪明。       再来说说人本计算的另一个侧面“以人为中心的计算(HCC)”。“基于人的计算(HBC)”谈的是从人（用户）那里获取知识的问题，而HCC谈的是面向人的应用模式设计问题。     一直以来，我们有一个误区，就是要把“机器智能”推向极致，把人的知识、人的智能赋予机器，让机器代替人脑去工作，人可以“垂手而天下治”。但反思多年来能够走向应用的智能技术，却发现他们无一例外不是以人机交互的方式在开展工作，而全自动的智能技术却始终无法走向市场。     这样的例子很多。全自动机器翻译在帮人写英文、翻译英文上用不起来，在浏览英文新闻时可以用，但很少有人用，相反辅助翻译，包括翻译记忆却产生了很大的市场价值。全自动的开放域问答系统已经有10年左右的时间，没有见到独立的应用，问答和检索的区别在于，检索是找出一些参考文献来，对不对由用户来判断，用户觉得检索结果都不满意，还可以修改查询再次检索，所以检索是交互式的，是人机协同的信息查找过程，而问答要解决问题，返还给用户的答案需要全面准确而没有冗余，这就难了，用户不看信息出处，无法判定答案是否正确。问答系统的准确率无法达到100%，所以不提供交互方式，问答系统就永远无法走向实用。因此，基于FAQ的问答是一条出路，而受限域问答不管受限到多小的领域也没法用。还有一个是听写机（全自动语音识别），到现在也没有看到应用的曙光，相反，拼音输入法是成功的，因为人可以对拼汉转换的错误进行修改，可以让机器记住自己经常输入的词语。     由此，我得出一个结论：在可预见的未来，智能技术的应用模式必须是人机交互，人机协同，或者是在机器的支持下的人人协同，而不会是人发命令，机器全自动完成。我们必须放弃“全自动”的理想，而对看上去不那么完美的“人机协同”满怀激情，让计算的负载在人脑和电脑之间求得平衡。电脑的优势在于快速低成本地对信息进行存储、复制、传输、比较、排序和检索，人脑的优势在于联想、推理、分析、归纳，而“群脑”的优势在于智慧的互补，所谓三个臭皮匠赛过诸葛亮。我们的目标不应再是追求电脑的智能极致，那是舍本逐末，好比本来要钓鱼，鱼竿只是手段，后来却迷上了鱼竿，开了鱼竿加工厂，我们的目标应该是追求提高人类的生产效率和生活体验。     从无用户参与，到人机协同，再到以网络为媒介，以计算为助力的人人协同，是由一个飞跃。在Web 2.0时代，这样的成功应用案例不胜枚举。自动问答系统不成功，但百度知道这样的社区型问答却很成功，计算技术在此帮助用户整理问答对，检索问答对，把问题推送给合适的人，起到穿针引线的作用，但不是唱主角。搜索引擎中的查询推荐，当当上的图书推荐，以至于双语例句检索和基于实例的机器翻译背后都是人人协同的理念，看看别人是怎么翻译的，我照猫画虎也能翻译。     除了用户自愿进行的人人协同工作外，还有付费形式的协同工作。比如“威客”，用户设定一个价格，要求其他网友为他设计一个logo，这是一种知识交易。联想开去，你帮我设计logo，我帮你做奥数题，还有中外用户互相帮助修改“外语”作文等等，这是一个“集体智慧(Collective Intelligence)”大发展的时代，我们设计新一代的互联网智能应用系统，必须把技术的姿态放低，充分地自然地辅助用户去完成他个人的任务，去与其他用户协同完成更复杂的任务。     在人类的各种通讯形式中，“多人远距离异步通讯”是最难的，互联网只为其他通讯形式提高了速度，降低了成本，比如即时通讯比电话的成本更低，电子邮件比传统的信件更快等，但“多人远距离异步通讯”以往几乎没有，最多看到领导之间由通讯员传递文件，依次批示。而在Web 2.0时代，最发达的就是这类通讯，包括：Wiki、论坛、贴吧、博客、微博、邮件组、知道等各种各样丰富多彩的形式。     至此，我又讲完了我关于“以人为中心的计算”的一些感悟。       我还要补充说明的是，作为信息检索与自然语言处理领域的研究者，我们的目标仍然是“搜索信息，理解语言，挖掘知识”，这种目标和使命并没有因为人本计算理念的影响而改变，但在Web 2.0时代，我们需要以人本计算的观念调整完善我们完成这些使命的方法，核心是重视人的参与。以往没有充分重视的一些研究点必须重视起来，比如版面分析、用户日志挖掘、人机交互界面设计、lifelong的机器学习、UGC（用户产生的内容，User Generated Content）的分析、基于游戏的知识采集、基于社会化标签的别名识别、基于FAQ的问答系统、个性化新闻推荐、协同式机器翻译等等。研究社会计算的学者可能利用用户日志走向社会关系网络，甚至人工社会等研究方向，但我们应该限制自己，坚持“利用语言技术帮助人们从互联网上获取知识”的宗旨，把我们获取到的多维信息（行为、结构、格式等）用于处理最初的那一维（内容）。此外，并不是说有了“人本计算”，第一代的专家知识加算法，第二代的专业人员语料库标注加统计学习的方法就都废弃，相反，他们仍然是需要的，我们有了大炮，但也需要匕首和步枪。        做个总结：“人本计算”就是“（知识）从群众中来，到群众中去（为群众服务）”，“一切依靠群众（提供知识），一切为了群众（获得知识服务）”，用英文说是“from the people, for the people”，人与机器共建“和谐”。计算技术发展到今天，从以计算为中心（比如，曾有人认为机器语言就够了，不用设计高级语言，现在也有人建议设计规范的查询语言而不是让用户自由输入查询），到以数据为中心（有人认为没有人工智能，只有数据，还有所谓“内容为王”的提法），到今天“以人为中心”，这是人本主义的“王者归来”，我们不会被自己制造的机器所奴役和压迫，神不是本，物也不是本，“人”才是本，机器只能向人学习，为人服务。       我曾在10年前完整地提出了类似“百度知道”平台的一揽子设计方案，当时命名为“全球信息交换GIE(Globle Information Exchange)”，但此后却没有真正重视互联网上的用户。Web 2.0的概念提出来也已经好几年了，我身在互联网圈里，却没有从中得到灵感，一直把自己现在文本检索上，而没有做互联网检索，实在是后知后觉。今年是哈工大信息检索研究中心成立10年（2000年9月成立），这几天我尽力排除各种事务的干扰，比较安心地思考着未来的研究计划，才有所醒悟，写出上面的文字，希望对各位读者有所帮助。","title":"关于Human-based computing（基于人的计算，人本计算）的一篇好文章"},{"content":"  数学之美 系列一 -- 统计语言模型 http://googlechinablog.com/2006/04/blog-post.html 数学之美 系列二 -- 谈谈中文分词 http://googlechinablog.com/2006/04/blog-post_10.html 数学之美 系列三 -- 隐含马尔可夫模型在语言处理中的应用 http://googlechinablog.com/2006/04/blog-post_17.html 数学之美 系列四 -- 怎样度量信息? http://googlechinablog.com/2006/04/4.html 数学之美 系列五 -- 简单之美：布尔代数和搜索引擎的索引 http://googlechinablog.com/2006/05/blog-post_10.html 数学之美 系列六 -- 图论和网络爬虫 (Web Crawlers) http://googlechinablog.com/2006/05/web-crawlers.html 数学之美 系列七 -- 信息论在信息处理中的应用 http://googlechinablog.com/2006/05/blog-post_25.html 数学之美 系列八-- 贾里尼克的故事和现代语言处理 http://googlechinablog.com/2006/06/blog-post_08.html 数学之美 系列九 -- 如何确定网页和查询的相关性 http://googlechinablog.com/2006/06/blog-post_27.html 数学之美 系列十 -- 有限状态机和地址识别 http://googlechinablog.com/2006/07/blog-post.html 数学之美 系列十一 -- Google 阿卡 47 的制造者阿米特.辛格博士 http://googlechinablog.com/2006/07/google-47.html 数学之美 系列十二 -- 余弦定理和新闻的分类 http://googlechinablog.com/2006/07/12.html 数学之美 系列十三 -- 信息指纹及其应用 http://googlechinablog.com/2006/08/blog-post.html 数学之美 系列十四 -- 谈谈数学模型的重要性 http://googlechinablog.com/2006/08/blog-post_09.html 数学之美 系列十五 -- 繁与简 自然语言处理的几位精英 http://googlechinablog.com/2006/08/blog-post_115634657041368311.html 数学之美 系列十六（上） 不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型 http://googlechinablog.com/2006/10/blog-post.html 数学之美 系列十六 （下）-- 不要把所有的鸡蛋放在一个篮子里　最大熵模型 http://googlechinablog.com/2006/11/blog-post.html 数学之美 系列十七 -- 闪光的不一定是金子 谈谈搜索引擎作弊问题(Search Engine Anti-SPAM) http://googlechinablog.com/2006/11/search-engine-anti-spam.html 数学之美 系列十八 -- 矩阵运算和文本处理中的分类问题 http://googlechinablog.com/2007/01/blog-post.html 数学之美 系列十九 -- 马尔可夫链的扩展 贝叶斯网络 (Bayesian Networks) http://googlechinablog.com/2007/01/bayesian-networks.html 数学之美 系列二十 －自然语言处理的教父 马库斯 http://googlechinablog.com/2007/04/blog-post_13.html 数学之美 系列二十一 － 布隆过滤器（Bloom Filter） http://googlechinablog.com/2007/07/bloom-filter.html   数学之美系列二十二 由电视剧《暗算》所想到的 — 谈谈密码学的数学原理 http://googlechinablog.com/2007/09/blog-post_13.html 数学之美系列 二十三 输入一个汉字需要敲多少个健 — 谈谈香农第一定律 http://googlechinablog.com/2007/12/blog-post.html 数学之美系列 二十四 从全球导航到输入法——谈谈动态规划 http://googlechinablog.com/2008/10/blog-post_14.html","title":"数学之美"},{"content":"  SCI或SCIE收录的本学科刊物清单请登陆 http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=K 和http://www.isinet.com/cgi-bin/jrnlst/jloptions.cgi?PC=D 下面列一些CS的顶级会议和期刊    有些是网上查到的，有些是某些人用SCI的IF排序做出来的： Computer Vision Conf.:   Best:     ICCV, Inter. Conf. on Computer Vision     CVPR, Inter. Conf. on Computer Vision and Pattern Recognition   Good:     ECCV, Euro. Conf. on Comp. Vision     ICIP, Inter. Conf. on Image Processing     ICPR, Inter. Conf. on Pattern Recognition     ACCV, Asia Conf. on Comp. Vision Computer Vision  Jour.:   Best:     PAMI, IEEE Trans. on Patt. Analysis and Machine Intelligence     IJCV, Inter. Jour. on Comp. Vision   Good:     CVIU, Computer Vision and Image Understanding PR, Pattern Reco. Network Conf.:     ACM/SigCOMM     ACM Special Interest Group of Communication     ACM/SigMetric Info Com Globe Com Network Jour.:     ToN (ACM/IEEE Transaction on Network) A.I.Conf.:     AAAI: American Association for Artificial Intelligence     ACM/SigIR IJCAI: International Joint Conference on Artificial Intelligence     NIPS: Neural Information Processing Systems     ICML: International Conference on Machine Learning A.I.Jour.:     Machine Learning     NEURAL COMPUTATION     ARTIFICIAL INTELLIGENCE PAMI     IEEE TRANSACTIONS ON FUZZY SYSTEMS     IEEE TRANSACTIONS ON NEURAL NETWORKS AI MAGAZINE     NEURAL NETWORKS     PATTERN RECOGNITION     IMAGE AND VISION COMPUTING     IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING     APPLIED INTELLIGENCE OS,System Conf.:     SOSP: The ACM Symposium on Operating Systems Principles     OSDI: USENIX Symposium on Operating Systems Design and Implementation Database Conf.:     ACM SIGMOD     VLDB:International Conference on Very Large Data Bases     ICDE:International Conference on Data Engineering Security Conf.:     IEEE Security and Privacy     CCS: ACM Computer and Communications Security NDSS (Network and Distributed Systems Security) Web Conf.:     WWW(International World Wide Web Conference) Theory Conf.:     STOC FOCS EDA Conf.: Best:     DAC: IEEE/ACM Design Automation Conference     ICCAD: IEEE International Conference on Computer Aided Design Good:     ISCAS: IEEE International Symposium on Circuits And Systems     ISPD: IEEE International Symposium on Physical Design     ICCD: IEEE International Conference on Computer Design     ASP-DAC: European Design Automation Conference     E-DAC: Asia and South Pacific Design Automation Conference Graphics Conf.:   Best:     Siggraph: ACM SigGraph   Good:     Euro Graph Jour.: IEEE(ACM) Trans. on Graphics     IEEE Trans. on Visualization and Computer Graphics CAD    Jour.: CAD CAGD Softe Engineering: conf.:     ICSE The International Conference on Software Engineering     FSE The Foundations of Software Engineering Conferences     ICASE IEEE International Conference on Automated Software Engineering     COMPSAC International Computer Software and Applications Conferences     ESEC The European Software Engineering Conferences Jour.:     SEN ACM SIGSOFT Software Engineering Notes     TSE IEEE Transactions on Software Engineering     ASE Automated Software Engineering SPE Software-Practice and Experience   EI收录的中国期刊：    来自http://www.ei.org.cn/twice/coverage.jsp ISSN 期 刊 名 相关链接 0567-7718 Acta Mechanica Sinica 1006-7191 Acta Metallurgica Sinica (English Letters) 0253-4827 Applied Mathematics and Mechanics (English Edition) 0890-5487 China Ocean Engineering 1004-5341 China Welding 1004-9541 Chinese Journal of Chemical Engineering 1022-4653 Chinese Journal of Electronics 1000-9345 Chinese Journal of Mechanical Engineering (English Edition) 学报网站 1671-7694 Chinese Optics Letters 学报网站 1673-7350 Frontiers of Computer Science in China 期刊网址 1006-6748 High Technology Letters 1674-4799 International Journal of Minerals, Metallurgy and Materials 1004-0579 Journal of Beijing Institute of Technology (English Edition) 学报编辑部 1005-9784 Journal of Central South University of Technology 学报网站 1672-5220 Journal of Donghua University (English Edition) 1005-9113 Journal of Harbin Institute of Technology (New Series) 1001-6058 Journal of Hydrodynamics 1005-0302 Journal of Materials Science and Technology 1002-0721 Journal of Rare Earths 1674-4926 Journal of Semiconductors 学报编辑部 1007-1172 Journal of Shanghai Jiaotong University (Science) 1003-7985 Journal of Southeast University (English Edition) 1004-4132 Journal of Systems Engineering and Electronics 1009-6124 Journal of Systems Science and Complexity 1003-2169 Journal of Thermal Science 1000-2413 Journal of Wuhan University of Technology -Materials Science Edition 1673-565X Journal of Zhejiang University SCIENCE A 1674-5264 Mining Science and Technology 1001-0521 Rare Metals 1006-9291 Science in China, Series B: Chemistry 1672-1799 Science in China, Series G: Physics, Astronomy 1005-8885 The Journal of China Universities of Posts and Telecommunications 1005-1120 Transactions of Nanjing University of Aeronautics and Astronautics 1003-6326 Transactions of Nonferrous Metals Society of China 1006-4982 Transactions of Tianjin University 1007-0214 Tsinghua Science and Technology Editor Information 1001-1455 爆炸与冲击 0254-0037 北京工业大学学报 1001-5965 北京航空航天大学学报 学报编辑部 1001-053X 北京科技大学学报 学报编辑部 1001-0645 北京理工大学学报 学报编辑部 1007-5321 北京邮电大学学报 学报编辑部 1000-1093 兵工学报 1001-4381 材料工程 1005-0299 材料科学与工艺 1009-6264 材料热处理学报 学报网站 1005-3093 材料研究学报 1001-1595 测绘学报 学报编辑部 1007-7294 船舶力学 1000-8608 大连理工大学学报 1004-499X 弹道学报 1000-2383 地球科学 学报网站 1005-0388 电波科学学报 1000-6753 电工技术学报 1007-449X 电机与控制学报 1000-1026 电力系统自动化 学报网站 1006-6047 电力自动化设备 1001-0548 电子科技大学学报 0372-2112 电子学报 1009-5896 电子与信息学报 1005-3026 东北大学学报 (自然科学版) 1001-0505 东南大学学报 (自然科学版) 1000-3851 复合材料学报 1003-6520 高电压技术 1000-7555 高分子材料科学与工程 1002-0470 高技术通讯 1003-9015 高校化学工程学报 1000-5773 高压物理学报 1000-4750 工程力学 0253-231X 工程热物理学报 1001-9731 功能材料 学报网站 1006-2793 固体火箭技术 0254-7805 固体力学学报 1005-0086 光电子.激光 1000-0593 光谱学与光谱分析 1004-924X 光学精密工程 学报网站 0253-2239 光学学报 学报网站 0454-5648 硅酸盐学报 1001-2486 国防科技大学学报 1006-7043 哈尔滨工程大学学报 学报网站 0367-6234 哈尔滨工业大学学报 0253-360X 焊接学报 1005-5053 航空材料学报 1000-8055 航空动力学报 编辑部网站 1000-6893 航空学报 学报网站 0258-0926 核动力工程 1001-9014 红外与毫米波学报 1000-2472 湖南大学学报 (自然科学版) 1000-565X 华南理工大学学报(自然科学版) 编辑部网站 1671-4512 华中科技大学学报(自然科学版) 0438-1157 化工学报 1002-0446 机器人 学报网站 0577-6686 机械工程学报 学报网站 1671-5497 吉林大学学报(工学版) 学报编辑部 1003-9775 计算机辅助设计与图形学学报 1006-5911 计算机集成制造系统 编辑部网站 0254-4164 计算机学报 1000-1239 计算机研究与发展 学报网站 1007-4708 计算力学学报 1001-246X 计算物理 1007-9629 建筑材料学报 1000-6869 建筑结构学报 1671-7775 江苏大学学报（自然科学版） 1009-3443 解放军理工大学学报（自然科学版） 0412-1961 金属学报 0258-1825 空气动力学学报 1000-8152 控制理论与应用 学报网站 1001-0920 控制与决策 0459-1879 力学学报 学报网站 0253-9993 煤炭学报 学报网站 1003-6059 模式识别与人工智能 1004-0595 摩擦学学报 1672-6030 纳米技术与精密工程 1005-2615 南京航空航天大学学报 1005-9830 南京理工大学学报 (自然科学版) 1000-0925 内燃机工程 1000-0909 内燃机学报 1002-6819 农业工程学报 学报编辑部 1000-1298 农业机械学报 学报编辑部 1001-4322 强激光与粒子束 学报编辑部 1000-0054 清华大学学报 (自然科学版) 0253-2409 燃料化学学报 1006-8740 燃烧科学与技术 1000-985X 人工晶体学报 无机材料期刊网 1000-9825 软件学报 学报编辑部 1006-2467 上海交通大学学报 1000-2618 深圳大学学报（理工版） 0371-0025 声学学报 1000-7210 石油地球物理勘探 1000-0747 石油勘探与开发 0253-2697 石油学报 1001-8719 石油学报:石油加工 学报网站 1672-9897 实验流体力学 学报网站 1001-6791 水科学进展 0559-9350 水利学报 学报编辑部 1003-1243 水力发电学报 1009-3087 四川大学学报(工程科学版) 学报编辑部 0254-0096 太阳能学报 学报编辑部 0493-2137 天津大学学报 学报编辑部 1001-8360 铁道学报 1000-436X 通信学报 0253-374X 同济大学学报 (自然科学版) 1000-131X 土木工程学报 学报网站 1674-4764 土木建筑与环境工程 学报编辑部 1001-4055 推进技术 1000-324X 无机材料学报 1671-8860 武汉大学学报(信息科学版) 1001-2400 西安电子科技大学学报 学报网站 0253-987X 西安交通大学学报 1000-2758 西北工业大学学报 0258-2724 西南交通大学学报 学报网站 1002-185X 稀有金属材料与工程 1000-6788 系统工程理论与实践 1001-506X 系统工程与电子技术 1007-8827 新型炭材料 1000-6915 岩石力学与工程学报 学报网站 1000-4548 岩土工程学报 1000-7598 岩土力学 期刊编辑部 0254-3087 仪器仪表学报 1005-0930 应用基础与工程科学学报 学报网站 1000-6931 原子能科学技术 1008-973X 浙江大学学报 (工学版) 1672-7126 真空科学与技术学报 1004-6801 振动测试与诊断 1004-4523 振动工程学报 1000-3835 振动与冲击 学报网站 0258-8013 中国电机工程学报 1001-7372 中国公路学报 0258-7025 中国激光 学报网站 1000-1964 中国矿业大学学报 1673-5005 中国石油大学学报 (自然科学版) 1001-4632 中国铁道科学 1004-0609 中国有色金属学报 学报网站 1672-7207 中南大学学报（自然科学版） 学报网站 0254-4156 自动化学报 学报网站 中科院计算所推荐国际会议 序号 会议名称 会议介绍 代表领域 1 ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左 右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录 用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。< /p> 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。 GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千 人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会 议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左 右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一 次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一 次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu. http://hpdc13.cs.ucsb.edu 高性能计算 42 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/  高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications 高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括 technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing 该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下 半年排名。 高性能计算 48 ACM International Conference on Supercomputing 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57 FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶 尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59 SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60 IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62 IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等 概念。 自主计算 63 Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64 International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66 IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67 USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68 IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69 International Symposium on Computer Architecture（ISCA ） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很 难 系统结构 70 International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72 IEEE International Conference on High Performance Computing IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73 Annual ACM International Conference on Supercomputing（ICS） 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半 年排名。 高性能计算 74 Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其 困难 操作系统 75 ACM Symposium on Operating Systems Principles （SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要 中极其困难 操作系统 76 Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困 难 操作系统，程序语言 77 Workshop on Hot Topics in Operating Systems （HOTOS） 操作系统最好的会议之一 操作系统 78 Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79 Annual IEEE Conference on Local Computer Networks（LCN） 网络 80 International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影 响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收 率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会 议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影 响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收 率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收 率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。< /p> 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统 等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影 响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术 121 ECOOP - European Conference on Object-Oriented Programming 关注object technology的各个领域。其中也包括一些虽然不直接和Object Oriented相关，但同样具有启发性的工作。 编译技术 122 ESOP - European Symposium on Programming ETAPS的member conference，是欧洲Software Science方向的重要会议。涵盖了程序语言的设计和实现，编程模型的研究，程序的自动生成和分析等方向。< /p> 编译技术 123 Euro-Par - European Conference on Parallel Computing 关注并行计算的诸多方面的国际会议。粗略可分为hardware, software,algorithms and application for parallel computing几个部分。 124 SAS - International Static Analysis Symposium 关注程序的静态分析的权威会议。 编译技术 125 CAV - Computer Aided Verification Rank1的国际会议，关注计算机辅助形式验证，涵盖从theoretical results到concrete applications的诸多方面，尤其是practial verification tools and the algorithms and techniques that needed for their implementation。影响因子1.88。 编译技术 126 FASE - Fundamental Approaches to Software Engineering ETAPS的member conference，主要关注Software Science，影响因子0.91。 编译技术 127 TACAS - Tools and Algorithms for the Construction and Analysis of Systems ETAPS的member conference，关注的领域包括formal methods, software and hardware verification, static analysis, programming languages, software engineering, real-time systems, and communications protocols。影响因子1.24 编译技术 128 VMCAI - Verification, Model Checking and Abstract Interpretation Rank2的国际会议。关注的领域包括Verification, Model Checking, and Abstract Interpretation, facilitating interaction, cross-fertilization, and advancement of hybrid methods that combine the three areas。 编译技术 129 ACL: The Association for Computational Linguistics 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。 人工智能 计算语言学 130 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索方面最好的会议, ACM 主办, 每年开。19％左右 信息检索技术 131 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining 数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右 132 WWW: The ACM International World Wide Web Conference 应用和媒体领域顶级国际会议 万维网 133 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库领域顶级国际 数据管理 134 CIKM: The ACM Conference on Information and Knowledge Management 数据库领域知名国际会议 数据管理 135 COLING: International Conference on Computational Linguistics 计算语言学知名国际会议 计算语言学 136 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 137 IEEE ICDM: International Conference on Data Mining 数据挖掘领域顶级国际会议 138 IJCAI: International Joint Conference on Artificial Intelligence 人工智能领域顶级国际会议，论文接受率18％左右 人工智能 139 VLDB: The ACM International Conference on Very Large Data Bases 数据库领域顶级国际 数据库 140 SIGGRAPH: ACM SIGGRAPH Conference 计算机图形学顶级国际会议，ACM主办，每年一次，几万人参加会议，论文录用率小于20％ 计算机图形学 141 EUROGRAPHICS： The Annual Conference of the European Association for Computer Graphics 欧洲举办的国际图形学会议，面向世界。接受率现在也有差不多20% 计算机图形学 142 AAAI: American Association for Artificial Intelligence 美国人工智能学会AAAI的年会，使该领域的顶级会议 人工智能 143 ACM Conference on Computer and Communications Security ACM通信和计算健全领域顶级学术会议 信息安全 144 ACM SIGCOMM: Special Interest Group on Data Communications 数据通信 145 ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval 信息检索领域的重要会议 信息检索 146 ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining ACM旗舰会议之一，是数据库与知识管理的顶级学术会议。每年举办一次。 通信与网络 147 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems ACM性能建模与评价领域顶级学术会议 通信与网络 148 ACM SIGMOD: ACM SIGMOD Conf on Management of Data 数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。 数据管理 149 ASPLOS: Architectural Support for Programming Languages and Operating Systems 编程语言和操作系统国际会议，涉及硬件体系结构、编译、操作系统、网格、应用的领域。 体系结构 150 CCGrid : IEEE/ACM International Symposium on Cluster Computing and the Grid 网格计算国际会议，网格平台、中间件 151 CIKM: The ACM Conference on Information and Knowledge Management 信息检索领域的会议，录用率为15% 信息检索 152 CLUSTER - IEEE International Conference on Cluster Computing 集群计算国际会议，涉及中间件、网格算法及应用、资源管理、集成等。 集群计算 153 CPM: Combinatorial Pattern Matching Symposium 组合模式匹配年会，是字符串匹配、模式匹配较好的会议。 模式匹配 154 FAST4: Third USENIX Conference on File and Storage Technologies, USENIX旗下关于文件和存储系统的顶级会议，会议于2002年召开第一届，召开地点都在美国加州。只有最好的工作能发表在FAST上。目前尚无大陆研究机构命中。 文件与存储 155 Grid : IEEE/ACM International Workshop on Grid Computing 网格计算国际会议，涉及计算模型、大规模数据访问和管理、资源管理和调度等。 网格计算 156 HPC: IEEE International Conference for High Performance Computing 157 HPDC: International Symposium on High Performance Distributed Computing 高性能分布计算国际会议，涉及告诉网格、分布计算、并行处理、大规模存储通信等领域。< /p> 高性能计算 158 ICDCS: IEEE International Conference on Distributed Computing Systems IEEE TCDP发起的关于分布式处理领域的会议，最为老牌强会，ICDLS 举办了26届，录用率为15%。 分布式计算 159 ICML: International Conference on Machine Learning 机器学习领域中的顶级会议 机器学习 160 ICWS: IEEE International Conference on Web Services Web服务国际会议，面向服务标准及规范、服务应用、语义服务等方面。 Web服务 161 IEEE CSB: Computer Society Bioinformatics 162 IEEE ICDM: International Conference on Data Mining 数据挖掘领域的著名会议，率用率为14%。 数据挖掘 163 IEEE ICNP: International Conference on Network Protocols IEEE 网络通信领域顶级学术会议，录用率在10%左右。 网络 164 IEEE ICON: IEEE International Conference on Networks 165 IEEE INFOCOM: conference on computer communications IEEE网络通信领域著名会议，领域广泛。 网络 166 IEEE IPCCC: International Performance Computing and Communications Conference IEEE性能领域著名学术会议，主要关注性能评价。 网络性能 167 IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval 字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。 字符串处理信息检索 168 IJCAI: International Joint Conference on AI 人工智能领域的顶级会议。 人工智能 169 IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference IMC: ACM-SIGCOMM/USENIX Internet Measurement Conference ACM SIGCOMM网络特征领域重要会议，主要涉及网络流特征。 170 International Symposium on High Performance Computer Architecture(HPCA) 服务计算 171 IPDPS: IEEE International Parallel and Distributed Processing Symposium IEEE TCPD牵头，IEEETCCA、TCDP与ACMSIGARH合办的关于并行处理与分布式处理会。录用率30%， 会议的优秀论文可能发表于JDPC杂志上。 并行计算分布式计算 172 ISMB: International conference on Intelligent Systems for Molecular Biology 无线 173 MobiCom: ACM/IEEE Conference on Mobile Computing and Networking 始于1995，无线、移动计算方面比较有历史和重要的会议。 安全 174 MobiSys: The International Conference on Mobile Systems, Applications, and Services 无线方面，2006年第4名。 无线 175 OSDI: USENIX Symposium on Operating Systems Design and Implementation USENIX操作系统领域重要会议，侧重操作系统各方面的新型技术。 操作系统 176 PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining 177 PDCAT: International Conference on Parallel and Distributed Computing, Applications and Technologies 关注并行于分布式计算领域众多问题的国际性会议，主要是亚太地区。 并行计算分布式计算 178 PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases 数据挖掘领域的重要会议，录用率为14%。 数据挖掘 179 SCC: IEEE International Conference on Services Computing 服务计算国际会议，侧重服务模型、发现体系结构、服务安全、服务质量、服务语义方面的研究。< /p> 服务计算 180 SDM: SIAM International Conference on Data Mining 数据挖掘领域的重要会议，录用率为14% 数据挖据 181 SOSE: IEEE International Workshop on Service-Oriented System Engineering 182 USENIX Sec: USENIX Security Symposium USENIX安全领域重要会议，侧重安全技术。 安全 183 USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI) USENIX网络领域重要会议，设计网络涉及各方面内容。 网络 184 VLDB: The ACM International Conference on Very Large Data Bases 数据管理 185 WWW: The ACM International World Wide Web Conference ACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。 Internet 186 RAID International Symposium on Recent Advances in Intrusion Detection 数据库顶级国际会议 187 IJCAI: International Joint Conference on Artificial Intelligence 人工智能顶级国际会议 人工智能 188 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据库 189 ICML: International Conference on Machine Learning 机器学习顶级国际会议 机器学习 190 PRICAI: Pacific Rim International Conference on Artificial Intelligence 亚太人工智能国际会议 人工智能 191 IFIP ICIIP: IFIP International Conference on Intelligent Information Processing IFIP智能信息处理国际会议 智能信息处理 192 NIPS: Neural Information Processing Systems 神经信息处理领域顶级国际会议 神经计算，机器学习 193 ISCA: International Symposium on Computer Architecture  体系结构领域的顶级会议 微处理器设计 194 International Symposium on Microarchitecture 体系结构领域的顶级会议 微处理器设计 195 HPCA：International Symposium on High Performance Computer Architecture 体系结构领域的顶级会议 微处理器设计 196 APCSAC: Asia-Pacific Computer Systems Architecture Conference  体系结构方面的重要会议 微处理器设计 197 ISLPED: International Symposium on Low Power Electronics and Design 低功耗设计的重要会议 微处理器设计 198 OSDI: Operation System Design & Implementation  操作系统方面的重要会议 微处理器设计 199 ASPLOS: Architecture Support for Programming Languages and Operation  体系结构方面的顶尖会议 微处理器设计 200 ICCD: IEEE International Conference on Computer Design 体系结构方面的顶尖会议 微处理器设计 201 DAC: Design Automation Conference 设计自动化领域的顶级会议 微处理器设计 202 IEEE/ACM International Conference on Computer Aided Design(ICCAD) 集成电路设计自动化方面的顶尖会议 微处理器设计 203 ASP-DAC: Asia and South Pacific Design Automation Conference  设计自动化领域的重要会议 微处理器设计 204 ISSCC: IEEE International Solid-State Circuits Conference 设计自动化领域的重要会议 微处理器设计 205 CICC: Custom Integrated Circuits Conference 集成电路设计方面的顶尖会议（公认排名第二） 微处理器设计 206 ESSCIRC: European Conference on Solid-State Circuits 集成电路设计方面的顶尖会议 微处理器设计 207 Symposium on VLSI Circuits 集成电路设计方面的顶尖会议 微处理器设计 208 IEEE International ASIC/SOC Conference 集成电路设计方面的重要会议 微处理器设计 209 Symposium on VLSI Technology 集成电路设计方面的重要会议 微处理器设计 210 ASSCC: Asian Conference on Solid-State Circuits 集成电路领域重要会议 微处理器设计 211 MWSCAS: Midwest Symposium on Circuits and Systems 集成电路领域重要会议 微处理器设计 212 ICECS: IEEE International Conference on Electronics, Circuits and Systems： 集成电路领域重要会议 微处理器设计 213 ISCAS: International Symposium Circuit and System 电路与系统方面的重要会议 微处理器设计 214 RFIC: IEEE Symposium on Radio Frequency Integrated Circuits 射频集成电路领域顶尖会议 微处理器设计 215 ACM RECOMB: Int. Conference on Research in Computational Molecular Biology RECOMB创办于1997年，每年举办一次，由ACM和/或国际计算生物学协会(ISCB)主办，强调计算生物学的数学和计算方面，近年录取率在 20%左右。 计算生物学 216 IEEE CSB: Computer Society Bioinformatics CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办。 计算生物学 217 International Conference of the IEEE Engineering in Medicine and Biology Society （EMBC） 生物信息学与计算生物学国际研讨会 计算生物学 218 PSB: Pacific Symposium on Biocomputing PSB创办于1996年，每年举办一次，从属于国际计算生物学协会(ISCB)，由美国的研究机构组织举 办。 计算生物学 219 WABI:Workshop on Algorithms in Bioinformatics WABI创办于2000年，每年举办一次，由欧洲理论计算机协会(EATCS)和国际计算生物学协会(ISCB)主办，强调生物信息学的算法方 面。 计算生物学 220 CSB: IEEE Computational Systems Bioinformatics Conference CSB创办于2002年，每年举办一次，2005年以前由IEEE协会主办，2006年开始改由生命科学协会(LSS)主 办 计算生物学 221 ISMB：Annual International Conference on Intelligent Systems for Molecular Biology ISMB创办于1993年，每年举办一次，由国际计算生物学协会(ISCB)主办，在生物信息学领域影响最大，论文集通常作为 Bioinformatics杂志的专刊发表，近年来录取率为15%左右。 生物信息 222 ECCB：European Conference on Computational Biology ECCB创办于2002年，每年举办一次，由国际计算生物学协会(ISCB)主办，有时与ISMB联合举办，论文集通常作为 Bioinformatics杂志的专刊发表，近年录取率在20%左右。 生物信息 223 APBC：Asia Pacific Bioinformatics Conference APBC创办于2003年，每年举办一次，由亚太国家的研究机构组织举办，近年录取率在35%左右。< /p> 生物信息 224 COCOON：Annual International Computing and Combinatorics Conference COCOON创办于1995年，每年举办一次，会议范畴为计算理论、算法、组合优化等，包括生物信息学方向，近年录取率在40%左 右。 生物信息 225 CPM：Annual Symposium on Combinatorial Pattern Matching CPM创办于1990年，每年举办一次，会议以串、树和图等复杂模式的搜索和匹配问题为主题，包括生物信息学方向，近年录取率在45%左 右。 生物信息 226 EMBC：IEEE International Conference of the Engineering in Medicine and Biology Society EMBC创办于1979年，每年举办一次，从属于IEEE医学与生物工程协会，会议内容覆盖范围很广，每年接收的论文数以千 计。 生物信息 227 Geospatial Information and Technology Association（GITA） Annual Conference 地球空间信息与技术协会年会 遥感与空间信息处理 228 International Geoscience and Remote Sensing Symposium （IGRSS） 地球科学与遥感国际会议 遥感与空间信息处理 229 International Society for Photogrammetry and Remote Sensing （ISPRS）Technical Commission Symposium 国际摄影测量与遥感学会专业委员会会议 遥感与空间信息处理 230 International Conference on Geoinformatics 地球信息国际会议 遥感与空间信息处理 231 IEEE SKG (Semantics, Knowledge and Grid) 由计算所发起的IEEE国际会议，每年有100人参加。 知识网格 232 WWW: The ACM International World Wide Web Conference Internet领域顶级国际会议 Internet 233 International Semantic Web Conference Semantic Web领域顶级会议，录用率17% Semantic Web 234 ACM SIGMOD: ACM SIGMOD Conf on Management of Data ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。 数据管理 235 ACM PODS Conference ACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。 数据管理 236 VLDB: The ACM International Conference on Very Large Data Bases 数据库顶级国际会议 数据管理 237 IEEE ICDE - International Conference on Data Engineering 数据库顶级国际会议 数据管理     转自：http://blog.csdn.net/a7758526/article/details/4894124","title":"计算机领域的顶级会议和期刊"},{"content":"  学习搜索有一段时间了，为了复习巩固和提高，特把学习的结果总结一下。本文章搜索只特指小型搜索系统。之所以特指是小型系统，是因为大型小型搜索系统虽然整体处理过程大体相似，但整体架构和要处理的数据量和响应速度是密切相关的，百万量级的和十亿量级的搜索系统是不可同日而语的。 搜索系统处理大体分为：蜘蛛、切词、索引、检索，下面逐个的描述。 1. 蜘蛛 蜘蛛是用来抓取网页的。所谓的抓，其实也就是通过socket向http服务器发送post或者get请求，服务器根据你的请求把页面送给你的过程。大家通常用的IE浏览器就是要先做这个工作，然后在根据返回的Html源码生成你看到的网页。蜘蛛当然不需要去生成网页给人看了，它只是把html存储起来给其它程序用。我看到有人问如何写一个下载动态网页的蜘蛛，其实动态页面和静态页面对于蜘蛛是一样的，都是发送请求和接受，没有实质区别。 搜索技术中，蜘蛛看起来是最容易的。大家学习了一个语言和熟悉TCP/IP后都可以编写一个下载网页的程序了。但是，要写一个适应性强的蜘蛛却是相当难的，因为网络环境太过于复杂，你总是很难想象到别人会发什么给你，也就是蜘蛛要有很强的例外处理能力。最常见的问题是： 网路故障 造成蜘蛛抓取超时，没有抓取到网页或者抓取不全。 网站链接 有些网站对错误的链接或者蜘蛛使用的ip，返回一个很怪的页面，例如它的内部错误信息。 页面问题 页面问题中网页编码是首要问题，各个网站用的编码五花八门，需要你判断和处理合理的编码。有朋友会问，http协议头信息中有charset，html中也有charset，可以方便的判断编码，没错，这是主要的判断方式，但是，如果这两个地方都得不到charset信息呢？ 其次，要提高蜘蛛的效率和速度，这就要涉及到待抓取url的权值值计算，因为并不是每个网页对我们来说都是同等重要的。由于网络的无边无际，所以采用广度搜索和深度搜索都不可取，只有采用种权值计算公式采用减枝方法做有效遍历。也有只依赖权值来决定搜索目标的。 最后是蜘蛛的性能。也就是并行处理方式。单个蜘蛛处理能力有限制，因此多采用多线程和多进程的方法，多个蜘蛛同时运行。另外，采用此方法的另一个重要原因是，网络速度是蜘蛛程序的瓶颈。 2.切词 切词仿佛是最简单的。采用最流行的倒序最大前序最大切词方法基本上就够用的了。但是切词也是让我最迷惑的东西，什么是切词？采用上述方法切出来的并不是标准的词语，更像是字符和分割组合，你查一下百度或者googel就可以看到它们的切词实现效果。切词程序真的不是在切词啊。 切词是深入处理中文信息的基本模块。在自然语言分析中也有切词，切词的基本方法和过程也基本相同，但我觉得在两者的是指导思想有些不一样。自然语言的分析很在意切词的准确性，而索引切词确更注重切词结果的覆盖情况。自然语言处理在切词后，需要词性标注和词法语法分析，因此必须切词准确，对新词、命名实体等的处理相对要求要高，而索引切词后就是做倒排索引了，只要能保证索引时切词和查询时的切词是同样的，基本上就可以正确使用索引，只是为了索引效果，需要尽可能的在符合词语的自然分段的基础上，把词的切割粒度定位在在效率和灵活性的一个调和位置，对新词和命名实体的识别就不那么看重。例如：北京大学，在第一中切词中会切割成一个词语，而在后一种切词中，为了保证能使用北京也可以搜索到，会被切割成北京和大学两个词。也有为了效率而做复合索引的，例如把北京大学切割为 北京大学、北京和大学这3个词，方便用户使用任何一个熟悉的词都能搜索到。 通过上面的讨论，分析到切词系统实际上是根据实际应用而不同的。就索引切词来说，切词的准确性并不重要，切词后的词能够快速准确找到索引，才重要。那么怎么切才能让后面的索引做的快速而且准确呢？想一想，我们为什么要切词呢，即便不切词，我们也可以在字上做索引，也可以在字节上做索引，开源搜索醒目lucence默认处理中国语言的方式就是双字切词，两个字两个字的切，例如：北京大学，切割后为，北京 京大 大学。但要讲到效率，根据搜索到的资料数据比较，还是基于词库的自然词切割是效果较好的。我想原因是：1，切割的完整性，不会遗漏词语，也不会有太多废词，例如 京大。因为使用索引查询的人的查询输入，也基本符合中文语法，基本不会出现 京大这样的词，相对的2字切词的处理方式就会产生废词，就要多做和维护一个基本用不到的索引。2，切割的后的词检索的效率，如果基于字或者字节做索引，虽然中国总数有2万多，但是常用的只有4~6千，要在这个范围内映射千万量级以上的文档，索引就比较大，每个字要对应万量级的文档，同时后期的归并和条件过滤处理工作量也比较大，而对应的词是中文语法结构的基本单位，常用词有5~10万，足以承担亿量级的文档索引分担。做了一段时间的索引程序后，我的感觉是词长在2~4之间比较好。 这里讨论的罗嗦的原因是为了比较说明索引切词并不是严格意义上的切词，只是融合了切词逻辑的字符切割。这种切割字符基本符合词语单位，我想是有必然的潜在的语言规则。我在学习切词时有走过弯路，总想把词切割的准确些，再准确些，结果程序效率下降，准确性也不能有明显提高，现在觉得，如果不涉及更深层次额语义处理，并不必要。 3.索引 对于学习过数据库，熟悉数据库基本原理的朋友们来说，做索引并不是很困难的事。现有的索引系统基本是采用倒排索引。基本含义就是在一个文件中记录下一个词在哪篇文章哪个位置出现，以便解析用户输入后可以直接读取对应的文章id，把对应的文章择要提取出来显示给用户。 我认为索引难做的一点是索引的维护，包括插入和删除。而索引的更新(update)，通常索引系统把它转化为删除(delete)和插入(insert)。信息的变更要尽可能快的反映到索引中，这个对索引系统的压力还是比较大的。想一下数据的删除过程，删除一片文章，要知道这篇文章都在哪些词的索引中，要逐个的清理数据，而这个数据清理，通常为了性能，又不是真实的清理，只是把文章对应的数据抹去（清零或做特定标记），这样索引中就像是留下了一个洞，时间一场，索引也就千疮百孔了，所以索引系统需要根据一定的算法计算索引中的空洞比率，在适合的条件下，整理索引，剔除空洞。 另外，插入索引也需要特别注意，简单的索引插入当然没有问题，因为文档id在索引中一般是排序的（整体排序或者分段排序），这样方便检索时快速处理。因此大批量或者频繁的插入时，就会有性能问题。通常的解决方案是采用大小索引的办法，新插入的索引并不直接插入到大索引中（数据处理量相对大），而是临时写入小索引文件，而检索时，从这两个索引中取值，然后归并就可以了。 另外，索引数据，为了提高读写速度，一般是经过简单压缩的，这时也就涉及到数据安全问题，为了避免万一问题，也可在代码中增加校验功能，保证数据完整，即便某个地方因计算错误而有问题，也要把他限定在最小范围内。 4.检索 我很怀疑google的pagerank计算公式，据说有超过一千的影响因素，这在我简单的脑袋里是不可想象的。我看google黑板报（google的中文官方blog）里曾经说到，简单就是最美的，公式的思想好像与这个相违背。总之pagerank的思想就是：我重要，你和我有友好关系，那么可以简单的推到出，你也重要。根据这样的思想，我们可以创建自己的计算公式。 我比较关心检索的组合，例如检索：测试 软件 程序员 -网站 。这个在检索时会处理成一个逻辑结果，好像，(a and b and c ) - d 。大家都知道这就是集合操作，熟悉数据库的朋友可以想到，这个很像数据查询语句的条件，没错，为了减少数据库和索引操作，分析和优化查询是很有必要的，处理过程完全可以借鉴数据库检索语句分析和优化过程。 总结一下，我写的这些是索引的入门。我对索引的理解是。根据处理的数据类型和数据量，索引系统有很大的不同。我记得百度掌门人说过，搜索人人可作，但要做大做好，就很困难了。具体索引的实现方式，要比描述的复杂写，有兴趣的朋友可以现学习lucence，非常棒的开源java程序，也有.net版本的开源。我对lucence的感觉是，它很经典，它的索引文件有点烦琐，它的检索方式很值得学习。","title":"搜索引擎导引"},{"content":"  最近的 DDP的背后 这次在NIPS 2010上发表的关于构造Dependent Dirichlet Processes (DDP)的paper在NIPS的官网已经可以下载了。 在这里只是想分享这篇文章背后的研究经历。 认识我的朋友们应该知道，我从本科开始直到现在，主要研究方向一直都是computer vision。但是，在硕士阶段和在博士阶段的研究目标却有着很大的不同，这和导师的风格有着很重要的关系。 在香港读硕士期间，我的导师汤老师是一个非常注重实际应用的人，因此当时做research的主要目标是提高实际性能或者建立新的应用。在方法上，更多地是借用现有的方法，或者略加改进。 到了MIT之后，我的导师Eric Grimson让我在John Fisher的指导下进行研究。John和LIDS的主任Alan Willsky关系很密切，因此我每周都要参加Alan的一个grouplet，并且有幸和Alan讨论学术问题。 Alan Willsky是我非常敬佩的一位教授，他有着很深的数学造诣。在每次grouplet的时候，他都听我们给他讲新的进展，而他则为下一步的研究提供方向性的意见。和汤老师的风格不一样，Alan是一位很典型的理论型的科学家。每次讨论时，他关注的重点不是实验结果，而是理论价值，比如某一个方法是不是能给这个field带来新的insight。在他前几年指导的工作里面，这一点得到了充分的体现，比如Tree reweighted approximation和Walk-sum analysis of Gaussian LBP都是probabilistic inference的重要进展，并且展现了对相关领域的深入而独到的理解。 在他的引导下，我在研究过程中更多地思考一个工作背后的理论基础。在这个过程中，我始终感到在本科和HK时期打下的数学基础并不足以支持在理论方面的深入探索，于是开始系统地学习和我的研究课题有关的数学。和运动分析有关的部分主要是Differential geometry和Lie algebra，和统计模型有关的是Measure theory，Modern probability theory，Stochastic processes和Convex analysis。以及这些学科所共同涉及的General topology和Functional analysis。 MIT要求每个PhD修一门minor，我当时从我的需要出发选择了数学(Course 18)。在这个过程中得到了进行严格数学推导和证明的训练。进行严格的数学分析和推演的能力也许在Computer Vision的大部分工作中并不是特别需要，但是在做NIPS这个工作的时候它的效用就显现出来了。理论上的东西，你要说服别人它是对的，必须给出严格的证明，而不仅仅是实验结果。 回到这篇NIPS paper吧。最初，我们是希望得到一种能随着时间变化的mixture model。在这个过程中，可能回增减其中的component。如果回到硕士的时代，也许，我们会通过工程的方法来解决这个问题——事实上很多现有的工作就是通过工程方法或者算法层面的设计来达到这个目标的。但是，基于我们组的风格，我们并不能满足这样的方法，而是希望每个方法都有一套严格的数学理论去支持。 在mixture演化的过程中component的个数会发生变化，因此Dirichlet Process也就成了很自然的一种选择。一开始的时候，我们也只是希望对原有的基于Polya Urn或者Stick breaking的方法加以改进来满足我们的需求，但是发现这其中在数学上存在很多困难，于是，我们开始尝试另起炉灶，从根源重新理解DP。 在我早前做关于大批运动物体的motion analysis的topic的时候，曾经读过一些关于Random Point processes的书，其中包括Kingman的Poisson processes。这本书中提到了(Spatial) Poisson process的很多很漂亮的数学性质，以及它和Gamma/Dirichlet Process的内在关系。最初读这本书的时候，只是惊叹于Poisson process的数学美，而并没有意识到它的实际价值。当我再次阅读这部书的时候，才形成了是否可以利用Poisson和Dirichlet的关系来建立我们的dynamic mixture model framework的想法。这种想法就是这篇NIPS的源头，paper中的section 2关于数学background的部分就是来自于这本书。 在我早前的blog中曾经论及空间泊松过程和随机测度，其实已经是在一定程度上介绍这篇paper的理论背景。只是当时paper还没有发表，并不便于讲得很深入。 除了Alan的影响，我也感谢John和Eric为我创造的研究环境。一直以来，对于在理论方面的探索，他们都是非常鼓励的，认为这样的探索非常有价值。虽然，我们的funding也需要一些项目来支持，但是，John尽了很大的努力排除这些项目对于研究的干扰——他向sponsor提出一个要求是，我们可以为他们提供新的方法或者模型，但是sponsor不应干扰或者介入具体的研究课题以及课题的选择。 在这次NIPS的会议上，我看到了很多很好的很有启发的工作。跟许多参加会议的学者相比，我目前所做的这些工作（包括这篇paper）其实还是非常有限的。这次提出的方法本身也有着很多的局限有待解决，比如目前只是支持sequential filtering，而且Sampler的efficiency的提高还有很大的空间。让这次提出的方法在领域内产生真正的影响，还需要做很多的事情。 12月 10, 2010 | Categories: Uncategorized | 15条评论» NIPS outstanding student paper award I am glad to share a good news with all my friends. My paper with title “Construction of Dependent Dirichlet Processes based on Poisson Processes” receives the outstanding student paper award in NIPS 2010. Relevant information can be found in the NIPS program booklet. Two papers out of 1200+ are awarded and two others got honorable mentions. My gratitude is delivered to Prof. Grimson, Fisher, and Willsky, as well as all the friends who have been supporting and encouraging me along my academic exploration. The basic idea of this work is to exploit the theoretical connections between Poisson, Gamma, and Dirichlet processes in constructing dependent Dirichlet processes. The construction proposed in this paper has essential differences from the traditional constructions that are based on either Polya urn (Chinese restaurant process) or stick breaking. I believe that this work is only a starting point along the line in exploiting this relation, and there remains a lot of room for future exploration. I would be really happy if there could be lots of efforts along this direction that make it into an important methodology in Nonparametric Bayesian learning and estimation. Had this happened, it could be much more significant than this paper itself. The paper is available by the following link: http://dahuasky.files.wordpress.com/2010/12/ddp_nips10.pdf Here is the supplemental document, which contains the description of relevant math concepts as well as the proofs of the theorems in paper: http://dahuasky.files.wordpress.com/2010/12/supplement_doc.pdf Any feedback is highly appreciated. Thank you. 12月 2, 2010 | Categories: Uncategorized | 12条评论» Blog搬家了 为 Live Space 默哀。。。。。。这个新家还真不习惯 09月 29, 2010 | Categories: Uncategorized | 16条评论» Learning和Vision中的小进展和大进展 首先祝朋友们中秋节快乐！ 因为过去三个月的实习工作很繁忙，这么已经很长时间没有更新了。这个夏天参加了两次会议(CVPR和ECCV)，在微软完成了一个新的project，这些经历都给了我新的启发。 不积跬步无以至千里 很多在这个领域做research的朋友抱怨，这个领域在过去相当长的时间没有“突破性”的进展了。在过去，我也一直抱有这样的看法。不过，如果比较最近两年的paper，以及20年前的paper，其实，还是可以看到，在很多具体的方向上，我们都已经取得了长足的进展。很多在当年只是处于雏型阶段的算法和模型，经过整个community这么多年的努力，现在的性能已经接近或者到达实用的水平。 虽然，在每年的各大会议中，非常激动人心的paper很少，可是，如果我们把某个方向过去10年的文章串在一起，我们会发现，这个方向的前沿已经推进了不少。这个过程有点类似于进化。在每年发表的成百上千的paper中，真正有价值的贡献只占很小的比例。但是这小部分的贡献能经历时间的考验，被积淀下来，并且被逐步被广泛地接受。当这样的进展积累到一定程度，整个方向就已是今非昔比。 在这个过程中，不同类型的paper其实发挥着不同的作用。举一个简单的例子，在很多问题的传统模型中，因为建模和计算的方便，都喜欢使用L2 norm来测量与观察数据的匹配程度。而近年来，越来越多的模型开始改用L1 norm来取代L2 norm，并且在性能上获得很大的提高。这样的变化起码经历了10年时间才逐步受到广泛的注意。在较为早期的工作里，部分的researcher在实践中发现似乎用L1 norm性能更好，但是大家并不是一开始就深入了解这背后的原理的。于是，这样的观察也许只散见于不同paper的experiment section或者implementation details里面。随着这种观察被反复验证，就会有人进行系统性的实验比较，使得这些观察形成更为可靠的结论。另一方面，理论分析也随之展开，希望能从更深的层次上来剖析其背后的原理，甚至建立严格的数学模型——于是一个本来只是实验中的heuristic的方法终于具有了稳固的理论根基。这些理论将启发人们提出新的方法和模型。 也许在很多人看来，从L2 norm到L1 norm的变化，只是一字之差，不值一提。但是，这种变化对于全领域的影响非常深远，不仅仅在很多具体的topic上带来性能提高，而且引导了学科的发展趋势——robust fitting, sparse coding / compressed sensing受到热情关注，和这种变化是密切相关的。 我在和一些同学交流的时候，发现有些人特别热衷于解决“根本问题”。壮志固然可嘉，但是，我始终认为，根本问题的解决离不开在具体问题上的积累和深刻理解。这种积累，既包括理论的，也包括实验的。至少，对于像我这样的普通人，我觉得，获得这种积累的唯一途径就是大量的实践，包括阅读paper，建立数学模型，推导求解算法，自己亲手把程序写出来，在实际数据中运行并观察结果。新的idea是思考出来的，但是，这种思考是需要建立在对问题的深刻理解上的。从石头缝里蹦出有价值的idea的概率，和彩票中奖没有什么区别。 什么是有价值的？ 每个人对于一个工作的价值会有不同的判断。我在这里只是想说说我个人的看法。Research 和 Engineer 不太一样的地方在于，后者强调work，而且倾向于使用已经proven的方法；而前者更强调novelty——创新是Research的生命。 一直以来，一些paper有这样的倾向，为了显示这个工作的“技术含量”，会在上面列出大段的数学推导，或者复杂的模型图。很多的推导只是把一些众所周知的线性代数结论重新推一遍，或者重新推一下kernel trick，又或者optimization里面的primal dual的变换。可是这些东西再多，在有经验的reviewer看来，只是在做标准作业，对于novelty加分为零。 真正的创新，在于你提出了别人没有提出过的东西。创新的内涵可以是多方面的： 建立了新的数学模型，或者提出了新的解法 提出的新的应用 提出新的框架，用新的方式来整合原有的方法 在比较性实验中获得新的观察 统一本来分开的领域，模型，或者方法 创新可以体现在从理论，建模，求解和实验的各个环节之中。判断创新与否的关键不在于有多高深的数学，不在于使用了多时髦的方法，不在于做一个多热门的topic，而在于是否make a difference。 另外，我觉得，创新的大小不能绝对而论。有一些在实验中用于improve performance的小trick，也许能被有理论基础的researcher开拓成全新的方法论，甚至建立严密的数学基础。很多paper中都埋藏着这样的金子，等待trained eyes的发掘（可能连paper的作者自己都没有意识到~~） 09月 23, 2010 | Categories: Uncategorized | 8条评论» 离开加州 在Microsoft Research Silicon Valley的intern结束了。感谢Simon，帮助我创造了一段愉快的工作经历。也要感谢Chen Wei，让这个本来枯燥的暑假增色不少。   在今天傍晚，将直接从San Francisco飞往希腊——开始一次让人憧憬的旅程。再见了，加州。         09月 3, 2010 | Categories: Uncategorized | 2条评论» From NIPS 2010 We are pleased to inform you that your NIPS 2010 submission \"Construction of Dependent Dirichlet Processes based on Compound Poisson Processes,\" paper ID 71, has been accepted for publication in the conference proceedings, with a full oral presentation at the conference.  There were a record 1219 submissions to NIPS this year, with many strong submissions. The program committee selected 293 papers for presentation at the conference; among these, only 20 papers were selected for full oral presentation, of which yours is one. Congratulations! －－－－－－－－－－ 过几天就要离开阳光加州（真是很阳光，三个月居然不见一滴雨～～～），启程参加ECCV了，希望在希腊有一个愉快的旅程，认识新的朋友。 08月 29, 2010 | Categories: Uncategorized | 11条评论» 本年度CVPR最有趣的文章 在今年的CVPR，见到了很多朋友，也看到很多Paper。 在这次会议里面让我最喜欢的一篇Paper，却不是在会议中正式发表的，而是在TC Panel派发的。这篇Paper的题目叫Paper Gestalt。文章以诙谐的笔调描述了一个基于vision + learning的自动paper review算法。 参加会议的朋友们可以很幸运的在会场获取这篇文章，至于没有来的朋友，我想只能向作者（这位兄弟（也许是姐妹）在paper中自称Carven von Bearnensquash, bearensquash@live.com）索要了。 这篇论文出炉的背景，就是最近几年CVPR或者ICCV的submission呈现急速的指数增长的趋势（在过去10年翻了三倍）。按照这个速度增长，在 10年后每次会议的投稿量就会超过5000篇！也许最好的办法是采用“货币杠杆”进行“宏观调控”——就是对投稿者收费。比如，对每个 submission征收100美元的费用，我相信对于投稿的数量和质量都会取得立竿见影的效果。一方面，很多纯粹是来碰运气的作者会掂量一下花100块 钱来博取1%的命中机会是不是值得；而持认真态度的作者则会对paper精益求精，免得投稿费白白浪费；而最终文章被录用的作者就可以减免注册费，会议方 面也有更多的funding来给有志于为Computer Vision奋斗的学术青年发放参加会议的路费。一举四得，何乐不为，呵呵。 当然了，涉及到钱的问题，自然要经历很多微妙的利益博弈——这些事情还是让Chair们去担心好了。这里，我们还是继续“奇文共欣赏”吧。文章的算法很简 单（前提是你对Machine Learning或者Computer Vision有一点了解），把8页的pdf文档并排成一张长的image，然后就在上面抽feature。做自然语言处理的朋友们请不要激动，这是 Vision的paper，自然用的是Vision圈子自己的方法。好了，抽什么feature呢？主要是HOG(Histogram of Gradients)，这是一种纯粹用于描述视觉观感的feature。显然，大段的文本，曲线图，图像，表格，数学公式，它们的feature应该是不 太一样的。然后作者用AdaBoost做feature selection训练得到一个分类器：纯粹根据paper的视觉观感来判断paper的好坏。 说到训练分类器，自然需要一个训练集。这篇文章的作者收集了CVPR 2008, ICCV 2009和CVPR 2009的全部1196篇paper构成正样本。那么负样本从何而来呢？被拒的paper显然作者是拿不到的。于是他很聪明的利用了一个众所周知但是大家 却不会公开明言的事实：workshop接纳的很多是在主要会议被拒收的paper。这样，很不幸的，workshop上发表的文章被用作负样本。在 Workshop上发表了论文的同志们不要打我——我只是讲述一篇别人的文章，这个主意不是我出的。 最有趣的部分要数实验结果了。从ROC曲线来看，结果其实还是不错的——以拒绝15%的正样本为代价，可以滤除一半的负样本。作者对于正负样本的特征做了 一些总结，也许对于大家以后投paper还是有点指导意义呢。。。 正样本的“视觉”特点： 1. 里面有几段公式，看上去文章显得似乎很专业，也显得作者似乎数学不错； 2. 实验部分里面多少要有几个曲线图，即使那几个曲线图说明不了什么。但是，只要有几个曲线图在那里，起码表示我做的是“科学实验”； 3. 最好在文章开头或者最后一页排列一堆图像。其实，我也注意到很多作者喜欢排列很多dataset里面的图像到paper上——即使那是一个 publically available的standard dataset——我不知道这样做的意义何在——除了审美效果。 4. 最好写满8页，代表分量足够。 负样本的特点： 1. 不够页数。在submission阶段，写不满6页的文章被录用的机会很小。虽然最后很多本来8页的文章还是能很神奇地被压缩到6页，如果作者想省掉 200美元的附加页费。题外话，我也一直不明白为什么多一页要多交100美元注册费。 2. 有很大的数字表，就是m行n列，排满数字那种。这篇文章表明，排列了很多曲线图和柱状图的文章比排列了很多数字表的文章有更大概率被接收。 3. 没有漂亮插图。 这篇文章的结果，我也做一些补充评论。 1. CVPR和ICCV的录用结果，对于文章的视觉观感，有着显著的统计相关。从我自己做Reviewer的经验，以及和其它reviewer的交谈来说，这 个确实在一定程度上影响了reviewer的第一印象，甚至是评价基调。一篇文章在first glance给人以专业和有内涵的感觉，会有利于它在reviewer心中树立良好印象。这与学术无关，但是，很不幸，却是一个普遍存在的事实。 对于NIPS这种理论取向的会议，虽然不需要那么多漂亮的图表，但是，文章要“长得”像这些会议的文章。让人觉得写文章的是一个有经验的研究者，而不是一 个打酱油的。 2. Workshop的文章和CVPR/ICCV主会似乎存在明显差距，以至于一个如此简单的分类器都能够在区分它们的任务中取得不俗的成绩。另外，作者使用 workshop paper作为负样本的做法虽然是个人选择，但是，起码在一定程度上反映了这","title":"MIT 大牛林达华"},{"content":"python3之python的核心数据类型(字符串) ---------- 字符串是用来记录文本信息的。它们是在python中作为序列(也就是说，一个包含其他对象的有序集合)提到的第一个例子。序列中的元素包含了一个从左到右的顺序---序列中的元素根据它们的相对位置进行存储和读取。从严格意义上来说，字符串是单个字符的字符串的序列，其他类型的序列还包括列表和元组。 序列的操作 作为序列，字符串支持假设其中各个元素包含位置顺序的操作。例如，如果我们有一个含有四个字符的字符串，我们通过内置的len函数验证其长度并通过索引操作得到其各个元素。 >>>S = 'Spam'>>>len(S)4>>>S[0]'S'>>>S[1]'p'在python ,索引是按照从最前面的偏移量进行编码的，也就是从0开始，第一项索引为0，第二项索引为1，依此类推。注意我们在这里是如何把字符串赋给一个名为S的变量的。我们随后将详细介绍这是如何做到的，但是，python变量不需要提前声明。当给一个变量赋值的时候就创建了它，可能赋的是任何类型的对象，并且当变量出现在一个表达式中的时候，就会用其值替换它。在使用变量的值之前必须对其赋值。我们需要把一个对象赋给一个变量以便保存它供随后使用。在python中，我们能够反向索引，从最后一个开始(正向索引是从左边开始计算，反向索引是从右边开始计算)。 >>>S[-1]'m'>>>S[-2]'a'一般来说，负的索引号会简单地与字符串的长度相加，因此，以下两个操作是等效的。 >>>S[-1]'m'>>>S[len(S)-1]'m'值得注意的是，我们能够在广括号中使用任意表达式，而不仅仅是使用数字常量---只要python需要一个值，我们可以使用一个常量、一个变量或任意表达式。python的语法在这方面是完全通用的。除了简单地从位置进行索引，序列也支持一种所谓分片(slice)的操作，这是一种一步就能够提取整个分片(slice)的方法。例如： >>>S'Spam'>>>S[1:3]'pa'也许认识分片的最简单的办法就是把它们看做是从一个字符串中一步就提取出一部分的方法。它们的一般形式为X[I:J],表示\"取出在X中从偏移为I，直到但不包括偏移量为J的内容\"。结果就是返回一个新的对象。在一个分片中，左边界默认为0，并且右边界默认为分片序列的长度。这引入了一些常用法的变体： >>>S[1:]'pam'>>>S'Spam'>>>S[0:3]'Spa'>>>S[:3]'Spa'>>>S[:-1]'Spa'>>>S[:]'Spam'最后，作为一个序列，字符串也支持使用加号进行合并(将两个字符串合成一个新的字符串),或者重复(通过再重复一次创建一个新的字符串): >>>S'Spam'>>>S+'xyz''Spamxyz'>>>S'Spamxyz'>>>S*8'SpamSpamSpamSpamSpamSpamSpamSpam'注意加号(+)对于不同的对象有不同的意义：对于数字为加法，对于字符串为合并。这是python的一般特性，也就是我们将会在本书后面提到的多态。简而言之，一个操作的意义取决于被操作的对象。正如将在学习动态类型时看到的那样，这种多态的特性给python代码带来了很大的简洁性和灵活性。 不可变性 注意：在之前的例子中，没有通过任何操作对原始的字符串进行改变。每个字符串都被定义为生成新的字符串作为其结果，因为字符串在python中具有不可变性---在创建后不能就地改变。例如，不能通过对其某一位置进行赋值而改变字符串，但是你总是可以通过建立一个新的字符串并以同一个变量名对其进行赋值。因为python在运行过程中会清理 旧的对象。 >>>S'Spam'>>>S[0]='z'...error text omitted...TypeError: 'str' object does not support item assignment>>>S='z'+S[1:]>>>S'zpam'在Python中的每一个对象都可以分为不可变性或者可变性。在核心类型中，数字、字符串和元组是不可变的；列表和字典不是这样(它们可以完全自由地改变)。 类型特定的方法 目前我们学习过的每一个字符串操作都是一个真正的序列操作。也就是说，这些操作在python中的其他序列中也会工作，包括列表和元组。尽管这样，除了一般的序列操作，字符串还有独有的一些操作方法存在。例如，字符串的find方法是一个基本的子字符串查找操作(它将返回一个传入子字符串的偏移量，或者没有找到的情况下返回-1)，而字符串的replace方法将会对全局进行搜索和替换。 >>>S.find('pa')1>>S'Spam'>>>S.replace('pa','XYZ')'SXYZm'>>>S'Spam'尽管这些字符串方法的命名有改变的含义，但在这里我们都不会改变原始的字符串,而是会创建一个新的字符串作为结果---因为字符串具有不可变性，我们必须这样做。字符串方法将是python中文本处理的头号工具。其他的方法还能够实现通过分隔符将字符串拆分为子字符串(作为一种解析的简单形式)，大小写变换，测试字符串的内容(数字、字母或其他),去掉字符串后的空格字符。 >>>line='aaa.bbb.ccccc.dd'>>>line.split(',')['aaa','bbb','ccccc','dd']>>>S='spam'>>>S.upper()'SPAM'>>>S.isalpha()True>>>line='aaa.bbb,ccccc,dd\\n'>>>line=line.rstrip()>>>line'aaa,bbb,ccccc,dd'字符串还支持一个叫做格式化的高级替代操作，可以以一个表达式的形式(最初的)和一个字符串方法调用(python2.6和python3.0中新引入的)形式使用： >>>'%s,eggs,and %s' % ('spam','SPAM!')'spam,eggs,and SPAM!'>>>'{0},eggs,and {1}'.format('spam','SPAM!')'spam,eggs,and SPAM!'注意：尽管序列操作是通用的，但方法不通用(虽然某些类型共享某些方法名，字符串的方法只能用于字符串)。一条简明的法则是这样的：可作用于多种类型的通用型操作都是以内置函数或表达式的形式出现的[例如，len(X),X[0]],但是类型特定的操作是以方法调用的形式出现的[例如，aString.upper()]。如果经常使用python，你会顺利地从这些分类中找到你所需要的工具。 寻求帮助 上一节介绍的方法很具有代表性，但是仅仅是少数的字符串的例子而已。对于更多细节，你可以调用内置的dir函数，将会返回一个列表，其中包含了对象的所有属性。由于方法是函数属性，它们也会在这个列表中出现。 >>>dir('aaa')dir函数简单地给出了方法的名称。要查询它们是做什么的，你可以将其传递给help函数。 >>>help('aaa'.replace)help是一个随python一起分发的面向系统代码的接口。另外你也能够对整个字符串提交查询，如： help('aaa'),一般最好去查询一个特定的方法，就像我们上边一样。 编写字符串的其它方法 到目前为止，我们学习了字符串对象的序列操作方法和类型特定的方法。python还提供了各种编写字符串的方法，我们将会在下面进行更深入的介绍。例如，反斜线转义序列表示特殊的字符。 >>>S='a\\nB\\tC'>>>len(S)5>>>ord('\\n')10>>>S='A\\oB\\oC'>>>len(S)5python允许字符串包括在单引号或双引号中(它们代表着相同的东西)。它也允许在三个引号(单引号或双引号)中包括多行字符串常量。当采用这种形式的时候，所有的行都合并在一起，并在每一行的末尾增加换行符。这是一个微妙的语法上的便捷方式，但是在python脚本中嵌入像HTML或XML这样的内容时，这是很方便的。 >>>msg=\"\"\"aaaaaaaaaaabbbb'''bbbbbb\"\"bbbbbb'bbbbccccccccccc\"\"\">>>msg'\\naaaaaaaaaa\\nbbb\\'\\'\\'bbbbbbbbb\"\"bbbbbb\\'bbbb\\nccccccccccccc' 模式匹配 值得关注的一点就是字符串对象的方法能够支持基于模式的文本处理。文本的模式匹配是本书范围之外的一个高级工具，但是有其他脚本语言背景的读者也许对在python中进行模式匹配很感兴趣，我们需要导入一个名为re的模块。这个模块包含了类似搜索、分割和替换等调用。 >>>import re>>>match=re.match(\"Hello[\\t]*(.*)world',\"Hello Python world')>>>match.group(1)\"python '这个例子的目的是搜索子字符串，这个子字符串以\"Hello\"开始，后面跟着零个或几个制表符或空格，接着有任意字符并将其保存到匹配的group中，最后以\"world\"结尾。如果找到了这样的子字符串，与模式中括号包含的部分匹配的子字符串的对应部分保存为组.如例： >>>match=re.match('/(.*)/(.*)/(.*)','/usr/home/lumberjack')>>>match.groups()('usr','home','lumberjack')模式匹配本身是一个相当高级的文本处理工具，但是在python中还支持更高级的语言处理工作，包括自然语言处理等。","title":"python3之python的核心数据类型(字符串)"},{"content":"IT行业一直是国家优先发展的重点行业，也是国内外人才需求量最大的行业之一。尤其是近几年，很多跨国公司陆续在中国成立软件研发中心，为计算机专业的高层次人才提供高质量的就业机会。这给计算机科学与技术专业的研究生提供了极好机遇，也使这一专业的报考火热。计算机科学与技术专业属于一级学科，下设计算机系统结构、计算机软件与理论、计算机应用技术等二级学科。由于计算机今年第一次参加统考，学校以及二级学科的选择显得尤为重要。在这里跨考网计算机教研室的张老师为大家介绍有关计算机这三个二级学科的有关情况。 计算机系统结构——深入“计”心 计算机系统结构是计算机科学与技术专业的重要学科之一，主要研究计算机硬件与软件的功能分配、软硬件界面的划分、计算机硬件结构组成与实现方法及技术，其中嵌入式系统无疑是当前最热门、最有发展前途的方向之一。随着家电智能化趋势的加强，嵌入式系统的重要性也日益凸显。嵌入式科学技术应用非常广，如日常生活中的手机、PDA、电子字典、可视电话、VCD/DVD/MP3 Player、数字相机（DC）、数字摄像机（DV）、机顶盒（Set Top Box）、高清电视（HDTV）、航天航空设备等。此外，人工智能也是计算机系统结构的热门方向之一，它主要研究如何利用计算机去模拟、延伸和扩展人的智能，如何设计和建造具有高智能水平的计算机应用系统（如专家系统软件、机器博弈软件等）。该领域的研究还包括机器人、语言识别、图像识别、自然语言处理等。 就业信息：计算机系统结构专业要求学生具有扎实的计算机软硬件基础，不仅能对计算机系统进行研究与设计，还要具有计算机应用、软件开发的能力。相对而言，该专业的毕业生还是比较好找工作的，适合于从事计算机网络、嵌入式技术、高性能计算、网络信息安全和多媒体信息处理等研究领域或工程技术领域的工作。该专业还涉及到一些硬件和计算机核心技术的知识，很多跨国公司和国内比较知名的大公司如Microsoft、IBM、HP、AMD、Autodesk、Intel、Sun、Oracle、SAP、Bayer、BEA、Sybase、NCR、中兴、华为、微创软件、汉得咨询、振华港机等对该专业人才的需求量都比较大。 推荐院校： 清华大学、华中科技大学、西安交通大学、上海交通大学、浙江大学、西安电子科技大学、武汉大学、复旦大学、哈尔滨工业大学、东北大学、北京大学、东南大学、北京航空航天大学、中国科学技术大学。 计算机软件与理论——“软硬”通吃 计算机软件与理论专业主要研究软件设计、开发、维护和使用过程中涉及的软件理论、方法和技术，探讨计算机科学与技术发展的理论基础。其研究方向众多，包括嵌入式软件、图形图像与多媒体、操作系统、计算机语言与编译系统、网络与信息安全等，北京大学该专业有20多个研究方向，软件与硬件方面均有所涉及。该专业竞争比较激烈，大部分研究方向如信息安全理论及应用、嵌入式系统、计算智能、信息安全、新型程序设计与方法学、软件自动化、分布计算与并行处理、软件工程、先进操作系统、计算机系统信息安全都是当今IT市场比较热门的方向。 近几年，嵌入式技术被广泛应用于通信、交通、电子、医疗、军事等众多领域，已成为国内IT产业发展的核心方向，我国软件产值中的40%来自嵌入式软件，大力发展嵌入式软件技术与应用已迫在眉捷，因为它已经成为我国软件产业实现跨越式发展的又一重要砝码。业内人士认为，目前已出现至少30万～50万的人才缺口，其中嵌入式专业人才缺口15万，移动增值专业人才缺口接近35万。 就业信息：计算机软件与理论专业可选择的就业方向很多，只要与计算机有关的，无论是网络、编程还是关于图形图像等工作能够很较快上手，可以说是“软硬”通吃。跨考网张老师提醒大家：虽然该专业就业前景看好，但是在找工作时，仅靠基础理论是绝对不够的，基础与技能并重才是成功就业的法则。毕竟最受企业欢迎的还是能独立解决问题并具有操作能力的毕业生。每年该专业的人数很多，要想在激烈的竞争中脱颖而出，必须有自己的独特优势。 推荐院校：上海交通大学、南京大学、北京大学、北京航空航天大学、吉林大学、清华大学、浙江大学、电子科技大学、大连理工大学、中山大学、北京理工大学、西北工业大学、武汉大学、山东大学、西安交通大学。 计算机应用技术——创新与开发能力并重 计算机应用技术的研究方向非常广泛，包括网络攻防技术、网络与数据库技术的应用、数据仓库与数据挖掘、多媒体与智能信息检索、数据网格与知识网格、计算机视觉与虚拟现实、模式识别与图像处理等。如比较热门的计算机网络与信息安全技术方向是以计算机网络技术和信息安全技术为核心，以数据加密技术、入侵检测技术和智能防火墙技术为突破口，进行网络与信息安全技术的研究。随着国内信息化产业的迅速推进及互联网的蓬勃发展，市场对网络工程师、网络管理员等技术人才的需求日渐看涨。跨考网张老师介绍：相较而言，网络工程师的就业机会比软件工程师要多，可在数据库管理、WEB开发、IT销售、互联网程序设计、数据库应用、网络开发和客户支持等领域发展。 就业信息：对于计算机应用技术专业的毕业生来说，拥有较好的创新能力和编程开发能力是非常重要的。目前最主流的数据仓库平台应是ORACLE的数据仓库工具，在国外，一些特殊数据仓库如NCR/TEREDATA的人才非常紧缺。所以对于数据仓库方向的同学来说，不妨选择一些人才稀缺的行业，不仅可以避开激烈的就业竞争，而且薪酬也比较可观。 也有很多企业招聘图像处理与模式识别算法工程师，识别、计算机视觉算法工程师，如美国虹软公司、中科院深圳研究院，都要求应聘者精通C++、VC、MATLAB，熟练图像处理基础算法，而且要有比较深厚的专业积累。尽管市场对图像处理与模式识别方向的要求较高，但已有很多院校在研究生期间就给学生提供了非常好的实习环境，如上海交通大学的图像处理与模式识别研究所已在图象处理、模式识别、数据融合、数据挖掘、人工智能领域主持承担30多项国家和省部级科研项目，包括医学图像处理、目标识别与跟踪、人脸识别、复杂时序信号的识别、中医舌脉像信息处理、传感器网络、生物信息学等方面。这不仅给学生创造了良好的学习和实践的环境，也大大增强了毕业生的就业竞争力。 推荐院校： 清华大学、浙江大学、哈尔滨工业大学、北京大学、东南大学、东北大学、西北工业大学、安徽大学、华中科技大学、西安电子科技大学、北京工业大学、大连理工大学、复旦大学、哈尔滨工程大学、武汉理工大学。 总的来说，计算机科学与技术专业的研究生教育应定位于应用型与研究型相结合，主要侧重于应用，无论选择哪个专业，知识与能力并重，才能成为IT行业中的强者。","title":"计算机考研－专业方向"},{"content":"  1） 语义网的头10年（2001-2011），是成功的10年，也是失败的10年。语义网的后一个10年，要强调实事求是、群众路线和“武装”斗争。 2）语义网的头10年是成功的，因为10年前除了在少数专业领域（比如医学和生物），几乎没有对广大Web用户有价值的数据集的存在。在10年中，各种标准语言，如OWL, SPARQL, RIF, SKOS, RDFa等被制定出来，并出现了数以百计的实现。专门从事语义网相关产业的公司有上百家，而主要的大IT公司和很多传统领域公司都有相关的团队进行语义网的应用或者预研。现在，有很多的开放域数据可供我们使用，例如DBPedia, Freebase, 和几百个Linked Data数据集。这些数据集，在提高某些问题解决方案（比如问答系统）的实践中，被证明是有价值的。数以千计的语义网的研究和技术人员被培养出来并进行各行各业，一些人已进入了决策阶层。 3）语义网的头10年是失败的，因为它发展的速度大大低于预期和Web本身。Web在1991年出现后，短短3、4年就为决策层高度重视，到2001年时，已经产生了百亿美元以上的市场，并几乎改变了IT和非IT产业的游戏规则。大约到2001年前后，目前意义上的社交网络（Social Web）开始形成（代表性事件是wikipedia的建立和blog进入主流）；到2011年，已经出现了数以千计的各种Social Web公司，十亿以上的用户和百亿以上的市场。语义网的成功，与Web本身或者Social Web，还有两个以上数量级的差别。2001年《科学美国人》上的文章，目前还被看成一种科幻。 4）语义网发展的相对缓慢，从宏观上，很大程度上源于三个脱离：脱离现实，脱离群众，脱离市场。近年来，已经有很多工作来解决这三大问题，但到目前为止，还是初步的。 5）说语义网脱离现实，是因为早期的语义网推动力，高校和W3C，集中注意力于技术的发展和规范。但是，一些实验室的方法无法推广到实际的Web应用去，例如如下问题： 认为推理能力是必要的，并强调推理的完备性和正确性。这使OWL使用描述逻辑DL作为建模基础 认为Web是分散的从而本体也是分散的和自主的，可以使用本体映射的方法来做数据集成 认为用户会使用本体编辑器或某些标注工具来提供元数据 认为用户可以进行简单的本体建模，如区分概念、关系和实例，建立概念分类树 现有的工具（如推理机和语义数据库）无法处理Web级的数据（这直到最近一两年才得到改观） 认为表达力的限制是制约语义网发展的核心问题（从而制定了OWL2和RIF） 6）相对的，语义网的发展，应该实事求是。应当放弃逻辑推理为主的想法。语义网的实现，应当是多种技术的综合使用，比如信息检索、机器学习、数据库、自然语言处理、数据采集、可视化等。推理在其中，是起到一个辅助的而不是主要的地位。应当从问题出发，探索既有技术的集成和发展，而不是从主义出发，至上而下规划技术的发展。从问题出发，就要多开发如IBM的沃森系统和TripIT（建立和旅行相关的元数据）这样的系统，在解决具体问题的过程中发现问题，解决问题。从主义出发，就是象OWL 2的制定一样，先定下技术基调，再通过OWL ED这样“自己人”的小圈子来寻找应用案例，最后用技术的考虑（例如推理的完备性和最差时间复杂性）而不是实际应用的考虑来决定取舍。应当重视Web工程系统的复杂性和现实数据的低质量性。Web科学和Web工程，如同化学和化工的区别，要考虑各种实验室中不会出现的问题。用户会“犯错”，数据必然是杂乱和充满噪声的，90%的正确率在实验室中是很好的结果而在应用中往往不可接受；绝大多数程序员还不能接受传统语义网技术（如RDF和OWL）所要求的知识建模能力；语义网技术规范集的复杂性超过了许多中小企业和开发团队的接受能力。具体如何制定可行的技术路线，应当从实践中来，到实践中去，大兴调查研究之风，而不是本本主义。某些W3C的工作组，制定Web的规范而几乎一个大Web公司的代表都没有，这是闭门造车，结果十九是碰壁。 7）说语义网脱离群众，是因为早期的语义网的“用户”，过多集中于特殊行业用户，如制药、医学、出版等大型企业和研究机构。从这些特定用户、特定案例获得的经验，难以推广到数以十亿计的普通Web用户那里。如何从普通用户那里获得数据？如何使语义网数据可以造福普通用户？长久以来，人们在呼唤语义网的“杀手级应用”，到目前为止还没有出现。我认为很重要的原因，就是脱离对普通用户的需求的关注，着眼的数据集既不是从普通用户那里来，也不是普通用户所需要的，哪里能建立好的、普通用户喜欢的应用？ 8）语义网要走群众路线，就是要急群众之所急，想群众之所想。普通Web用户需要什么？衣食住行，饮食男女。找工作，有找工作的数据；买房子，有买房子的数据；炒股，有炒股的数据。看电影是数据，打电话也是数据；买菜是数据；治病也是数据。先有了语义网的数据，才有了语义网的应用。先有了语义网的高质量数据，才有了语义网的高质量应用。Social Web之所以成功，一是它利用了Web Form，解决了数据输入的门槛；二是利用了社交粘性，解决了数据发布的即时回报效用。语义网的高质量数据，也要首先降低结构化数据输入的门槛，使数据在用户的自然活动中产生并被采集、提取，甚至不一定需要键盘、鼠标或者触摸屏的输入；其次，要使用户在发布数据的短期内就可以得到回报，保持其数据发布的意愿。一个杀手级应用，设计之初就应当面向百万之上的用户，而且与普通人的日常生活相结合。群众是要分两亩地不是要共产主义；群众是要搜仓老师的作品而不是仓老师的foaf。 9）说语义网脱离市场，是说早期的语义网推广路线，不是依赖Web公司循序渐进，而是企图至上而下，事先规划了行业的发展方向。语义网是一种革命，好的革命的路线要也要有人事的配合，也要有“武装”的支持——这里的武装，就是市场，具体就是投资。作为规划，既要有远期（3年以上）的蓝图，也要有近期（6-12个月）的规划。Google和Microsoft等大公司在语义网技术的采用上是谨慎的，当然并不是保守的。他们通过并购的方式吸纳语义网的小公司（如Freebase和Powerset），并试图消化语义网的团队技术到他们的主流产品中，这是一种渐进而稳妥的方式。我们期待更多的这种并购出现，比如在Facebook或者LinkedIn。 10）语义网要坚持“武装”斗争，就是要充分利用现有的产业平台，而不是另起炉灶。要先降低姿态，以初级阶段的语义网技术来辅助现有的产业平台来提高某些应用的性能，也许只能有几个百分点的提高。关键是通过关键应用（比如搜索、广告和推荐）的这几个百分点的提高，起到语义网技术的示范作用。要让语义网的技术沾满铜臭，然后才能有进一步的良性循环。 11）总结：我认为，语义网的三个（还有其他n个）未来应着眼的重点是 实事求是：放弃实验室思维，重视工程实践 群众路线：面向普通用户采集数据，为普通用户开发应用 “武装”斗争：从人事和资金上，加强利用现有产业平台（主要是大Web公司），渐进实现技术-市场的良性循环 P.S. (2011-06-17) 中国革命开始的设想，是工人城市暴动。这个方法不是很成功。首先，在社会的主要组成群体还是农民的时候，工人不与农民相结合，不能有大的作为。其次，革命的武装，其主要成员必然是农民；在革命的早期阶段，也要根植与农村，提出农民可以接受的、可以理解的口号和政策。语义网的今天，“农民”和“农村”就是现有的最广泛的、面向普通Web用户的应用。现有的大多数“应用”，无论是Protege, Swoogle, Sindice, NCBO Index (2010 SWC winner), TrialX (2009 SWC winner)，都不是为普通用户设计的。这一点应该在不久的未来得到改变。 P.S.2 (2011-06-18) 文中提到的决策层，既包括政府，也包括商业和各种非营利组织的政策制定者。语义网技术从开始到现在最主要的支持者就是美国军方，从DARPA, IARPA到ARL(Army Research Lab)。Data.gov计划和英国政府数据计划都是在政府的大力支持下展开的；这些计划，是争取了包括英国首相和美国CIO在内的高级政府官员才得以展开。","title":"语义网的红旗到底能打多久？"},{"content":"计算机视觉库 OpenCV OpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业... 更多OpenCV信息 最近更新： OpenCV-2.3.0rc 发布了 发布于 2个月前 人脸识别 faceservice.cgi faceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。 更多faceservice.cgi信息 运动检测程序 QMotion QMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。 更多QMotion信息 视频监控系统 OpenVSS OpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。 更多OpenVSS信息 手势识别 hand-gesture-detection 手势识别，用OpenCV实现 更多hand-gesture-detection信息 人脸检测识别 mcvai-tracking 提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);... 更多mcvai-tracking信息 人脸检测与跟踪库 asmlibrary Active Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。 更多asmlibrary信息 开放模式识别项目 OpenPR Pattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。 更多OpenPR信息 图像特征提取 cvBlob cvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取. 更多cvBlob信息 视频捕获 API VideoMan VideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。 更多VideoMan信息 基于QT的计算机视觉库 QVision 基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。 更多QVision信息 3D视觉库 fvision2010 基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ... 更多fvision2010信息 图像处理和计算机视觉常用算法库 LTI-Lib LTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具 更多LTI-Lib信息 实时图像/视频处理滤波开发包 GShow GShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。... 更多GShow信息 C++计算机视觉库 Integrating Vision Toolkit Integrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV 更多Integrating Vision Toolkit信息 视觉快速开发平台 qcv 计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。 更多qcv信息 模式识别和视觉库 RAVL Recognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。 更多RAVL信息 OpenCV优化 opencv-dsp-acceleration 优化了OpenCV库在DSP上的速度。 更多opencv-dsp-acceleration信息 计算机视觉算法 OpenVIDIA OpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API'... 更多OpenVIDIA信息 图像捕获 libv4l2cam 对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出 更多libv4l2cam信息 高斯模型点集配准算法 gmmreg 实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口... 更多gmmreg信息 OpenCV的扩展库 ImageNets ImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。 更多ImageNets信息 STAIR Vision Library STAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。 更多STAIR Vision Library信息 libvideogfx 视频处理、计算机视觉和计算机图形学的快速开发库。 更多libvideogfx信息","title":"C/C++ 计算机视觉库/人脸识别开源软件"},{"content":"本文为淘宝广告技术部广告算法负责人、淘宝网研究员吴雪军在8月3日CTO俱乐部沙龙演讲实录 转载自: http://news.csdn.net/a/20110809/302848.html 编者按：本文为淘宝广告技术部广告算法负责人、淘宝网研究员吴雪军在8月3日CTO俱乐部沙龙演讲实录，全文如下： 我今天演讲的题目是自然语言处理技术在搜索和广告中的应用。搜索和广告是技术非常密集两个互联网产品，它们前端都非常简单，但后台系统架构极其复杂。 今天主要讲三个方面的内容：第一、主要是介绍一比较典型的互联网应用体系；第二、主要介绍NLP技术在搜索中的应用；第三、介绍NLP技术在物联网广告中的应用。 NLP技术体系 首先介绍NLP技术体系，NLP技术体系在不同的应用需求、不同的领域下，拥有不同的组织形式。下图是一种比较典型的面向互联网应用的技术体系。 在05年之前NLP技术在实际应用中，特别是互联网应用还中比较多。07年时我参加自然语言学者技术研讨会。会上很多人都是国内做NLP技术、自然语言处理技术的前沿代表人物，当时我们讨论的主要问题就是NLP技术在实际应用中有没有价值？ 底层为数据层，包含三种类型的数据，1.词典，词条译本在分词或一些词法分析内可用到；2.知识库，内包含一些语言，语义分析处理是比较重要的功能；3.统计的数据，主要是词汇共现、ngram数据。以上比较有代表性的三个数据。 第二层是Term级。分为词法分析、Term语义表和Term关系。1.词法分析包含分词分词、词性标注和未登录词识别；2.Term语义表包含属性/类别和语义的表示；3.Term关系包含同义关系、词汇见关系和知识库构建。 第三个层短串涉及一些变化，分为短串解析、短串语义表示和短串变换。1.短串解析分为结构分析/浅层句法分析和Term重要性分析；2.短串语义表示包含短串主题分类短串语义表示；3.短串变换包含同义词替换、语义归一化以及省略纠错 第四个层为篇章级，分为单文档分析和多文档分析，在研究领域应用较多，在分析的领域，有诸如PLSA、LDA这样海量的文本分析技术。 NLP技术在搜索中的应用 侧重介绍在NPL在搜索引擎和互联网广告的应用，下图为一个简单的搜索引擎基础架构，第一块为最基本的网页抓取，第二块是网页的分析、索引。第三大块为一个查询。蓝色的三块是NLP技术应用比较多的三个方向，我将介绍这三个方向中NLP的应用。 query分析/Rank  一、短串分析技术，涉及到结构的分析，Term重要性的标注，对短串进行初步的处理。为后续查询和语义的相关度计算做一些基础的分析。由于查询需求有很多不同的表示方法，我们会对query进行改写，使其能比较好的召回。这其中其实最主要的技术是短串的语义相关性。    二、语义规化，即相同语义用不同方法表示，这种语义规化技术在搜索引擎中应用广泛。语义短串在这里能很好的被应用，用一种相同的形式表示，然后计算它们之间的关联。 三、纠错，我们需要分析用户需要什么，对query需求的识别能针对性的满足用户不同的需求，或者整合成特定的数据库用来满足精确的需求。快速需求识别是比较重要的应用，其中的技术可被理解为对query语义类别的识别，即短串的分类。 在排序上的应用，NLP技术在这里面体现是相关性计算，query和网页相关性。在搜索内Rank代表两个体系，像百度、谷歌规则为主的系统以及像微软、雅虎用这种机器学习的Rank系统。在机器学习的Rank起到的作用还是最基本的query文本的语义表示，这些特征。规则系统里面会涉及到一些语义的计算、相关性的识别和相关性计算。 在网页分析和索引中应用主要涉及对象是网页title、Term权重计算、网页语义表示和网页类别识别。 互联网广告技术整体架构 NLP在互联网广告技术中应用包含了三种类型的广告技术：用户行为分析、网页内容分析/站点分析和Query分析，涉及到很多基本的广告库分析，索引。  如果能定向广告会涉及到用户行为分析。要把用户行为表成一个能去检索广告的形式，如果用户行为很多，这里面会涉及到行为排序。如果广告库很小，我们可能对这个行为的表示可能会抽象一些。如果整个广告集合很多，侯选广告很多可以分层次。对于内容广告而言，会涉及比较多的对网页内容的分析，这方面主要涉及到最基本网页主题的提取，另外还会涉及到关联内容分析，因为广告一般都具有商业价值的内容，我们会把广告内容关联到有商业价值的内容上去，做到对广告的匹配。 对于搜索广告而言，请求分析主要是query分析，与搜索大体一样。对用户的基本请求解析完之后，会涉及到怎么去匹配广告如果广告集合很大我们不可能对每个广告做相关性计算，所以先要保证能够把相关的广告召回做一个集合，然后对这个集合进行相关性计算。     最后一方面的应用是广告排序，收费在搜索广告里是很重要的形式，按点击量来收费，所以排序最主要的是预测其点击率作为排序的依据。预测点击率后我们根据其价格算出其基本的收益。其最核心的技术是预估点击率，CTR是排序的核心。语义特征的表示，以及相关性的机损，广告可以有一部分特征语义来表示。","title":"淘宝吴雪军：自然语言处理技术在搜索和广告中的应用"},{"content":"WordNet 1 WordNet简介  传统词典一般都是按字母顺序组织词条信息的，这样的词典在解决用词和选义问题上是有价值的。然而，它们有一个共同的缺陷，就是忽略了词典中同义信息的组织问题。20世纪以来，语言学家和心理学家们开始从一个崭新的角度来探索现代语言学知识结构以及特定的词典结构，终于由Princeton大学研制成功了一个联机英语词汇检索系统—WordNet，它作为语言学本体库，同时又是一部语义词典，在自然语言处理研究方面应用非常广泛。 WordNet与其他标准词典最显著的不同在于：它将词汇分成五个大类：名词、动词、形容词、副词和虚词。实际上，WordNet仅包含名词、动词、形容词和副词。虚词通常是作为语言句法成分的一部分，WordNet忽略了英语中较小的虚词集。 WordNet最具特色之处是根据词义而不是词形来组织词汇信息。可以说WordNet是一部语义词典。但是与按字母排列的语义词典以及按主题排列的语义词典都不同，它是按照词汇的矩阵模型组织的。如表2.1所示。同义词集合（synonymy set）可以看作是词形（word form）之间一种具有中心角色的语义关系。WordNet的2.0版本中，有115424个同义词集合，其中名词同义词集合就有79685个。基本上涵盖了我们常用的英语名词词汇。 表1 词汇矩阵概念示意：F1和F2为同义词；F2是多义词   词形 F1 F2 F3 … … Fn M1 E(1, 1) E(1, 2)         M2   E(2, 2)         M3     E(3, 3)       …             Mm           E(m, n) 表1简单说明了词汇矩阵的设想：假定表中的列代表词形，行代表词义，矩阵中的表元素对应列上的词形可以被用来表示相应表行上的词义（在一个适当的上下文环境中）。这样，表元素E(1,1)就表示：词形F1可以表示词义M1；如果同一表列中有两个表元素，则该词形具有两个义项，是个多义词（polysemy）；如果同一表行中有两个表元素，则对应的两个词形是同义的，相应的两个词是同义词（synonymy）。 2 WordNet中的语义关系  WordNet中除了具有中心角色的同义关系外，还有反义关系、上下位关系和部分关系。 2.1 反义关系  反义关系（antonymy）是很难定义的一种语义关系。一个词x的反义词有时并不是非x。例如，“富有（rich）”和“贫穷（poor）”是一对反义词，但是要说某个人不富有并不意味着一定穷；许多人认为自己既不富也不穷。反义词似乎是一种简单的对称关系，实际上却是相当复杂的。反义词是一种词形间的语义关系，而不是词义间的语义关系。例如，词义｛升高，上升｝和｛下落，下降｝可能在概念上是相对的，其中[升高/下落]是反义词，[上升/下降]也是反义词。但是，如果说“升高”与“下降”与“上升”于“下落”是否是反义词，就要考虑一下了。所以有必要区分词形之间的语义关系和词义之间的语义关系。反义关系为WordNet中的形容词和副词提供了一种中心组织原则。 2.2 上下位关系  与同义词和反义词都是词形之间的词汇关系不同，上位关系（hypernymy）/下位关系（hyponymy）是词义之间的语义关系。例如：｛樟树｝是｛树｝的下位词，｛树｝又是｛植物｝的下位词。下位/上位关系也称为从属/上属关系，子集/超集关系，或ISA关系。如果以英语为母语接受以“An x is a (kind of) y”框架构造的句子，则同义词集合｛x1,x2,…｝表示的概念与同义词集合｛y1,y2,…｝表达的概念是下位概念和上位概念的关系。上下位关系具有某种限制，而且是一种不对称的关系。通常情况下，一个同义词集合如果有与之是下位概念和上位概念的关系的同义词集合，则也只有惟一的一个。即便是不惟一，同为上位概念的关系的同义词集合之间差别也是非常小的。这就产生了一种层次语义结构，其中下位词位于其上属关系的下层。这样的层次表达方法，Touretzky称作继承体系，它意味着下位词继承了上位词更一般化概念的所有性质，并且至少增加一种属性，以区别它与它的上位词以及该上位词的其他下位词。例如，“枫树”继承了其上位词“树”的属性，但却以其坚硬的木质、叶片的形状等特性区别于其他的树。这种方法为WordNet中的名词提供了一种核心的组织原则。在2.5.2节中我们将根据WordNet名词体系中的这一继承体系的特点，定义基于一个概念（同义词集合）的概念链。 2.3 部分关系  同义关系，反义关系和上下位关系都是比较容易理解的语义关系。另一种语义关系称为“部分-整体关系”（简记为HASA），语言学家称之为部分词（meronym）/整体词（holonym）的关系。如果以英语为母语接受以“A y is an x”或“An x is a part of y”框架构造的句子，则同义词集合｛x1,x2,…｝表示的概念与同义词集合｛y1,y2,…｝表达的概念是部分概念和整体概念的关系。部分关系也具有某种限制，且是不对称的关系，可以构造一种部分等级关系。 3 WordNet名词体系中相关概念 下面，我们介绍WordNet名词体系中一些重要概念。 (1)    独立起始概念（Unique Beginner） 如果有一同义词集合（即概念）没有上位同义词集合（即上位概念），则称之为独立起始概念（Unique Beginner）。在WordNet名词体系中，共有25个独立起始概念。其他名词通过上位/下位关系与这25个独立起始概念构成25个独立的层次结构。也就是说，标识着某个起始概念特点的属性将它的所有下位概念所继承，而这个起始概念就可以看作为是该语义领域内的所有概念（同义词集合）的一个原始语义元素。如表2所示。 表2 WordNet名词体系的25个独立起始概念 ｛动作，行为，行动｝ ｛自然物｝ ｛动物，动物系｝ ｛自然现象｝ ｛人工物｝ ｛人，人类｝ ｛属性，特征｝ ｛植物，植物系｝ ｛身体，躯体｝ ｛所有物｝ ｛认知，知识｝ ｛作用，方法｝ ｛信息，通信｝ ｛量，数量｝ ｛事件｝ ｛关系｝ ｛知觉，情感｝ ｛形状｝ ｛食物｝ ｛状态，情形｝ ｛团体，组织｝ ｛物质｝ ｛场所，位置｝ ｛时间｝ ｛目的｝     (2) 词典编撰ID（Lexicographer ID） 每一个同义词集合（synonymy set）均有惟一的一个编号，这个编号就称为词典编撰ID（Lexicographer ID）。 (3)    概念链（Concept Chain） 概念链一般的定义是这样一种结构：:=(C,<)，其中C代表的是概念集合，<代表概念间的下位/上位关系。也就是说概念链是由C概念集合中的概念通过概念间的上位/下位关系连接而成。 (4)    WordNet名词体系中的概念链（Concept Chain） 在WordNet名词体系中，我们定义概念链（Concept Chain）如下： :=((C,<)<UBCi)，其中UBCi表示WordNet名词体系的一个独立起始概念，C代表的是概念集合，<代表概念间的下位/上位关系。也就是说概念链是以一个独立起始概念UBCi为链首，通过概念间的上位/下位关系连接与C概念集合连接而成。同时C概念集合中的概念也是通过概念间的上位/下位关系进行连接。如图1所示。 图1 一个概念链（Concept Chain）的例子 图1展示的就是一个概念链的示意范例。小三角形代表的是词“football”。每一个小圆圈都代表WordNet中的一个同义词集合（也就是概念）。小圆圈旁边的注释就是该同义词集合的内容。注释中的数字是该同义词集合的词典编撰ID（Lexicographer ID）。这个示意范例表达的是：词“football”有两个义项，即它在两个同义词集合中出现，也就是对应图中的两个小圆圈。小圆圈之间用带箭头的线连接，表示的是小圆圈所代表的概念通过下位/上位关系联系起来，从而构成概念链。概念链的首端对应的就是WordNet中的独立起始概念。比如：概念链ch1可以表示为：（3255461）<（2681909）<（3289024）<（3174243）<（3443493）<（19244）<（2645）<（16236）<（1740）。其中（3255461）作为概念链的末端代表的是词“football”的一个义项，而（1740）是WordNet中的独立起始概念，成为概念链的首端。概念“game equipment”（3289024）是概念“ball”（2681909）的上层概念，表达的语义更抽象。 URL of Wordnet: http://wordnet.princeton.edu","title":"WordNet介绍"},{"content":" ·概述     以前曾经撰文讲过Topic Engine的过去、现在和未来。Topic Engine是一个生生不息的应用方向，因为从News Group、邮件列表、聊天室、论坛、Google News、博客圈子、群组。。。，人们一直因话题（有人也叫主题，英文为Topic）而聚集而交友，话题一直在生生不息层出不穷，组织形式在不断变异。     现在再讲讲个性化阅读的过去、现在和未来，也算是这个话题的延续。 一、概念定义     泛泛地说，只要是根据用户的历史行为（发言、标签等数据，点击流、分享、收藏、推荐、跳过等动作），动态决定哪些资讯内容（论坛帖子、新闻资讯、博客、微博、等）呈现给用户，都叫个性化阅读。 二、历史阶段 2005年～2007年：     这个阶段还没有Social数据，所以： 首先需要用户选定对哪些分类频道感兴趣，比如历史、人文、明星、体育等。稍微聪明一点的做法，不让用户选分类，而是问用户几个问题，然后就大致匹配出用户的兴趣点。 其次，系统决定给用户展现哪些分类的资讯。 随着用户点击，资讯实时不断变化，点击越多，系统越了解用户的阅读喜好。 这阶段的问题是： 1、利用成熟的协同过滤算法，但由于都在追求实时计算，运算量较大，有一定技术门槛； 2、对用户背景还是不够了解，仅仅通过用户点击流终究太浅。 3、普遍存在冷启动问题。 2008年～2010年：     有了Twitter，有了Facebook，有了Social Graph，个性化阅读器纷纷利用Twitter/Facebook帐号登录，展现的资讯是用户自己好友的Timeline聚合，主要是合并那些被诸多好友推荐的热点链接、图片和视频。不过，这波潮过去之后，像http://thoora.com/ 、http://twittertim.es/等都没有找到足够的用户群，还没有像2005年杀出来的TechMeme那么成功。 这阶段的问题是： 1、依赖于Twitter/Facebook的Social Graph，依赖于好友推送，可供阅读的数据过少，可供计算的数据过少，限制了自身应用的发展； 2、除非与Twitter保持良好的关系，能拿到 Streaming Firehose 接口，提前积累用户数据，否则用户Timeline信息需要积累一段时间，造成大量用户登录后没有可阅读的数据。 2010年：     FlipBoard杀出重围，自动排版技术独步天下。 2011年：     随着国内新浪微博、豆瓣等拥有Interest Graph（兴趣图谱）+Social Graph（社交图谱）海量数据的网站崛起，成为主流数据源，如何把2005年到2010年这些探索择其优点都整合起来，成为一个大课题。     Zite的横空出世，被众人热捧为“Flipboard Killer”，强调的是基于社会化关系的个性化推荐阅读方式。而Flipboard目前的战略重点主要还是集成各种社会化应用及内容源，并以其创造性的阅读体验方式展现出来。国内已经有几家也在Zite的方向上，尤其是iPad应用上，动了起来。 三、热门？还是个性化？     在2009年SXSW大会上，SheGeeks 直言不讳：『 热门内容（Popularity）已经过时了，某种程度上令人讨厌。 我不想知道什么是最流行的，Techmeme已经帮我做到了。我想知道什么东西和我相关。我们需要更多“相关性过滤服务”。』     此时，会有几种做法： 1、以热点资讯为主（先有蛋），以社交图谱为辅（后引入鸡）：将社交图谱引入热点资讯阅读中，像Quora（或中国的知乎）一样按人来隔离不同话题（不同热点）的讨论。Zite的方式类似于此。 2、以社交图谱为主：组织一度好友和二度好友的数据，做好数据挖掘。曾经有人在很久远的年代说过，“建立一个Social Network，每一个用户都推荐出自己喜欢的内容，那么被推荐得最多的，就一定是大多数人最受欢迎的内容。如果把这些推荐内容的用户区分成不同的群体， 就会得到特定群体欢迎的内容。Digg的想法就源于此。不过，这需要用户有足够的动力去推荐自己喜欢的内容，否则，Network也无法形成”。 3、以人为阅读中心：有人很多年前说过“许多人的blog阅读体验和阅读闲谈专栏是相似的，他们选择读什么不读什么的判断依据不是话题，而是作者，因为只有这样才能保证阅读到的内容的质量”。 4、以Topic为中心：用户定义或发掘用户感兴趣的Topic，只要是一篇文章谈及了用户关注的某一个主题，那么就推送给他。或者来自于不同人的文章集中地探讨某个话题，那么把这些文章自动聚合为一个Dialogue（虚拟对话），推送给用户。     除了第一种做法之外，我曾经尝试过其他三种做法。在中国的大环境下，要么数据过少，要么数据质量不高，都不能很好地做到有“发现、探索”、“新鲜、有趣”的冲击力。     当Social能完整地提供三重元素时： 1、 你的身份标识（Indentity）：Who you are； 2、 你的联系人或圈子（Contacts）：Who you know； 3、 你的网际行为（Activities）：What you do 。     那么，Social Graph，Interrest Graph，再联合热点资讯，揉入2005年以来的协同过滤算法，至少能做到make something people want吧。 四、Interest Graph的变化     以前，郑昀针对不同人群做的信息聚合，单纯从内容分类（也就是靠自然语言处理的自动分类算法）做，属于从信息本身下手。这种方式有一个问题：    某一类人群，虽然有一些集中的阅读点，但还有边缘的共同兴趣。举例，如IT人群，虽然共享和推荐的大多数是IT科技文章，但也涌现出很多受欢迎的兴趣点，如韩寒的文章，如冷笑话，如创意趣味产品。     这也就是为何基于 Tag 方式的阅读模式，以及基于指定主题的追踪模式，都不容易持久耐用的原因。一个人群的阅读兴趣点是比较模糊的。对于一个人来说，如果一个信息过滤器供应点科技，供应点娱乐，适当补充些人文历史，就能保证一定的粘度。     所以，郑昀后来觉得从内容分类，由于不引入人工，只靠比较大条的自然语言处理分类，对于博文、微博、论坛帖子等文字质量不稳定的信息会分得很粗糙，所以改变思路，从人群分类开始做。     也就是，划分出目标人群，依靠人群来挑拣信息，NLP算法为辅。这样有一个额外的好处，人群的兴趣点在动态变，短期地变，长期地变，但由于锁定人群，所以筛选出来的信息也在变。而相比之下，自动分类做出的信息，隔几个月或半年后，就要重新训练机器，因为往往信息包含的语言特征变了。     这也是信息聚合中的一个实际考虑点。     现在，中国也有了自己的Interest Graph，比如新浪微博，它的数据天然就表明一个人的兴趣喜好，以及连续波动，都可以跟踪和挖掘出来。以前依靠遍历Twitter、Google Reader、FriendFeed的好友所得到的社群分离，现在通过新浪微博等Social Graph都可以得到类似的。 五、人员配比     一般我对这个领域（Topic Engine啦、个性化阅读啦、Meme Tracker啦），研发人员配比是这么建议的： 爬虫2人， 文本挖掘4人（新词发现+分词+分类一个人，实体识别与发现+情感趋势分析一个人，事件识别与发现一个人，User Interest Profile一个人）， 数据挖掘和分析2人， Web前端展现（包括手持设备）3人， 产品经理1人， 12人是一个比较不错的开局。 前面说到Topic Engine/个性化阅读/Meme Tracker这几个方向所需要的研发团队大致是12个人起。下面着重说一下在现如今如何做个性化阅读。     Zite 的战略就是直接从Social Graph+Interest Graph切入，通过将Google Reader、Twitter、Facebook等拥有Interest Graph图谱的社会化数据导入，从而获得用户初始的兴趣爱好及社会化关系，由此引发阅读推荐，有效避免了推荐引擎的“冷启动”问题。如果用户不提供这些账号，Zite 需要你选择一些话题，然后就开始给你呈现相关的内容，这就和郑昀上一篇文章所介绍的2005～2007年活跃的个性化推荐没什么区别了。据说，国内已经有一家公司在做类似的产品，最近就会推出。 第一步，资讯聚合     热门资讯聚合有几种方法，郑昀以前在一、二里都讲过： 一、基于链接检测的聚合模式     这个模式非常好理解。只不过，我定义之所以叫链接“检测”，是因为链接并不显著，或在正文中隐藏，或在 Tweets 中隐藏，需要你特地提取出来。     2005年9月上线的 Techmeme 作为本模式的最优秀代表，就深刻地教育了 mashup 开发者，原来 链接检测 混搭 适当的A-List 有如此高的信息过滤效率。 Techmeme 在 Blog 时代称雄一时。到了 Twitter 时代，后起之秀是 TweetMeme ，上线之初，它并没有像 Techmeme 一样大放光彩，但随着 Twitter 的如日中天，它终于爆发了，它的 Alexa全球排名已经抵达在500名左右。 二、基于重复文字检测的聚合模式     Google News和百度新闻的新闻聚合，都属于本模式。它们可以通过检测近期发布的资讯之间的内容重合度，能将同一个主题的资讯合并在一起，也就是以文本相似性为技术基础的。     本模式一般是广泛收集新闻媒体信源，标记不同的权重度，做成扫描列表；然后通过爬虫抓取最新的新闻。通过对最近一段时间内的新闻计算文本相似性，可以获知哪些文章的相似度高于预设阈值，那么就说明这些文章是近似一个话题，可以合并。 三、Reddit模式     让新鲜且投票数还不足够多的文章能快速突破进入热门榜单，是很重要的。所以郑昀曾经在《榜单类应用我所喜欢的算法》中写道：“Reddit算法是我最喜欢用的算法。这个算法的解释参见我的文章：《Hacker News与Reddit的算法比较》”。     在郑昀撰写的《从Social Media海量数据中寻找专家的五大手法》中，SPEAR模式认为：“专家应该是发现者，而不是趋势的跟随者。experts应该是第一批收藏和标记高质量文章的人，从而召唤起社区内其他用户的围观。用户发现优质内容越早，表明该用户专业程度越高。所以，要区分“Discoverers”和“Followers”。”Reddit 正是通过log10 的使用，使得早期的投票（即Discoverers）获得更大的权重。比如，前10票获得的权重，与11到101票所获得的权重是一样的。     以Reddit算法为依托，针对资讯或社会化媒体数据做出不同分类的榜单，即热文列表。 四、Seeds模式     这是一种第三方应用深入某个 Social Media的常见刺探式统计方法。事先选定一个key users集合（比如创始人以及其他核心用户，被称之为“seeds”），然后从这批用户开始扫描建立Social Graph，通过统计inbound links和好友关系，得出被扫描的social media的不同指标的排行榜，这就是Spinn3r rank所用到的手法。这种模式并不限于计算Top Users。     它所用到的两个技巧倒是经常看到： 从 Approved Sources 开始扫描：一个好的算法，当然要从好源开始，Techmeme和玩聚SR都是这么做的； 遍历 friendship ：spammers或水平不那么高的用户，要想从 seeds 这里获得连接显然是不大可能的。     资讯聚合之后，要做到自动分类。此处用到了NLP的东西。 第二步，Interest Graph的建立     收集用户初始数据。此时有两种方式。     第一种，授权式： 让用户以自己的新浪微博、豆瓣、Google(Reader)等Social帐号登录； 获得用户OAuth授权后，获得用户（以及他的好友的）timeline，实时分析其潜在阅读喜好，构建Interest Graph。     第二种，预先计算式： 拿到Twitter、GR、新浪微博的类似Firehose（Streaming API）接口，提前存储大多数用户的社会化数据； 用户一旦输入（或叫“绑定”）自己的社会化帐号，后台根据已收集好的数据，立刻开始计算Interest Graph。 如何计算Interest Graph？     郑昀认为，可以把计算一个（微博/twitter）的Interest Graph视为短文本分类问题。     那么可以采用改进后的LDA算法，区分并且去除了容易造成主题混淆的关键词，只考虑主题明晰化的关键词。 什么是LDA？     即Latent Dirichlet allocation（硬翻译为 潜在狄利克雷分布或隐含狄利克雷分配）。关联关键词：Topic Modeling。     参考《基于LDA的Topic Model变形》或《LDA模型理解》。比较通俗的解释可以看这篇SEO的：《Latent Dirichlet Allocation (LDA)与Google排名有着相当显著的相关性》。 具体如何做？     去年曾经有一位rickjin在新浪微博上如是说，颇有参考价值：     从代码层次上如何具体做，我就不说了。（注：你要是手头没工具包，也没做过NLP，也可以参考这么一个工具ROST CM。） 第三步，提供关联阅读列表     开源的推荐引擎，一个是easyrec.org。 背景1：http://en.wikipedia.org/wiki/Easyrec easyrec is an open source Web application that provides personalized recommendations using RESTful Web services to be integrated into Web enabled applications. It is distributed under the GNU General Public License by the Studio Smart Agent Technologies and hosted at SourceForge. It is written in Java, uses a MySQL database and comes with an administration tool.     另一个是Apache Mahout 。 背景2：http://www.ibm.com/developerworks/cn/java/j-lo-mahout/ Apache Mahout 是 Apache Software Foundation（ASF） 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。经典算法包括聚类、分类、协同过滤、进化编程等。 Taste 是 Apache Mahout 提供的一个协同过滤算法的高效实现，它是一个基于 Java 实现的可扩展的，高效的推荐引擎。Taste 既实现了最基本的基于用户的和基于内容的推荐算法，同时也提供了扩展接口，使用户可以方便的定义和实现自己的推荐算法。同时，Taste 不仅仅只适用于 Java 应用程序，它可以作为内部服务器的一个组件以 HTTP 和 Web Service 的形式向外界提供推荐的逻辑。     此处的难点是，如何根据前面计算的Interest Graph，给用户推荐资讯。最简单的做法是，把Interest Graph映射到不同分类，这样只需要先期计算好每一个分类下的热门资讯，那么知道用户喜欢哪些分类，就直接推送对应的热文即可；然后根据用户点击流和分享收藏等动作做个性化推荐，调整资讯内容。 第四步，再次分发、共享和传播     不能让传播链在这里断掉，所以必须鼓励用户再次分享、推荐、共享、收藏到其他社会化媒体渠道。 第一篇：《个性化阅读的过去、现在和未来（一）·概述》； 第二篇：《个性化阅读的过去、现在和未来（二）·实作》。 例行赠图一枚： 我的最新推特流： 1、 这个故事发生在距离中国不远的某俄罗斯边陲小镇硬盘维修中心。某天一位顾客带来了一块损坏的500Gb移动硬盘，这块硬盘来自河对岸的中国店铺，价格低廉的不可思议。这块移动硬盘竟然是由一个128MB的U盘和两个大螺帽组成。http://t.cn/hBgycH 2、 RT @shifeike: 这大半年来，没有一件让人开心的事儿，总是坏消息连着坏消息。我总在想，等到解放以后，罄竹难书和倒行逆施这两个成语，该拿出来用多少次啊！ 3、 当《风声传奇》最后顾晓梦的父亲意欲杀死老潘，节奏越来越急促的背景音乐响起，两岁半的宝宝突然说：好像要死人了。。。 4、 下午百分点周涛教授给我们讲了个性化推荐如何与电商实际结合，非常精彩，信息量极大，深入浅出，全是干货，很久没听过这么酣畅淋漓的讲座了。","title":"个性化阅读的过去、现在和未来"},{"content":"2008年的时候就看过这本书的介绍，今天借到中文版，可以好好研读一下了。 一、“真正的”数据仓库         DW2.0中，作者认为数据仓库很快演变为一个被称为企业信息工厂的架构。包括：源系统、ODS、ETL、企业数据仓库、数据集市和探索仓库等组件。这一点基本上还是DW中的观点。现在电子商务应用中基于云计算的数据仓库已经发生了根本变化，可能只需一个云数据仓库，即能满足数据集市、数据挖掘等业务需求，ODS可能会被省略，ETL过程被实时数据流所代替。几乎可无限扩展的存储代替了高昂的专业存储，线性提升的计算代替了复杂昂贵的商用系统。真正的数据仓库的面纱已经打开了！ 二、逻辑分层         DW2.0中提出划分为四个区：交换区、整合区、近线区、归档区。因为云数据仓库的出现，高性能计算和天文级存储不再是高不可攀，这点不再重要，但仍具有参考意义。 三、DW2.0流动性-语义常态和暂态数据         DW2.0提出数据流动性：语义常态和暂态数据，以此应对业务需求变化和技术架构难以变化的矛盾。云数据仓库中，因为基于列/列族（key/value/stamp）的数据存储和应用，无疑更有优势。这点仍有参考意义和实际意义。 四、DW是应用而非项目         过去认为DW是一个长期的项目，数据仓库和应用需求可以分离，结果发现应用时根本无法使用。DW2.0中，认为DW是应用，而非项目。由此，数据仓库在规划、设计、建设时需结合应用需求“总体规划、分步实施”，避免数据和应用脱节。 五、数据模型         DW2.0中提出建立企业数据模型，并完成模型转换。我认为，在云数据仓库中同样需要整合数据模型，汇总模型和明细模型，并近可能使用宽表，提供统一视图。以此，提高数据适用性，提供高性能计算能力。 六、ETL与DW2.0         在DW2.0中，ETL仍是一个非常重要的工作，但在云数据仓库中，需满足实时的要求，因此ETL被实时数据流所代替。 七、元数据与DW2.0         元数据在DW中无从提及，在DW2.0中多处提到，但元数据的用途阐述的还不够深入，在实际应用中，需要借助于元数据管理系统实现。 八、数据质量         数据质量也是在DW2.0中首次提及，在技术上，借助于元数据管理系统可有效管理数据质量问题，管理上，可借助IMO（信心管理组织）来管理数据质量问题。 九、非结构化数据         针对非结构化数据，DW2.0充分认识到了其重要性，可以用来作为参考，也可以借助于NLP（自然语言处理）、AI（人工智能）等直接进行聚合，提取有价值的信息。           以上，对DW2.0进行些概要的分析，以后在云数据仓库的实践中再进一步完善。","title":"《DW2.0下一代数据仓库的构架》研读心得"},{"content":"TrainTClassifier，基于TF/IDF算法的分类器，必须先把要语料库放到各自所属的分类文件夹中，比如：与金融相关的文章就放到金融这个文件夹中，我这的根目录是f:/data/category，训练完后会生成一个分类器模型tclassifier，之后其它文本的分类的确定就是通过它。 /** * 使用 Lingpipe的TF/IDF分类器训练语料 *  * @author laigood */public class TrainTClassifier {\t//训练语料文件夹\tprivate static File TDIR = new File(\"f:\\\\data\\\\category\");\t//定义分类\tprivate static String[] CATEGORIES = { \"金融\", \"军事\", \"医学\", \"饮食\" };\tpublic static void main(String[] args) throws ClassNotFoundException,\t\t\tIOException {\t\t\t\tTfIdfClassifierTrainer<CharSequence> classifier = new TfIdfClassifierTrainer<CharSequence>(\t\t\t\tnew TokenFeatureExtractor(CharacterTokenizerFactory.INSTANCE));\t\t// 开始训练\t\tfor (int i = 0; i < CATEGORIES.length; i++) {\t\t\tFile classDir = new File(TDIR, CATEGORIES[i]);\t\t\tif (!classDir.isDirectory()) {\t\t\t\tSystem.out.println(\"不能找到目录=\" + classDir);\t\t\t}\t\t\t// 训练器遍历分类文件夹下的所有文件\t\t\tfor (File file : classDir.listFiles()) {\t\t\t\tString text = Files.readFromFile(file, \"utf-8\");\t\t\t\tSystem.out.println(\"正在训练 \" + CATEGORIES[i] + file.getName());\t\t\t\tClassification classification = new Classification(\t\t\t\t\t\tCATEGORIES[i]);\t\t\t\tClassified<CharSequence> classified = new Classified<CharSequence>(\t\t\t\t\t\ttext, classification);\t\t\t\tclassifier.handle(classified);\t\t\t} \t\t}\t\t\t\t// 把分类器模型写到文件上\t\tSystem.out.println(\"开始生成分类器\");\t\tString modelFile = \"f:\\\\data\\\\category\\\\tclassifier\";\t\tObjectOutputStream os = new ObjectOutputStream(new FileOutputStream(\t\t\t\tmodelFile));\t\tclassifier.compileTo(os);\t\tos.close();\t\t\t\tSystem.out.println(\"分类器生成完成\");\t}} TestTClassifier ,测试分类的准确度，测试数据的存放与上面的类似 /** * 测试TF/IDF分类器的准确度  *  * @author laigood */public class TestTClassifier {\t//测试语料的存放目录\tprivate static File TDIR = new File(\"f:\\\\data\\\\test\");\tprivate static String[] CATEGORIES = { \"金融\", \"军事\", \"医学\", \"饮食\" };\tpublic static void main(String[] args) throws ClassNotFoundException {\t\t\t\t//分类器模型存放地址\t\tString modelFile = \"f:\\\\data\\\\category\\\\tclassifier\";\t\tScoredClassifier<CharSequence> compiledClassifier = null;\t\ttry {\t\t\tObjectInputStream oi = new ObjectInputStream(new FileInputStream(\t\t\t\t\tmodelFile));\t\t\tcompiledClassifier = (ScoredClassifier<CharSequence>) oi\t\t\t\t\t.readObject();\t\t\toi.close();\t\t} catch (IOException ie) {\t\t\tSystem.out.println(\"IO Error: Model file \" + modelFile + \" missing\");\t\t}\t\t// 遍历分类目录中的文件测试分类准确度\t\tConfusionMatrix confMatrix = new ConfusionMatrix(CATEGORIES);\t\tNumberFormat nf = NumberFormat.getInstance();\t\tnf.setMaximumIntegerDigits(1);\t\tnf.setMaximumFractionDigits(3);\t\tfor (int i = 0; i < CATEGORIES.length; ++i) {\t\t\tFile classDir = new File(TDIR, CATEGORIES[i]);\t\t\t//对于每一个文件，通过分类器找出最适合的分类\t\t\tfor (File file : classDir.listFiles()) {\t\t\t\tString text = \"\";\t\t\t\ttry {\t\t\t\t\ttext = Files.readFromFile(file, \"utf-8\");\t\t\t\t} catch (IOException ie) {\t\t\t\t\tSystem.out.println(\"不能读取 \" + file.getName());\t\t\t\t}\t\t\t\tSystem.out.println(\"测试 \" + CATEGORIES[i]\t\t\t\t\t\t+ File.separator + file.getName());\t\t\t\tScoredClassification classification = compiledClassifier\t\t\t\t\t\t.classify(text.subSequence(0, text.length()));\t\t\t\tconfMatrix.increment(CATEGORIES[i],\t\t\t\t\t\tclassification.bestCategory());\t\t\t\tSystem.out.println(\"最适合的分类: \"\t\t\t\t\t\t+ classification.bestCategory());\t\t\t} \t\t} \t\tSystem.out.println(\"--------------------------------------------\");\t\tSystem.out.println(\"- 结果 \");\t\tSystem.out.println(\"--------------------------------------------\");\t\tint[][] imatrix = confMatrix.matrix();\t\tStringBuffer sb = new StringBuffer();\t\tsb.append(StringTools.fillin(\"CATEGORY\", 10, true, ' '));\t\tfor (int i = 0; i < CATEGORIES.length; i++)\t\t\tsb.append(StringTools.fillin(CATEGORIES[i], 8, false, ' '));\t\tSystem.out.println(sb.toString());\t\tfor (int i = 0; i < imatrix.length; i++) {\t\t\tsb = new StringBuffer();\t\t\tsb.append(StringTools.fillin(CATEGORIES[i], 10, true, ' ',\t\t\t\t\t10 - CATEGORIES[i].length()));\t\t\tfor (int j = 0; j < imatrix.length; j++) {\t\t\t\tString out = \"\" + imatrix[i][j];\t\t\t\tsb.append(StringTools.fillin(out, 8, false, ' ',\t\t\t\t\t\t8 - out.length()));\t\t\t}\t\t\tSystem.out.println(sb.toString());\t\t}\t\tSystem.out.println(\"准确度: \"\t\t\t\t+ nf.format(confMatrix.totalAccuracy()));\t\tSystem.out.println(\"总共正确数 : \" + confMatrix.totalCorrect());\t\tSystem.out.println(\"总数：\" + confMatrix.totalCount());\t}} 补上StringTools /** * A class containing a bunch of string utilities - <br> * a. filterChars: Remove extraneous characters from a string and return a * \"clean\" string. <br> * b. getSuffix: Given a file name return its extension. <br> * c. fillin: pad or truncate a string to a fixed number of characters. <br> * d. removeAmpersandStrings: remove strings that start with ampersand <br> * e. shaDigest: Compute the 40 byte digest signature of a string <br> */public class StringTools {  public static final Locale LOCALE = new Locale(\"en\");  // * -- String limit for StringTools  private static int STRING_TOOLS_LIMIT = 1000000;  // *-- pre-compiled RE patterns  private static Pattern extPattern = Pattern.compile(\"^.*[.](.*?){1}quot;);  private static Pattern spacesPattern = Pattern.compile(\"\\\\s+\");  private static Pattern removeAmpersandPattern = Pattern.compile(\"&[^;]*?;\");  /**   * Removes non-printable spaces and replaces with a single space   *    * @param in   *          String with mixed characters   * @return String with collapsed spaces and printable characters   */  public static String filterChars(String in) {    return (filterChars(in, \"\", ' ', true));  }  public static String filterChars(String in, boolean newLine) {    return (filterChars(in, \"\", ' ', newLine));  }  public static String filterChars(String in, String badChars) {    return (filterChars(in, badChars, ' ', true));  }  public static String filterChars(String in, char replaceChar) {    return (filterChars(in, \"\", replaceChar, true));  }  public static String filterChars(String in, String badChars,      char replaceChar, boolean newLine) {    if (in == null)      return \"\";    int inLen = in.length();    if (inLen > STRING_TOOLS_LIMIT)      return in;    try {      // **-- replace non-recognizable characters with spaces      StringBuffer out = new StringBuffer();      int badLen = badChars.length();      for (int i = 0; i < inLen; i++) {        char ch = in.charAt(i);        if ((badLen != 0) && removeChar(ch, badChars)) {          ch = replaceChar;        } else if (!Character.isDefined(ch) && !Character.isSpaceChar(ch)) {          ch = replaceChar;        }        out.append(ch);      }      // *-- replace new lines with space      Matcher matcher = null;      in = out.toString();      // *-- replace consecutive spaces with single space and remove      // leading/trailing spaces      in = in.trim();      matcher = spacesPattern.matcher(in);      in = matcher.replaceAll(\" \");    } catch (OutOfMemoryError e) {      return in;    }    return in;  }  // *-- remove any chars found in the badChars string  private static boolean removeChar(char ch, String badChars) {    if (badChars.length() == 0)      return false;    for (int i = 0; i < badChars.length(); i++) {      if (ch == badChars.charAt(i))        return true;    }    return false;  }  /**   * Return the extension of a file, if possible.   *    * @param filename   * @return string   */  public static String getSuffix(String filename) {    if (filename.length() > STRING_TOOLS_LIMIT)      return (\"\");    Matcher matcher = extPattern.matcher(filename);    if (!matcher.matches())      return \"\";    return (matcher.group(1).toLowerCase(LOCALE));  }  public static String fillin(String in, int len) {    return fillin(in, len, true, ' ', 3);  }  public static String fillin(String in, int len, char fillinChar) {    return fillin(in, len, true, fillinChar, 3);  }  public static String fillin(String in, int len, boolean right) {    return fillin(in, len, right, ' ', 3);  }  public static String fillin(String in, int len, boolean right, char fillinChar) {    return fillin(in, len, right, fillinChar, 3);  }  /**   * Return a string concatenated or padded to the specified length   *    * @param in   *          string to be truncated or padded   * @param len   *          int length for string   * @param right   *          boolean fillin from the left or right   * @param fillinChar   *          char to pad the string   * @param numFills   *          int number of characters to pad   * @return String of specified length   */  public static String fillin(String in, int len, boolean right,      char fillinChar, int numFills) {    // *-- return if string is of required length    int slen = in.length();    if ((slen == len) || (slen > STRING_TOOLS_LIMIT))      return (in);    // *-- build the fillin string    StringBuffer fillinStb = new StringBuffer();    for (int i = 0; i < numFills; i++)      fillinStb.append(fillinChar);    String fillinString = fillinStb.toString();    // *-- truncate and pad string if length exceeds required length    if (slen > len) {      if (right)        return (in.substring(0, len - numFills) + fillinString);      else        return (fillinString + in.substring(slen - len + numFills, slen));    }    // *-- pad string if length is less than required length DatabaseEntry    // dbe = dbt.getNextKey(); String dbkey = new String (dbe.getData());    StringBuffer sb = new StringBuffer();    if (right)      sb.append(in);    sb.append(fillinString);    if (!right)      sb.append(in);    return (sb.toString());  }  /**   * Remove ampersand strings such as \\    *    * @param in   *          Text string extracted from Web pages   * @return String Text string without ampersand strings   */  public static String removeAmpersandStrings(String in) {    if (in.length() > STRING_TOOLS_LIMIT)      return (in);    Matcher matcher = removeAmpersandPattern.matcher(in);    return (matcher.replaceAll(\"\"));  }  /**   * Escape back slashes   *    * @param in   *          Text to be escaped   * @return String Escaped test   */  public static String escapeText(String in) {    StringBuffer sb = new StringBuffer();    for (int i = 0; i < in.length(); i++) {      char ch = in.charAt(i);      if (ch == '\\\\')        sb.append(\"\\\\\\\\\");      else        sb.append(ch);    }    return (sb.toString());  }  /**   * Get the SHA signature of a string   *    * @param in   *          String   * @return String SHA signature of in   */  public static String shaDigest(String in) {    StringBuffer out = new StringBuffer();    if ((in == null) || (in.length() == 0))      return (\"\");    try {      // *-- create a message digest instance and compute the hash      // byte array      MessageDigest md = MessageDigest.getInstance(\"SHA-1\");      md.reset();      md.update(in.getBytes());      byte[] hash = md.digest();      // *--- Convert the hash byte array to hexadecimal format, pad      // hex chars with leading zeroes      // *--- to get a signature of consistent length (40) for all      // strings.      for (int i = 0; i < hash.length; i++) {        out.append(fillin(Integer.toString(0xFF & hash[i], 16), 2, false, '0',            1));      }    } catch (OutOfMemoryError e) {      return (\"<-------------OUT_OF_MEMORY------------>\");    } catch (NoSuchAlgorithmException e) {      return (\"<------SHA digest algorithm not found--->\");    }    return (out.toString());  }  /**   * Return the string with the first letter upper cased   *    * @param in   * @return String   */  public static String firstLetterUC(String in) {    if ((in == null) || (in.length() == 0))      return (\"\");    String out = in.toLowerCase(LOCALE);    String part1 = out.substring(0, 1);    String part2 = out.substring(1, in.length());    return (part1.toUpperCase(LOCALE) + part2.toLowerCase(LOCALE));  }  /**   * Return a pattern that can be used to collapse consecutive patterns of the   * same type   *    * @param entityTypes   *          A list of entity types   * @return Regex pattern for the entity types   */  public static Pattern getCollapsePattern(String[] entityTypes) {    Pattern collapsePattern = null;    StringBuffer collapseStr = new StringBuffer();    for (int i = 0; i < entityTypes.length; i++) {      collapseStr.append(\"(<\\\\/\");      collapseStr.append(entityTypes[i]);      collapseStr.append(\">\\\\s+\");      collapseStr.append(\"<\");      collapseStr.append(entityTypes[i]);      collapseStr.append(\">)|\");    }    collapsePattern = Pattern.compile(collapseStr.toString().substring(0,        collapseStr.length() - 1));    return (collapsePattern);  }  /**   * return a double that indicates the degree of similarity between two strings   * Use the Jaccard similarity, i.e. the ratio of A intersection B to A union B   *    * @param first   *          string   * @param second   *          string   * @return double degreee of similarity   */  public static double stringSimilarity(String first, String second) {    if ((first == null) || (second == null))      return (0.0);    String[] a = first.split(\"\\\\s+\");    String[] b = second.split(\"\\\\s+\");    // *-- compute a union b    HashSet<String> aUnionb = new HashSet<String>();    HashSet<String> aTokens = new HashSet<String>();    HashSet<String> bTokens = new HashSet<String>();    for (int i = 0; i < a.length; i++) {      aUnionb.add(a[i]);      aTokens.add(a[i]);    }    for (int i = 0; i < b.length; i++) {      aUnionb.add(b[i]);      bTokens.add(b[i]);    }    int sizeAunionB = aUnionb.size();    // *-- compute a intersect b    Iterator <String> iter = aUnionb.iterator();    int sizeAinterB = 0;    while (iter != null && iter.hasNext()) {      String token = (String) iter.next();      if (aTokens.contains(token) && bTokens.contains(token))        sizeAinterB++;    }    return ((sizeAunionB > 0) ? (sizeAinterB + 0.0) / sizeAunionB : 0.0);  }  /**   * Return the edit distance between the two strings   *    * @param s1   * @param s2   * @return double   */  public static double editDistance(String s1, String s2) {    if ((s1.length() == 0) || (s2.length() == 0))      return (0.0);    return EditDistance.editDistance(s1.subSequence(0, s1.length()), s2        .subSequence(0, s2.length()), false);  }  /**   * Return a string with the contents from the passed reader   *    * @param r Reader   * @return String   */  public static String readerToString(Reader r) {    int charValue;    StringBuffer sb = new StringBuffer(1024);    try {      while ((charValue = r.read()) != -1)        sb.append((char) charValue);    } catch (IOException ie) {      sb.setLength(0);    }    return (sb.toString());  }  /**   * Clean up a sentence by consecutive non-alphanumeric chars with a single   * non-alphanumeric char   *    * @param in Array of chars   * @return String   */  public static String cleanString(char[] in) {    int len = in.length;    boolean prevOK = true;    for (int i = 0; i < len; i++) {      if (Character.isLetterOrDigit(in[i]) || Character.isWhitespace(in[i]))        prevOK = true;      else {        if (!prevOK)          in[i] = ' ';        prevOK = false;      }    }    return (new String(in));  }  /**   * Return a clean file name   *    * @param filename   * @return String   */  public static String parseFile(String filename) {    return (filterChars(filename, \"\\\\/_:.\"));  }}  ","title":"使用lingpipe自然语言处理包进行文本分类"},{"content":"注：文章系转载 http://bdonline.sqe.com/ 一个关于网站测试方面的网页,对这方面感兴趣的人可以参考 http://citeseer.nj.nec.com/ 一个丰富的电子书库,内容很多,而且提供著作的相关文档参考和下载,是作者非常推荐的一个资料参考网站 http://groups.yahoo.com/group/LoadRunner 性能测试工具LoadRunner的一个论坛 http://groups.yahoo.com/grorp/testing-paperannou-nce/messages 提供网站上当前发布的软件测试资料列表 http://satc.gsfc.nasa.gov/homepage.html 软件保证中心是美国国家航天局（NASA)投资设立的一个软件可靠性和安全性研究中心，研究包括了度量、工具、风险等各个方面 http://seg.iit.nrc.ca/English/index.html 加拿大的一个研究软件工程质量方面的组织，可以提供研究论文的下载 http://sepo.nosc.mil/ 内容来自美国SAN DIEGO的软件工程机构（Sofrware Engineering Process Office)主页，包括软件工程知识方面的资料 http://www.asq.org/ 是世界上最大的一个质量团体组织之一，有着比较丰富的论文资源，不过是收费的 http://www.automated-testing.com/ 一个自动化软件测试和自然语言处理研究页面，属于个人网页，上面有些资源可供下载 http://www.benchmarkresources.com/ 提供有关标杆方面的资料，也有一些其它软件测试方面的资料 http://www.betasoft.com/ 包含一些流行测试工具的介绍、下载和讨论，还提供测试方面的资料 http://www.brunel.ac.uk/~csstmmh2/vast/home.html VASTT研究组织，主要从事通过切片技术、测试技术和转换技术来验证和分析系统，对这方面技术感兴趣的人是可以在这里参考一些研究的项目及相关的一些主题信息 http://www.cc.gatech.edu/aristotle/ Aristole研究组织，研究软件系统分析、测试和维护等方面的技术，在测试方面的研究包括了回归测试、测试套最小化、面向对象软件测试等内容，该网站有丰富的论文资源可供下载 http://www.computer.org/ IEEE是世界上最悠久，也是在最大的计算机社会团体，它的电子图书馆拥有众多计算机方面的论文资料，是研究计算机方面的一个重要资源参考来源 http://www.cs.colostate.edu/testing/ 可靠性研究网站，有一些可靠性方面的论文资料 http://www.cs.york.ac.uk/testsig/ 约克大学的测试专业兴趣研究组网页，有比较丰富的资料下载，内容涵盖了测试的多个方面，包括测试自动化、测试数据生成、面向对象软件测试、验证确认过程等 http://www.csr.ncl.ac.uk/index.html 学校里面的一个软件可靠性研究中心，提供有关软件可靠性研究方面的一些信息和资料，对这方面感兴趣的人可以参考 http://www.dcs.shef.ac.uk/research/groups/vt/ 学校里的一个验证和测试研究机构，有一些相关项目和论文可供参考 http://www.esi.es/en/main/ ESI（欧洲软件组织），提供包括CMM评估方面的各种服务 http://www.europeindia.org/cd02/index.htm 一个可靠性研究网站，有可靠性方面的一些资料提供参考 http://www.fortest.org.uk/ 一个测试研究网站，研究包括了静态测试技术（如模型检查、理论证明）和动态测试（如测试自动化、特定缺陷的检查、测试有效性分析等） http://www.grove.co.uk/ 一个有关软件测试和咨询机构的网站，有一些测试方面的课程和资料供下载 http://www.hq.nasa.gov/office/codeq/relpract/prcls-23.htm NASA可靠性设计实践资料 http://www.io.com/~wazmo/ Bret Pettichord的主页，他的一个热点测试页面连接非常有价值，从中可以获得相当大的测试资料，很有价值 http://www.iso.ch/iso/en/ISOOnline.frontpage 国际标准化组织，提供包括ISO标准系统方面的各类参考资料 http://www.isse.gmu.edu/faculty/ofut/classes/ 821-ootest/papers.html 提供面向对象和基于构架的测试方面著作下载，对这方面感兴趣的读者可以参考该网站，肯定有价值 http://www.ivv.nasa.gov/ NASA设立的独立验证和确认机构，该机构提出了软件开发的全面验证和确认，在此可以获得这方面的研究资料 http://www.kaner.com/ 著名的测试专家Cem Kanner的主页，里面有许多关于测试的专题文章，相信对大家都有用。Cem Kanner关于测试的最著名的书要算Testing Software,这本书已成为一个测试人员的标准参考书 http://www.library.cmu.edu/Re-search/Engineer- ingAndSciences/CS+ECE/index.html 卡耐基梅陇大学网上图书馆，在这里你可以获得有关计算机方面各类论文资料，内容极其庞大，是研究软件测试不可获取的资料来源之一 http://www.loadtester.com/ 一个性能测试方面的网站，提供有关性能测试、性能监控等方面的资源，包括论文、论坛以及一些相关链接 http://www.mareinig.ch/mt/index.html 关于软件工程和应用开发领域的各种免费的实践知识、时事信息和资料文件下载，包括了测试方面的内容 http://www.mtsu.ceu/-storm/ 软件测试在线资源，包括提供目前有哪些人在研究测试，测试工具列表连接，测试会议，测试新闻和讨论，软件测试文学（包括各种测试杂志，测试报告），各种测试研究组织等内容 http://www.psqtcomference.com/ 实用软件质量技术和实用软件测试技术国际学术会议宣传网站，每年都会举行两次 http://www.qacity.com/front.htm 测试工程师资源网站，包含各种测试技术及相关资料下载 http://www.qaforums.com/ 关于软件质量保证方面的一个论坛，需要注册 http://www.qaiusa.com/ QAI是一个提供质量保证方面咨询的国际著名机构，提供各种质量和测试方面证书认证 http://www.qualitytree.com/ 一个测试咨询提供商，有一些测试可供下载，有几篇关于缺陷管理方面的文章值得参考 http://www.rational.com/ IBM Rational的官方网站，可以在这里寻找测试方面的工具信息。IBM Rational提供测试方面一系列的工具，比较全面 http://rexblackconsulting.com/Pages/publicat-ions.htm Rex Black的个人主页，有一些测试和测试管理方面的资料可供下载 http://www.riceconsulting.com/ 一个测试咨询提供商，有一些测试资料可供下载，但不多 http://www.satisfice.com/ 包含James Bach关于软件测试和过程方面的很多论文，尤其在启发式测试策略方面值得参考 http://www.satisfice.com/seminars.shtml 一个黑盒软件测试方面的研讨会，主要由测试专家Cem Kanar和James Bach组织，有一些值得下载的资料 http://www.sdmagazine.com/ 软件开发杂志，经常会有一些关于测试方面好的论文资料，同时还包括了项目和过程改进方面的课题，并且定期会有一些关于质量和测试方面的问题讨论 http://www.sei.cmu.edu/ 著名的软件工程组织，承担美国国防部众多软件工程研究项目，在这里你可以获俄各类关于工程质量和测试方面的资料。该网站提供强有力的搜索功能，可以快速检索到你想要的论文资料，并且可以免费下载 http://www.soft.com/Institute/HotList/ 提供了网上软件质量热点连接，包括：专业团体组织连接、教育机构连接、商业咨询公司连接、质量相关技术会议连接、各类测试技术专题连接等 http://www.soft.com/News/QTN-Online/ 质量技术时事，提供有关测试质量方面的一些时事介绍信息，对于关心测试和质量发展的人士来说是很有价值的 http://www.softwaredioxide.com/ 包括软件工程（CMM,CMMI,项目管理）软件测试等方面的资源 http://www.softwareqatest.com/ 软件质量/测试资源中心。该中心提供了常见的有关测试方面的FAQ资料，各质量/测试网站介绍，各质量/测试工具介绍，各质量/策划书籍介绍以及与测试相关的工作网站介绍 http://www.softwaretestinginstitute.com/ 一个软件测试机构，提供软件质量/测试方面的调查分析，测试计划模板，测试WWW的技术，如何获得测试证书的指导，测试方面书籍介绍，并且提供了一个测试论坛 http://www.sqatester.com/index.htm 一个包含各种测试和质量保证方面的技术网站，提供咨询和培训服务，并有一些测试人员社团组织，特色内容是缺陷处理方面的技术 http://www.sqe.com/ 一个软件质量工程服务性网站，组织软件测试自动化、STAR-EASE、STARWEST等方面的测试学术会议，并提供一些相关信息资料和课程服务 http://www.stickyminds.com/ 提供关于软件测试和质量保证方面的当前发展信息资料，论文等资源 http://www.stqemagazine.com/ 软件策划和质量工程杂志，经常有一些好的论文供下载，不过数量较少，更多地需要通过订购获得，内容还是很有价值的 http://www.tantara.ab.ca/ 软件质量方面的一个咨询网站，有过程改进方面的一些资料提供 http://www.tcse.org/ IEEE的一个软件工程技术委员会，提供技术论文下载，并有一个功能强大的分类下载搜索功能，可以搜索到测试类型、测试管理、 测试分析等各方面资料 http://www.testing.com/ 测试技术专家Brain Marick的主页，包含了Marick 研究的一些资料和论文，该网页提供了测试模式方面的资料，值得研究。总之，如果对测试实践感兴趣，该网站一定不能错过 http://www.testingcenter.com/ 有一些测试方面的课程体系，有一些价值 http://www.testingconferences.com/asiastar/home 著名的AsiaStar测试国际学术会议官方网站，感兴趣的人一定不能错过 http://www.testingstuff.com/ Kerry Zallar的个人主页，提供一些有关培训、工具、会议、论文方面的参考信息 http://www-sqi.cit.gu.edu.au/ 软件质量机构，有一些技术资料可以供下载，包括软件产品质量模型、再工程、软件质量改进等","title":"63个国外优秀测试网站地址"},{"content":" 前言 也许大家不相信，数学是解决信息检索和自然语言处理的最好工具。它能非常清晰地描述这些领域的实际问题并且给出漂亮的解决办法。每当人们应用数学工具解决一个语言问题时，总会感叹数学之美。我们希望利用 Google 中文黑板报这块园地，介绍一些数学工具，以及我们是如何利用这些工具来开发 Google 产品的。 系列一： 统计语言模型 (Statistical Language Models) Google 的使命是整合全球的信息，所以我们一直致力于研究如何让机器对信息、语言做最好的理解和处理。长期以来，人类一直梦想着能让机器代替人来翻译语言、识别语音、认识文字（不论是印刷体或手写体）和进行海量文献的自动检索，这就需要让机器理解语言。但是人类的语言可以说是信息里最复杂最动态的一部分。为了解决这个问题，人们容易想到的办法就是让机器模拟人类进行学习 - 学习人类的语法、分析语句等等。尤其是在乔姆斯基（Noam Chomsky 有史以来最伟大的语言学家）提出 “形式语言” 以后，人们更坚定了利用语法规则的办法进行文字处理的信念。遗憾的是，几十年过去了，在计算机处理语言领域，基于这个语法规则的方法几乎毫无突破。 其实早在几十年前，数学家兼信息论的祖师爷 香农 (Claude Shannon)就提出了用数学的办法处理自然语言的想法。遗憾的是当时的计算机条件根本无法满足大量信息处理的需要，所以他这个想法当时并没有被人们重视。七十年代初，有了大规模集成电路的快速计算机后，香农的梦想才得以实现。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师贾里尼克 (Fred Jelinek)。当时贾里尼克在 IBM 公司做学术休假 (Sabbatical Leave)，领导了一批杰出的科学家利用大型计算机来处理人类语言问题。统计语言模型就是在那个时候提出的。 给大家举个例子：在很多涉及到自然语言处理的领域，如机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询中，我们都需要知道一个文字序列是否能构成一个大家能理解的句子，显示给使用者。对这个问题，我们可以用一个简单的统计模型来解决这个问题。 如果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的概率等于每一个词出现的概率相乘，于是P(S) 可展开为： P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1) 其中 P (w1) 表示第一个词w1 出现的概率；P (w2|w1) 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词wn，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。因此我们假定任意一个词wi的出现概率只同它前面的词 wi-1 有关(即马尔可夫假设），于是问题就变得很简单了。现在，S 出现的概率就变为： P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)… (当然，也可以假设一个词又前面N-1个词决定，模型稍微复杂些。） 接下来的问题就是如何估计 P (wi|wi-1)。现在有了大量机读文本后，这个问题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次，以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可以了,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。 也许很多人不相信用这么简单的数学模型能解决复杂的语音识别、机器翻译等问题。其实不光是常人，就连很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法都有效。比如在 Google 的中英文自动翻译中，用的最重要的就是这个统计语言模型。去年美国标准局(NIST) 对所有的机器翻译系统进行了评测，Google 的系统是不仅是全世界最好的，而且高出所有基于规则的系统很多。 现在，读者也许已经能感受到数学的美妙之处了，它把一些复杂的问题变得如此的简单。当然，真正实现一个好的统计语言模型还有许多细节问题需要解决。贾里尼克和他的同事的贡献在于提出了统计语言模型，而且很漂亮地解决了所有的细节问题。十几年后，李开复用统计语言模型把 997 词语音识别的问题简化成了一个 20 词的识别问题，实现了有史以来第一次大词汇量非特定人连续语音的识别。 我是一名科学研究人员 ，我在工作中经常惊叹于数学语言应用于解决实际问题上时的神奇。我也希望把这种神奇讲解给大家听。当然，归根结底，不管什莫样的科学方法、无论多莫奇妙的解决手段都是为人服务的。我希望 Google 多努力一分，用户就多一分搜索的喜悦。   来自：http://www.kuqin.com/math/20071204/2775.html","title":"数学之美系列一：统计语言模型"},{"content":"1.        结构化程序设计 为了提高程序的可读性、可重用性等，逐渐出现了将程序开发中经常用到的相同的功能，比如数学函数运算、字符串操作等，独立出来编写成函数，然后按照相互关系或应用领域汇集在相同的文件里，这些文件构成了函数库。 函数库是一种对信息的封装，将常用的函数封装起来，人们不必知道如何实现它们。只需要了解如何调用它们即可。函数库可以被多个应用程序共享，在具体编程环境中，一般都有一个头文件相伴，在这个头文件中以标准的方式定义了库中每个函数的接口，根据这些接口形式可以在程序中的任何地方调用所需的函数。 由于函数、库、模块等一系列概念和技术的出现，程序设计逐渐变成如图所示的风格。程序被分解成一个个函数模块，其中既有系统函数，也有用户定义的函数。通过对函数的调用，程序的运行逐步被展开。阅读程序时，由于每一块的功能相对独立，因此对程序结构的理解相对容易，在一定程度上缓解了程序代码可读性和可重用件的矛盾，但并未彻底解决矛盾。随着计算机程序的规模越来越大，这个问题变得更加尖锐，于是出现了另一种编程风格——结构化程序设计。 在结构化程序设计中，任何程序段的编写都基于3种结构：分支结构、循环结构和顺序结构。程序具有明显的模块化特征，每个程序模块具有惟一的出口和入口语句。结构化程序的结构简单清晰，模块化强，描述方式贴近人们习惯的推理式思维方式。因此可读性强，在软件重用性、软件维护等方面都有所进步，在大型软件开发尤其是大型科学与工程运算软件的开发中发挥了重要作用。因此到目前为止，仍有许多应用程序的开发采用结构化程序设计技术和方法。即使在目前流行的面向对象软件开发中也不能完全脱离结构化程序设计。   2.        面向对象程序设计 面向对象的程序役计方法是程序设计的一种新方法。所有面向对象的程序设计语言一般都含有三个方面的语法机制，即对象和类、多态性、继承性。 1．对象和类 对象的概念、原理和方法是面向对象的理序设计语言晕重要的特征。对象是用户定义的类型（称为类）的变量。一个对象是既包含数据又包合操作该数据的代码（函数）的逻辑实体。对象中的这些数据和函数称为对象的成员，即成员数据和成员函数。对象中的成员分为公有的和私有的。公有成员是对象与外界的接口界面。外界只能通过调用访问一个对象的公有成员来实现该对象的功能。私有成员体现一个对象的组织形式和功能的实现细节。外界无法对私有成员进行操作。类对象按照规范进行操作，将描述客观事物的数据表达及对数据的操作处理封装在一起，成功地实现了面向对象的程序设计。当用户定义了一个类类型后，就可以在该类型的名下定义变量（即对象）了。类是结构体类型的扩充。结构体中引入成员函数并规定了其访问和继承原则后便成了类。 2．多态性 面向对象的程序设计语言支持“多态性”，把一个接口用于一类活动。即“一个接口多种算法”。具体实施时该选择哪一个算法是由特定的语法机制确定的。C++编译时和运行时都支持多态性。编译时的多态性体现在重载函数和重载运算符等方面。运行时的多态性体现在继承关系及虚函数等方面。 3．继承性 C++程序中，由一个类（称为基类）可以派生出新类（称为派生类）。这种派生的语法机制使得新类的出现轻松自然，使得一个复杂事物可以被顺理成章地归结为由逐层派生的对象描述。“派生”使得程序中定义的类呈层次结构。处于子层的对参既具有其父层对象的共性．又具有自身的特性。继承性是一个类对象获得其基类对象特性的过程。C++中严格地规定了派生类对其基类的继承原则和访问权限，使得程序中对数据和函数的访间，需在家族和朋友间严格区分。   3.        事件驱动的程序设计 事件驱动的程序设计实际上是面向对象程序设计的一个应用，但它目前仅适用于windows系列操作系统。windows环境中的应用程序与MS－DOS环境中的应用程序运行机制不同、设计程序的方式也不一样。windows程序采用事件驱动机制运行，这种事件驱动程序由事件的发生与否来控制，系统中每个对象状态副改变都是事件发生的原由或结果，设计程序时需以一种非顺序方式处理事件，与顺序的、过程驱动的传统程序设计方法迥异。 事件也称消息，含义比较广泛，常见的事件有鼠标事件(如民标移动、单击、掠过窗口边界)、键盘事件(如按键的压下与拾起)等多种。应用程序运行经过一系列必要的初始化后，将进入等待状态，等待有事件发生，一旦事件出现，程序就被激活并进行相应处理。 事件驱动程序设计是围绕着消息的产生与处理进行的．消息可来自程序中的某个对象，也可由用户、wlndow s或运行着的其他应用程序产生。每当事件发生时，Windows俘获有关事件，然后将消息分别转发到相关应用程序中的有关对象，需要对消息作出反应的对象应该提供消息处理函数，通过这个消息处理函数实现对象的一种功能或行为。所以编写事件驱动程序的大部分工作是为各个对象(类)添加各种消息的处理函数。由于一个对象可以是消息的接收者，同时也可能是消息的发送者，所发送的消息与接收到的消息也可以是相同的消息，而有些消息的发出时间是无法预知的(比如关于键盘的消息)，因此应用程序的执行顺序是无法预知的。 4.        逻辑式对象程序设计 逻辑式程序设计的概念来自逻辑式程序设计语言Prolog这一曾经在计算机领域引起震动的日本“第五代”计算机的基本系统语言，在这种“第五代”计算机中，Prolog的地位相当于当前计算机中的机器语言。 Prolog主要应用在人工智能领域，在自然语言处理、数据库查询、算法描述等方面都有应用，尤其适于作为专家系统的开发工具。 Prolog是一种陈述式语言，它不是一种严格的通用程序设计语言，使用Prolog编写程序不需要描述具体的解题过程、只需结出一些必要的事实和规则，这些规则是解决问题方法的规范说明，根据这些规则和事实．计算机利用渭词逻辑，通过演绎推理得到求解问题的执行序列。 5.        并行程序设计 一个有实际应用的并行算法，最终总要在并行机上实现，为此首先就要将并行算法转化为并行程序，此过程就是所谓的并行程序设计(Parallel Program)。它要求算法设计者、系统结构师和软件工作者广泛频繁的交互。因为设计并行程序涉及到的知识面较广，主要包括操作系统中的有关知识和优化编译方面的知识。操作系统内容非常丰富，并行程序中最基本的计算要素如任务、进程、线程等基本概念、同步机制和通信操作等。 目前并行程序设计的状况是：⑴并行软件的发展落后于并行硬件；⑵和串行系统与应用软件相比，现今的并行系统与应用软件甚少且不成熟；⑶并行软件的缺乏是发展并行计算的主要障碍；⑷不幸的是，这种状态似乎仍在继续着。究其原因是并行程序设计远比串行程序设计复杂：⑴并行程序设计不但包含了串行程序设计，面且还包含了更多的富有挑战性的问题；⑵串行程序设计仅有一个普遍被接受的冯·诺依曼计算模型，而并行计算模型虽有好多，但没有一个可被共同认可的像冯·诺依曼那样的优秀模型；⑶并行程序设计对环境工具(如编译、查错等)的要求远比串行程序设计先进得多；⑷串行程序设计比较适合于自然习惯，且人们在过去积累了大量的编程知识、经验和宝贵的软件财富。 转自 http://www.cppblog.com/changshoumeng/archive/2010/07/24/121176.html","title":"五种程序设计方法"},{"content":" 编者按：原文作者Alan Skorkin是一名软件开发人员，他在博客中分享对软件开发相关的心得，其中有很多优秀的文章，本文就是其中一篇，作者认为：成为优秀的开发人员，可以没有数学技能，但成为卓越的开发人员，不能没有。 　　不久之前，我开始思索数学。你也知道，到目前为止，我编写软件也有几年了。老实说，在我的工作当中，我还没有发现有关数学的需求。我要学习和掌握许多新东西，包括语言、框架、工具、流程、沟通技巧和可以用来做你想到的任何东西的库。在我学的新东西中，数学并没有帮助。当然了，这不足为奇，我所做的工作，大部分都是CRUD类型（编注：CRUD是Create、Read、Update和Delete的首字母缩写）。在互联网时代，这也是我们多数开发人员所做的大部分工作。如果你做顾问，你主要是在做网站；你在大公司上班，你主要是在做网站；你做自由职业者，你主要是在做网站。我很清楚我是在总结，但请忍耐一下，我跑偏了。 　　最后你对此有些厌倦了，我也如此。别误会我，这可以是项有趣并有挑战性的工作，有机会解决问题，并和有趣的人一起互动，在工作时间做这个，我高兴。但在我个人时间中搭建更多的网站，这种想法已经稍微失去其光泽，于是你开始寻找一些更加有趣/酷/好玩的事情，我再一次地也如此。（所以，）有些人转移到前台和图像技术，比如视觉反馈就比较诱人。但我并不是其中一员（虽然我和别人一样都喜爱前台，但它真的不能让我兴奋。）这就是当我遇到一些搜索相关的问题时，我为什么决定深入挖掘的原因了。这把我带回到故事的一开始，因为一旦我抓到第一把充满搜索的铁铲，一旦我“撞到”数学时，我才真正意识到，我的技能恶化的程度。数学并不像骑自行车，长期不用就会忘记。 　　拓展视野 　　多对搜索的一些了解，让我接触到各种有趣的软件和计算机科学相关的事情和问题（包括机器学习、自然语言处理、算法分析等）。现在，在我接触的各方面，我都看到了数学，所以我更加强烈地感觉到自己技能缺乏。我已经意识到，如果你想利用计算机做又酷又有趣的事，你需要达到一个像样的数学能力水平。除了上面说的三个，还有一些，如：密码学、游戏人工智能、压缩算法、遗传算法、3D图形算法等。在理解之后，如果你想要编写我们正讨论的那些库和工具，而不是仅仅使用它们（即：做一个“消费者”，而不是“生产者”），那你需要数学（知识）来理解这些领域背后的你能应用的理论。即便如果你不想编写任何库，当你真正理解事情的原理，你在构建软件时，它能给带来更多的成就感，绝非仅仅把它们连起来，就希望它们去做任何它们应该能做的。 　　虽然大多数开发人员会告诉你，他们在工作中从来不需要数学(就像我前面说的 :))，但是经过一番沉思后，我有了个想法（突发灵感）：就是反马斯洛的锤子理论。你知道这个吧，当你有一把锤子，你会把一切看成是钉子。（注：伯乐在线编译的《每位开发人员都应铭记的10句编程谚语》中的第7条就是锤子理论。）这是一个隐喻，也就是说人们乐于使用自己钟爱的工具，即便这并不是手中工作的最好工具。数学就是我们的一个相反的锤子。我们知道有这个锤子，但并不太子的如何使用。所以，当我们遇到问题，我们的锤子是解决问题的最佳工具时，我们却从未认真考虑过它。对我祖父而言，螺丝刀够用了；对我父亲来说，也很好；对我来说，同样如此。谁还需要锤子？数学的技巧在于，人们惧怕它，甚至大多数程序员，你认为我们不会怕，但我们确实怕。所以，我们把自己的话转变为可以自我实现的预言。这并不是我在工作中不需要数学，这只是我真的不知道，即便我知道，我也不知道如何使用它。所以我并没有使用它，当缺少某些东西时，如果你长期将就，不久后你甚至不会察觉它的缺失，所以对其需要更少了，这是自我实现的预言。 　　针对思索接近我们内心世界，这里有一些的“粮食”——学习新技术。作为一名协作世界的开发人员，你努力成为一名通才型的专才（如果你不知道我在说什么，可以看看这本书《The Passionate Programmer:Creating A Remarkable Career In Software Development》）。你尽力在多数事情上做的体面，并在有些事情上做的优秀。但是你擅长什么？一般来说，人们会选择一两个框架或一门语言，然后与之相伴，这样是不错。但是要看到，框架和较小范围内的语言都有保质期。如果你要做一名Hibernate、Rails或Struts专家（使用struts的朋友现在真的应该担忧一下了），当新框架取代当前的框架时，你在几年内将不得不重新洗牌。所以，这也许是你真正的最好投资，但也可能不是。另一方面，数学是不会很快消逝的。在我们领域中所做的一切，都是建立在稳固的数学原理之上（算法和数据结构正是这样的例证），所以用在数学上的时间绝不是浪费，这不可辩论。再重复一次，总结起来就是：要真正理解东西，而不是非死记硬背地使用。当涉及到计算机时，数学能有助你更深入地理解你所做的。事实上，正如Steve Yegge所言，作为程序员我们所做的事很像数学，只是我们甚至都没有意识到这一点。 　　什么/谁造就了与众不同？ （唐纳德） 　　你不相信我？那请你想想：在我们的领域中，几乎人人普遍尊敬的卓越程序员同样也是大数学家。我是说像唐纳德·克努斯、艾兹格·迪杰斯特拉、诺姆·乔姆斯基、彼得·诺维格（Google研究院总监）这一类人。但是这些家伙并非真正的开发人员，他们是计算机科学家，这能真正算数么？我再一次觉得，在我们写出的纯代码行数能达到这些人所写的十分之一之前，也许我们不应该再去讨论这些问题了。当然，不当科学家，你也能获得成功和名誉，大家都听过加文·金（Gavin King，Hibernate创始人）或戴维·海涅梅艾尔·汉森（DHH，Ruby on Rails创始人）。这还挺真实的（是不是有很多人听说过加文和戴维，虽然这还有待确认），但是“听说过”和普遍尊敬是不同的，这种差别就如同创建一个框架，和在你的领域中为人类知识所做出的全部重大推动两者之间的差别。（不要误会我，我尊重加文和戴维，他们所做的事，远远超过我，但是这不能影响我所说的事实）。所有的这些相关么？我不知道，可能不相干，但在我们反省之后，我想无论如何要把它“扔掉”。 　　如今的世界正充满着数据，每日都增加更多的数据。而在以前，我们在相对少量的数据下享受工作。我们今日编写的软件必须高效处理海量数据。甚至在协作世界，这也是愈加明显的事实。这也就是说，你更不可能只“启动东西”，就想看其如何运作，因为你要处理的数据量将困住你，除非你非常了解它。我的预测是：算法分析将对于 Lay Programmer 越来越重要，以前不仅如此，以后也更加如此。如果要成为一位体面的算法设计专家，需要什么？你猜到了，是一些数学技能。（编注：Lay Programmer是指那些不认为自己是程序员的程序员，详情请见Martin Fowler的解释。我暂未想到合适的简短叫法，如果哪位朋友知道，请在评论中说明。） 　　所以，我该怎么办呢？嗯，我已决定一点一点地建立或恢复我的数学技能，虽然还有大量的书要看，大量的代码要写，但我会尽力抽时间放在数学上，这就像锻炼，时不常的锻炼总聊胜于无（再次引用Steve Yegge的话）。说到数学，我袖中当然还藏有一张王牌，它对我有利，但很幸运，有这个博客，我们都会受益的。（我知道你好奇，一会告诉你 :) ）。 　　你在5年内的规划如何？ （极限水上滑板） 　　那么，数学对所有事都有利么？这事先很难说，我对我现在的处境十分满意，或许你也如此，但这都和潜能有关系。如果你是协作世界的一名开发人员，你真的不需要数学。如果你乐于你的整个职业生涯是这样的：在工作时间中做企业CRUD应用，或在闲暇时间滑翔跳伞或极限水上滑板（或其他各种时髦的极客运动），也分配较多时间在Spring、Hibernate、Visual Studio或其它东西上。（其实）那些特殊的职位并没有真正限制你的潜力，你能变得极具价值，甚至可深入追求。但是如果你想为多样化的职业生涯而奋斗，想要有能力尝试几乎所有涉及代码的事，从信息检索到Linux内核。总之，如果你想成为一个开发人员、程序员和计算机科学家的完美组合，你必须确保你的数学技能达到标准（哎，你还是可以去玩滑翔跳伞或极限水上滑板）。长话短说，如果你在数学方面有一定天赋，那在软件开发领域中没有向你关着的门，如果没有，那一切都是CRUD型工作！","title":"数学是成就卓越开发人员的必备技能"},{"content":"世界计算机算法最权威会议SODA---全称ACM-SIAM Symposium on Discrete Algorithms。 世界计算机科学领域最顶级期刊JACM---全称Journal of the Association for Computing Machinery，该期刊只发表世界计算机科学领域具有最重要意义的研究工作，每年仅收录30多篇。 世界数据库领域最顶级的期刊ACM TODS---全称ACM Transactions on Database Systems，该期刊全年在全世界范围不过收录30篇高水平论文 世界计算机存储领域顶尖期刊ACM Transactions on Storage---该期刊全年收录文章不超过20篇 世界程序语言设计领域顶级学术会议PLDI2007---全称ACM SIGPLAN Conference on Programming Language Design and Implementation 世界物理学最权威学术刊PRL---全称Physical Review Letter，国内大学计算机系目前只有清华计算机系发过两篇PRL 世界理论计算机领域顶级会议STOC---全称ACM Symp on Theory of Computing 世界人工智能方面最顶级会议IJCAI---全称International Joint Conferences on Artificial Intelligence 世界计算机视觉和模式识别领域顶级国际会CVPR---全称IEEE Conference on Computer Vision and Pattern Recognition 世界信息检索领域顶级会议SIGIR---全称ACM SIGIR Special Interest Group on Information Retrieval 世界数据挖掘领域最权威国际期刊IEEE TKDE---全称IEEE Transactions on Knowledge and Data Engineering 世界数据库领域最顶级会议SIGMOD---全称ACM's Special Interest Group on Management Of Data 世界计算机图形学最权威国际会议ACM SIGGRAPH 世界计算语言/自然语言处理领域最顶级会议ACL---全称Association for Computational Linguistics 世界理论计算机科学顶级学术期刊Theoretical Computer Science 世界计算复杂性领域顶级会议CCC---全称IEEE Conference on Computational Complexity 世界计算机视觉和模式识别领域顶尖期刊IEEE PAMI---全称IEEE Transactions on Pattern Analysis and Machine Intelligence 世界集成电路设计领域最顶级会议DAC---全称Design Automation Conference 世界人工智能领域顶级学术会议AAAI---全称Association for the Advancement of Artificial Intelligence 世界互联网领域顶级会议WWW---全称World Wide Web Conference 世界通信与计算机网络领域顶级学术会议Infocom---全称IEEE Conference on Computer Communications， 世界信息科学理论顶级期刊IEEE Transactions on Information Theory 世界数据挖掘领域一流会议SDM---全称SIAM International Conference on Data Mining 世界声学与信号处理一流会议ICASSP---全称IEEE International Conference on Acoustics, Speech, and Signal Processing 世界计算机算法与理论领域一流会议STACS---全称Symp on Theoretical Aspects of Computer Science 世界计算机理论科学领域一流会议ICALP---全称International Colloquium on Automata, Languages and Programming 世界数据挖掘领域一流会议ICME---全称IEEE International Conference on Multimedia & Expo 世界计算机图形学领域一流会议EuroGraphics 世界集成电路领域一流会议ISVLS","title":"世界计算机各领域顶尖期刊会议"},{"content":"赵世奇，毕业于哈尔滨工业大学计算机科学与技术学院，是百度博士后工作站成立以来的第一名博士后，到2010年年底，赵世奇已经在百度工作了一整年。 话不多，这是赵世奇给人的第一印象，他的自我介绍也颇为简单：“我在百度工作，研究语义搜索。” 但是话匣子一旦打开，就会发现赵世奇并非一个只钻技术的狂人，他思维敏捷，爱好广泛，对生活抱以随性的平实态度。 “与技术保持距离。”这是赵世奇的研究态度，对他而言，技术的极致与否要从用户的需求出发，如果埋头技术不问需求，反而会走进一个技术“怪圈”。 歪打正着 计算机语言学。2003年，大三刚一结束，赵世奇得知已被保送研究生，在大四就要开始实验室生涯，但这门学科是赵世奇鲜有听闻，从未接触过的领域。 自然语言处理技术的发展，得益于搜索引擎的发展，而在当时，搜索引擎方兴未艾，自然语言处理完全是个大冷门，眼看同学们都在网络安全与数据库的研究范畴里拼抢，赵世奇想到了退出。 为此，学科带头人多次找到赵世奇，将这一交叉学科的兴趣点罗列出来。“计算机科学、语言学、数学融合在一起，少了单一学科的枯燥，领域更为宽广。” “那就读吧。”在赵世奇的回忆中，他其实不是一个目标清晰的人，更多的是随遇而安，这样的心态反而让他不必左顾右盼而专心于这一领域。“这就是一个阴差阳错的安排，我没想到我能做好。慢慢的，一年后，我有了些感觉。” 在日后的研究中，赵世奇从一名“小白”逐渐找到了自己的方式。真正挑动起赵世奇积极性的是每次做出的小系统和程序，能够进一步理解人类语言，这样的成就感在一点一滴的积累中，推动赵世奇不断向前。 3年后，赵世奇被保送攻读博士学位。 赵世奇一直保持了“随遇而安”的心态。在同学都为着出国深造削尖脑袋考托福时，赵世奇慢了半拍。最终让赵世奇放弃出国念头的还是在于他内心对“研究”的看法，如果出国，他将学习另一种研究思路和范式，但如果能在荷枪实弹的实际环境中磨练，将研究用于实用，会更有价值，赵世奇选择了后者。 转换思路 也正是抱着“实干”的态度，在攻读完博士后，赵世奇进入百度，这和他所想要的荷枪实弹恰好呼应，他看中了百度广阔的发展平台。“首先，百度搜索引擎很倚重自然语言处理技术，这与我的博士课题方向非常一致；其次，百度是一个非常大且非常有活力的平台，在这样的企业中工作，自己的成长也会更快。” 赵世奇所在的自然语言处理团队，是百度最核心的技术团队之一，其成员全部由顶尖学者和工程师组成，规模已达60余人。百度为这些“赵世奇们”从事的前沿技术探索提供了庞大用户数据库资源和强有力的技术支持：从用户量上看，百度已覆盖95%以上的中国网民，拥有超过两亿注册用户；从技术投入上看，百度拥有3000多名全球顶级的搜索技术工程师，技术研发和产品投入更是超过了公司年度营收的1/3。 “单独做研究可以天马行空，有趣、离奇、新颖或者前瞻。”赵世奇刚进入百度，就发现在企业做科研与在院校的研究院有根本区别，将研究成果转化为实际的产品才是最难的。 在百度，对技术研发方向的确认必须经过“层层把关”。经过公司的内部考量，与现有产品的嫁接，以及用户的考验后，此项研究开发才能被确认其价值。赵世奇时常提醒自己，在理论研究和应用实践中找到平衡点。 “当我们考虑问题的时候，我们常常回到原点，返回用户搜索行为的源头去思考，如何让搜索引擎读懂用户想说什么、想找什么。”赵世奇在百度的一年时间里，理解到技术层面的更新不是最重要的，更重要的是一名技术人员思维方式的转变。 让机器读懂人的思想 “我不是一个技术狂人，也不痴迷于技术。”这是赵世奇的科研态度。在大学做研究时，他并不爱时常泡实验室，在百度工作，加班也不是他的风格。“对技术保持若即若离，更有利于研发。” 对于赵世奇而言，让机器读懂人的思想，是他的追求。 “当我们看到用户的查询时，就在想能不能去反推或者猜测他脑子里的问题到底是什么？从研究者的角度来讲，这也是很有趣的问题。”而对于资源的精细加工，赵世奇觉得当前搜索引擎对于互联网资源的利用还不够深入，未来将会把信息抽取和数据挖掘技术更多地应用进来，使得用户检索的对象不再是网页，而是网页中的信息和知识。 2009年，百度CEO李彦宏提出了“框计算”理念，这是一种简单、可依赖的互联网需求交互模式，用户只要在框中输入服务需求，系统就能明确识别这种需求，并将该需求分配给最优的应用或内容资源提供商处理，最终返回给用户相匹配的结果。 赵世奇认为，“语义搜索”是框计算理念得以实现的重要技术保证。“‘语义搜索’未来的重点可能包括‘查询的深入理解’和‘资源的精细加工’。” 搜索引擎的未来会变成什么样子，赵世奇心中已经有了更加明确的方向，他做了一个巧妙的比喻:“当电灯发明之前，大家觉得点蜡烛也蛮好的，晚上可以照亮。但是当电灯发明后，人们才发现原来蜡烛是满足不了需求的。因此，如果我们依据当前对搜索引擎的定义来看搜索，其实它已经差不多达到了需求，但是这不代表用户内心里真的没有更高的需求。如果我们能够努力地突破这一层，在更上一层回头看搜索引擎的时候，才能看到它有什么不足的地方。”  ","title":"赵世奇：和技术保持距离"},{"content":"  【嘉宾介绍】 王海峰，1999年3月毕业于哈尔滨工业大学，获博士学位，现任百度高级科学家，兼任哈尔滨工业大学兼职教授、博士生导师、中国中文信息学会理事、中国计算机学会高级会员及多个学会的专委会委员、全国信息技术标准化技术委员会委员等。在国际学术界，王海峰是自然语言处理领域世界上影响力最大、也最具活力的国际学术组织ACL（Association for Computational Linguistics）五十年历史上唯一当选副主席的华人，他还曾担任或正在担任国际期刊副编辑、客座编辑、国内期刊编委、顶级国际会议的程序委员会主席、Workshop主席、Tutorial主席及Industry track主席等。   规格严格、功夫到家 “哈工大的方方面面都让我难忘，包括严谨治学的老师、风华正茂的同学、丰富多彩的大学生活、雄伟壮观的主楼、风格各异的体育场馆、全国评比屡拔头筹的食堂、学校周边的小餐馆和录像厅，还有那据说是亚洲最大的第二学生宿舍等等。但对我影响最大的，还是哈工大‘规格严格，功夫到家’的校训，直到现在，这也是我做事的准则。”提起从本科到博士九年半的哈工大求学生涯，王海峰饱含着对母校的深情娓娓道来。 “其实，早在上大学之前，我就已经领略了哈工大的‘规格严格、功夫到家’。记得上高中时，作为全国物理竞赛黑龙江省赛区入围决赛的十几位同学之一，我第一次来到了位于机械楼的哈工大物理实验室参加最终的实验竞赛。实验竞赛，不仅要看最终的实验结果，还要考察操作过程，连续几个小时的实验，作为考官的哈工大物理老师始终站在旁边注视着我的每一个操作，还不时就一些关键点向我提问，当时提到的‘回程误差’等一些概念直到今天仍记忆犹新。那一天的经历带给我的对科学技术的向往和敬畏，更是二十几年来一直激励着我。后来，当我以大学生的身份重新走进哈工大物理实验室时，‘规格严格、功夫到家’已由中学时代的模糊感受变成了行为自觉，我求真务实地进行每一个操作、一丝不苟地完成每一份实验报告，当时我每次物理实验报告的成绩都是全班最高。” 上研究生后，王海峰在李生、赵铁军两位老师的指导下，进入了机器翻译这个充满挑战的领域，并仅用一年的时间就开发出了当时在国家863评测获得第一的汉英机器翻译系统，并获得部级科技进步奖。那时，他每天都早7点以前到达实验室，直到晚10点以后才离开，踏踏实实地践行着“规格严格、功夫到家”的校训，也实实在在地追逐着年轻学子的梦想。 工作后，“规格严格、功夫到家”的行事准则仍伴随王海峰不断做出突破性成果、达到新的高度。当接二连三地发表世界级研究成果时，在同行眼中，他的论文不仅学术价值高，而且写作也十分严谨，在他的论文中，连个标点符号的疏忽都极难找到。当带领团队在国际口语翻译评测中击败来自世界各地的顶尖团队，以绝对优势获得第一时，他凭的不仅是方法的创新，还有求真务实地做好每一个细节的态度。   百度是一个实现梦想的平台 1999年初博士毕业时，面对大量诱人的机会，王海峰果断地选择了当时刚刚成立不久的微软中国研究院，他说：“希望我的所学能转化为优秀的产品，真正为亿万用户带来实实在在的帮助。” 这个胸怀梦想的年轻人，从1999年初加入微软，到通过香港特区政府的优秀人才计划任isilk.com的研究科学家，再到在东芝（中国）研究开发中心担任副所长兼研究部部长、首席研究员，经过十余年的磨练，他带着厚重的积累和丰富的经验加盟了百度。“因为在这里，我有机会让更多的用户受惠于我的工作成果”。十余年后，王海峰依然在追逐着同样的梦想。 2010年，王海峰参加了ACL副主席的竞选，并最终成功当选，成为ACL五十年历史上唯一当选副主席的华人。在竞选宣言中，他这样描述了在百度的工作： “I have realized what I dreamed 17 years ago when I started pursuing NLP: to create good enough NLP technology for billions of daily user requirements. And I am aware of my privilege to further promote NLP technology, both \"from research to real users\" and \"from real users to research\".” 回顾十二年的职业生涯，王海峰说，“美国式的开拓创新、香港式的目标现实、日本式的严谨务实都使我受益匪浅。而在百度，感受到的是一种别样的激情与活力。百度的活力，是一种特别的活力，它将现代中国的活力、互联网行业的活力、国际化企业的活力和年轻人的活力完美融合，形成了‘简单可依赖’的特有文化，创造了每天满足亿万用户需求的产品。我的每一个成果都有机会影响亿万用户，为他们带来用户体验的提升。百度还提供了足够的空间与平台，在企业高速发展的同时，更能够使每个员工实现个人能力与价值的飞速提升。” 百度是一个实现梦想的平台，它热情地欢迎更多充满活力与梦想的人前来加入。   写给学弟学妹：脚踏实地才是最有用的 在采访的最后，王海峰向所有的学弟学妹们提出了两点中肯的建议。 首先是注重基础能力的培养，他提到两个重要的基础能力：软件开发基础和数学基础。不少立志于搞研究的同学容易忽视软件开发能力，实际上，“工欲善其事，必先利其器”，具备优秀的开发能力，会让自己的工作更加高效，也可以快速尝试各种不同的点子。另一方面，有很多同学认为数学在工作学习中并不经常使用，但在实际工作中，你会逐渐意识到数学拥有的魔力，数学思维不仅可以带来更精妙的算法，还可以使你思维更加敏捷，逻辑更加清晰。 除了提高自身能力，还要培养求真务实的做事态度，不可浮躁。当别人走所谓“捷径”的时候不要为之所动，须知踏实走好每一步才是最重要的。“规格严格、功夫到家”地做好每件事，成功才会离自己越来越近。","title":"百度高级科学家——王海峰"},{"content":"Nutch中自带对搜索结果的聚类，使用开源的Carrot2，以插件形式被调用，大概看了一下nutch关于clustering这一块的搜索源码，它会显示出URL和title，可是用mahout做文本聚类的话，最后的聚类结果中，只有向量， 当然自己可以将URL加进去，但如何显示标题呢？title不是存放在parse_text中的，标题是存放在parse_data中的，难不成再反过来根据URL去查parse_data，可以调nutch的命令根据URL查，返回来的估计是个String，这样还要在这个String里去匹配title字段，这个过程中是不是复杂了点呢?parse_data还存放每个URL解析出的外部连接和元数据Metadata 看了一下抓取网页和抓取pdf文件后生成的parse_data，抓取网页后的parse_data中有标题，即网页源码中的<title>标签包含的字段，而抓取pdf文件后生成的parse_data中，title字段为空，而在其对应的parse_text中，发现pdf文档的标题被当作正文放在parse_text中了 看nutch对搜索结果的聚类，每个簇都有个标签，一个簇下有属于这个簇的点，在对应簇下显示出相应点的标题及URL。如果是用mahout聚类后，取不到这个值，这个可以仔细看一下nutch搜索源码，看它是如何做到的，然后看能不能用mahout实现一下。同时建议看一下topic model，在机器学习和自然语言处理领域，topic model指一种统计模型，用来从一批文档的集合中发现抽象的主题/论题。 在nutch对结果搜索聚类中是用HitsCluster.getDescriptionLabels() 来获得聚类的标签的，文档标题是通过HitsCluster.getHits().getValue(\"title\")来获得，这个只是粗浅的看，具体实现细节还不知道，还需要时间继续向下挖掘，看它是如何利用Carrot2来做搜索结果聚类的 好了，上述提的问题，等后面有时间再看，现在言归正传，如何将mahout的聚类结果通过网页来展示呢？如果能在eclipse将聚类结果成功打在控制台中，那么把它移到web容器中，如tomcat，这个就不是难事了 现假设调用mahout kmens命令生成的聚类结果在本地，那么问题就转化成如何读取这些聚类结果文件，前面文章里有对聚类结果文件介绍，有兴趣的可以去找找。读取聚类结果，大家可以参照mahout clusterdump这个命令的源文件是如何读取的，本人主要是在读取过程中遇到了少包的问题，现记录下来，以便后面查看 需要在tomcat的web项目下的WEB-INF/lib中，因为页面中引用了一些类，根据页面提示，导入hadoop-core-0.20.2.jar，mahout-core-0.4.jar，mahout-math-0.4.jar，mahout-utils-0.4.jar，此时页面没有错误提示，但一运行会报错，依次根据报的提示，导入commons-logging-1.1.1.jar，mahout-collections-1.0.jar，gson-1.3.jar，google-collections-1.0-rc2.jar，commons-cli-2.0-mahout.jar，slf4j-api-1.6.0.jar，slf-jcl-1.6.0.jar，完成后，终于在网页上看到了聚类结果  ","title":"将聚类结果展示在网页上"},{"content":"转载 http://blog.csdn.net/pongba/article/details/3456240 一直以来伴随我的一些学习习惯（四）——程序员的知识结构 By 刘未鹏(pongba) C++的罗浮宫(http://blog.csdn.net/pongba) TopLanguage(https://groups.google.com/group/pongba)   自从建立了 TopLanguage 以来，发现在上面待的时间越来越多，与高手讨论问题是个粘性十足的事情，一方面，分享自己的认识是整理不成熟的想法的极好途径，另一方面，互相之间视角不同，所以往往自己忽视的地方会被别人发现。在讨论中不断精化既有的知识体系。以下这段基本上摘抄自（略有整理和添加）在 TopLanguage 上的发言： 抓住不变量 我喜欢把知识分为essential的和non-essential的。对于前者采取提前深入掌握牢靠的办法，对于后者采取待用到的时刻RTM (Read the manual)方法（用本）。 如何区分essential和non-essential的知识想必绝大多数时候大家心里都有数，我举几个例子：对程序员来说，硬件体系结构是essential的，操作系统的一些重要的实现机制是essential的，主流编程范式（OO、FP）是为了满足什么需求出现的（出现是为了解决什么问题），是怎么解决的，自身又引入了哪些新的问题，从而适用哪些场景）。 这些我认为都是essential的。我想补充一点的是，并不是说硬件体系结构就要了解到逻辑门、晶体管层面才行（其实要了解到这个层面代价也很小，一两本好书就行了），也并不是说就要通读《Computer Architecture: Quantitative Approach》才行。而是关键要了解那些重要的思想（很长时间不变的东西），而不是很细的技术细节（易变的东西）。《Computer Systems: A Programmer’s Perspective》就是为此目的，针对程序员的需求总结出那些essential knowledge的好书。 再来说一下为什么需要预先牢靠掌握这些essential的知识： 根据Joel Spolsky同学的说法（原文），编程语言技术是对底层设备的封装，然而封装总是会出现漏洞的，于是程序员被迫下到“下水道”当中去解决问题，一旦往下走，漂亮的OO、N层抽象就不复存在了，这时候不具备坚硬的底层知识就会无法解决问题。简而言之就是这些底层知识会无可避免的需要用到，既然肯定会被用到那还是预先掌握的好，否则一来用到的时候再查是来不及的，因为essential的知识也往往正是那些需要较长时间消化掌握的东西，不像Ruby的mixin或closure这种翻一下manual就能掌握的东西。（英语也是这样的essential knowledge——上次在PyCN上看到一个招Python开发人员的帖子将英语列为必备技能，却并不将自然语言处理列为必备技能，正是因为英语不是可以临阵磨枪的东西，而且作为知识的主要载体，任何时候都少不了它，如果不具备英语能力，这个就会成为个人知识结构的短板或瓶颈，而且由于需要长时间才能获得这项能力，所以这个瓶颈将持续很长时间存在。我们曾经在 TopLanguage 上讨论过如何花最少的时间掌握英语）另一方面，在问题解决当中，如果不具备必要的知识，是根本无从思考的，再好的分析能力也并不是每个问题都能分析出该用哪些知识然后再去查手册的，很多时候是在工具和问题之间比较，联想，试探性的拼凑来解决问题；这就使得一个好的既有知识基变得至关重要。（实际上以上这个是一个较大的话题，希望有一天我能够把它详细展开说清:)） 如果你不知道某个工具的存在，遇到问题的时候是很难想到需要使用这么样一个工具的，essential knowldge就是使用最为广泛的工具，编程当中遇到某些问题之后，如果缺乏底层知识，你甚至都不知道需要去补充哪些底层知识才能解决这个问题。 你必须首先熟悉你的工具，才能有效地使用它（须知工具的强是无敌的，但这一切得以“了解你的工具”为前提，甚至得以“了解目前可能有哪些工具适合你的问题”为前提）。一门语言，你必须了解它的适用场景，不适用场景（比如继承能解决你的问题不代表继承就是解决你的问题的最适合的方案，须知问题是一个复杂系统，解决方案总是常常引入新的问题）。你必须了解它支持的主要编程范式，此外你还必须了解它的traps和pitfalls（缺陷和陷阱，如果不知道陷阱的存在，掉进去也不知道怎么掉的。）这些都是essential knowledge，如果不事先掌握，指望用的时候查manual，是很浪费时间的，而且正如第2点所说，正因为你不知道这些知识（如适用场景），从而用sub-optimal的方式使用了一门语言自己可能还不知道（最小白的例子是，如果你不知道语言支持foreach，那么可能每次都要写一个冗长的循环，较常见的例子是不知道有很方便的库设施可以解决手头的问题所以傻乎乎的自己写了一堆代码），因为人的评价标准常常是：只要解决了最醒目的问题并且引入的新问题尚能忍受，就行。注意，熟悉并非指熟悉所有细节，而是那些重要的，或者无法在需要用到的时候按需查找的知识。比如上面提到的：适用场景不适用场景，编程范式，主要语言特性，缺陷和陷阱。 当然，以上作为程序员的essential knowledge列表并不完备，关键是自己在学习新知识的时候带着第三只眼来敏锐地判断这个知识是否是不变量，或不易变的量，是否完全可以在用的时候查手册即可，还是需要提前掌握（一些判断方法在上文也有所提及）。并且学会在纷繁的知识中抽象出那些重要的，本质的，不变的东西。我在之前的part里面也提到我在学习新知识的时候常常问自己三个问题：该知识的（体系或层次）结构是什么、本质是什么、第一原则是什么。 另外还有一些我认为是essential knowledge的例子：分析问题解决问题的思维方法（这个东西很难读一两本书就掌握，需要很长时间的锻炼和反思）、判断与决策的方法（生活中需要进行判断与决策的地方远远多于我们的想象），波普尔曾经说过：All Life is Problem-Solving。而判断与决策又是其中最常见的一类Problem Solving。尽管生活中面临重大决策的时候并不多，但另一方面我们时时刻刻都在进行最重大的决策：如：决定自己的日常时间到底投入到什么地方去。如：你能想象有人宁可天天花时间剪报纸上的优惠券，却对于房价的1%的优惠无动于衷吗？（《别做正常的傻瓜》、《Predictably Irrational》）如：你知道为什么当手头股票的股价不可抑止地滑向深渊时我们却一边揪着头发一边愣是不肯撤出吗？（是的，我们适应远古时代的心理机制根本不适应金融市场。）糟糕的判断与决策令我们的生活变得糟糕，这还不是最关键的，最关键的是我们从来不会去质疑自己的判断，而是总是能“找到”其他为自己辩护的理由（《错不在我（Mistakes were made, but not by me）》）又，现在是一个信息泛滥的时代，于是另一个问题也出现：如何在海洋中有效筛选好的信息，以及避免被不好的信息左右我们的大脑（Critical Thinking）关于以上提到的几点我在豆瓣上有一个专门的豆列（“学会思考”），希望有一天我能够积累出足够多的认识对这个主题展开一些详细介绍。 最后分享一个学习小Tip： 学习一个小领域的时候，时时把“最终能够写出一篇漂亮的Survey”放在大脑中提醒自己，就能有助于在阅读和实践的时候有意无意地整理知识的结构、本质和重点，经过整理之后的知识理解更深刻，更不容易忘记，更容易被提取。 杨军在 TopLanguage 上也曾分享了三篇非常棒的学习心得的文章，字字珠玑： [1] 有些事情做起来比想象中容易 [2] 有关读书方法的一点想法 [3] 一件事情如果你没有说清楚，十有八九不能做好","title":"一直以来伴随我的一些学习习惯（四）——程序员的知识结构"},{"content":"数据稀疏 协同过滤的精度主要取决于用户数据的多少。如果一个系统有很多用户的历史数据，他就能更好的对用户的喜欢做出预测。所以，目前推荐系统做的最好的都是那些有着很大量用户数据的公司，比如Google, Yahoo, Netflix, Amazon等等。但是，即使拥有很多数据，数据还是不够多，因为推荐系统的历史还不够长，还没有积累足够的数据。在目前处理稀疏数据的算法中，软性SVD是一种最好的方法。 新用户问题 这个问题和数据稀疏问题有一些相似性，他是指如何对新用户做出推荐。当一个新用户进入一个网络时，我们对他的兴趣爱好还一无所知，这时如何做出推荐是一个很重要的问题。一般在这个时候，我们只是向用户推荐那写普遍反映比较好的物品，也就是说，推荐完全是基于物品的。 新用户问题还有一个变种就是长尾(long tail)问题，在Amazon中，不是所有的用户都对很多书给出了评分，很多用户只给少数的书给出了评分，这些用户就处在一个长尾中，如何处理那些不太表露自己兴趣的用户，也是推荐系统的一个主要问题。 隐性喜好发现 在现在的推荐系统中，用户的喜欢是通过用户对某些物品进行评分获得的。这种获得用户兴趣的方法是一种很直接的方法。但在实际的互联网中，用户有很多隐性的方法表露他们的喜欢。比如用户的文字评论，我们可以通过自然语言处理从用户的评论中获得用户的兴趣；或者是用户的浏览行为，比如用户长时间的浏览一个物品，或者用户经常浏览一个物品，或者用户 购买了一个物品，这些行为都可以作为模式识别系统中的特征。 所以，发现用户的隐性喜好，相对于模式识别的特征提取，这方面的研究也很热门。 用户兴趣的变化 我们知道，用户的兴趣不是永远不变的，随着年龄和阅历的变化，用户的行为会发生变化。也就是说，协同过滤其实还应该加入一个时间因子。目前对于变化的用户兴趣的研究还处于起步阶段，主要是因为现有的系统历史都不是很久，大多数用户的兴趣还是比较稳定的，但是随着互联网的发展，用户兴趣的变化对推荐系统的影响将会越来越明显，所以这方面的研究也将越来越重要。 偏激的用户和全新的物品 我们知道，这个世界上有一些用户是很偏激的。他们和大多数人的观点是相反的。对于这种用户，现有的推荐系统做出的预测往往是很差的。如何处理偏激的用户，是推荐系统中的一个重要问题。 和偏激用户相对应的，是全新的物品。比如有一部新电影，他是颠覆性的，和以前的电影都不太相似。用户对于这个电影的爱好和用户以前的兴趣是没有太大关系的，因为用户从来没见过这种电影，这个问题也是导致现有的推荐系统精度不高的主要原因。 马太效应以及推荐系统对互联网的影响 我们知道，被推荐系统所推荐的物品将会越来越热门，这就导致了大量很好的物品可能会被推荐系统所淹没。在互联网中，物品实在是太多了，而推荐系统只能推荐有限的物品。解决这个问题的主要方法是增加推荐系统的多样性，比如一个推荐系统发现一个用户非常喜欢吃德芙巧克力，那么他给这个用户推荐10个产品，不需要都是德芙巧克力，也可以推荐别的一些巧克力，或者一些和巧克力相似的甜品。在推荐时，不仅要推荐用户喜欢的东西，而且要通过推荐让用户喜欢一些东西，有的时候，用户自己也不知道他喜欢什么，通过推荐系统，他可能会发现一些新东西他比较喜欢。 推荐系统中的作弊 只要涉及到经济利益，就有人作弊。搜索引擎作弊是一个被研究了很久的问题，因为在搜索引擎中，自己的网站排名越高，就能获得越多的经济利益。在推荐系统中也是如此，比如在淘宝中，如果一个卖家的物品经常被推荐，他就可能获得很多经济利益。这样，很多电子商务的推荐系统都遭受到了作弊的干扰，一些人通过一些技术手段，对自己卖的物品给出非常高的评分，这就是一种作弊行为。 推荐系统中的作弊在电子商务网站中越来越严重，特别是在美国这种互联网比较发达的国家，已经受到一些研究者的重视。作弊行为相当于人为的向系统中注入了噪声。目前解决作弊的算法主要是基于信任度和信用的。现在很多电子商务网站都引入了信用系统，比如淘宝等等。如何设计信用系统和推荐系统更好的融合，是一个重要的研究问题。 原文见http://xlvector.net/blog/?p=145，这是很不错的关于推荐算法的博客。 http://bbs.sciencenet.cn/home.php?mod=space&uid=3075&do=blog&id=459442学校互联网实验室的杰出代表。 http://www.resyschina.com/2010/03/five_problems_of_resys.html提出了相似的5个问题： 1. 缺少数据 对于推荐系统来说，可能最大的问题就是需要大量的数据才能产生推荐结果。这也是为什么那些表现最突出的推荐系统都是来自于有数据的大公司，比如Google，Amazon，Netflix，Last.fm。正如Strands公司在他的演讲中提到的那样，一个好的推荐系统首先要获得内容数据，接着必须获得和分析用户数据（行为事件），最后才是算法的工作。内容和用户数据越多，获得好的推荐的比率就会越高。但是这也是一个“鸡和蛋”的问题——推荐系统的目的就是带来更多的用户点击和购买，而好的推荐系统需要大量的用户，你才能为推荐系统提供需要的数据。 2. 变化的数据 这个问题曾经被智能推荐系统公司 Clicktorch CEO Paul Edmunds在以前的文章评论中提到过，Paul 指出推荐系统常常充斥着老的内容，而很难推荐出新的东西。时尚发烧友社区StyleHop 的David Reinke 在他的一篇博客中举了一个例子——“流行趋势总是在变化，因此用户的过去行为并不是一个好的工具”。显然纯算法的方式是不太可能跟上流行的趋势的。大多数非时尚认识，我就属于这一类，只信赖那些可信的对时尚很谨慎的朋友和家人的推荐。 David Reinke 要说明的是往往产品中有很多时尚因素，比如肥瘦、价格、颜色、款式、材料、品牌等，对于同一个消费者来说，每个时尚元素在不同的时间都会有不同的重要程度，因此产品的推荐往往会效果不好，他还指出也许“社会化推荐”会解决这个问题。 3. 变化的用户喜好 同样也是由 Paul Edmunds提出的一个问题是，今天我们在Amazon上怀着某一特殊目的浏览，而明天我们的目的就会变化。一个经典的例子，某天我在Amazon上为自己找一本书，另一天我有可能会在Amazon上为我的妹妹找一个生日礼物。 另外一个有关用户喜好的话题就是推荐系统有可能会给用户打上错误的标签，比如经典的2002年华尔街日报上的笑话——If TiVo Thinks You Are Gay, Here’s How to Set It Straight. 4. 无法预测的事物 在我们关于Netflix竞赛（由影片在线租赁公司Netflix举办的100万美金的推荐系统竞赛活动）的文章中，曾经提到关于某些“怪异电影”的问题，这些影片用户常常会表现出偏激的喜欢或者讨厌，比如《炸弹头拿破仑》（又名《大人物拿破仑》Napoleon Dynamite）。这些类别的影片很难做出推荐，因为用户的反应是多样化和不可预测的。 在音乐中有很多这样的内容。你能猜出我同时喜欢Metallica和Carpenters的音乐吗？我怀疑Last.fm不太能做出这样的推荐。 5. 推荐系统是复杂的！ 我们上面只是说了一些表面的现象，Strands公司曾经介绍过，即使实施一个非常简单的推荐系统，也需要许多的变量（我们猜想以下的这些变量恐怕还仅仅是一小部分）。  迄今为止，只有少数几家公司可以为用户提供高满意的推荐——Amazon，Netflix（他们仍然在寻求对算法的改进），Google也算是一个。尽管只有这么少的成功故事，成百上千的其它网站和应用程序仍在努力探寻着推荐系统的魔法公式——以给他们的用户产生满意的推荐。 事实上，我们在读写网上也非常希望让读者围绕我们的网站产生更多点击，发现其他内容。我们尝试了一些插件和方法，以实现这一点——但我们仍未感到满意。 我们错过了什么东西？ 在实施推荐系统过程中会遇到很多问题——比如有些仅仅简单了提供“大众化”的推荐；有些不能够形成作古的长尾效应，只能给出一些显而易见的结果；还有的会有异常推荐等问题。随着应用和技术的进步，我们还会发现其它的一些问题。 译者点评： 以上提出的五大问题，在实施个性化推荐服务的过程中，或多或少的都会遇到，但是在不同的类型的网站、不同的内容和用户、不同的实施阶段和不同的目的，所面临问题的严重程度也会不同。 对于数据影响推荐质量的问题：为什么有大量数据的公司会做出效果更好的推荐服务，首先推荐系统本身需要数据，其次在海量数据引起的信息过载问题更加严重，需求更加迫切。因此推荐系统的作用更加明显。因此，用户数据的绝对数量并不是限制推荐系统实施的门槛，而用户数据的稀疏程度会直接影响推荐的效果。 对于内容数据的变化问题：我们大部分人都不是时尚达人——在一般的服装电子商务网站上实施个性化推荐还是可以满足大部分人的需求的。虽然文中举出的例子有些极端，更像是在长尾的尾部。但是对于某些对时间性有要求的内容也还是会面临这样的问题。解决的方法除了对算法本身的选择和改进以外，还需要在产品设计的过程中加入时间因素的条件。 对于用户的喜好变化问题：实际上文中提出的例子倒不是喜好的变化，而是目的的变化。因此在进行推荐系统设计时，需要建立更加完整的用户模型。而对于文中的例子，比较好的处理办法是单独设计一个礼物的购物通道，比如 Amazon 上的礼物页面：http://www.amazon.com/gp/gift-central/ 推荐系统确实是非常复杂的，不仅涉及文中提出的数据获取的问题，还有对数据的处理，对算法的选择，对参数的优化，产品和服务设计，反馈收集，效果测试和改进，是一个螺旋式上升的过程，它不仅仅是一个或几个推荐服务新的功能开发，而是需要长期维护和改进，需要专业的团队和持续的投入才能完成的工作。 实际上效果不错的推荐服务不仅仅包括文中提到的网站，比如 digg, overstock, yahoo, AT&T等一大批网站也在他们的业务中加入了不错的推荐服务。还有国内著名的豆瓣，当当网都有较高用户满意度的推荐产品，所以，请暂时忘了上面的这些问题，Just do it，我们才能克服这些困难。","title":"推荐系统和协同过滤面临的主要问题"},{"content":"转载一篇奇文    [交流]CVPR2010奇文一篇共欣赏 http://emuch.net/bbs/viewthread.php?tid=3455612&fpage=1   这篇Paper的题目叫Paper Gestalt。文章以诙谐的笔调描述了一个基于vision + learning的自动paper review算法。 这篇Paper不是在会议中正式发表的，而是在TC Panel派发的。参加会议的朋友们可以很幸运的在会场获取这篇文章，至于没有来的朋友，我想只能向作者（这位兄弟（也许是姐妹）在paper中自称Carven von Bearnensquash, bearensquash@live.com）索要了。 这篇论文出炉的背景，就是最近几年CVPR或者ICCV的submission呈现急速的指数增长的趋势（在过去10年翻了三倍）。按照这个速度增长，在 10年后每次会议的投稿量就会超过5000篇！ 文章的算法很简单（前提是你对Machine Learning或者Computer Vision有一点了解），把8页的pdf文档并排成一张长的image，然后就在上面抽feature。做自然语言处理的朋友们请不要激动，这是 Vision的paper，自然用的是Vision圈子自己的方法。好了，抽什么feature呢？主要是HOG(Histogram of Gradients)，这是一种纯粹用于描述视觉观感的feature。显然，大段的文本，曲线图，图像，表格，数学公式，它们的feature应该是不 太一样的。然后作者用AdaBoost做feature selection训练得到一个分类器：纯粹根据paper的视觉观感来判断paper的好坏。 这篇文章的作者收集了CVPR 2008, ICCV 2009和CVPR 2009的全部1196篇paper构成正样本。那么负样本从何而来呢？被拒的paper显然作者是拿不到的。于是他很聪明的利用了一个众所周知但是大家 却不会公开明言的事实：workshop接纳的很多是在主要会议被拒收的paper。这样，很不幸的，workshop上发表的文章被用作负样本。 最有趣的部分要数实验结果了。从ROC曲线来看，结果其实还是不错的——以拒绝15%的正样本为代价，可以滤除一半的负样本。作者对于正负样本的特征做了 一些总结，也许对于大家以后投paper还是有点指导意义呢... 正样本的“视觉”特点： 1. 里面有几段公式，看上去文章显得似乎很专业，也显得作者似乎数学不错； 2. 实验部分里面多少要有几个曲线图，即使那几个曲线图说明不了什么。但是，只要有几个曲线图在那里，起码表示我做的是“科学实验”； 3. 最好在文章开头或者最后一页排列一堆图像。其实，我也注意到很多作者喜欢排列很多dataset里面的图像到paper上——即使那是一个 publically available的standard dataset——我不知道这样做的意义何在——除了审美效果。 4. 最好写满8页，代表分量足够。 负样本的特点： 1. 不够页数。在submission阶段，写不满6页的文章被录用的机会很小。虽然最后很多本来8页的文章还是能很神奇地被压缩到6页，如果作者想省掉 200美元的附加页费。题外话，我也一直不明白为什么多一页要多交100美元注册费。 2. 有很大的数字表，就是m行n列，排满数字那种。这篇文章表明，排列了很多曲线图和柱状图的文章比排列了很多数字表的文章有更大概率被接收。 3. 没有漂亮插图。","title":"CVPR2010奇文一篇共欣赏"},{"content":"Superjiju是我的一个好友，专注于自然语言处理。 其博客思想内涵丰富，涉及很多自然语言处理相关技术，值得学习。 推荐的这个博文写得很好，有困难的时候，就可以拿出来读读。 http://superjiju.wordpress.com/2010/04/27/%e5%87%ba%e6%9d%a5%e6%b7%b7%ef%bc%8c%e8%bf%9f%e6%97%a9%e8%a6%81%e8%bf%98%e7%9a%84/","title":"推荐一个博客Superjiju和一个博文"},{"content":"张宏江的艰难抉择 是继续留在驾轻就熟的位置，还是从零开始接受创业的挑战？不少从业多年的人士都曾面对这样的选择题，张宏江也不例外。 工程院成立典礼结束几个星期后的一天，张宏江正坐在自己的办公室里，这时，张亚勤从外面走了进来。 张宏江抬头一看，张亚勤已经坐在对面看着他。不久前，张亚勤告诉张宏江，他有10%的机会被调到雷蒙德总部去管理一个新的产品部门，就像当年李开复被调到总部负责.NET的用户界面工作一样。现在，张亚勤开玩笑地和张宏江说：“结果这个10%变成了100%。” 听到这个消息，张宏江的第一个反应是伤感，因为共事四年多的张亚勤要离他而去。可是，当张亚勤告诉他，将由他接任微软亚洲工程院院长一职时，他的伤感即刻变为震惊。 张宏江回忆道：“我问：‘为什么？’亚勤说：‘看看目前工程院的项目，事实上有一半都是你带领的研究团队做出来的。’”更为重要的是，张宏江是微软在中国的团队里最受尊敬的高层之一，在研究和技术转化两个领域都非常有经验，而后者要归功于他从前在惠普工作的日子。还有谁比他更适合领导这项新事业呢？ 里克·雷斯特也支持这个决定。几天之后的一次会议上，他告诉张宏江：“这是一项艰巨的工作，但我们有信心你能把它做好！” 与张亚勤和沈向洋不同的是，张宏江并非什么少年天才，也没有就读于顶尖大学的经历，他取得成就的过程，更像是一名寒门子弟的奋斗史。 张宏江的父亲是原电子部下属的一家工厂的技术工人，母亲也是一名工人。40年前，当张宏江还是个小男孩的时候，他随父母离开武汉，来到河南叶县黄莹坡——中国古代寓言“叶公好龙”中的叶公居住的地方。20世纪70年代，黄莹坡是一个只有2000多人口的小镇，也是一个专供知识分子和干部“劳动改造”的“流放地”。 1976年，15岁的张宏江就要从高中毕业了。那时候的社会风尚与今天大不一样，人人都把这样一句话挂在嘴边：“农村是一片广阔的天地，到那里是可以大有作为的”。张宏江的哥哥已经在两年以前到农村去了，现在又轮到他。他的父母却开始忧虑。在那个年代，父母通常没有什么“望子成龙”的念头。张宏江的父母只希望大儿子赶快从农村回来，二儿子别再到“广阔天地”去了——就算没有工作，在家闲逛都是好的。但是，1977年7月，张宏江高中一毕业，他的城市户口便立即被取消，并且转到了农村。按照规定，他除了追随哥哥到农村去，别无他途。 但就在这时，高考制度恢复了。1977年12月，张宏江和哥哥一起参加了高考。 很快，这个焦急等待的家庭迎来了第一份录取通知书——是老大的。左邻右舍都来祝贺，张宏江的父母既兴奋又心焦：一个儿子“解放”了，但是他们还有一个儿子啊！ 张宏江的学习成绩一向比哥哥好，又有自信。他对父母说：“哥哥能拿到，我也能拿到。” 但是张宏江的父母不太敢相信——他们不是不相信张宏江，而是实在不敢想象能拥有两个儿子同时考上大学的幸运。 一家人在兴奋和焦虑中又过了三天。第四天傍晚，有个人忽然跑进来，满口说着祝贺的话。父亲以为又是祝贺大儿子的，就说：“都过了四天啦，还祝贺……”可就在这时，父亲呆住了，眼睛直直地望着面前的邮递员和他手上的录取通知书——它来自郑州大学电子系，是张宏江的！ 母亲一声惊叫，父亲一声不吭，只抓着那张录取通知书，左看右看。等到终于相信眼前发生的事情全是真的，父亲才一个劲儿地说：“喝酒，喝酒。” 进入郑州大学之后，尽管张宏江在电子工程专业的学习成绩十分出色，但毕业之后，他又一次选择了一条不寻常的发展道路。他没有去麻省理工学院或者卡耐基梅隆大学这样的美国大学，而是到丹麦技术大学攻读博士学位。后来，张宏江曾任职于新加坡国立大学系统科学学院，20世纪90年代中期又到位于美国加州帕罗奥多（Palo Alto）的惠普实验室做了四年的研究主管。 在攻读博士学位时，张宏江选择的研究方向是遥感图像处理，从此，他与图像处理和检索结下了不解之缘，并最终成为基于内容的视频检索和查询方面的学科领袖。 1993年6月，张宏江在《多媒体系统》杂志的创刊号上发表了他在这个领域的第一篇论文。这篇论文后来成为现代多媒体研究领域的经典之作，并为现代视频检索和内容查询的研究建立了基本框架。1994年，张宏江写作的《多媒体系统中视频和图像的处理》一书在美国出版，成为该领域的第一本专著。随后，他开发出了一套视频检索系统并获得了专利许可，柯达、英特尔等知名公司纷纷购买这项专利，并将该技术应用到一系列的产品研究和开发实践中。在美国，甚至有数家公司在张宏江开创的理论框架下创业成功。张宏江成为这一多媒体研究领域名副其实的开山鼻祖。1998年年底，张宏江应邀担任ACM多媒体世界大会技术委员会主席，成为第一位担任此职务的华人。 在美国的日子虽然有些单调，但舒适惬意——生活方面，经济来源稳定；工作方面，从事的又是自己心爱的科研工作。有时候，张宏江甚至认为自己的下半辈子都将在硅谷的实验室里度过。但是，偶然看到的一部国产电视剧却彻底改变了他的人生轨迹。 一个周末，张宏江和家人去一家亚洲超市采购食品。付完款后，他拿到了一张租借录像带的优惠券。张宏江注意到货架上有一部中国电视剧——《爱你没商量》，就把它租了下来。 “（虽然）剧情和角色都不记得了，但我现在还记得，在看这部剧集时，我被其中人物生活和工作的环境，以及他们之间的对白深深震撼——我突然感到，自己好像太久地远离所生长的那片故土，对她的变化是如此陌生。”张宏江说。从那以后，回国的想法就不断在他脑海里闪现，驱使他寻找各种回国工作的可能。 半年后，张宏江得知微软将在北京成立一家基础研究机构，领头人正是业内赫赫有名的李开复和张亚勤，而他们正在招募计算机领域的顶尖人才回国加盟。很快，李开复就找到了张宏江。随后，张亚勤又对他进行了说服工作。 那个时候，对张宏江而言，回国的念头已经不可遏制。但基于多年来理性判断的惯性思维，他还是决定在回国之前先回去看看未来的工作和生活环境。 当时正值1999年初春。一天，张宏江来到北京的一个房屋租赁中心了解租房的行情。中午，他在一家饭馆吃饭，窗外霎时狂风大作，天空变得昏黄。张宏江心里“咯噔”一下，旋即明白，这就是传说中的“沙尘暴”。 沙尘暴和污染，正是一些朋友劝他“慎行”的原因。但张宏江却立即决定，这件事必须从给太太的“考察报告”中删除。 后来，他回忆道：“那时，我的心其实已经回来了。感性的‘东风’压倒了理性的‘西风’，所以回国看到的一切都能从正面支持自己的决定，负面的印象都能够被潜意识所忽略。”1999年4月13日，张宏江回国并加入微软中国研究院。 现在，在张宏江办公室的墙上，仍挂着他获得的10项美国专利的纪念匾；书架上摆着一摞微软公司颁发的“专利石”（Patent Cubes），每块石头上都刻着他的名字、专利名称和提交申请的日期。此外，张宏江独自撰写及与他人合著的论文数量达到400篇。 由于张宏江在搜索和信息检索领域所取得的重要研究成果，加盟微软仅两年，他就被提升至Partner（合伙人）级。在微软的级别体系中，一名研发人员能成为“合伙人”，意味着他已经被认为是不可缺少的人才。2003年，美国电气电子工程协会授予张宏江院士（Fellow of  IEEE）称号。 除了个人在学术上的成就，张宏江在研究院带领的多个团队也颇有建树——多媒体计算、自然语言处理、分布式计算系统和搜索这4个项目都是对微软而言举足轻重的技术，其中，搜索项目当时刚刚被确定为微软的战略重点。 在微软亚洲研究院度过异常忙碌的四年后，张宏江意气风发。他心里盘算：“是时候放松一下，和孩子们玩玩了。”但没想到，这时张亚勤提出让他带头做工程院，打破了他的“美梦”。 “如果接着做研究院的工作，应当是驾轻就熟的，而且能预见到未来的成功。但是去做工程院，开发和研究有天壤之别。这意味着把过去的成功、未来的安逸统统放弃，所有的努力推倒重来。”张宏江说。一方面，对未来的不确定性让他犹豫，甚至有些恐惧；而另一方面，正是开创新事业的这种不确定性，又对他有一种难以名状的吸引力。这才是最让他纠结的地方。 2003年圣诞节期间，张亚勤、张宏江和沈向洋带着各自的家人，一起到海南三亚度假。他们三人在工作中是好伙伴，他们的孩子也是同学，因此他们的家人之间十分熟络。 张亚勤、张宏江和沈向洋很清楚，这次度假结束之后，他们三个人都可能要走上新的工作岗位：张亚勤赴总部任副总裁，沈向洋将接替张亚勤任研究院院长，而张宏江则将成为工程院的院长。 “我表面装做轻松，但其实心事重重。因为我还在考虑到底对亚勤说‘Yes’还是‘No’。”张宏江说，他当时内心挣扎得很厉害。 张宏江想起自己在过去几十年中作出的几次重大的抉择，似乎都没有这么困难，即便是面对回国工作这样重大的问题，他也是很痛快地作出了选择。张宏江在想：是什么让自己变得患得患失？是年纪大了吗？还是获得的越多，就越害怕失去？ “既然‘上面’这么支持，这应当是一个大展拳脚的机会、一个可以把事情做大的机会！”张宏江忽然意识到，他们开创工程院是一件独一无二、前无古人的事情，不仅在微软未曾有过类似的机构，而且在当时中国的软件业界也不见得存在类似的能够从事高水平核心软件产品开发的机构。 当离开三亚回到北京的时候，张宏江已经说服了自己，从张亚勤手里接过了工程院院长的重任。他开始构思整个计划。他回忆道：“我的感觉是，又迎来了一个全新的开始。” 张宏江说：“当了工程院院长之后，有时我仍然会在心里假设和比较，如果继续做研究会怎么样，成就会不会比今天更大。但当时有一种难以描述的动力让我接下了这份工作，你可以把它称为‘使命感’。” 2004年1月7日，微软召开了一次新闻发布会，宣布张亚勤升任全球副总裁，并将离开微软亚洲研究院院长的职位，前往微软总部负责移动通信及嵌入式系统在全球的开发业务。这一天也是张亚勤37岁的生日。 研究院和工程院的新掌门人也在这次发布会上亮相：原微软亚洲研究院副院长沈向洋晋升为新一任院长，而另一位副院长张宏江则升任微软亚洲工程院院长。沈向洋和张宏江都将直接向负责微软研究院全球事务的高级副总裁里克·雷斯特汇报工作。 同时，研究院的几名资深人员在此时被宣布成为工程院的高管：原微软亚洲研究院院长技术助理张益肇升任微软亚洲工程院副院长，原微软亚洲研究院新技术开发部经理林斌出任微软亚洲工程院工程总监。另外，来自雷德蒙研究院的高级总监丹尼斯·阿德勒将兼任工程院副院长。 张益肇、林斌，以及随后从微软总部回国的幺宝钢和萧圣璇，后来被称为工程院的“四大金刚”，他们是工程院早期四名直接向张宏江汇报工作的“大将”。     本文节选自《创业在微软——微软亚洲工程院成长启示（双色） 》一书。 图书详细信息:http://blog.csdn.net/broadview2006/article/details/6617589  ","title":"张宏江的艰难抉择"},{"content":"  记得第一次了解中文分词算法是在 Google 黑板报 上看到的，当初看到那个算法时我彻底被震撼住了，想不到一个看似不可能完成的任务竟然有如此神奇巧妙的算法。最近在詹卫东老师的《中文信息处理导论》课上 再次学到中文分词算法，才知道这并不是中文分词算法研究的全部，前前后后还有很多故事可讲。在没有建立统计语言模型时，人们还在语言学的角度对自动分词进 行研究，期间诞生了很多有意思的理论。 中文分词的主要困难在于分词歧义。“结婚的和尚未结婚的”，应该分成“结婚／的／和／尚未／结婚／的”，还是“结婚／的／和尚／未／结婚／ 的”？人来判断很容易，要交给计算机来处理就麻烦了。问题的关键就是，“和尚未”里的“和尚”也是一个词，“尚未”也是一个词，从计算机的角度看上去，两 者似乎都有可能。对于计算机来说，这样的分词困境就叫做“交集型歧义”。 有时候，交集型歧义的“歧义链”有可能会更长。“中外科学名著”里，“中外”、“外科”、“科学”、“学名”、“名著”全是词，光从词库的 角度来看，随便切几刀下去，得出的切分都是合理的。类似的例子数不胜数，“提高产品质量”、“鞭炮声响彻夜空”、“努力学习语法规则”等句子都有这样的现 象。在这些极端例子下，分词算法谁优谁劣可谓是一试便知。 最简单的，也是最容易想到的自动分词算法，便是“最大匹配法”了。也就是说，从句子左端开始，不断匹配最长的词（组不了词的单字则单独划开），直 到把句子划分完。算法的理由很简单：人在阅读时也是从左往右逐字读入的，最大匹配法是与人的习惯相符的。而在大多数情况下，这种算法也的确能侥幸成功。不 过，这种算法并不可靠，构造反例可以不费吹灰之力。例如，“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来 ／应聘”。 维护一个特殊规则表，可以修正一些很机械的问题，效果相当不错。例如，“不可能”要划分成“不／可能”，“会诊”后面接“断”、“疗”、“脉”、“治”时要把“会”单独切出，“的确切”后面是抽象名词时要把“的确切”分成“的／确切”，等等。 还有一个适用范围相当广的特殊规则，这个强大的规则能修正很多交集型歧义的划分错误。首先我们要维护一个一般不单独成词的字表，比如 “民”、“尘”、“伟”、“习”等等；这些字通常不会单独划出来，都要跟旁边的字一块儿组成一个词。在分词过程中时，一旦发现这些字被孤立出来，都重新考 虑它与前面的字组词的可能。例如，在用最大匹配法切分“为人民服务”时，算法会先划出“为人”一词，而后发现“民”字只能单独成词了。查表却发现，“民” 并不能单独划出，于是考虑进行修正——把“为人”的“人”字分配给“民”字。巧在这下“为”和“人民”正好都能成词，据此便可得出正确的划分“为／人民／ 服务”。 不过，上述算法归根结底，都是在像人一样从左到右地扫描文字。为了把问题变得更加形式化，充分利用计算机的优势，我们还有一种与人的阅读习惯完全 不同的算法思路：把句子作为一个整体来考虑，从全局的角度评价一个句子划分方案的好坏。设计自动分词算法的问题，也就变成了如何评估分词方案优劣的问题。 最初所用的办法就是，寻找词数最少的划分。注意，每次都匹配最长的词，得出的划分不见得是词数最少的，错误的贪心很可能会不慎错过一些更优的路。因而，在 有的情况下，最少词数法比最大匹配法效果更好。若用最大匹配法来划分，“独立自主和平等互利的原则”将被分成“独立自主／和平／等／互利／的／原则”，一 共有 6 个词；但词数更少的方案则是“独立自主／和／平等互利／的／原则”，一共只有 5 个词。 当然，最少词数法也会有踩大便的时候。“为人民办公益”的最大匹配划分和最少词数划分都是“为人／民办／公益”，而正确的划分则是“为／人 民／办／公益”。同时，很多句子也有不止一个词数最少的分词方案，最少词数法并不能从中选出一个最佳答案。不过，把之前提到的“不成词字表”装备到最少词 数法上，我们就有了一种简明而强大的算法： 对于一种分词方案，里面有多少词，就罚多少分；每出现一个不成词的单字，就加罚一分。最好的分词方案，也就是罚分最少的方案。 这种算法的效果出人意料的好。“他说的确实在理”是一个很困难的测试用例，“的确”和“实在”碰巧也成词，这给自动分词带来了很大的障碍。但是“确”、“实”、“理”通常都不单独成词的，因此很多切分方案都会被扣掉不少分： 他／说／的／确实／在理 （罚分：1+1+1+1+1 = 5 ） 他／说／的确／实／在理 （罚分：1+1+1+2+1 = 6 ） 他／说／的确／实在／理 （罚分：1+1+1+1+2 = 6 ） 正确答案胜出。 需要指出的是，这个算法并不需要枚举所有的划分可能。整个问题可以转化为图论中的最短路径问题，利用动态规划效率则会更高。 算法还有进一步加强的余地。大家或许已经想到了，“字不成词”有一个程度的问题。“民”是一个不成词的语素，它是绝对不会单独成词的。 “鸭”一般不单独成词，但在儿歌童谣和科技语体中除外。“见”则是一个可以单独成词的语素，只是平时我们不常说罢了。换句话说，每个字成词都有一定的概 率，每个词出现的频率也是不同的。 何不用每个词出现的概率，来衡量分词的优劣？于是我们有了一个更标准、更连续、更自动的改进算法：先统计大量真实语料中各个词出现的频率，然后把每种分词方案中各词的出现概率乘起来作为这种方案的得分。利用动态规划，不难求出得分最高的方案。 以“有意见分歧”为例，让我们看看最大概率法是如何工作的。查表可知，在大量真实语料中，“有”、“有意”、“意见”、“见”、“分歧”的 出现概率分别是 0.0181 、 0.0005 、 0.0010 、 0.0002 、 0.0001 ，因此“有／意见／分歧”的得分为 1.8×10-9 ，但“有意／见／分歧”的得分只有 1.0×10-11 ，正确方案完胜。 这里的假设是，用词造句无非是随机选词连在一块儿，是一个简单的一元过程。显然，这个假设理想得有点不合理，必然会有很多问题。考虑下面这句话： 这／事／的确／定／不／下来 但是概率算法却会把这个句子分成： 这／事／的／确定／不／下来 原因是，“的”字的出现概率太高了，它几乎总会从“的确”中挣脱出来。 其实，以上所有的分词算法都还有一个共同的大缺陷：它们虽然已经能很好地处理交集型歧义的问题，却完全无法解决另外一种被称为“组合型歧 义”的问题。所谓组合型歧义，就是指同一个字串既可合又可分。比如说，“个人恩怨”中的“个人”就是一个词，“这个人”里的“个人”就必须拆开；“这扇门 的把手”中的“把手”就是一个词，“把手抬起来”的“把手”就必须拆开；“学生会宣传部”中的“学生会”就是一个词，“学生会主动完成作业”里的“学生 会”就必须拆开。这样的例子非常多，“难过”、“马上”、“将来”、“才能”、“过人”、“研究所”、“原子能”都有此问题。究竟是合还是分，还得取决于 它两侧的词语。到目前为止，所有算法对划分方案的评价标准都是基于每个词固有性质的，完全不考虑相邻词语之间的影响；因而一旦涉及到组合型歧义的问题，最 大匹配、最少词数、概率最大等所有策略都不能实现具体情况具体分析。 于是，我们不得不跳出一元假设。此时，便有了那个 Google 黑板报上提到的统计语言模型算法。对于任意两个词语 w1 、 w2 ，统计在语料库中词语 w1 后面恰好是 w2 的概率 P(w1, w2) 。这样便会生成一个很大的二维表。再定义一个句子的划分方案的得分为 P(∅, w1) · P(w1, w2) · … · P(wn-1, wn) ，其中 w1, w2, …, wn 依次表示分出的词。我们同样可以利用动态规划求出得分最高的分词方案。这真是一个天才的模型，这个模型一并解决了词类标注、语音识别等各类自然语言处理问题。 至此，中文自动分词算是有了一个漂亮而实用的算法。 但是，随便拿份报纸读读，你就会发现我们之前给出的测试用例都太理想了，简直就是用来喂给计算机的。在中文分词中，还有一个比分词歧义更令人头疼 的东西——未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？最近十年来，中文分词领域都在集中攻克这一难关。 在汉语的未定义词中，中国人名的规律是最强的了。根据统计，汉语姓氏大约有 1000 多个，其中“王”、“陈”、“李”、“张”、“刘”五大姓氏的覆盖率高达 32% ，前 400 个姓氏覆盖率高达 99% 。人名的用字也比较集中，“英”、“华”、“玉”、“秀”、“明”、“珍”六个字的覆盖率就有 10.35% ，最常用的 400 字则有 90% 的覆盖率。虽然这些字分布在包括文言虚词在内的各种词类里，但就用字的感情色彩来看，人名多用褒义字和中性字，少有不雅用字，因此规律性还是非常强的。根 据这些信息，我们足以计算一个字符串能成为名字的概率，结合预先设置的阈值便能很好地识别出可能的人名。 可是，如何把人名从句子中切出来呢？换句话说，如果句中几个连续字都是姓名常用字，人名究竟应该从哪儿取到哪儿呢？人名以姓氏为左边界，相 对容易判定一些。人名的右边界则可以从下文的提示确定出来：人名后面通常会接“先生”、“同志”、“校长”、“主任”、“医生”等身份词，以及“是”、 “说”、“报道”、“参加”、“访问”、“表示”等动作词。 但麻烦的情况也是有的。一些高频姓氏本身也是经常单独成词的常用字，例如“于”、“马”、“黄”、“常”、“高”等等。很多反映时代性的名 字也是本身就成词的，例如“建国”、“建设”、“国庆”、“跃进”等等。更讨厌的就是那些整个名字本身就是常用词的人了，他们会彻底打乱之前的各种模型。 如果分词程序也有智能的话，他一定会把所有叫“高峰”、“汪洋”、”庞博“的人拖出去斩了。 还有那些恰好与上下文组合成词的人名，例如： 费孝通向人大常委会提交书面报告 邓颖超生前使用过的物品 这就是最考验分词算法的句子了。 相比之下，中国地名的用字就分散得多了，重庆就有一个叫做“犀牛屙屎”的地方。不过，中国地名委员会编写了《中华人民共和国地名录》，收录 了从高原盆地到桥梁电站共 10 万多个地名，这让中国地名的识别便利了很多。外文人名和地名的用字非常集中，识别的正确率要高出许多。 真正有些困难的就是识别机构名了，虽然机构名的后缀比较集中，但左边界的判断就有些难了。更难的就是品牌名了。如今各行各业大打创意战，品牌名可以说是无奇不有，而且经常本身就包含常用词，更是给自动分词添加了不少障碍。 最难识别的未登录词就是缩略语了。“教改”、“发改委”、“北医三院”都是比较好认的缩略语了，有些缩略语搞得连人也是丈二和尚摸不着头脑。你能猜到“人影办”是什么机构的简称吗？打死你都想不到，是“人工影响天气办公室”。 汉语中构造缩略语的规律很诡异，目前也没有一个定论。初次听到这个问题，几乎每个人都会做出这样的猜想：缩略语都是选用各个成分中最核心的 字，比如“安全检查”缩成“安检”，“人民警察”缩成“民警”等等。不过，反例也是有的，“邮政编码”就被缩成了“邮编”，但“码”无疑是更能概括“编 码”一词的。当然，这几个缩略语已经逐渐成词，可以加进词库了；不过新近出现的或者临时构造的缩略语该怎么办，还真是个大问题。 说到新词，网络新词的大量出现才是分词系统真正的敌人。这些新词汇的来源千奇百怪，几乎没有固定的产生机制。要想实现对网络文章的自动分词，目前来看可以说是相当困难的。革命尚未成功，分词算法还有很多进步的余地。 注：转载请注明出处“我爱自然语言处理”：http://www.52nlp.cn 本文链接地址：http://www.52nlp.cn/matrix67-漫话中文分词算法","title":"Matrix67：漫话中文分词算法"},{"content":"       开了该技术类的博客好久了，却迟迟没有发表任何文章，之前的打算也完全变成了泡影。这就是我，总是不能坚持自己的想法。。哎~鉴于最近一直处于浑浑噩噩的状态，上周以及结束全部的研一的课程，这就表示我正式步入研二的队伍。但我这个状态真的是万万不可取的，还是给自己定个方向和目标计划来实时的鞭策自己。     我所做的方向目前是在做抽取的工作，但是自己却没有任何的提高。我觉得我对于自然语言处理方向非常感兴趣，我想自己朝着这个方向去学习和发展。接下来继续看几乎被我荒废的《算法导论》，晚上回到宿舍可以看一些，希望自己能够坚持下去。然后就是对于数据挖掘进行梳理和了解。     白天做好自己的东西，不能偷懒和荒废，加油↖(^ω^)↗    ","title":"关于以后的工作以及学习的计划和方向"},{"content":"from http://blog.csdn.net/ictextr9/archive/2009/03/20/4008703.aspx Wordnet是一个词典。 每个词语(word)可能有多个不同的语义，对应不同的sense。 而每个不同的语义（sense）又可能对应多个词，如topic和subject在某些情况下是同义的，一个sense中的多个消除了多义性的词语叫做lemma。 例如，“publish”是一个word，它可能有多个sense： 1. (39) print,publish -- (put into print; \"The newspaper published the news of the royalcouple's divorce\"; \"These news should not be printed\") 2. (14) publish,bring out, put out, issue, release -- (prepare and issue for publicdistribution or sale; \"publish a magazine or newspaper\") 3. (4) publish,write -- (have (one's written work) issued for publication; \"How manybooks did Georges Simenon write?\"; \"She published 25 books during herlong career\")   在第一个sense中，print和publish都是lemma。Sense 1括号内的数字39表示publish以sense 1在某外部语料中出现的次数。显然，publish大多数时候以sense 1出现，很少以sense 3出现。   WordNet的具体用法 NLTK是python的一个自然语言处理工具，其中提供了访问wordnet各种功能的函数。下面简单列举一些常用功能：   得到wordnet本身： from nltk.corpusimport wordnet   获得一个词的所有sense，包括词语的各种变形的sense： wordnet.synsets('published') [Synset('print.v.01'),  Synset('publish.v.02'),  Synset('publish.v.03'),  Synset('published.a.01'),  Synset('promulgated.s.01')]   得到synset的词性： >>>related.pos 's'   得到一个sense的所有lemma： >>>wordnet.synsets('publish')[0].lemmas [Lemma('print.v.01.print'), Lemma('print.v.01.publish')]   得到Lemma出现的次数： >>> wordnet.synsets('publish')[0].lemmas[1].count() 39   在wordnet中，名词和动词被组织成了完整的层次式分类体系，因此可以通过计算两个sense在分类树中的距离，这个距离反应了它们的语义相似度： >>> x =wordnet.synsets('recommended')[-1] >>> y =wordnet.synsets('suggested')[-1] >>> x.shortest_path_distance(y) 0   形容词和副词的相似度计算方法： 形容词和副词没有被组织成分类体系，所以不能用path_distance。 >>> a =wordnet.synsets('beautiful')[0] >>> b =wordnet.synsets('good')[0] >>>a.shortest_path_distance(b) -1 形容词和副词最有用的关系是similar to。 >>> a =wordnet.synsets('glorious')[0] >>>a.similar_tos() [Synset('incandescent.s.02'),  Synset('divine.s.06'), ……]","title":"WordNet"},{"content":"  原文转自：http://blog.sina.com.cn/s/blog_4ecd024b0100sjuv.html 中文互联网数据资料来源   平台机构 易观国际 互联网信息中心 淘宝数据平台 百度数据研究中心 艾瑞咨询 电子商务研究中心 IDC中国 百度数据中心 计世资讯 互联网数据中心 智库数据 梅花网 CNZZ数据中心 缔元信互联网数据 第一财经 中国经济网数据中心 投资界 赛迪顾问         >>>待补充     分析论坛 人大经济论坛 中国商业智能网 中国统计网 SAS爱好者 市场调研论坛 数据挖掘研究院 数据挖掘论坛 SAS中文论坛 沃顿知识在线 中国数据分析网 数据仓库之路 SPSS论坛 统计家园 BI Think商业智能网 统计之都 中文自然语言处理 智能中国网 EXCEL HOME技术论坛 百岛潮论坛 经济学家       >>>待补充       其他 月光博客 互联网的那点事 中文互联网数据资讯中心 36氪 游戏大观 洪波的偏见 亿邦电商 草根网 互联网的一些事 游戏邦 关于营销的那点事 派代网 IT 商业新闻网 业网内 网络推广方案 中国网站排名             >>>待补充 中文博客站点 数据挖掘与分析 小蚊子乐园 数据挖掘与数据分析 数据挖掘者 未来趋势—车品觉 数据化管理 沈浩老师 数据文化 数据挖掘营销应用 郑来轶 ExcelPro的图表博客 数据元素 庖丁的小刀 让数据说话 Flystarhj的博客 数据小兵 SAS数据挖掘 数据&分析 统计软件学习 图说企业文化 Keep On Fighting 数据之路 数据分析与研究 诸葛小川 小象的博客 杜牛牛 SAS博客列表           >>>待补充     网站分析 网站数据分析 网站分析在中国 MAR’S 观点 上海WA非官方组织 网站分析 Tenly的互联网哲学 在线广告分析 邮件营销在中国 香港网站分析 蓝鲸的网站分析笔记 搜索引擎营销 So Marketing博客 SOYAN天空 网站分析与电子商务 搜索营销智库 数据营销宝典 芒果运营 Keven网站分析博客 像风一样自由 IWOM研究 CIC网络口碑 互联网营销博客           >>>待补充   用户研究 腾讯用户研究 UCD中国用户研究 淘宝用户研究 阿里用户研究 当当用户研究 支付宝用户研究 搜狐用户研究 顺网用户研究 网易用户研究 百度泛用户体验 口碑用户研究 5173用户研究 19楼用户研究             >>>待补充  ","title":"数据分析站点导航"},{"content":" 《智能Web算法》 基本信息 原书名： Algorithms of the Intelligent Web 原出版社： Manning Publications 作者： (美)Haralambos Marmanis    Dmitry Babenko    [作译者介绍] 译者： 阿稳 陈钢 出版社：电子工业出版社 ISBN：9787121139192 上架时间：2011-8-1 出版日期：2011 年7月 http://product.china-pub.com/198425 《智能Web算法》电子书在线阅读   内容简介　     《智能web算法》涵盖了五类重要的智能算法：搜索、推荐、聚类、分类和分类器组合，并结合具体的案例讨论了它们在web 应用中的角色及要注意的问题。除了第1 章的概要性介绍以及第7 章对所有技术的整合应用外，第2～6 章以代码示例的形式分别对这五类算法进行了介绍。 　　《智能web算法》面向的是广大普通读者，特别是对算法感兴趣的工程师与学生，所以对于读者的知识背景并没有过多的要求。本书中的例子和思想应用广泛，所以对于希望从业务角度更好地理解有关技术的技术经理、产品经理和管理层来说，本书也有一定的价值。 目录 《智能web算法》 1 什么是智能web？ 1 1.1 智能web应用实例 3 1.2 智能应用的基本要素 4 1.3 什么应用会受益于智能? 5 1.3.1 社交网络 6 1.3.2 mashup 7 1.3.3 门户网站 8 1.3.4 维基 9 1.3.5 文件分享网站 9 1.3.6 网络游戏 11 1.4 如何构建智能应用？ 11 1.4.1 检查功能和数据 12 1.4.2 获取更多的数据 12 1.5 机器学习、数据挖掘及其他 16 1.6 智能应用中八个常见的误区 17 1.6.1 误区1：数据是可靠的 18 1.6.2 误区2：计算能马上完成 19 1.6.3 误区3：不用考虑数据规模 19 1.6.4 误区4：不考虑解决方案的可扩展性 19 1.6.5 误区5：随处使用同样的方法 19 1.6.6 误区6：总是能知道计算时间 20 1.6.7 误区7：复杂的模型更好 20 1.6.8 误区8：存在无偏见的模型 20 1.7 小结 20 1.8 参考资料 21 2 搜索 22 2.1 用lucene实现搜索 23 2.1.1 理解lucene代码 24 2.1.2 搜索的基本步骤 31 2.2 为什么搜索不仅仅是索引？ 33 2.3 用链接分析改进搜索结果 35 2.3.1 pagerank简介 35 2.3.2 计算pagerank向量 37 2.3.3 alpha：网页间跳转的影响 38 2.3.4 理解幂方法 40 2.3.5 结合索引分值和pagerank分值 45 2.4 根据用户点击改进搜索结果 47 2.4.1 用户点击初探 48 2.4.2 朴素贝叶斯分类器的使用 50 2.4.3 整合lucene索引、pagerank和用户点击 54 2.5 word、pdf等无链接文档的排序 58 2.5.1 docrank算法简介 58 2.5.2 docrank的原理 60 2.6 大规模实现的有关问题 65 2.7 用户得到了想要的结果吗？精确度和查全率 67 2.8 总结 69 2.9 to do 70 2.10 参考资料 72 3 推荐系统 73 3.1 一个在线音乐商店：基本概念 74 3.1.1 距离与相似度的概念 75 3.1.2 走近相似度的计算 80 3.1.3 什么才是最好的相似度计算公式？ 83 3.2 推荐引擎是怎么工作的 84 3.2.1 基于相似用户的推荐 85 3.2.2 基于相似条目的推荐 94 3.2.3 基于内容的推荐 98 3.3 推荐朋友、文章与新闻报道 104 3.3.1 mydiggspace.com简介 105 3.3.2 发现朋友 106 3.3.3 diggdelphi的内部工作机制 108 3.4 像netflix.com那样推荐电影 114 3.4.1 电影数据集的介绍及推荐器 114 3.4.2 数据标准化与相关系数 117 3.5 大规模的实现与评估 123 3.6 总结 124 3.7 to do 125 3.8 参考资料 127 4 聚类：事物的分组 128 4.1 聚类的需求 129 4.1.1 网站中的用户组：案例研究 129 4.1.2 用sql order by子句分组 131 4.1.3 用数组排序分组 132 4.2 聚类算法概述 135 4.2.1 基于分组结构的聚类算法分类 136 4.2.2 基于数据类型和结构的聚类算法分类 137 4.2.3 根据数据规模的聚类算法分类 137 4.3 基于链接的算法 138 4.3.1 树状图：基本的聚类数据结构 139 4.3.2 基于链接的算法概况 141 4.3.3 单链接算法 142 4.3.4 平均链接算法 144 4.3.5 最小生成树算法 147 4.4 k-means算法 149 4.4.1 初识k-means算法 150 4.4.2 k-means的内部原理 151 4.5 鲁棒的链接型聚类（rock） 153 4.5.1 rock简介 154 4.5.2 为什么rock这么强大？ 154 4.6 dbscan 159 4.6.1 基于密度的算法简介 159 4.6.2 dbscan的原理 162 4.7 超大规模数据聚类 165 4.7.1 计算复杂性 166 4.7.2 高维度 167 4.8 总结 168 4.9 to do 169 4.10 参考资料 171 5 分类：把事物放到它该在的地方 172 5.1 对分类的需求 173 5.2 分类器的概述 177 5.2.1 结构分类算法 178 5.2.2 统计分类算法 180 5.2.3 分类器的生命周期 181 5.3 邮件的自动归类与垃圾邮件过滤 182 5.3.1 朴素贝叶斯分类 184 5.3.2 基于规则的分类 197 5.4 用神经网络做欺诈检测 210 5.4.1 交易数据中关于欺诈检测的一个用例 210 5.4.2 神经网络概览 212 5.4.3 一个可用的神经网络欺诈检测器 214 5.4.4 神经网络欺诈检测器剖析 218 5.4.5 创建通用神经网络的基类 226 5.5 你的结果可信吗？ 232 5.6 大数据集的分类 235 5.7 总结 237 5.8 to do 239 5.9 参考资料 242 6 分类器组合 244 6.1 信贷价值：分类器组合案例研究 246 6.1.1 数据的简要说明 247 6.1.2 为真实问题生成人工数据 250 6.2 用单分类器做信用评估 255 6.2.1 朴素贝叶斯的基准线 255 6.2.2 决策树基准线 258 6.2.3 神经网络基线 260 6.3 在同一个数据集中比较多个分类器 263 6.3.1 mcnemar检验 264 6.3.2 差额比例检验 266 6.3.3 cochran q检验与f检验 268 6.4 bagging: bootstrap聚合（bootstrap aggregating） 270 6.4.1 bagging实例 272 6.4.2 bagging分类器底层细节 274 6.4.3 分类器集成 276 6.5 boosting：一种迭代提高的方法 279 6.5.1 boosting分类器实例 280 6.5.2 boosting分类器底层细节 282 6.6 总结 286 6.7 to do 288 6.8 参考资料 292 7 智能技术大汇集：一个智能新闻门户 293 7.1 功能概览 295 7.2 获取并清洗内容 296 7.2.1 各就位、预备、开抓！ 296 7.2.2 搜索预备知识回顾 298 7.2.3 一个抓取并处理好的新闻数据集 299 7.3 搜索新闻 301 7.4 分配新闻类别 304 7.4.1 顺序问题 304 7.4.2 使用newsprocessor类进行分类 309 7.4.3 分类器 310 7.4.4 分类策略：超越底层的分类 313 7.5 用newsprocessor类创建新闻分组 316 7.5.1 聚类全部文章 317 7.5.2 在一个新闻类别中聚类文章 321 7.6 基于用户评分的动态内容展示 325 7.7 总结 328 7.8 to do 329 7.9 参考资料 333 附录a beanshell简介 334 a.1 什么是beanshell？ 334 a.2 为什么使用beanshell？ 335 a.3 运行beanshell 335 a.4 参考资料 336 附录b 网络采集 337 b.1 爬虫组件概况 337 b.1.1 采集的步骤 338 b.1.2 我们的简单爬虫 338 b.1.3 开源web爬虫 339 b.2 参考资料 340 附录c 数学知识回顾 341 c.1 向量和矩阵 341 c.2 距离的度量 342 c.3 高级矩阵方法 344 c.4 参考资料 344 附录d 自然语言处理 345 d.1 参考资料 347 附录e 神经网络 348 e.1 参考资料 349 索引 350    ","title":"《智能Web算法》"},{"content":"导读：批改网是一家基于语料库的英语作文自动批改服务，可有效提高老师批改英语作文的工作效率，提高学生的英语写作能力，属国内首款。它的创立背景是什么？背后又有怎样的创新故事？它的成功有哪些经验可以借鉴呢？带着这些疑问，CSDN记者专访了批改网创始人张跃。 CSDN记者：请首先介绍一下批改网，（与同类型产品相比）它具有哪些创新之处？ 张跃：批改网（www.pigai.org）是基于语料库的英语作文自动批改服务，能够有效提高老师批改英语作文的工作效率，提高学生的英语写作能力。与同类产品相比，我们的创新点包括： 一键收发：考虑到老师和学生的日常教学和学习任务都非常繁重，我们在设计系统的时候着重考虑到系统的用户体验，让用户使用起来非常简单，比如我们已经做到了老师一键布置作文和学生提交作文，同时我们还提供多种方式让老师收发作文，比如可以提供一个固定的网址或者提供作文号让学生提交作文，希望做到比传统的邮件收发作文还要好用。 自动批改：学生提交作文后，我们会立即给出分数、评语和按句点评，其中机器评分和人工评分的基本一致率达到91.55%，评语和点评可以分别从总体和句子两个层面给学生反馈，帮助学生改进作文。 中式英语：目前国外的ETS、麦格劳希尔、皮尔森等知名机构都有类似的作文自动批改系统，但他们主要针对英语作为本族语的学习者，而我们主要针对的是英语作为第二外语的中国学习者，中国学习者最容易范的错误就是中式英语（Chinglish），我们因为掌握了大量的中国学习者语料库和国外本族语语料库，通过语料库比对，我们能够识别多数中式英语。 开放平台：我们底层的打分公式、自动评语和按句点评都是开放可以定制的，比如我们目前有四六级打分公式，在我们有相应训练集和测试集得情况下，我们就可以定制出雅思、托福、中考、高考的打分公式出来，对于评语和点评也一样。 薄弱分析：除了这些以外，我们还可以基于学习者作文的数据为老师提供教学和科研方面的应用，比较典型的例子是薄弱点分析，老师可以基于数据分析学习者的个性薄弱点和共性薄弱点，老师在课堂上可以做针对性的改进。 另外我还想强调一点，作文自动批改系统目前只有美国ETS、麦格劳希尔、皮尔森等少数权威机构或国际大公司掌握了核心技术，而批改网是完全自主研发的国产软件，从核心引擎到批改应用都是完全自主知识产权的，这一点能够保证我们能够快速和创造性的满足客户的需求。 CSDN记者：请介绍一下创建该网站的背景。当初怎么想到要做这么一个产品呢？ 张跃：我们公司是2007年成立的，之前一直是做垂直搜索引擎和双语语料库应用的技术公司，很偶然的机会我们拜访了南京大学大学外语部的王海啸教授，希望推广我们的语料库教研平台。王老师一席话让我们决定转型做批改网这个产品，他认为：“目前大学英语考试中学生得分最低的就是作文，主要的原因是练的不够多和没有老师批改；但从老师的角度来说，也没法给学生全部做批改，因为现在英语老师的生师比约是1:130，每篇作文如果认真批改至少要10分钟，这样老师至少要花掉20小时的时间去批改一篇作文，大学老师除了教学还有科研任务，不可能花这么多时间去批改作文；另外还有一点是，老师可能也没有这个能力去帮助学生批改作文，因为批改作文第一要判断错误，第二要改正错误，第三提供建议，而语言又是动态变化的，这些都对老师的能力提供了挑战。” 王老师当时就建议我们做英语作文自动批改系统，因为首先，我们有英语语料库分析方面的积累，某种意义上来说作文的分数可以看做作文和语料库之间的距离；其次，我们有自然语言处理技术（搜索引擎）方面的积累；最后，我们有英语教学软件和互联网产品研发方面的积累。所以王老师认为从技术积累上我们非常适合开发这个系统，我们最缺的就是来自一线教学的需求和英语教育测量方面的理论支持，而南京大学和王老师恰恰是需求的来源和英语教育测量方面的专家。 在这种背景下，我们和南京大学很快就打成共识，共同研发这个系统，这就是批改网的研发背景。 CSDN记者：该产品的适用群体有哪些？ 张跃：批改网目前主要适用于各类教育机构，包括高校、培训机构、中小学等，通过辅助老师批改作文来提高学生的英语写作能力。同时我们也跟知名英语学习网站合作直接为个人提供英语作文批改服务，比如我们和大耳朵英语合作推出了大耳朵英语作文批改网（http://writing.ebigear.com/）。 CSDN记者：该产品的核心技术点有哪些？ 张跃：批改网的核心算法是计算学生作文和标准语料库之间的距离，再通过一个映射将距离转化成作文分数和评语。核心技术点是对于每一篇输入的作文，批改网将它分析成可测量的192个维度，这个分析过程用到了大量的自然语言技术和机器学习的方法，每篇作文先被自动切分成句子，然后每一个句子都进行深度的语义分析，从中抽取词、搭配、词组等结构化单元。 CSDN记者：您认为在设计、开发该产品时最困难的阶段是什么时候？都遇到了哪些难点？是如何克服的？ 张跃：研发第一版时最困难，当时时间紧，我们没做太多调研的情况下推出了第一版作文打分公式，在实际应用中发现了很多问题，比如区分度不够等。 为了提高作文打分公式的区分度和有效性，我们仔细调研了国内外的相关论文，最终在新版设计了一个打分平台，支持了192个作文维度，包含了国内外几个主流作文自动打分系统的维度，新的打分平台支持自主选择训练集训练，并支持维度配置，通过该平台上可以定制一些常见的打分公式，比如我们知道著名的美国ETS打分公式有12个维度，我们也可以在批改网上配置一个“ETS”版的打分公式。 CSDN记者：在开发此产品时，有没有一些难忘的经历，请分享一下。 张跃：批改网研发的过程就是一个不断和一线老师接触的过程和内部争论的过程，我们内部有两个不成文的规定“不接触一线就没有发言权”、“否定与自我否定”。我们曾经为一个“抄袭检测”的按钮是放在上面还是下面？是隐藏还是不隐藏？是显示数字链接还是显示按钮？反复修改了3次。 CSDN记者：假设可以重新设计该产品，您将侧重于哪些方面的改进？ 张跃：如果重新设计的话，我们会将作文批改整合到统一的语料库平台中，从批改网上线以来，作文数据增长非常快，这个数据中蕴藏着大量有价值的信息，比如学生的共性薄弱点，某个学生的成长轨迹等等，批改网也计划提供作文数据挖掘接口，帮助老师挖掘数据中的规律。 CSDN记者：该产品发布至今，累计有多少高校入住，他们对该产品的评价如何？ 张跃：批改网的应用分三个阶段，第一个阶段是产品磨合阶段，通过南京大学实际的应用来磨合产品；第二个阶段试点推广阶段，主要是选择部分典型高校进行试点推广，更大范围的收集用户的反馈，目前试点的高校有二十多所；第三个阶段是大范围推广阶段，即将启动这个阶段，不过我们已经做了一些准备工作，比如我们跟外研社签订了合作协议，在他们的资源库产品中捆绑批改网，预计将直接覆盖到300多所高校，同时我们也在尝试跟一些培训机构和出版机构进行合作。 老师们对产品的评价也分三个阶段，最开始认为机器评分完全不靠谱；后来发现机器评分和人工评分其实相差不大，而且还能够提供反馈，有些反馈比老师给的还要细致；到现在，老师已经开始主动给我们提建设性意见，主动帮助我们完善系统，比如发现系统不能识别的错误，老师们会通过系统的点评工具记录到系统的知识库中，以后再有类似的错误系统就可以自动识别了。 学生的评价从被动的提交老师布置的作文到主动在批改网上练习和反复提交作文看分数的变化，这一点是让我们最开心的。 CSDN记者：您认为该产品的发展前景如何？对该产品的未来发展有哪些规划？ 张跃：我们相信任何行业和互联网结合都是必然的规律，比如普通邮件升级为电子邮件、贸易升级为电子商务，纸质作业也一样必然会升级为电子作业为主，电子作业相比纸质作业有几个优势，第一、绿色，电子作业完全是绿色无污染，还可以节省大量的纸张；第二、高效，计算机可以高效并且不知疲倦地处理大量重复的工作；第三、可管理，计算机可以将学生的全部作业记录保存下来，老师可查询、管理和分析。 我们的愿景是成为全球领先的电子作业批改服务提供商，不仅可以批改英语作文也可以批改英语翻译、听写、口语更多等作业类型；逐步支持中文、数学、物理等其它学科。 CSDN记者：最后请总结一下，设计该产品的过程中所得到的经验或教训。 张跃：在这过程中确实有很多经验和教训，我们也总结了产品设计的两个凡是，“凡是拍脑袋出来的需求最后一定会调整”、“凡是多余的设计最后一定会去掉”，所以我们建议设计产品一定把握以下两点。 （1） 深入客户 深入客户一方面要求我们要广泛接触一线教师，收集教师的需求和问题；另一方面，我们也招聘了四位英语老师以真实老师的身份去使用系统，深度了解老师的需求。 比如作文截止时间一定设在给定日期的23:59分而不是设在24:00，这就是来自一线老师的需求。 （2） 化繁为简 80%老师经常使用的是20%的功能，针对20%常用的功能一定要将用户体验做到极致，不常使用的80%功能要学会舍弃或隐藏。 CSDN链接：http://news.csdn.net/a/20110731/302431.html","title":"专访批改网创始人张跃：设计产品一定要遵循两个“凡是”"},{"content":"最小编辑距离是用来衡量两个字符串的相似度 比如从“cabc”到“abc”，我们删除最前面的‘c’即可。 对于字符串的操作主要包括三类：删除，插入和替换（替换可以看作是删除和插入的组合） 我们设置删除和插入的代价为1，那么替换的代价则为2. 详细参考《自然语言处理总理》5.6最小编辑距离。 java实现版本 public class MED{\tpublic static void main(String[] args){\t\tMED med = new MED();\t\tmed.min_edit_dic(\"abc\", \"cabc\");\t}\tpublic int min_edit_dic( String target ,String source){\t\tint n = target.length();\t\tint m = source.length();\t\tint[][] distance = new int[ n+1 ][ m+1 ];\t\tdistance[ 0 ][ 0 ] = 0;\t\tfor ( int i = 1; i <= n; i++){\t\t\tdistance[ i ][ 0 ] = distance[ i-1 ][ 0 ] + ins_cost(target.charAt(i-1));\t\t}\t\tfor ( int j = 1; j <= n; j++){\t\t\tdistance[ 0 ][ j ] = distance[ 0 ][ j-1 ] + ins_cost(target.charAt(j-1));\t\t}\t\tfor ( int i = 1; i <= n; i++){\t\t\tfor ( int j = 1; j <= m; j++){\t\t\t\tint ins = distance[ i-1 ][ j ] +ins_cost(target.charAt(i-1));\t\t\t\tint sub = distance[ i-1 ][ j-1 ] + subs_cost(target.charAt(i-1),source.charAt(j-1));\t\t\t\tint del = distance[ i ][ j -1 ] + del_cost(source.charAt(j-1));\t\t\t\tdistance[i][j] =  min( ins, min(sub,del));\t\t\t}\t\t}\t\t\t\tfor ( int i = 0; i <= n; i++){\t\t\tfor ( int j = 0; j <= m; j++){\t\t\t\tSystem.out.print(distance[i][j]+\"\\t\");\t\t\t}\t\t\tSystem.out.println();\t\t}\t\t\t\treturn 1;\t}\t\tprivate int min(int d1, int d2){\t\treturn d1 < d2 ? d1: d2;\t}\t\tprivate int ins_cost(char c){\t\treturn 1;\t} \t\tprivate int del_cost(char c){\t\treturn 1;\t\t}\t\tprivate int subs_cost(char c1 , char c2){\t\treturn c1 != c2 ? 2 : 0;\t\t\t}}","title":"最小编辑距离"},{"content":"1：互联网技术 http://research.google.com/                       http://research.google.com/people/jeff/                          Research Scientists and Engineers http://research.google.com/people/sanjay/               Research Scientists and Engineers http://stblog.baidu-tech.com/                                           百度搜索研发技术官方博客 http://curl.haxx.se/libcurl/                                     Libcurl为一个免费开源的，客户端url传输库   2:自然语言处理(NLP)                   http://superjiju.wordpress.com/    自然语言处理自然语言处理相关技术  http://www.52nlp.cn/                           我爱自然语言处理（很有研究价值的自然语言入门参考资料） http://www.datatang.com/                                                 科研数据共享平台 http://nlp.csai.tsinghua.edu.cn/~zkx/cws/bib.html       中文分词贡献者列表 http://hunch.net/               机器学习的英文博客 http://www.datatang.com/member/5878              中科院自动化所“自动化学科创新方法课题”数据专区 http://research.microsoft.com/en-us/um/people/hoifung/    Machine Reading,NLP http://xh.5156edu.com/                                              汉语语言、语法资源 http://www.openpr.org.cn/                    模式识别国家重点实验室的开源工具 3:Web开发 http://flowplayer.org/tools/index.html                                     Jqery tools  jQuery 特效例子及教程 http://witzhao.blog.163.com/blog/static/2858082201173102549642/              13个Jqeury特效及教程","title":"[置顶] 记录有用的网址---不断保持更新"},{"content":"USNEWS 2003 PhD Program Ranking (Computer Science)   1. Carnegie Mellon University (PA) 4.9 1. Massachusetts Institute of Technology 4.9 1. Stanford University (CA) 4.9 1. University of California?Berkeley 4.9 5. University of Illinois?Urbana-Champaign 4.6 6. Cornell University (NY) 4.5 7. University of Texas?Austin 4.4 7. University of Washington 4.4 9. Princeton University (NJ) 4.3 10. California Institute of Technology 4.1 10. University of Wisconsin?Madison 4.1 12. Georgia Institute of Technology 4.0 12. University of Maryland?College Park 4.0 14. Brown University (RI) 3.9 14. University of California?Los Angeles 3.9 14. University of Michigan?Ann Arbor 3.9 17. Rice University (TX) 3.8 17. University of North Carolina?Chapel Hill 3.8 17. University of Pennsylvania 3.8 20. Columbia University (NY) 3.7 20. Duke University (NC) 3.7 20. Harvard University (MA) 3.7 20. Purdue University?West Lafayette (IN) 3.7 20. University of California?San Diego 3.7 美国计算机专业前20名学校点评 约定：CS＝计算机科学 (系) 总的来说，前20的CS可以分成三波： 1. 4 个最为优秀的CS Program: Stanford, UC. Berkeley, MIT, CMU 2. 7 个其他前10的： UIUC， Cornell, UT-Austin, U. of Washington, Princeton,   UW Madison 和 Caltech, 其中UIUC, Cornell, U. of Washington和UW-Madison几乎从 未出过前10。 3. 其他非常非常优秀的CS：UMCP, Georgia Tech, Brown, UCLA, Michigan, Rice,   UPenn, UNC, Harvard, Purdue, Columbia, Duke, UCSD.   约定每个学校的介绍采用如下的数据格式： Ranking(2003): URL；介绍 1. URL: http://www.(cs.)stanford.edu (8个图灵奖, 其中4人为校友) Stanford的CS是个很大个的CS，拥有40人以上的Faculty成员，其中不乏响当当硬梆梆的 图灵奖得主(Edward A.Feigenbaum, John McCarthy) 和各个学科领域的大腕人物，比如 理论方面的权威 Donald E. Knuth；数据库方面的大牛Jeffrey D. Ullman(他还写过那 本著名的编译原理，此人出自Princeton)；以及RISC技术挑头人之一的John Hennessy。 相信CS的 同学对此并不陌生。该系每年毕业30多名Ph.D.以及更多的Master。学生的出 路自然是如鱼得水，无论学术界还是工业界，Stanford的学生倍受青睐。几乎所有前10 的CS中都有Stanford的毕业生在充当教授。当然同样享 有如此地位的还包括其他三头巨 牛：UC. Berkeley, MIT 和 CMU. 毕业于U. of Utah的Jim Clark 曾经在Stanford CS当教授。后来就是这个人创办了高性 能计算机和科学计算可视化方面巨牛的SGI公司。SUN 公司名字的来历是：Stanford Un iversityNetwork. 顺便提一下，创办 YAHOO的华人杨致远曾在斯坦福的EE攻读博士，后 来中途辍学办了YAHOO。 CS科研方面，斯坦福无论在理论，数据库，软件，硬件，AI 等各个领域都是实力强劲的 顶级高手。斯坦福的RISC技术后来成为SGI/MIPS的 Rx000系列微处理器的核心技术； D ASH，FLASH 项目更是多处理器并行计算机研究的前沿；SUIF并行化编译器成为国家资助 的重点项目，在国际学术论文中SUIF编译器的提及似乎也为某些平庸的论文平添几分姿 色。 Stanford有学生14000多，其中研究生7000多。CS有175人攻读博士， 350人攻读硕士， 每年招的学生数不详，估计少不了，但不要忘了，每年申请StanfordCS的申请学生接近千 人。 申请费高达 80$。 斯坦福大学位于信息世界的心脏地带－－ 硅谷。加州宜人的气候， 美丽的风景使得 S tanford堪称CS的天堂。33.1平方公里的校园面积怕是 够学子们翻江蹈海，叱姹风云的 了。 申请斯坦福是很难成功的，但也并非不可为之。近年来复旦好象没有去斯坦福CS的学生 ，但该系是否有前复旦学子从其他地方转入偶就不清楚了。偶本科的好友曾申请过，遭 据。他可是各方面都很优秀的人， 去斯坦福这样的牛校，运气很重要，牛人的推荐也很 重要，可惜复旦…… 1. URL: http://www.mit.edu (5个图灵奖, 其中4人为校友) MIT 招生好象不看GRE成绩，偶又没有牛人推荐，所以没有怎么注意 它。但MIT的CS是巨 牛的，99年最新排名上它和斯坦福被打了5.0 的满分， 并列第一。MIT的CS曾为CS的发 展作出不可磨灭的贡献，数据流计算的思 想和数据流计算机、 人工智能方面的许多重 大成就，以及影响了整个 UNIX界的X-Window…… MIT和斯坦福，CMU， UC. BERKELEY一 样，都是几乎在CS界样样巨牛的学校。 据AGOU大侠提供的资料：MIT 的 Media Arts and Sciences 其知名 度不在Computer D epartment下。主要是多媒体技术，信息处理，人工智 能…… 有一大批著名的教授， 如Marvin Minsky (Turing Award)…… 1. URL: http://www.(eecs.)berkeley.edu (8个图灵奖, 其中5人为校友) 这是俺许多年来的梦中情人。可惜，她竞残忍地将偶拒之门外 …… 同样地处旧金山湾畔，硅谷地带，离Stanford只有大约 50公里的加州大学伯克利校区： UC.Berkeley是美国最激进的学校之一。60年代的嬉皮文化，反越战，东方神秘主义，回 归自然文化都起源于此。诗人爱伦金 斯堡是当年 Berkeley的代言人。 在当今高科技领域C. Berkeley 在缔造新的神话，在文学，数学，化学，新闻等20多个 大的学科领域中位居前3. 16个诺贝尔奖得主，总数近 200的科学院院士、工程院院士， 连同众多在硅谷商战中成为亿万富翁的伯克利人撑起了一面汇集天下之英才 的大旗。I NTEL总裁AndrewGrove，就是那个写 > 的家伙，毕业于UC. Berkeley。 BSD版的UNIX影响了整个OS界，伯克利的RISC技术后来成为了 SUN公司SPARC微处理器的 核心技术，巨牛人物David Patterson最近刚接下了一个6亿美元的项目用于新型计算机 体系结构，特别是IRAM的研究开发。 UC. Berkeley有学生30000多，研究生超过8500。申请费和其他加州 大学的分校一样， 40$。据一项最近的调查，伯克利已经成为美国大学生 最向往的研究生院，高居榜首， 其申请的难度可想而知。UC.Berkeley的 DEADLINE一般很早，12月中就截至了，但去年 偶不慎从和他们的接触中得知，其内部的实际DEADLINE其实要迟一些 …… Berkeley的CS是个大系，Faculty中有图灵奖得主以及象 Patterson这样的巨牛。学生的 出路同Stanford，MIT，CMU一样，光 圆 烂，前程锦绣，这里不再赘述。CS科研方面， Berkeley也是样样强，门门巨牛。 旧金山湛蓝起伏的海湾，苍翠绵延的山峦，舒心宜人的气候， 以及 近在咫尺的硅谷… … 这一切的一切不也使得 UC.Berkeley 俨然一个CS 学子的世外桃源么？ 唉，不说了 …… 1. URL: http://www.(cs.)cmu.edu (6个图灵奖, 其中1人为校友) CMU是个位于匹兹堡的不大的学校，学生7000多，校园好象也不大。但这个学校在工程及 其他一些领域却是顶尖的学堂。 CMU的 CS 不单单是个系，而是一个学院，其规模之大 ，可能只有Stanford, UIUC可比。 教 师学生的情况同前面3个类似，不再赘述。Mach操 作系统，PVM，C.mmp等 都有CMU的巨大贡献。 申请CMU的难度很大，因为尽管CMU的 CS Faculty很多，但每年只招不足30人的研究生队 伍。令人可喜的是，复旦已有学人在 CMU的CS了，我想我还是止住为好，以免有班门弄 斧之疑。   5. URL: http://www.(cs.)uiuc.edu (2个图灵奖, 其中2人为校友) UIUC的工程院在全美堪称至尊级的巨牛，其CS，ECE，EE在历史上都 屡建战功。在CS方 面，从早期的超级计算机ILLIAC I, II, III, IV到后来的 CEDAR，都是CS发展史上，特 别是并行计算机发展史上的重要事件，影响，引导了很长时期的发展。 David Kuck曾是 并行处理界的一代先驱。 超级计算机研究开发中心：CSRD，美国国家超级计算及应用中心：NCSA等众多的机构， 使得UIUC的CS常常成为研发的领军头领。 大家可能还记得，Netscape-Navigator 的最初开发人员中有个Marc,Anderssen。这位来 自WISCONSIN的小伙在UIUC读本科，大四的时候在NCSA参与编写了MOSAIC，后来他去了硅 谷，并在那里遇到了前面提到过的大牛: Jim Clark,SGI的前创始人，两人一见如故，联 手创办了著名的网景，并 一度在浏览器市场上独霸武林。 随着一代代至尊大师的离去，UIUC 的 Faculty看上去似乎并不引人 注目。但啊拉得提 醒你， UIUC的CS向来以实干著称。我期待着他们下一 个惊世之举。 UIUC是个大学校，学生数过35000，研究生院的近万。 UIUC的CS很大个，40余个Faculty提供了全面的CS教育和科研项目。每年30多个博士的毕 业数目似乎只有斯坦福可以匹敌。 UIUC的Polaris并行化编译器是这个领域和斯坦福的SUIF直接叫板的拳头产品。清华开发 并行编程环境时选用了这个系统。只是代码庞大，运作缓慢的Polaris搞的清华哥们有那 么一点点瘪西西... UIUC 在计算机硬件，软件，AI，DB，等各个领域都相当巨牛。特别 是硬件，前面提到 的ILLIAC，CEDAR.....事实上，UIUC在超级计算机系 统的研究开发方面决不逊于CS四大 天王中的任何一个，甚至有过之而无不 及。NCSA建立在UIUC这一事实本身就是佐证。 复旦有校友在UIUC的CS。非常遗憾，偶没有能够拿到该系的全奖，于是作罢。 UIUC-CS 的学生毕业后去学术界的不少，Stanford, Berkeley...都 有UIUC的博士挑大 梁。但更多UIUC-CS学人还是进入业界，成为业界实干 的中坚。 6. URL: http://www.(cs.)cornell.edu (2个图灵奖, 其中0人为校友) 作为 IVY LEAGUE的成员和一所私立学校，Cornell有其独到的优势。 在美国，私立学校一般比公立学校难进，其学生也是经过很严的选拔才录 取的，Corne ll的CS学生入校后多能享受FELLOW的待遇，其个人经济条件 非公立学校可比，加上贵族 式校友的提拔，私立学校的出路是很诱人的。 Cornell在理论计算机方面一直是顶级高手，但在其他 CS领域并不总能在前10，偶得承认 对其跟踪的不够，就此打住。 Cornell学生18000多，研究生过5000。CS每年招攻读Ph.D.的学生25 人左右。 复旦至少有两位大虾在那里攻读博士，后来人可去WWW上了解。 7. URL: http://www.(cs.)washington.edu 位于 Seattle的 UW 得天独厚--计算机界的巨牛MS就在西雅图，而且 更 为要命的是， Bill Gates就是那里儿的人。这位Harvard 的辍学者给 了哈福许 多MONEY, 但同给UW的 钱财相比，实在是小巫见大巫。 U. of Washington位于分光秀丽的WASHINGTON湖畔，气候四季如春。33000多学生中研究 生有8000。Seattle最令人厌恶的地方可能就是一年有 160天会降水。 UW的CS较大，30多名Faculty成员，每年近20个优质博士毕业，以及大量的Master。估计 每年的招生数应该不低，UW的CS在各个方面比较均衡， 最强的软件排名第5，而其他领 域也一般都能位居前10，好象没有明显弱 的地方。 图灵奖得主 Dick Karp从Berkeley告老还乡后又被返聘到了UW的CS。 U. of Washington的 CS要求很高，Ph.D.学生入学的平均 GPA 高达 3.86, GRE2160+， 加上一般较早的DEADLINE，申请UW是相当有难度的。 7. URL: http://www.(cs.)utexas.edu (1个图灵奖, 其中0人为校友) UT-Austin的CS较大。Faculty中好象有个图灵奖得主。( agou证实， 那人名叫：Edsge r Wybe Dijkstra，是那个搞算法的)。 偶不喜欢炎热的气候，上海的副高实在让俺受够了，所以偶没申请UT-Austin，故而对它 也不是很了解，好象该系发展比较平衡，最好的AI 排第5，其他几个专业也多能挤进前 十。 UT-Austin是个巨大的学校，5万学生，研究生院的可能有1.3万。但学校的主校区却好象 面积不足，仅140公顷，按美国大学的标准， 太不足了。偶曾见到一张照片，校园周围 高楼林立，可能是位于市中心的缘故吧。 复旦有校友在 U. of Texas-Austin 的 CS，就是比偶大约高两届的， 但偶不认识。 9. URL: http://www.(cs.)princeton.edu (7个图灵奖, 其中5人为校友) Princeton是个令人神往的地方，这里曾经是科学的世界中心。 Princeton的CS不大， 18个Faculty成员，学生数也不算多。科研上 除了排名第 5的理论，似乎俺还没注意到其他闪光点，望知情人补充。但是，Princeton无疑培养出 了大量计算机界的优秀人物，Jeffrey D. Ullman, John McCarthy等巨牛人物均出自大 名鼎鼎的Princeton. 在Princeton领 受的教育是最好的教育熏陶。 Princeton学校不大，只有6000多学生，研究生不过1700。 Princeton 的 CS录取很严，虽然已有不少华人学生就读 Princeton，但迄今为止，好象 还没有复旦校友在那里。俺在即将丢出申请材料的那一 瞬，放弃了 Princeton 转投了 WISCONSIN...   10. URL: http://www.(cs.)wisc.edu 这是俺未来学习生活的地方，比较了解。 UW-Madison的CS较大，35个Faculty, 200多个研究生，每年招60-70 个新生。 目前几乎 1/4 的 Faculty 来自Berkeley，博士生毕业后有去 Stanford, Berkeley等牛校挑大梁 的，但和UIUC类似，似乎进入业界的更多些。然而要在这里拿到博士学位可不容易。超 过 7成的人，会在中途找到比较理想的工作后，拿着硕士文凭撒丫子就跑，免得被那些 无穷无尽的 科研项目给整瘪了。一位 WISCONSIN的哥们在回答我关于 \"该做些什么 准 备\" 的提问时说: 尽情欢乐享受吧，这样可以 \" Bring A healthy and energetic you toMadison to survive those projects. \" UW-Madison的数据库一直在前 3 位，经常是第 1位。这里的数据库 由于在设计实现 D BMS系统上的传统优势，使得其在业界的声誉相当崇高， MicroSoft 里据说有一帮WISC ONSIN的校友从中兴风作浪，Oracle也格外 青睐WISCONSIN-Madison的学子。可惜，偶似 乎对数据库并不是很感兴趣。 WISCONSIN的硬件，计算机体系结构实力巨牛，99排名第 6，对业界相当有影响力。微处 理器中的超标量技术（SuperScalar） 源于此地；多 处理机CACHE一致性的总线侦听SN OOPING协议，IEEE SCI协议等，都是源 于此地。正在研究开发中的MultiScalar技术和 DataScalar技术据吹可以 把微处理器每个时钟周期的指令发射数提到10以上，大大地提 高微处理器 的计算能力。WISCONSIN的软件99排名第 7。主要是在系统软件方面做OS 的 设计与实现，WEB上的 CACHE策略，支持共享主存和消息传递两种并行 编程模式及其混 合的并行程序设计语言和编译器，以及由 MIDSHIP项目挑起的关于并行与分布式计算， OODB，科学数据库，支持图象查询的新型查询语言以及图象处理等方面的研究。由于美 国有大量的卫星图象需要及时 处理，加上迫切需要GIS系统的研究开发，这方面的研发 使得UW-Madison 捞到了不少经费。 WISCONSIN和UIUC的CS理论都是10名左右。WISCONSIN的Carl de Boor 是逼近理论方面的 大牛。 U. of Wisconsin-Madison是个大型综合性的学校，40000 学生中研究生院的超过10000 ,这万人中有博士生5000，硕士生3500，法学院、医学院、护院、兽医院的职业学生200 0人。2200多 Faculty中有多位诺贝尔奖 得主, 52个院士，18个工程院院士。 130个科系几乎涵盖了所有科研领域。 科研经费常年位居全美前 4。 wisconsin 的研究 生院稳居 TOP20，而且由于它的大而全，在科研排名上能进前10。UW-Madison在95年NR C 的41项评价中，16项位居TOP10，35项排进了TOP25。 U. of WISCONSIN-Madison的校园位于风景如画的湖畔林荫中. 现代化风格和古典欧美风 格的建筑物在平缓起伏的湖岸上交相呼应。学校自吹 拥为世界上最美的校园之一.偶不 知道其他校园的场景，单从他们在网页 上提供的照片来看，的确很美。 WISCONSIN的冬天很冷，很长，而且大雪纷飞，寒风凛冽。 UW-Madison 的 CS 曾经至少有三个复旦校友就读，但目前似乎只有 即将过去的偶。需 要注意的是，WISCONSIN的CS有点不同于许多其他学校， 它隶属于College of Letters & Science. 而不在College of Engineering下面，因此许多偏硬件的项目，比如嵌入 式系统，网络硬件、路由，多媒体，通信，自控以及数字信号处理及等项目不在CS Dep t， 而是在工程院 下的 Dept. of Electrical & ComputerEngineering 即 ECE系。那 个系 也挺大个，比CS还要大不少。98年在工程类排名的计算机工程一项上也排 了第9。 但偶将来怕跟他们没多少来往。 伊拉的网址：工程院： http://www.engr.wisc.edu E CE系， http://www.ece.wisc.edu 10. URL: http://www.(cs.)caltech.edu (2个图灵奖, 其中2人为校友) CalTech的CS很小，只有大约5位教授，每年招很少的学生。虽然申请CalTech是免费的， 但建议轻易不要尝试。（也别让我这话给吓趴下了） 由于系太小，CalTech 好象只是在计算机硬件，和科学计算的可视化方面很强。该系多 年以来一直稳坐 NO.11,12几乎没动过窝, 类似的情况 还有斯坦福、MIT，稳居NO. 1,2 , Cornell稳居NO.5, UW-Madison, 稳居 No.9,10.CalTech的CS和其他系，比如数学，物 理，生物等需要大量科学 计算的部门联系很紧密。 CalTech 学校也很小，2000名学生中研究生占1100人。 Faculty人数也不多，但几乎个 个是巨牛，按平均水平看，CalTech 可能是世界上最牛 的学校了。 不要说复旦校友，偶好象就没见到来自大陆的学子在该系，可能是啊拉孤陋寡闻吧。 总的来说，前 10 的 CS由于在当前国际计算机行业普遍热门的情况 下，很难申请，但 决不是不可为之的!如果你是复旦CS，EE，数学或其他 相近专业的TOP5%；蛘?OP10%的 学生，计算机动手能力强、理论背景扎实，英文功底深， 尽可申请，没什么好怕的。   虚名很重要，但偶认为实力加运气加上明智的决策是成功的关键所在。失败不可怕，   可怕的是懦弱的胆气和愚蠢的策略，去了差一些的学校并不表明你不如别人，机会仍然 在处处等你，只要你有心有眼；成功了也不值得嚣张，俗人进了牛校，除了浪费资源，赢得 虚名，还能干什么呢？偶只是担心一些纯真的复旦人被传 言或自己的畏惧吓倒。那才是复旦 这面旗帜的最大悲哀。 12. URL: http://www.(cs.)umd.edu (U. of Maryland at CP) 这是一个实力相当强劲的CS，软件(8)，数据库(4)，AI(9) 三个专业 都挤进了前10，偶当初没有考虑该系的事实，其实是被掩盖掉的一大失误。俺是搞并行 计算的，伊拉的TINY系统相当有知名度，但偶居然视而不见。 其他情况就更不了解了。 12. URL：http://www.gatech.edu Georgia Tech是个较大的学校，具体数字记不清了。 Georgia Tech的工程院很利害，研发经费仅次于MIT，和UIUC, Umich差不多。 CS系的数据 库第7，GUI第4。其他没有列在前10，偶也没有去仔细了解过，就一概的不清楚了。听说有位 复 旦往届巨牛的师姐，将去那里读书。 14. URL: http://www.(eecs.)umich.edu (1个图灵奖, 其中1人为校友) U. of Michigan是个非常了不起的学校。在BIG TEN里，从综合的角 度上说它可算的上 是领头羊了，当然UIUC, UW-Madison也紧随其后。 这里的CS偏硬的更利害些，硬件排在第9，而计算机工程(7)，EE(5) 都是前10中的巨牛 。 MICHIGAN 的 CS 和EE合在一起称为 EECS系。是个 相当大个的系，每年招的学生当 不在少数。今年复旦有人去EE，而且MICHIGAN的复旦校友还是颇有一些的。鉴于他们EE ，CS合在一起，CS还是偏硬的利 害。偶就不再多言了。 MICHIGAN的CS估计在历史上也相当牛，UW-Madison CS里的两位来自umich的教授都是院 士，在其他CS系里，比如UIUC的，也大有UMICH的牛人 在。如前述，UIUC的CS在硬件上 极强，而UMICH的CS中有许多UIUC的哥们在那里当老师。 14. URL: http://www.(cs.)ucla.edu 历史上 UCLA 的 CS曾经一度辉煌，上到过第6 (NRC'82)，但近年来一直徘徊在13－15。 而且CS的各个专业细目几乎没有一个能进前10。尽管 如此，UCLA的CS还是十分强大的。 UCLA辉煌的历史可能在于它对Internet的发展，所作出的巨大贡献。 六十年代美国的 ARPA在搞网络互连的开创性研究， ARPA网的四个 节点是：UCLA，Stanford 的 SRI, U CSB 和 U. of Utah。 此时一位来自美国新英格兰地区的青年: Vinton Cerf不去离家咫 尺的 Yale大学，远涉千里，来到了加州。他先在Stanford获得数学学士，然后到UCLA拿 下了CS 的硕士和博士。 毕业后Cerf一直在SRI从事ARPA网的研究，特别是如何让它无法正常工作。几年后，Cer f与MIT的一位到业界闯荡的数学教授Kyhn(?)合作研发，搞出了一套软件系统用于网络互 连(1973)。这就是TCP/IP协议 的诞生。 UCLA 作为 INTERNET 的先驱，地处阳光灿烂的南加州，应当成为CS学生的乐土。可惜偶 当初也被'加州学校难申请'的说法吓倒了。没敢动土。加州的学校的确难申请，但也是 可以一试的。1999年2月1日偶收到的第一 个OFFER，就是来自加州的 UC. Irvine 。 UCLA有学生33000人，其中研究院的占 9900人。地处落杉矶的 UCLA,周围几乎有玩不尽 的地方，DISNEY，HOLLYWOOD..... 由于位于 大城市， 校园不是很大，但风景似乎非常 美丽。 UCLA的CS较大，规模应该和 U. of Washington 和 UW-Madison类似， 偶记不清了。 14. URL：http://www.(cs.)brown.edu Brown的规模不大，具体数字记不清了。 这所 IVY LEAGUE 的私立学校可能拥有一些类似于CORNELL的优势。CS的GUI可以列在NO .6，好象还有许多关于语音识别等偏人工智能 方面的研发项目。但偶从来没有注意过这 所学校，不能多说了。 我真的很后悔从事这项介绍前 20 CS 的重任，既没这个资格，也没 这个学识。现在已 经骑虎难下。头也大了，手也快抬不起来了， 我真的真的好后悔.... 17. URL: http://www.(cs.)rice.edu Rice是个位于休斯顿的小学校， 4000个学生，研究生有1600左右吧， 希望偶没记错， 误人子弟。 CS也不大。优势在于软件，排在第9。别的情况偶不了解，但偶特别 想告诉大家的是， 该系的 KEN KENNEDY是个巨牛的人物。伊是美国 HPCC 常委的关键人物之一,好象还是总 统在信息科学方面的特别顾问。KENNEDY是并行计算领域的大牛牛。 前几年,伊义无反顾 地承担起高性能 FORTRAN 语言(HPF)的编译器研制工作，项目之大，投入人力之巨，加 上伊的权威地位，被人们普遍寄予厚望。可惜后来项目失败了。从此并行计算界陷入 了 一阵低潮。这几年 KENNEDY 好象转向去作针对特定处理器的后端优化 技术了。 Rice CS 学生的出路相当好。 17. URL: http://www.(cis.)upenn.edu (University of Pennsylvania) 17. URL: http://www.(cs.)unc.edu (1个图灵奖, 其中0人为校友) 北卡GUI方面是顶级牛校。 20. URL: http://www.harvard.edu (5个图灵奖, 其中4人为校友) 在CS的早期发展史上，Harvard曾经是泰斗级的人物，毕竟CS和数学， 物理的渊源太深 太长了。可惜Harvard并不重视工程化的东西，现在伊的 CS已不能和圈里的巨牛，甚至 伊的当初相提并论了。好象王安是这里出来的， Bill Gates也是这里出来的， Harvar d 毕竟是 Harvard， 总是名 人辈出。毕竟Harvard总是可以招到最优秀的人，甚至是在 它很瘪的领域 里。 但千万别以为哈佛人人牛。据说美国人的调侃中，专门有一条是说 哈佛的某些学生 是如何令人叹为观止的愚蠢.... 偶还没有身在美国，不知是真是假。 Harvard 不喜欢带工程色彩的东西，CS 是挂在Arts & Science学院 下面的 Division of Engineering and Applied Science,连独立的一个 系好象都不是。除了理论可以排 进前10，其他项目怕也拿不出多少货色了。 但是，如果再给我一次机会的话，我一定申请Harvard。因为这里是Harvard，你可以学 到许多在别的地方难以学到的东西。 专业知识并不是全部，况且哈佛的教育是不会差的 ，虽然它在 CS 的科研上没什么好吹的。 哈佛的研究生每年超过 20000$ 的 FELLOWSHIP 是你安心寒窗苦读的强大 后盾。 哈佛大学学生18000人，其中研究生院的11000人。Harvard大学拥有世界上最多的诺贝尔 奖得主，150多个美国国家科学院院士.......哈佛是个巨牛云集的超级牛圈。 ) 哈佛的 CS 估计不会是个大个子， 招的学生可能也不会多，申请的 难度应当很大。 20. URL: http://www.(cs.)purdue.edu 可能许多人还不知道，Purdue 的计算机系是美国最早成立的计算机 系。建系之初一直 处于TOP10。在70年代由于本人不甚了解的原因，没落 了。Purdue的排名也不太稳定从 13到 30的排法偶似乎都见过。Purdue CS有些什么特点从俺接触的有限材料上，俺好象 没有看出什么， 俺由于 某些原因放弃了Purdue，对她也不甚了解。 今年有两位复旦校友将 开赴 Purdue的CS。复旦往届在普度CS的校友，? Purdue是个大个的学校，有35000学生。其 工程院很出名。 20. URL: http://www.(cs.)columbia.edu (1个图灵奖, 其中1人为校友) Columbia在AI，语音识别，自然语言处理等方面颇有造诣. 20. URL: http://www.(cs.)duke.edu (Duke) 20. URL: http://www.(cs.)ucsd.edu (University of California?San Diego) 25. URL: http://www.(cs.)yale.edu (1个图灵奖, 其中0人为校友) YALE 曾经也进过前 10，NRC'82 的排名上，是 YALE和 UCLA而不是 Princeton和 UT-A ustin 位于前 10 的榜上.YALE的 CS不大， 十几个老 师加上为数不多的学生，每年只 招六个博士研究生。 和 Harvard这样很重文理的学校一样，YALE 的 CS在理论上比较强。 但不同于哈佛，Y ALE 有独立的CS系，受到较高的重视。YALE-CS 在 AI， 软件方面比较强。著名的 LIN DA 并行编程模式是在这里提出并实现的。 YALE 的毕业生中到学术界的比到业界的似乎 要多，哈佛似乎也是这样。 好了，终于可以下虎背了。 俺学识尚浅，这里只随便罗列了一些俺顺口拈来的东西，仅 供参考。 其实 CS其他很好的学校还有很多， 比如： USC 等等。可惜偶实在不想吹了。 就此搁笔。","title":"美国计算机专业前20名学校点评"},{"content":"3.2 PAIRS（对）和STRIPES（条纹）   在MapReduce程序中同步的一个普遍做法是通过构建复杂的键和值这样一个途径来使数据自然地适应执行框架。我们在之前的章节中涉及到这个技术，即把部分总数和计数“打包”成一个复合值（例如pair），依次从mapper传到combiner再传到reducer。以之前的出版物为基础【54，94】，这节介绍两个常见的设计模式，我们称为pairs（对）和strips（条纹）。   作为一个运行时的例子，我们关注于在大型数据上建立单词同现矩阵，这是语料库语言学和自然语言处理的共同任务。正式来说，语料库中的同现矩阵是一个在语料库中以n个不同单词（即词汇量）为基础的n×n矩阵。一个mij包含单词wi与wj在具体语境（像句子，段落，文档或某些窗口上的m词，m词是应用程序依赖的属性）下共同出现的次数。矩阵的上下三角形是同样的因为同现是一个对称关系，虽然一般来说单词之间的关系不必相对称。例如，一个同现矩阵M，mij是单词i和单词j同现的次数，它通常不能均衡。   这个任务在文本处理和为其它算法提供初始数据时很普遍，例如，逐点信息交互的统计，无人监管的辨别聚集，还有很多，词典语义的大部分工作是基于词语的分布式情景模式，追溯到1950年和1960年的Firth [55] 和 Harris [69]。这个任务也可以应用于信息检索（例如，同义词词典构建和填充），另外一些相关的领域例如文本挖掘。更重要的是这些问题代表着一个从大量观测值中的不相关的joint事件的分布的任务的特殊实例，统计自然语言处理的一个共同任务是MapReduce的解决方案。实际上这里展示的观念在第六章讨论最大期望值算法时也会用到。   除了文本处理，很多应用领域的问题都有相同的特性。例如，大的零售商会分析销售点的交易记录来识别出购买的产品之间的关系（例如，顾客们买这个的话就会想买那个），这有助于库存管理和产品在货架上的摆放位置。同样地，一个智能的分析希望分辨重复的金融交易，它将提供恶意买卖的线索。这节讨论的算法可以解决类似的问题。   很明显，单词同现问题的算法复杂度是O(n2)， 其中n是词库大大小，现实中的英语单词全部加起来可能有10万多个，在web规模中甚至达到10亿个。如果把整个单词同现矩阵放到内存中，计算这个矩阵是非常容易的，然而，由于这个矩阵太大以致内存放不下，一种在单机上很慢无经验的实现是把内存保存到磁盘上。虽然压缩计数能够提高单机构建单词同现矩阵的规模，但是它明显存在限制伸缩性的问题。我们会为这个任务提供两个MapReduce算法来使其能适用于大规模的数据集。   1: class Mapper 2:      method Map(docid a, doc d) 3:               for all term w ∈ doc d do 4:                         for all term u ∈ Neighbors(w) do 5:                                  Emit(pair (w, u), count 1)    //出现一次发送一次计数   1: class Reducer 2:      method Reduce(pair p, counts [c1, c2, . . . ]) 3:               s ← 0 4:               for all count c 2 counts [c1, c2, . . . ] do 5:               s ← s + c                       //统计出现的次数 6:               Emit(pair p, count s) 图 3.8: 大数据集中计算单词的同现矩阵的伪代码   图3.8展示了我们称之为“pairs”的第一个算法的伪代码。像往常一样，文档的id和相关的内容组成输入的键值对。Mapper处理每一个输入文档和发送同现词对作为键1（即计数）作为值的中间键值对。这由两个嵌套循环来完成：外循环遍历每一个词语（pair中的左元素），内循环遍历第一个词语（pair中的右元素）的所有邻接词。MapReduce执行框架保证同一键的所有值都会在reducer中集合。因此，在这种情况下reducer只是用同一单词同现键值对获得文档中joint事件的绝对数量来进行简单的统计，这些值将作为最终键值对发送出去。每一个键值对相当于单词同现矩阵的一个值。这个算法说明了使用复杂的键来协调分布式计算。   1: class Mapper 2:      method Map(docid a, doc d) 3:               for all term w ∈ doc d do 4:                        H ← new AssociativeArray 5:               for all term u ∈ Neighbors(w) do 6:                        H{u} ← H{u} + 1                //统计和w同时出现的单词的计数 7:               Emit(Term w, Stripe H)     1: class Reducer 2:      method Reduce(term w, stripes [H1,H2,H3, . . . ]) 3:               Hf ← new AssociativeArray 4:               for all stripe H ∈ stripes [H1,H2,H3, . . . ] do 5:                        Sum(Hf ,H)                    //按元素进行统计Element-wise sum 6:               Emit(term w, stripe Hf ) 图 3.9:用stripes的方法来计算单词的同现矩阵， 注：element wise就是按元素进行运算，将两个不同矩阵内部的对应元素相乘 图3.9展示了另一个可选的方法---“stripes”方法。和pairs方法一样，同现词的键值对由两个嵌套循环来生成。然而，和之前方法主要的不同是，同现的信息首先被存放在关联数组H中而不是发送每一个同现词对的中间键值对。Mapper用词语作为key并把对应的关联数组作为value发送出去，每一个关联数组记录着某一个词语的相邻元素（如：它的上下文中出现的词语）的同现次数。MapReduce执行框架会使所有相同key的关联数组到reduce阶段一起处理。Reducer根据相同的key来进行统计运算（element-wise sum），积累的计数相当于同现矩阵中的同一个单元（cell）。最后的关联数组以相同的词作为主键发送出去。相比于pairs方法，stripes方法中每个最终键值对包含同现矩阵中的一行。   很明显，pairs算法相对stripes算法来说要生成很多键值对。Stripes表现得更加紧凑，因为pairs算法中的左元素代表着每一同现词对。Stripes方法则生成更加少而短的中间键，因此，在框架中执行时不需要太多的排序。但是，stripes的值更加复杂，也比pairs算法有着更多的序列化与反序列化操作。   这两种算法都得益于使用combiners，因为它们运行在reducers（额外和元素智能的关联数组的个数）的程序都是可交换和可结合的。然而，stripes方法中的combiners有更多的机会执行局部聚集，因为主要是词库占用空间，关联数组能在mapper多次遇到某个单词时被更新。相比之下，pairs的主要占用空间的是它自己和词典相交的空间，一个mapper观察到多次同一同现对时只能计数只能聚集（它和在stripes中观察一个单词的多次出现不同）。   对这两种算法而言，之前章节提到的in-mapper combining优化方法也可以对它们使用；因为这个修改比较简单我们把它留给读者作为练习。然而，上面提到的警告仍然有：因为缺少中间值的存储空间，pairs方法将有比较少的机会做到部分聚集。缺少空间也限制了in-mapper combining的效率，因为在所有文档都被处理之前mapper就有可能已经用完了内存，这样就必须周期地发送出键值对（更多地限制执行部分聚集的机会）。同样地，对stripes方法来说，它的内存的管理对于简单的单词统计例子来说更加复杂。对于常见的词语，关联数组会变得特别大，需要周期地清除内存中的数据。   考虑到每个算法潜在的伸缩性瓶颈是重要的。Stripes方法假定，在任何时候，每一个关联数组都要足够小来使之适合内存---否则，内存的分页会显著地影响性能。关联数组的大小受限于词典大小，而词典大小和文档的大小无关（回忆之前讨论过的内存不足问题）。因此，当文档的大小增加时，这将成为一个紧迫的问题---可能对于GB级别的数据还没有什么，但可以肯定未来将常见到的TB和PB级别的数据一定会遇到。Pairs方法，在另一方面，没有这种限制，因为它不需要在内存中保存中间数据。   鉴于此讨论，那一个方法更快呢？这里我们引用已发表的结果[94]来回答这个问题。我们在Hadoop中实现了这两个算法，并把它们应用在美联社Worldstream栏目（APW）中总计5.7GB的由2.27百万个文档组成的文档集中。在Hadoop中运行之前，文档集需先做下面处理：把所有XML标记移除，然后是用Lucene搜索引擎提供的基本工具来做分词和去除停止词。了更有效的编码所有分词都被唯一的整数代替为。图3.10对比了pairs和stripes在同一文档集中运行时的不同分数，这个实验是执行在有19个节点的Hadoop集群中，每个节点有一个双核处理器和两个磁盘。   这个结果说明了stripes方法比pairs方法要快很多：处理5.7GB的数据分别用666秒（11分钟）和3758秒（62分钟）。Pairs方法中的mappers生成26亿个总计31.2GB的中间键值对。经过combiners处理后，减少到11亿个键值对，这确定了需要通过网络传输的中间数据的数据量。最后，reducers总共发送1.42亿个最终键值对（同现矩阵中不为零的值的数量）。在另一方法，Pairs方法中的mappers生成6.53亿个总计48.1GB的中间键值对。经过combiners处理后，只剩0.288亿个键值对，最后reducers总共发送169万个最终键值对（同现矩阵中的行数）。像我们期望那样，stripes方法提供更多的机会来让combiners聚集中间结果，因此大大的减少了清洗（shuffle）和排序时的网络传输。图3.1.0也看到了两种算法展现出的高伸缩性---输入数据数量的线性。这由运行时间的线性回归决定，它产生出的R2 值接近1。   图 3.10: 使用不同百分比的APW文集作为实验数据测试pairs和stripes算法计算单词的同现矩阵所使用的时间，这个实验的环境是一个有着19个子节点的Hadoop集群，每个子节点都有两个处理器和两个硬盘。   图 3.11:（左边）用不同规模的EC2服务器组成的Hadoop集群来测试stripes算法在APW文集中执行的时间。（右边）根据增加Hadoop集群的规模得到的标度特征（相对的运行速度的提升）。   额外的一系列实验探索了stripes方法另一方面的伸缩性：集群的数量。这个实验可以用亚马逊的EC2服务来做，它允许用户很快地提供聚群来，EC2中的虚拟化计算单元被称为实例，用户根据实例的使用时间来交费。图3.11（左）展示了stripes算法的时间（同一数据集，与之前相同的设置），在不同数量的集群上，从20个节点的“小”实例到80个节点的实例（沿着x坐标）。运行时间由实心方块标示。图3.11（右）重构同样的结果来说明伸缩性。圆圈标出在EC2实验中规模的大小和增速，关于20个节点的集群。这个结果展示了非常理想的线性标度特征（即加倍集群数量使任务时间加快一倍）。这由线性回归中R2值接近1决定的。   从抽象层面看，pairs和stripes算法代表两个计算大量观测值中的重现事件的不同方法。这两个算法抓住了很多算法的特点，包括文本处理，数据挖掘和分析复杂生物资料。由于这个原因，这两种设计模式可以广泛而且频繁地用在不同的程序中。   总的来说，pairs方法分别记录每个同现事件，stripes方法记录所有重现时间关心的调节事件。我们把整个词典拆分成b个部分（即通过哈希查找），wi的同现词会分成b个小的“子stripes”，与10个不同的键分开(wi; 1)， (wi; 2) …(wi; b)。这是应对stripes方法中内存限制的合理方法，因为每个子stripes会更小。对b=|V|而言，|V|是词典大小，这与pairs方法是相同的。对b=1而言，这与标准的stripes方法相同。","title":"Data-Intensive Text Processing with MapReduce第三章（3）-MapReduce算法设计-3.2 PAIRS（对）和STRIPES（条纹）"},{"content":"关于字符串的的编辑距离的计算，最经典的两个方法就是树编辑距离和串编辑距离。网页的相似度就是根据这两个方法做到的。 （1）字符串编辑距离算法（转自http://hxraid.iteye.com/blog/615469，谢谢Heart.X.Raid啦~~~） 我们来看一个实际应用。现代搜索技术的发展很多以提供优质、高效的服务作为目标。比如说：baidu、google、sousou等知名全文搜索系统。当我们输入一个错误的query=\"Jave\" 的时候，返回中有大量包含正确的拼写 \"Java\"的网页。当然这里面用到的技术绝对不会是我们今天讲的怎么简单。但我想说的是：字符串的相似度计算也是做到这一点的方法之一。 字符串编辑距离： 是一种字符串之间相似度计算的方法。给定两个字符串S、T，将S转换成T所需要的删除，插入，替换操作的数量就叫做S到T的编辑路径。而最短的编辑路径就叫做字符串S和T的编辑距离。 举个例子：S=“eeba” T=\"abac\" 我们可以按照这样的步骤转变：(1) 将S中的第一个e变成a;(2) 删除S中的第二个e;(3)在S中最后添加一个c; 那么S到T的编辑路径就等于3。当然，这种变换并不是唯一的，但如果3是所有变换中最小值的话。那么我们就可以说S和T的编辑距离等于3了。 动态规划解决编辑距离 动态规划(dynamic programming)是一种解决复杂问题最优解的策略。它的基本思路就是：将一个复杂的最优解问题分解成一系列较为简单的最优解问题，再将较为简单的的最优解问题进一步分解，直到可以一眼看出最优解为止。 动态规划算法是解决复杂问题最优解的重要算法。其算法的难度并不在于算法本身的递归难以实现，而主要是编程者对问题本身的认识是否符合动态规划的思想。现在我们就来看看动态规划是如何解决编辑距离的。 还是这个例子：S=“eeba” T=\"abac\" 。我们发现当S只有一个字符e、T只有一个字符a的时候，我们马上就能得到S和T的编辑距离edit(0,0)=1(将e替换成a)。那么如果S中有1个字符e、T中有两个字符ab的时候，我们是不是可以这样分解：edit(0,1)=edit(0,0)+1(将e替换成a后，在添加一个b)。如果S中有两个字符ee，T中有两个字符ab的时候，我们是不是可以分解成：edit(1,1)=min(edit(0,1)+1, edit(1,0)+1, edit(0,0)+f(1,1)). 这样我们可以得到这样一些动态规划公式： 如果i=0且j=0 edit(0, 0)=1 如果i=0且j>0 edit(0, j )=edit(0, j-1)+1 如果i>0且j=0 edit( i, 0 )=edit(i-1, 0)+1 如果i>0且j>0 edit(i, j)=min(edit(i-1, j)+1, edit(i,j-1)+1, edit(i-1,j-1)+f(i , j) ) 小注：edit(i,j)表示S中[0.... i]的子串 si 到T中[0....j]的子串t1的编辑距离。f(i,j)表示S中第i个字符s(i)转换到T中第j个字符s(j)所需要的操作次数，如果s(i)==s(j)，则不需要任何操作f(i, j)=0； 否则，需要替换操作，f(i, j)=1 。 这就是将长字符串间的编辑距离问题一步一步转换成短字符串间的编辑距离问题，直至只有1个字符的串间编辑距离为1。 编辑距离的实际应用 在信息检索领域的应用我们在文章开始的时候就提到了。另外，编辑距离在自然语言文本处理领域(NLP)中是计算字符串相似度的重要方法。一般而言，对于中文语句的相似度处理，我们很多时候都是将词作为一个基本操作单位，而不是字(字符)。 字符串编辑距离源代码 Java代码 package net.hr.algorithm.stroper; /** * 字符串编辑距离 * * 这是一种字符串之间相似度计算的方法。 * 给定字符串S、T，将S转换T所需要的插入、删除、替代操作的数量叫做S到T的编辑路径。 * 其中最短的路径叫做编辑距离。 * * 这里使用了一种动态规划的思想求编辑距离。 * * @author heartraid * */ public class StrEditDistance { /**字符串X*/ private String strX=\"\"; /**字符串Y*/ private String strY=\"\"; /**字符串X的字符数组*/ private char[] charArrayX=null; /**字符串Y的字符数组*/ private char[] charArrayY=null; public StrEditDistance(String sa,String sb){ this.strX=sa; this.strY=sb; } /** * 得到编辑距离 * @return 编辑距离 */ public int getDistance(){ charArrayX=strX.toCharArray(); charArrayY=strY.toCharArray(); return editDistance(charArrayX.length-1,charArrayY.length-1); } /** * 动态规划解决编辑距离 * * editDistance(i,j)表示字符串X中[0.... i]的子串 Xi 到字符串Y中[0....j]的子串Y1的编辑距离。 * * @param i 字符串X第i个字符 * @param j 字符串Y第j个字符 * @return 字符串X(0...i)与字符串Y(0...j)的编辑距离 */ private int editDistance(int i,int j){ if(i==0&&j==0){ //System.out.println(\"edit[\"+i+\",\"+j+\"]=\"+isModify(i,j)); return isModify(i,j); } else if(i==0||j==0){ if(j>0){ //System.out.println(\"edit[\"+i+\",\"+j+\"]=edit[\"+i+\",\"+(j-1)+\"]+1\"); if(isModify(i,j) == 0) return j; return editDistance(i, j-1) + 1; } else{ //System.out.println(\"edit[\"+i+\",\"+j+\"]=edit[\"+(i-1)+\",\"+j+\"]+1\"); if(isModify(i,j) == 0) return i; return editDistance(i-1,j)+1; } } else { //System.out.println(\"edit[\"+i+\",\"+j+\"]=min( edit[\"+(i-1)+\",\"+j+\"]+1,edit[\"+i+\",\"+(j-1)+\"]+1,edit[\"+(i-1)+\",\"+(j-1)+\"]+isModify(\"+i+\",\"+j+\")\"); int ccc=minDistance(editDistance(i-1,j)+1,editDistance(i,j-1)+1,editDistance(i-1,j-1)+isModify(i,j)); return ccc; } } /** * 求最小值 * @param disa 编辑距离a * @param disb 编辑距离b * @param disc 编辑距离c */ private int minDistance(int disa,int disb,int disc){ int dismin=Integer.MAX_VALUE; if(dismin>disa) dismin=disa; if(dismin>disb) dismin=disb; if(dismin>disc) dismin=disc; return dismin; } /** * 单字符间是否替换 * * isModify(i,j)表示X中第i个字符x(i)转换到Y中第j个字符y(j)所需要的操作次数。 * 如果x(i)==y(j)，则不需要任何操作isModify(i, j)=0； 否则，需要替换操作，isModify(i, j)=1。 * @param i 字符串X第i个字符 * @param j 字符串Y第j个字符 * @return 需要替换，返回1；否则，返回0 */ private int isModify(int i,int j){ if(charArrayX[i]==charArrayY[j]) return 0; else return 1; } /** * 测试 * @param args */ public static void main(String[] args) { System.out.println(\"编辑距离是：\"+new StrEditDistance(\"eeba\",\"abac\").getDistance()); } } （2）PAT Tree子串匹配结构（转自http://hxraid.iteye.com/blog/615295，还得谢谢Heart.X.Raid啦~~~） Patricia Tree 简称PAT tree。 它是 trie 结构的一种特殊形式。是目前信息检索领域应用十分成功的索引方 法，它是1992年由Connel根据《PATRICIA——Patrical Algorithm to Retrieve Information Coded in Alphanumeric》算法发展起来的。 PAT tree 在字符串子串匹配 上有这非常优异的表现，这使得它经常成为一种高效的全文检索算法，在自然语言处理领域也有广泛的应用。其算法中最突出的特点就是采用半无限长字串(semi-infinite string 简称 sistring) 作为字符串的查找结构。 采用半无限长字串(sistring): 一种特殊的子串信息存储方式。 比如一个字符串CUHK。它的子串有C、CU、CUH、CUHK、U、UH、UHK、H、HK、K十种。如果有n个字符的串，就会有n(n+1)/2种子串，其中最长的子串长度为n。因此我们不得不开辟 n(n+1)/2个长度为n的数组来存储它们，那么存储的空间复杂度将达到惊人的O(n^3)级别。 但是我们发现这样一个特点： CUHK —— 完全可以表示 C、CU、CUH、CUHK UHK —— 完全可以表示 U、UH、UHK HK —— 完全可以表示 H、HK、 K —— 完全可以表示 K 这样我们就得到了4个sistring: CUHK、UHK、HK和K。 PAT tree的存储结构 如果直接用单个字符作为存储结点，势必构造出一棵多叉树(如果是中文字符的话，那就完蛋了)。检索起来将会相当不便。事实上，PAT tree是一棵压缩存储的二叉树结构。现在我们用“CUHK”来构造出这样一棵PAT tree 。 开始先介绍一下PAT tree的结点结构(看了后面的过程就再来理解这些概念) * 内部结点：用椭圆形表示，用来存储不同的bit位在整个完整bit sequence中的位置。 * 外部节点(叶子结点)： 用方形表示，用来记录sistring的首字符在完整sistring中的开始位置(字符索引)和sistring出现的频次。 * 左指针：如果 待存储的sistring在 内部结点所存储的bit位置上的数据 是0，则将这个sistring存储在该结点的左子树中。 * 右指针：若数据是1，则存储在右子树中。 (1) 将所有sistring的字符转化成1 bytes的ASCII码值，用二进制位来表示。形成一个bit sequence pattern(没有的空字符我们用0来填充)。 sistring bit sequence 完整sistring -> CUHK 010 00011 01010101 01001000 01001011 <- 完整bit sequence UHK0 010 10101 01001000 01001011 00000000 HK00 01001000 01001011 00000000 00000000 K000 01001011 00000000 00000000 00000000 (2) 从第一个bit开始我们发现所有sistring的前3个bit位都相同010，那么相同的这些0/1串对于匹配来说就毫无意义了，因此我们接下来发现第4个bit开始有所不同了。UHK 的第4个bit是1，而CUHK、HK、K的第4个bit是0。则先构造一个内部结点iNode.bitSize=4（第4个bit），然后将UHK的字符索引 cIndex=2(UHK的开始字符U在完整的CUHK的第2位置上)构造成叶子结点插入到iNode的左孩子上，而CUHK、HK、K放在iNode右子树中。(如下图2) (3) 递归执行第2步，将CUHK、HK、K进一步插入到PAT tree中。流程如下图所示。所有sistring都插入以后结束。 注意：既然PAT tree 是二叉查找树，那么一定要满足二叉查找树的特点。所以，内部结点中的bit 位就需要满足，左孩子的bit位< 结点bit 位< 右孩子的bit 位。 PAT tree的检索过程 利用PAT tree可以实现对语料的快速检索，检索过程就是根据查询字串在PAT tree中从根结点寻找路径的过程。当比较完查询字串所有位置后，搜索路径达到PAT tree的某一结点。 若该结点为叶子结点，则判断查询字串是否为叶子结点所指的半无限长字串的前缀，如果判断为真，则查询字串在语料中出现的频次即为叶子结点中记录的频次；否则，该查询字串在语料中不存在。 若该结点为内部结点，则判断查询字串是否为该结点所辖子树中任一叶子结点所指的半无限长字串的前缀。如果判断为真，该子树中所有叶子结点记录的频次之和即为查询字串的出现频次。否则，查询字串在语料中不存在。 这样，通过PAT tree可以检索原文中任意长度的字串及其出现频次，所以，PAT tree也是可变长统计语言模型优良的检索结构。 例如：要查找string= “CU ”(bit sequence=010 00 0 1 1 01010101) 是不是在CUHK 中。 (1) 根据“CUHK ”的PAT tree 结构( 如上图) ，根结点r 的bit position=4 ，那么查找bit sequence 的第4 个bit=0 。然后查找R 的左孩子rc 。 (2) rc 的bit position=5 ，在bit sequence 的第5 个bit=0 。则查找rc 的左孩子rcc 。 (3) rcc= ” CUHK ” 已经是叶子结点了，则确定一下CU 是不是CUHK 的前缀即可。 PAT tree 的效率 特点：PAT tree查找的时间复杂度和树的深度有关，由于树的构造取决于不同bit位上0,1的分布。因此PAT tree有点像二叉查找树 ，最坏情况下是单支树(如上图例子)，此时的时间复杂度是O(n-1)，n为字符串的长度。最好情况下是平衡二叉树 结构，时间复杂度是O(log2(N))。另外，作为压缩的二叉查找树，其存储的空间代价大大减少了。 PAT tree的实际应用 PAT tree在子串匹配上有很好的效率，这一点和Suffix Tree(后缀树)，KMP算法的优点相同。因此PAT tree在信息检索和自然语言处理领域是非常常用的工具。比如：关键字提取，新词发现等NLP领域经常使用这种结构。","title":"串和序列的编辑方法"},{"content":"说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern   Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可   以给到1+, 也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比   IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的   介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字   “Advances in Neural Information Processing Systems”, 所以, 与ICML/ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML/ECML/COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示/推理/学习等很多方面, AUAI   (Association of UAI) 主办, 每年开. tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal   Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁 道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说). 转自：http://blog.csdn.net/daiyuchao/article/details/5660622","title":"AI会议的总结（by南大周志华）"},{"content":"简单回顾一下咱在CSDN混的历史吧 02-03年刚毕业，搞.net，当时国内搞的人还少，资料少，经常混.net版。 04-05年，慢慢就不混了，偶尔提几个问题，也没有得到解答，还是自己搞定的。 05-07年，开了博客，也写过一些文章，主要是asp.net/winform/webservice/sqlserver的，那时候没有意识到这是个好的宣传自己的方式，把自己的探索过程写下来也是给予其他人帮助的一种方式，同时，也是自己的一个备忘的文档库。 07年开始，转型了，从windows下的.net转到了linux/c/python，从做平台转到做后台，时间也稍微多了些，写的比以前频繁了些，既有技术架构的，也有各种算法相关的。 CSDN是非常不思进取，这话说的有点重，看javaeye就知道，变为iteye了，明显从一个单一技术论坛转为综合性网站了，我也考虑过转到javaeye去，但是文章迁移是件麻烦事，最后作罢；而且，混javaeye的人的水平明显比csdn要高，csdn偏向于初级程序员，以及windows开发的程序员。 看到新界面，终于有Web2.0的感觉了，有点像中年男人焕发第二春，我觉得这是个好的开端，希望下一步，CSDN博主和网友的层次高一些，同时更多的关注开源社区。 最近在研究自然语言处理、检索、统计学习等方面的内容，有时间再总结总结吧。","title":"看到新的CSDN的博客界面，泪流满面"},{"content":"探索推荐引擎内部的秘密，第 1 部分: 推荐引擎初探 http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy1/index.html   探索推荐引擎内部的秘密，第 2 部分: 深入推荐引擎相关算法 - 协同过滤 http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/index.html   探索推荐引擎内部的秘密，第 3 部分: 深入推荐引擎相关算法 - 聚类 http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy3/index.html   LDA http://www.shamoxia.com/html/y2010/1384.html   SVD http://www.shamoxia.com/html/y2011/2840.html   NGram http://fendou.org/2010/07/23/n-gram-study-case/   后续会继续添加  ","title":"近期推荐引擎、机器学习、自然语言处理看到的一些资料"},{"content":"   The bag-of-words model is a simplifying assumption used in natural language processing and information retrieval. In this model, a text (such as a sentence or a document) is represented as an unordered collection of words, disregarding grammar and even word order.    词袋模型是在自然语言处理和信息检索中的一种简单假设。在这种模型中，文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序。    The bag-of-words model is used in some methods of document classification. When a Naive Bayes classifier is applied to text, for example, the conditional independence assumption leads to the bag-of-words model. [1] Other methods of document classification that use this model are latent Dirichlet allocation and latent semantic analysis.[2]    词袋模型被用在文本分类的一些方法当中。当传统的贝叶斯分类被应用到文本当中时，贝叶斯中的条件独立性假设导致词袋模型。另外一些文本分类方法如LDA和LSA也使用了这个模型。      Example: Spam filtering    In Bayesian spam filtering, an e-mail message is modeled as an unordered collection of words selected from one of two probability distributions: one representing spam and one representing legitimate e-mail (\"ham\"). Imagine that there are two literal bags full of words. One bag is filled with words found in spam messages, and the other bag is filled with words found in legitimate e-mail. While any given word is likely to be found somewhere in both bags, the \"spam\" bag will contain spam-related words such as \"stock\", \"Viagra\", and \"buy\" much more frequently, while the \"ham\" bag will contain more words related to the user's friends or workplace.     在贝叶斯垃圾邮件过滤中，一封邮件被看作无序的词汇集合，这些词汇从两种概率分布中被选出。一个代表垃圾邮件，一个代表合法的电子邮件。这里假设有两个装满词汇的袋子。一个袋子里面装的是在垃圾邮件中发现的词汇。另一个袋子装的是合法邮件中的词汇。尽管给定的一个词可能出现在两个袋子中，装垃圾邮件的袋子更有可能包含垃圾邮件相关的词汇，如股票，伟哥，“买”，而合法的邮件更可能包含邮件用户的朋友和工作地点的词汇。     To classify an e-mail message, the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags, and uses Bayesian probability to determine which bag it is more likely to be.     为了将邮件分类，贝叶斯邮件分类器假设邮件来自于两个词袋中中的一个，并使用贝叶斯概率条件概率来决定那个袋子更可能产生这样的一封邮件。","title":"Bag of words model (词袋模型)"},{"content":"网站开发工程师 1. 大学本科或硕士学历，计算机相关专业毕业。英语4级以上，能熟练阅读英文文档； 2. 两年以上相关经验 3. PHP、MySQL、C/C++、JAVA等至少一个领域属于专家水平； 4. 精通Linux, Apache, MySQL, PHP (LAMP)环境，各种应用配置，熟练掌握shell 脚本； 5. 了解常见数据结构和算法，并能熟练且适当的应用； 6. 具备良好的代码编程习惯及较强的文档编写能力； 7. 熟悉MySQL数据库应用开发，了解MySQL的配置管理、性能优化等操作技能； 8. 熟悉互联网基本原理及Ajax，DHTML，CSS等Web相关前端技术； 9. 具备软件系统设计及性能优化相关经验； 10. 有大规模、分布式、可扩展系统开发或管理经验者优先； 11. 具备强烈的进取心、求知欲及团队合作精神，认可组织文化和价值观。   资深网站开发工程师 1. 见多识广，遇到难题能够迅速的做出正确判断，并有效找出解决方案； 2. 在行业中有丰富的人脉关系，可以在招聘、技术交流、公司形象推广、合作机会探寻等方面，做出贡献； 3. 视角广阔，能够从各方面考虑问题，但又不忘自己的本行及专业是技术； 4. 能够承受压力，并在压力中，较好的掌握在寻求技术方案时，“快”与“好”两者之间的平衡； 5. 具备一定的领导能力，能够在技术指导、示范，或人事管理上，对于新进人员提供较大的帮助； 6. 四年以上相关经验, 有管理经验者优先； 7. 具体技术经验需求，见“网站开发工程师”职位需求。 系统架构工程师 1. 大学本科或硕士学历，计算机相关专业毕业。英语4级以上，能熟练阅读英文文档； 2. 两年以上相关经验 3. 参与过或深刻了解大型网站（如大型SNS网站）的技术架构的设计与优化； 4. 具备迅速理解、分析、解决网站运行中各种常见问题的能力； 5. 深刻认识网站speed，scalability，availability，security等需求，理解网站流量快速增长所带来的挑战并能妥善应对； 6. 熟悉LAMP环境，各种应用配置，熟练掌握shell 脚本； 7. 在Linux、C/C++、Mysql、Security、Networking等至少一个领域属专家水平； 8. 了解常见数据结构和算法，能熟练应用； 9. 具备良好的编程习惯及较强的文档编写能力； 10. 具备强烈的进取心、求知欲及团队合作精神，认可组织文化和价值观。 Flash 工程师 1. 熟练掌握Flash及相关周边交互多媒体设计软件，熟悉Flash的优化及后期发布流程，能够创作高效流畅应用； 2. 两年以上相关经验 3. 深度掌握Flash AS3 编程语言, 熟悉AS3的架构体系，能够用AS3创作高效完整的交互程序，熟练掌握Flash与后台之间的工作流程，能与后台开发工程师进行顺畅配合； 4. 具有美术功底、创意构思能力及网站设计经验者更佳； 5. 熟悉 PHP等后台编程语言更佳； 6. 具备强烈的进取心、求知欲及团队合作精神，认可组织文化和价值观； 7. 需附近期个人作品。 算法和数据挖掘工程师 1. 大学本科或本科以上数学、计算机或其他相关专业，英语4级以上，能熟练阅读英文文档； 2. 两年以上相关经验 3. 具备扎实的数学基础，严谨的思维逻辑； 4. 在机器学习、数据挖掘、推荐引擎、自然语言处理等至少一个领域属专家水平； 5. 扎实的编码能力与算法基础，精通C/C++语言开发； 6. 能够较好的掌握算法精准与效率之间的权衡； 7. 能够独立承担或带领队伍开展某个方面的研发工作； 8. 有大型项目开发经验者优先； 9. 具备强烈的进取心、求知欲及团队合作精神，认可组织文化和价值观。 广告销售经理 职位描述： 1.公司广告销售； 2.发掘新的网络广告客户和维系原有客户关系； 3.客户的认可度及投放比率、以及费用额度； 4.网络广告销售季度及年度任务。 职位要求： 1.大学本科或以上学历，市场营销专业优先； 2.有过网络广告销售经验，熟悉网络广告流程及销售工作； 3.有1-2年网络广告销售经验；1年以上4A公司网络广告客户经理经验；1-2年网络广告公司媒介经验；熟悉国际化公司办事流程及经验丰富者优先； 4.有门户网站销售经验或一定的客户基础者优先； 5.理解、表达、沟通能力强，有非常强的学习能力和客户开拓能力。 广告策划经理 职位描述： 1.广告销售策划支持； 2.大客户互联网营销方案撰写及提案； 3.产品的广告形式策划开发及包装； 4.行业数据调查收集分析，形成定期报告。 职位要求： 1.大学本科或以上学历，新闻.广告.中文.市场营销相关专业； 2.熟练使用OFFICE办公软件，熟练的英文书写和口语表达； 3.有在互联网相关行业的经验，熟悉互联网网站产品的业务模式；对社交网站产品有很好的理解，具有商业产品规划及推动经验； 4.3年以上互联网商业广告产品经验和大客户市场推广策划经验。 产品经理      职位描述： 1.负责网站产品策划、设计，提供产品设计、需求分析、功能描述等文档； 2.负责产品开发过程中的进度管理与跟踪，组织产品测试； 3.负责与协助产品上线以后的改进：BUG跟踪、收集改进意见、提供改进方案，引导用户熟悉使用产品； 4.负责babytree产品的数据及用户行为分析； 5.组织定期对行业相关产品的跟踪与分析。 职位要求： 1.本科以上学历，3年以上互联网产品策划、设计经验；善于分析用户需求，对网络用户体验有较多研究； 2.具备一定的项目管理经验，习惯于用严谨科学的统筹方法来保证工作按计划进行； 3.对国内外产品与技术发展有敏锐的观察力和极高的求知欲； 4.有良好的沟通能力和团队合作能力，有很高的执行力，善于沟通，耐心细致，心理承受力强； 5.具有社区网站、SNS网站、电子商务网站开发经验者更佳。 儿童教育产品策划 职位描述与要求：  1．热爱儿童教育事业，对孩子的教育问题充满探索精神；  2.  对儿童心理学、儿童教育理念有一定的了解，并有独到的教育理念；  3.  具有独立内容策划、文字编辑能力。正规出版机构编辑经验者优先；  4．教育相关专业优先；  5.  良好的团队协作能力，有创意意识；  6.  了解儿童教育市场。﻿     ","title":"07年的职位却不知当时在想什么"},{"content":"  NLP相关网站 §  ACL §  ACL Anthology §  ACL Anthology Network §  ACL Wiki §  CLSP §  CWB中文词库 §  EuroMatrix §  Freebase §  JHU Workshop §  LDC §  Moses §  nlper §  Powerset §  SRILM §  Statistical Machine Translation §  中国万维网联盟 §  中国中文信息学会 §  中文自然语言处理开放平台 §  机器翻译档案计划 §  欧洲议会平行语料库 §  知网 §  自然语言处理与信息检索共享平台","title":"nlp 相关网址"},{"content":"说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern   Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可   以给到1+, 也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比   IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的   介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字   “Advances in Neural Information Processing Systems”, 所以, 与ICML\\ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML\\ECML\\COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示\\推理\\学习等很多方面, AUAI   (Association of UAI) 主办, 每年开. tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal   Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说).","title":"会议排名"},{"content":"记得早年第一次从《人工科学》（西蒙 著，武夷山 译）看到 维诺格拉德（T.Winograd）设计的SHRDLU 系统， 给我留下了很深刻的印象：维诺格拉德（T.Winograd）设计的SHRDLU 系统，仅用一组有限的几何物体，就把 人机之间的对话活动描述得如此贴切，既可感，又可知。 可以说它既是自然人展示自己如何借助计算机实现自然语言理解的一个经典示例， 也是可怎样应用计算机有效进行自然语言处理的一个里程碑。 后来我又想，它究竟可以给我们哪些启示呢？ 首先，让我想到的是我们小时候练习绘画所用的一组有限的几何物体，可以说，比他用的那一组有限的几何物体的 种类还丰富，但是，其中，揭示的道理却几乎是一致的，这就是： 自然人如何观察、思考和交流，计算机可怎样模仿自然人所进行的观察、思考和交流，......。 同时，让我还想到是正因为计算机模拟自然人的观察和思考以及机器人模拟自然人的对话与活动均是严格受限的， 这才把自然科学和工程技术之间的关系给展现了出来，而且，活龙活现。 至于，哲学、人文、心智和社会科学诸方面的复杂情形， 在严格的逻辑、数学、自然科学和工程技术的限定条件之下，几乎均可暂时不用考虑。  致谢： 感谢冯老（志伟）和白硕先生，正是因为看到他的帖子“SHRDLU 人机对话系统”以及白硕先生的回帖或评论【附录1】， 这才又勾起了我的回忆和思考。还要感谢武夷山先生，因为我最早就是通过他翻译的《人工科学》读到SHRDLU 系统的。 当然，更要感谢维诺格拉德（T.Winograd）先生，因为如没他设计的SHRDLU 系统，也就谈不上其他人的介绍和对话。 附录1： 白硕2011-07-12 13:54:16 Winograd对于自然语言处理的贡献，不仅在于这个SHRDLU系统能懂得多少句话，更在于他利用积木世界给语义找到了一个很扎实的根基——有本体，有状态和关系，有改变状态和改变关系的动作，有动作得以成立的前提条件。这样的“理解”，虽然针对的是一个封闭的人工构建的世界，但是五脏俱全。对这个领域的后来者来说，他是一个不能跳过的里程碑式的人物。 附录2：维诺格拉德（T.Winograd）和他设计的SHRDLU 系统 T. Winograd http://hci.stanford.edu/winograd/  SHRDLU http://hci.stanford.edu/~winograd/shrdlu/ How SHRDLU got its name http://hci.stanford.edu/~winograd/shrdlu/name.html","title":"维诺格拉德（T.Winograd）设计的SHRDLU 系统及其给我的一些启示"},{"content":"  2008-11-16 20:21 来自水木，标题我给改了下 发信人: pennyliang (pennyliang), 信区: SearchEngineTech 标 题: 总结一下我的一些提问和感想 发信站: 水木社区 (Sun Nov 16 08:19:10 2008), 站内 Latent Dirichlet Allocation(LDA)模型是近年来提出的一种具有文本主题表示能力的非监督学习模型。 rocchio算法，读作“Rockey-O”。       LDA，就是将原来向量空间的词的维度转变为Topic的维度，这一点是十分有意义的。    例如，如果一个文档A，包含电脑这个关键词，那么A向量化后可能是,比如电脑这个词是    100万词汇中的第2维（便于举例），微机这个词是100万词汇中的第3维，维上的投影简单看作是tf，即文档中出现的次数。    A={x,2,0,...,x} 表示文档A中电脑出现了2次.x表示出现次数不care    B={x,0,3,...,x} 表示文档B中微机次数出现了3次。    如果是用词做维度的向量空间，做聚类也好，分类也好，A和B在电脑和微机上的这种向量表示，机器理解为A和B完全在表示不同的意义。而事实上，如果在词的高维空间上看，电脑和微机的维是很近似的，正交性是很低的。       如果能够将高维空间上，近义词或者表示接近的词的维度“捏“成一个维度，比如电脑和微机这两个词被捏成了第2维，但是每个词在这个维上的权重给与不同的度量（比如概率）。    这样上诉例子变为    A={x,2*pi,x,...x}，pi表示电脑这个词到Topic2的转移概率。    B={x,3*pj,x,...x}    这样，A和B看上去在第二个Topic上显示了一定的相关性。    由于Topic是被捏后的产物，每个Topic的正交性直观上看都很强，LDA开源的工具做出的结果可以把转移到TOpic最Top的那些词提取出来，都是十分相关或近似的词。而Topic与Topic之间显示出很大的差异性。    短文本分类的商业价值是很大的，在视频分类，广告分类上都可以看作是短文本分类问题，我有幸做了一些这方面的工作，其中提到的短文本的扩展是很好的思路。    问答系统商业价值也很巨大，特别是封闭领域的问答系统，可以拦截投诉，用户提问，降低人工成本。开放领域的问答系统商业上感觉前途有限，当然把搜索引擎的 搜索结果进一步精化的思路肯定是搜索引擎的一个方向，用户会越来越懒，搜索引擎已经让用户懒了一些，还需要让用户继续懒下去。 下面是baidu知道中有人对LDA的解释 lda是一个集合概率模型，主要用于处理离散的数据集合，目前主要用在数据挖掘（dm）中的text mining和自然语言处理中，主要是用来降低维度的。据说效果不错。 以下是在tm中对lda的定义： Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. 其实它还可以用在别的方面，早期是被用在自然语言处理的文本表示方面。因为他提供了一个理解相关词为什么在同一文档出现的框架解释模型。","title":"短文本分类或lda的分析(ZZ)（转载pennyliang (pennyliang),）"},{"content":"中科院计算所会议 序号     会议名称       会议介绍         代表领域 1ACM SIGCOMM: ACM Conf on Communication Architectures, Protocols & Apps ACM的旗舰会议之一，也是网络领域顶级学术会议，内容侧重于有线网络，每年举办一次，录用率约为10%左右。 网络通信领域 2 IEEE INFOCOM: The Conference on Computer Communications IEEE计算机和通信分会联合年会，由IEEE计算机通信技术委员会和IEEE通信协会联合举办，是信息通信领域规模最大的顶尖国际学术会议，录用率约为16%左右。这个每年一度的会议的主要议题是计算机通信，重点是流量管理和协议。 网络通信领域 3 IEEE International conference on communications IEEE国际通信大会，是IEEE通信学会的两大旗舰会议之一。每年举办一次，录用率约为30%左右。 网络通信领域 4 IEEE Globecom: Global Telecommunications Conference IEEE全球电信会议，IEEE通信学会的两大旗舰会议之一。覆盖包括语音、数据、图像和多媒体通信等热点问题的技术和其它活动。GLOBECOM每年一次，一般都在十一月举行，录用率约为30%-40%。 网络通信领域 5 IEEE ITC: International Test Conference 创始于1970年，是测试领域顶级学术会议，对工业界影响巨大。每年举办一次，近年参会人员规模达到数千人。 网络通信领域 6 IEEE The International Conference on Dependable Systems and Networks IEEE可靠系统和网络会议，是IEEE容错计算技术委员会主办的最重要的国际会议，也是可靠系统和网络领域历史最悠久，地位非常高的学术会议。 网络通信领域 7 ACM MobiCom: International Conference on Mobile Computing and Networking 无线网络领域顶级会议，录用率约为10%，每年举行一次。 无线网络领域 8 ACM SIGMETRICS: Conference on Measurement and Modeling of Computer Systems 偏重于建模和测量的重要国际会议，内容覆盖系统和网络，录用率为10%左右。 网络通信领域 9 MOBIHOC: ACM International Symposium on Mobile Ad Hoc Networking and Computing 无线网络领域新兴的重要国际会议，内容侧重于adhoc网络。 无线网络领域 10 IEEE International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。录用率约为18%左右。 分布式计算系统领域 11 IMC: Internet Measurement Conference 网络测量领域顶级的专业会议 网络测量 12 ICCV: IEEE International Conference on Computer Vision 领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇 计算机视觉，模式识别，多媒体计算 13 CVPR: IEEE Conf on Comp Vision and Pattern Recognition 领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 14 ECCV: European Conference on Computer Vision 领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇 模式识别，计算机视觉，多媒体计算 15 DCC: Data Compression Conference 领域顶级国际会议，录取率很低，每年一次，目前完全国内论文极少 数据压缩 16 ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少 机器学习，模式识别 17 NIPS: Neural Information Processing Systems 领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇） 神经计算，机器学习 18 ACM MM: ACM Multimedia Conference 领域顶级国际会议，全文的录取率极低，但Poster比较容易 多媒体技术，数据压缩 19 IEEE ICIP: International conference on Image Processing 图像处理领域最具影响力国际会议，一年一次 图像处理 20 IEEE ICME: International Conference on Multimedia and Expo 多媒体领域重要国际会议，一年一次 多媒体技术 21 IEEE VR:IEEE Virtual Reality IEEE虚拟现实会议，每年一次 虚拟现实领域 22 ACM VRST:ACM Virtual Reality Software and Technology 虚拟现实软件与技术ACM年会，一年一次 虚拟现实领域 23 CGI:Computer Graphics International 国际图形学会议，一年一次 图形学领域 24 ACL: The Association for Computational Linguistics 国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次 计算语言学，自然语言处理 25 COLING: International Conference on Computational Linguistics 计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次 计算语言学，自然语言处理 26 IEEE ICASSP: International Conference on Acoustics, Speech and Signal Processing 是语音和声学信号处理领域最权威的国际学术会议之一，也是图像、视频信号处理领域的权威会议之一，每年举办一次 信号处理 27 IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一次 自然语言处理 28 IEEE/ACM Design Automation Conference 顶级会议，在美国召开 IC设计领域 29 IEEE VLSI Test Symposium 一级会议，在美国召开 测试领域 30 IEEE/ACM Design, Automation and Test in Europe 一级会议，在欧洲召开 设计和测试领域 31 IEEE Asian Test Symposium 一级会议，在亚洲召开 测试领域 32 Ubicomp: International Conference on Ubiquitous Computing 国际普适计算年会，本领域最权威的学术会议之一，每年一次 普适计算 33 PerCom: IEEE International Conference on Pervasive Computing and Communications 本领域最权威的学术会议之一，每年一次 普适计算 34 EUC: The IFIP International Conference on Embedded And Ubiquitous Computing 普适计算与嵌入式系统峰会，一年一次，不仅仅是学术讨论，也有工业界和政府代表参加 普适计算与嵌入式系统 35 ICPS: IEEE International Conference on Pervasive Services 普适计算与服务会议，一年一次 普适计算 36 SenSys，ACM Conference on Embedded NEtworked Sensor Systems ACM主办传感器网络最有影响力的会议，由SIGCOMM, SIGMOBILE, SIGARCH, SIGOPS, SIGMETRICS, SIGBED等ACM的Special Interest Groups提供学术资助。从2003年开始，已经连续举办4届：03年收录24篇；04年收录21篇；05收录21篇文章；06年24篇收录文章。目前只能查到2004年的录用率，为14.5％。 传感器网络 37 SECON, IEEE Communication Society Conference on Sensor and Ad Hoc COmmunications and Networks, 由IEEE发起的会议，基本每年举行一次。近三年的录取率（04，05，06）分别为18.1% 27.2%和25.9％ 传感器网络 38 MASS, IEEE International Conference on Mobile Ad hoc and Sensor Systems 由IEEE、DARPA、 NSF和Army Research Office 发起的国际会议，基本每年举行一次。2006年录用率24% 传感器网络 39 The International Conference for High Performance Computing and Communication 每年11月举行（始于1989年）， Conference on High Performance Networking and Computing , http://www.sc-conference.org/ 高性能计算 40 CLUSTER 4, IEEE Int’l Conf. on Cluster Computing, http://grail.sdsc.edu/cluster2004/ 高性能计算 41 HPDC-, th IEEE Int’l Symp. on High-Performance Distributed Computing, Honolulu. http://hpdc13.cs.ucsb.edu 高性能计算 42 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 每年一次，http://storageconference.org/ 高性能计算 43 SuperComputing：The International Conference for High Performance Computing and Communications 高性能计算机方向公认的高水平会议之一，第一届会议是1988年，每年11月举行,4、5月份投稿。会议内容包括technical and education programs，workshops，tutorials以及展览的一系列活动供与会者参加。SuperComputing“is the one place that attendees can see tomorrow's technology being used to solve world-class challenge problems today”。官方网站为http://www.sc-conference.org 高性能计算 44 IEEE Int'l Conf. on Cluster Computing 该会议一般会以Cluster+年份作为会议名称，比如Cluster 2005。该会议主要讨论商业集群相关技术，包括“To achieve higher performance, scalability, and usability, research and development challenges remain in virtually all areas of cluster computing, including middleware, networking, algorithms and applications, resource management, platform deployment and maintenance, and integration with grid computing”。每年9月份举行，4、5月份投稿。 高性能计算 45 [ICDCS] International Conference on Distributed Computing Systems 由IEEE主办，开始于1979年，从84年起每年举办一次。这是分布式计算系统领域中历史最悠久的会议。ICDCS provides a forum for engineers and scientists in academia, industry, and government to present and discuss their latest research findings on a broad array of topics in distributed computing. 高性能计算 46 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing This conference is “a forum for presenting the latest research findings on the design and use of highly networked systems for computing, collaboration, data analysis, and other innovative tasks”. 每年6、7月份举行，2月份截稿，3月底确定 高性能计算 47 International Conference for High Performance Computing and Communications (IEEE/ACM Supercomputing Conference) 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年12月份召开，会上发布TOP500的下半年排名。 高性能计算 48 ACM International Conference on Supercomputing 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。 高性能计算 49 IEEE International Parallel & Distributed Processing Symposium IEEE 和ACM SIGARCH发起的并行处理国际会议。每年一次 高性能计算 50 IEEE International Conference on Parallel Processing IEEE发起的并行处理国际会议。每年一次 高性能计算 51 IEEE International Conference on High Performance Computing IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 52 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. Performance presents papers on the development and application of state of the art, broadly applicable analytic, simulation, and measurement-based performance evaluation techniques. We are interested in techniques whose aim is to evaluate a system's dependability, security, correctness, or power consumption as well as more traditional performance metrics. Of particular interest is work that furthers the state of the art in performance evaluation methods, or that creatively applies previously developed methods to gain important insights into key design trade-offs in complex computer and communication systems. 高性能计算 53 IEEE Annual Workshops on Workload Characterization. The meeting that began as Workshop on Workload Characterization (WWC) in 1998 is becoming a Symposium. New computer applications and programming paradigms are constantly emerging to complement new and improving technology. The design of next generation microprocessors and computer systems should be based on an understanding of today's emerging workloads. 高性能计算 54 International Symposium on Computer Architecture(ISCA) ISCA is the premier forum for computer architecture research 高性能计算 55 International Symposium on High Performance Computer Architecture(HPCA) 　 高性能计算 56 International Symposium on Microarchitecture (MICRO) The annual MICRO conference (co-sponsored by SIGMICRO) has been a key forum for presenting major breakthroughs in computing architecture, and has established itself as the premier conference on instruction level parallelism. 高性能计算 57 FAST: USENIX Conference on File and Storage Technologies, 存储领域最好的专业会议，该会议只针对存储相关的内容，属于本领域最顶级的会议。录取率非常低，现在的状况是基本上只有美国和加拿大最顶尖的研究小组在上面发表文章。每年举办一届。 存储领域 58 NASA/IEEE Conference on Mass Storage Systems and Technologies (MSST4)， 存储领域的专业会议，历史很长，在业界比较有影响 存储领域 59 SNAPI’4: International Workshop on Storage Network Architecture and Parallel I/Os, Antibes Juan-les-spins, French, 存储领域较好的专业会议 存储领域 60 IEEE SC: SC-High Performance Computing, Networking and Storage Conference 高性能计算领域最好会议之一 存储领域 61 IEEE International Workshop on Networking, Architecture, and Storages（IWNAS） 国内办的存储领域的国际会议 存储领域 62 IEEE International Conference on Autonomic Computing（ICAC） 自主计算领域的国际专业会议，从2004开始，每年举办一次。针对大规模计算机系统或软件系统而提出的自管理、自配置、自优化、自保护等概念。 自主计算 63 Proceedings of the International Conference on Measurements and Modeling of Computer Systems 性能测试、分析与模拟方面的顶级会议。一年一届，已经举办12届 性能研究 64 International Symposium on High Performance Computer Architecture(HPCA) 高性能计算领域最好会议之一，基本上都是最顶尖的研究小组在上面发文章 高性能计算 65 [HPDC] IEEE Int'l Symp. On High Performance Distributed Computing 高性能分布式计算领域的会议，一年一届，已经举办15届 高性能计算 66 IEEE Int'l Conf. on Cluster Computing 集群和高性能计算很有影响的会议 分布式系统 67 USENIX Annual Technical Conference 操作系统、体系结构方面最好的会议之一 计算机系统 68 IEEE/ACM Int'l Symp. on Cluster Computing & the Grid 集群和网格计算领域很好的会议 集群 69 International Symposium on Computer Architecture（ISCA ） 系统结构最好的会议，系统结构的旗舰会议。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 70 International Symposium on Microarchitecture（MICRO） 系统结构最好的会议之一。基本上是美国最顶尖的研究小组在上面发表文章，国内的人员很难 系统结构 71 HPCC：The International Conference for High Performance Computing and Communications 高性能计算领域较高的会议 高性能计算 72 IEEE International Conference on High Performance Computing IEEE发起的高性能计算国际会议。每年一次在印度举行。 高性能计算 73 Annual ACM International Conference on Supercomputing（ICS） 高性能计算领域的顶级会议，全世界从事高性能计算事业的每年一次的最重要的盛会之一。每年6月份召开，会上发布TOP500的上半年排名。 高性能计算 74 Symposium on Operating System Design and Implementation（OSDI） 操作系统最好的会议和SOSP交替举行，每两年一届，操作系统的旗舰会议。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难 操作系统 75 ACM Symposium on Operating Systems Principles （SOSP） 操作系统最好的会议和OSDI交替举行，每两年一届，操作系统旗舰会议操作系统。基本上是美国最顶尖的研究小组在上面发文章，其他地区要中极其困难 操作系统 76 Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems （ASPLOS） 操作系统和程序语言最好的会议之一，录取率也非常低，也是基本上只有美国最顶尖的研究小组能够在上面发文，其他地区的极其困难 操作系统，程序语言 77 Workshop on Hot Topics in Operating Systems （HOTOS） 操作系统最好的会议之一 操作系统 78 Proceedings of the International Conference on Parallel Processing（ICPP） 并行计算非常有影响的会议 并行计算 79 Annual IEEE Conference on Local Computer Networks（LCN） 　 网络 80 International Conference on Distributed Computing Systems（ICDCS） 分布式计算非常有影响的会议，每年一次 分布式计算 81 International Conference on Parallel and Distributed Computing, Applications and Technologies（PDCAT） 分布式计算很好的会议，每年一次，已经举办7届 分布式计算 82 IEEE International Parallel and Distributed Processing Symposium（IPDPS） 并行与分布式计算领域非常有影响的会议，每年一次 并行与分布式计算 83 ASPLOS: Architectural Support for Programming Languages and Operating Systems ASPLOS是由ACM主办的国际会议，每年一次。主要关注硬件、体系结构、编译、操作系统等研究方向，在国内外学术界很高的影响。 编译技术 84 CASES: International Conference on Compilers, Architectures and Synthesis for Embedded Systems CASES是ACM主办的国际会议，每年一次，主要关注编译器，体系结构和嵌入式系统等研究方向，录取率在20％－30％ 编译技术 85 CODES: International Conference on Hardware Software Codesign CODES是ACM主办的国际权威会议，每年一次。始于1994年。主要关注hardware/software co-design和嵌入式系统的system-level design。近两年的接收率为25％左右。 编译技术 86 DAC: Annual ACM IEEE Design Automation Conference DAC是电子电路设计方面的国际权威会议，始于1984年。主要关注芯片、电路以及系统设计的新工具和新方法。近两年的接收率为20％左右。 编译技术 87 ICFP: International Conference on Functional Programming Functional Programming方向的国际会议。主要关注functional programming的设计、实现、概念和使用。接收率在30％左右。 编译技术 88 ICS: International Conference on Supercomputing ICS是由ACM SIGARCH主办的国际会议，每年一次。主要关注高性能计算机和计算等方面的研究，在国内外学术界很高的影响。接收率30％左右。 编译技术 89 ICSE: International Conference on Software Engineering 软件工程方向的权威会议，接收率不到20％。除了main conference之外，还包括tutorials, workshops, symposia以及collocated conferences。 编译技术 90 ISCA: International Conference on Computer Architecture ISCA是由IEEE 和ACM主办的国际会议，每年一次。主要关注处理器结构、存储结构、功耗等方面的研究，在国内外学术界很高的影响。接收率20％左右。 编译技术 91 ISMM: International Symposium on Memory Management 内存管理方向的国际会议，主要关注garbage collection, dynamic storage allocation, storage management implementation techniques，另外也包括interactions with languages and operating systems, and empirical studies of programs' memory allocation and referencing behavior 编译技术 92 ISSTA: International Symposium on Software Testing and Analysis ISSTA是国际顶级的关于software test and analysis的会议。近年来两年举办一次。和它同时举行的还有Formal Methods in Software Practice Workshop。 编译技术 93 LCTES: Language, Compiler and Tool Support for Embedded Systems 关注languages, compilers和tools for embedded systems的国际会议。每年一次，接收率在25％左右。 编译技术 94 MICRO: International Symposium on Microarchitecture 计算机体系结构方向的国际权威会议。关注计算机体系结构领域的重大发展。同时也是指令级并行方向的顶级会议。 编译技术 95 OOPSLA: Conference on Object Oriented Programming Systems Languages and Applications Object technology和面向对象程序设计领域的国际权威会议。涉及的具体方向有patterns, refactoring, aspect-oriented programming, dynamic compilation and optimization, unified modeling language, and agile methods。接收率20％左右。 编译技术 96 PLDI: Conference on Programming Language Design and Implementation PLDI是由ACM主办的国际会议，每年一次。主要关注编程语言的设计与实现等方面的研究工作，在国内外学术界很高的影响。 编译技术 97 PODC: Annual ACM Symposium on Principles of Distributed Computing 关注分布式系统的理论，设计，实现，规范等领域的国际会议。近年的接收率在25％左右。 编译技术 98 POPL: Annual Symposium on Principles of Programming Languages 关注programming languages, programming systems, and programming interfaces的design, definition, analysis, and implementation的国际权威会议，始于1973年，接收率不到20％ 编译技术 99 PPoPP: Principles and Practice of Parallel Programming PPoPP是由ACM主办的国际会议，两年一次。主要关注并行编程方面的研究，在国内外学术界很高的影响。接收率30％。 编译技术 100 SIGMETRICS: Joint International Conference on Measurement and Modeling of Computer Systems 关注计算机系统性能方面的theory, practice and case studies的国际会议 编译技术 101 SIGSOFT: Foundations of Software Engineering Software Engineering领域的权威会议。2005年以前的接收率一般不足20％。 编译技术 102 ASE - IEEE International Conference on Automated Software Engineering 关注软件开发自动化的顶级会议。前身为KBSE(Knowledge-Based Software Engineering)，始于1997年。 编译技术 103 CGO - International Symposium on Code Generation and Optimization CGO是由IEEE CS和ACM SIGMICRO主办的国际会议，每年一次。主要关注代码生成和优化等方面的研究，在国内外学术界很高的影响。接收率30％左右。 编译技术 104 CLUSTER - IEEE International Conference on Cluster Computing CLUSTER是由IEEE主办的国际会议，每年一次。主要关注群集计算方面的研究，在国内外学术界很高的影响。 编译技术 105 DATE - Design, Automation, and Test in Europe 关注Design methodologies, CAD languages, algorithms and tools, Testing of electronic circuits and systems, and Designer experiences的国际会议 编译技术 106 EURO-PDP - Euromicro Conference on Parallel, Distributed and Network-Based Processing EUROMICRO的一个workshop，关注并行和分布式计算。 编译技术 107 HPCA - International Symposium on High-Performance Computer Architecture HPCA是由IEEE主办的国际会议，每年一次。主要关注高性能计算方面的研究，在国内外学术界较高的影响。 编译技术 108 HPCS - Annual International Symposium on High Performance Computing Systems and Applications IEEE主办，每年在加拿大的大学召开。但参会者不局限于加拿大的研究人员。其内容涵盖了HPC的各个领域 编译技术 109 ICDCS - International Conference on Distributed Computing Systems IEEE主办，涉及分布式计算方面各个领域的权威会议。起始于1979年。 编译技术 110 ICPADS - International Conference on Parallel and Distributed Systems IEEE主办，关注并行和分布式系统的国际会议 编译技术 111 IISWC - IEEE International Symposium on Workload Characterization 关注characterization of computing system workload的国际会议。涵盖applications, middleware, system的behavior。Benchmark的构造和分析，以及程序行为的modeling等领域。 编译技术 112 IPDPS - International Parallel and Distributed Processing Symposium IEEE主办，并行和分布式处理相关的国际会议。包括并行处理的算法，应用，体系结构，以及和并行处理相关的软件如语言，编译器，运行时系统等。 编译技术 113 ISPASS - IEEE International Symposium on Performance Analysis of Systems and Software IEEE主办，关注计算机软硬件设计中的性能分析。 编译技术 114 PACT - International Conference on Parallel Architectures and Compilation Techniques PACT是由IEEE CS、ACM SIGARCH和IFIP主办的国际会议，每年一次。主要关注并行体系结构、针对并行计算机系统的编译等方面的研究，在国内外学术界很高的影响。 编译技术 115 RTSS - IEEE Real-Time Systems Symposium 实时系统研究的顶级会议，IEEE主办，已经举行了27届。 编译技术 116 RTAS - IEEE Real-Time and Embedded Technology and Applications Symposium 关注实时和嵌入式计算的基础结构，理论，system support的国际会议。 编译技术 117 SC - IEEE/ACM SC Conference SuperComputing的简称。关注的领域为HPC，networking，storage and analysis。ACM和IEEE合办。 编译技术 118 LCPC - International Workshop on Languages and Compilers for Parallel Computing 始于1988年，涉及编程语言的各个方面，包括compiler techniques, run-time environments, and compiler-related performance evaluation for parallel and high-performance computing。 编译技术 119 CC: International Conference on Compiler Construction 关注的领域涉及程序的各个方面，包括编译器构造，run-time技术，prgramming tools, 新编程语言。也包括一些特别的领域如parallel, ditributed, embedded, mobile, low power code, hardware等。是ETAPS的member conference。影响因子0.83 编译技术 120 HiPEAC - International Conference on High Performance Embedded Architectures & Compilers 关注嵌入式系统的发展，包括处理器设计，编译优化等。 编译技术","title":"中科院计算所会议"},{"content":"《浪潮之巅》豆瓣主页：http://book.douban.com/subject/6709783/ 互动网预定页面：http://product.china-pub.com/198405 快书包预定页面：http://www.kuaishubao.com/62/1199.html序最早看到吴军的《浪潮之巅》，是在Google（谷歌）黑板报上。2007年，时任研究员的吴军，应邀为谷歌黑板报撰写文章，介绍他对互联网和IT业界兴衰变化的观察和思考。由于文章篇幅较长，被单列为“浪潮之巅”栏目分次刊出。设立该栏目的直接收获就是，谷歌黑板报随后人气大增，增加了大批的追随者。作为《浪潮之巅》的最早一批读者，我当时就感觉，这个系列完全应该编纂成书，如今，这个感觉变成了现实。对于吴军，我比较熟悉，因为在语音识别领域，我们都有着共同的研究兴趣，并曾作为同事有过很多交流。吴军早年在清华大学获得学士和硕士学位后，赴美国约翰霍普金斯大学攻读计算机博士学位，致力于语音识别、自然语言处理等领域的研究。我在2005年加入Google时，吴军已经在Google工作，参与主持了许多研发项目，并在国内外发表过数十篇论文、获得和申请了近十项美国和国际专利。我认识很多顶尖的工程师，但具备强大叙事能力的优秀工程师，我认识的可以说是凤毛麟角，而吴军是其中之一。从AT&T、微软、Google、思科等引领整个时代浪潮的公司历史叙述，到硅谷之所以成为科技中心所依靠的天时、地利、人和因素，再到科技公司发展壮大过程中风险投资、银行、产业规律各自扮演的角色，以及新时代背景下金融危机和云计算为科技产业带来的冲击和革命…虽然每个人的观点不尽相同，但是通过这本书中看似波澜不惊的行文，你会读出一个从事互联网行业十多年的“老行家”个人独到的见解，以及一个身处“浪潮”中的“弄潮儿”的切身体会。作为“兼才”，《浪潮之巅》恰恰因此具备了两方面的优势。首先，作为一位曾每天与程序、算法、科研打交道的Google最优秀的研究员，势必能更客观地描述那些科技公司的兴衰得失，不会人云亦云，更不至于离题万里；第二，作为一位拥有写作天赋的工程师，吴军能够确保文章的有趣与可读，不会容忍自己的作品成为一本呆板的教科书式读物。《浪潮之巅》又不仅是一部提供“快乐阅读”的大公司商业史，它融汇了作者多年来的所见所闻，更包含了大量的独立思考与独特见解。这份心血，不仅是他个人的天赋使然，也是他始终在研究领域孜孜不倦的成果。值得一提的是，吴军的文章，没有将目光局限在大洋彼岸，内容上也不仅是停留在对若干巨头企业的探查。作者试图从整个产业链上向读者揭示科技公司的运作规律，并通过大量的调研与观察，客观分析中国本土企业在这次科技浪潮中的地位与影响。实际上，作者吴军本人也已离开了Google，目前正在一家中国著名互联网公司担任其核心业务的领军人物。《浪潮之巅》不是一本历史书，因为书中着力描述的，很多尚在普及或将要发生，比如微博与云计算，又比如对下一代互联网科技产业浪潮的判断和预测。从文字中可以看出，作者对科技、对创新、对互联网都充满“虔诚”信仰，并为之激情四射。我想，对所有身处并热爱高科技行业的人来说，对所有渴望创新、欣赏创新的中国创业者来说，《浪潮之巅》都是一本可读性很强的作品，足以做到“开卷有益”！李开复","title":"李开复：身处浪潮中的“弄潮儿”的切身体会"},{"content":"Twitter 替代 微博时代行将过渡，微媒时代即将到来! 不错，所谓的高官|明星|名人|红人就在这里哈www.gg3m.com! 马上关注鸽姆微媒吧，再不来你就要OUT勒~ [导读]微软亚洲研究院的技术依然是这次技术节的重要组成部分，约有36个项目被选定在这次技术节上展示，所涉领域涵盖自然用户界面、语音识别、自然语言处理与机器翻译、交互设计等领域。 2010年微软技术节(TechFest2010)公开日于3月2日在美国总部正式举行，微软技术节是微软研究院年度内最大的技术展示活动，今年共有来自全球6大微软研究院的150多个项目齐聚一堂，接受微软内部产品部门的检阅。与往年一样，微软亚洲研究院的技术依然是这次技术节的重要组成部分，约有36个项目被选定在这次技术节上展示，所涉领域涵盖自然用户界面、语音识别、自然语言处理与机器翻译、交互设计等领域。下面让我们一起来看看微软亚洲研究院在本次技术节上的精彩亮相吧! 一、移动3维交互场(MobileSurface) 智能触控技术为移动计算提供了一个新的人机交互界面。移动3维交互场(MobileSurface)通过使用摄像投影传感系统能够将任意的，比如办公桌面、餐台、茶几等表面或一张普通的纸变成可触控的平台，从而让用户能够在移动设备，如手机上，体会到和微软Surface类似的智能触控体验，而且这项技术在平面多点智能触控的基础上，还能够为移动设备提供3D空间的自然交互。比如用户将手机的内容投影到任意桌面，可以触控或3D的手势实现交互操作。而且这个系统还能实时捕获放入投影区域物体表面的数据模型，从而为增强现实应用提供了支持平台。移动自然交互面能够为手机/移动设备提供一个自然的交互平台，可以使你随时随地的与你身边的数字信息进行自然交互。 这是在技术节现场搭建的表面触控平台，从画面上可以看到有两只鼓投影在了桌面上。用户可以拿着鼓棒直接在空中或者表面敲打，后面的小屏幕会马上重现你的整个演奏的乐点。 一个摄像头、一个投影仪就可以在桌面上打起虚拟的鼓来。如果你觉得拿着鼓棒敲击麻烦，你也可以直接用手去敲打。 二、置身云端——全新云计算交互体验(Insidecloud:newinteractionfor cloud computing) 云计算开启了新型交互时代，云计算的强大能力让用户可以随时随地的访问他们的个人数据，可以尽情享用丰富多彩的新型网络服务，也给了用户更大的空间和能力在信息时代更自由的遨游。置身云端研究项目在探讨新型云计算交互体验，包括新型交互设备——云鼠标，信息组织形式——云界面。     每个人都会有一片属于自己的云，有一个自己的云鼠标，它是通向个人云端的钥匙，同时它又是一个拥有6个自由度、触控反馈体验的自然交互设备。云界面在现有的二维界面的基础上增加时间、空间维度，将用户纷繁复杂的信息有机的整理在一起，并以更自然的方式呈现。云鼠标结合新的自然的云界面，用户如身处云端，沉浸在个性化、智能化、有组织的个人信息数据和浩瀚的网络数据与应用之中。所有的对象漂浮在空中，用户可以使用云鼠标在其中轻松的漫游，自然的交互。 三、同声传译电话(T3:TheTranslating!Telephone) 这是一个能同声传译电话的系统。道格拉斯·亚当斯的巴别鱼激发了构建无拘无束的通用通讯的梦想。虽然我们还远未实现这一目标，但是目前有限的精确度仍能在许多场景创造价值。我们在电话通话场景中的目标就是在没有其他通信手段存在的条件下，提供一个跨语言沟通的辅助工具。该系统充分使用了说话人自适应技术，以达到合理的实时语音到文字转录的准确性。然后，又通过机器翻译来提供语音到文本的翻译，并进一步利用文语转换系统来最终实现语音到语音的翻译。识别文字和翻译文本都会显示给用户，使他们能够验证他们的意图。我们将用一个德语和英语的现场会话来演示这个系统。     四、肌肉运动指挥计算机(InputwithMuscle-ComputerInterfaces) 微软公司一直致力于研究利用传感器等元件来实现新颖的并具有较强交互感的物理设备。微软亚洲研究院的人机交互组负责人DesneyTan博士展示了一项最新技术——将手臂的运动直接作为鼠标来使用——依赖于肌肉的运动和变化来指挥计算机进行各式各样的交互体验。只要在手臂上戴上传感器，可以握着电脑中的吉他自如地弹奏起来。在空中轻轻挥动手指，计算机便可以跟踪运动手指的轨迹……等等。 五、基于人脸识别、事件匹配的社会化相册聚合服务OneAlbum 微软以色列研发中心创新实验室在TechFest2010上演示了OneAlbum–可自动根据人脸、事件等条件智能聚合来自社会化相册服务Flickr、Facebook的照片。 或许，OneAlbum所实现的功能才是WindowsLivePhotos相册服务该有的社会化气质： 照片的人脸识别[1]，而且可从你好友的社会化相册服务中自动去识别和你相册中出现的人脸有关的照片;事件(活动、聚会)匹配，综合拍摄日期、照片内的人脸、照片颜色识别出同一事件的照片。 结合这两项功能，再加上些社会化和自我算法改进的功能就可开始将与你有关的照片整合于OneAlbum中，这些照片可以是来自Flickr、Facebook的。 更关键的是，OneAlbum无需人工干预，甚至你都不需要输入描述标签，它自动可以从照片本身的属性中挖掘与你的有关的照片。 我始终认为，微软有必要将OneAlbum概念整合入不温不火的WindowsLivePhotos服务底层。至少从4的整合社会化的趋势来看，OneAlbum概念绝对可行，而且不会添乱，甚至可以想象基于OneAlbum概念的Bing社会化图片搜索引擎。 我想说的就是上面这些了，下面附上图例一枚。 六、手机搜索本地缓存技术SONGO SONGO全称为SearchOntheGo(译为移动搜索)，是由微软研究院在TechFest2010上演示了手机搜索和广告缓存技术，支持用户即时搜索个人文件、Web搜索结果、本地搜索结果，而且是跨平台的，这意味着可通过手机、PC、云访问这些缓存的信息。 更酷的是，SONGO可不依靠互联网，本地执行用户的搜索。 目前已经有两款SONGO原型应用已经完成了，其一是Facebook实时搜索，另一款是QuickAds实时商家查询应用。下面附上微软研究院的SONGO原理框图和具体讲解： 首先，SONGO在手机端本地缓存根据其他用户搜索产生的流行搜索结果、商家和广告的记录数据，并保证每天更新。随着用户的使用，SONGO将分析用户点击，以完成以下两个任务：1)扩展缓存库数据量;2)调整缓存数据的重要排名，以提供个性化搜索体验。通过对1亿的移动搜索查询量的分析：平均66%的搜索可通过缓存了2,500链接的1MB缓存数据库来提供合适的结果。 WindowsPhone平台上的原型应用测试数据表明：SONGO比通过3G网络快16倍，并能省25倍的电量。 SONGO是基于Bing的，一旦时机和技术成熟，SONGO将毫无悬念地整合于Bing移动版。不过，我仍很怀疑上面加粗的分析结果。 另外，我将在下篇中介绍微软研究院在TechFest2010上演示的图像&视频、照片分享相关的项目。换句话说，这些技术也可能影响WindowsLivePhotos、WindowsLive Photo Gallery 和WindowsLiveMovie Maker的未来。 七、Project Gustav ProjectGustav是一款现实数字绘画板原型项目，给予使用者最接近真实绘画体验，其包含目前世界最先进的自然绘图、混色、笔触算法。恐怕ProjectGustav也是一款Surface衍生项目，看图：  ","title":"微软技术节（TechFest 2010）最前沿技术汇总"},{"content":"前 前言 有幸见证历史近一百多年来，总有一些公司很幸运地、有意识或无意识地站在技术革命 的浪尖之上。一旦处在了那个位置，即使不做任何事，也可以随着波浪顺 顺当当地向前漂十年，甚至更长的时间。在这十几年到几十年间，它们代 表着科技的浪潮，直到下一波浪潮的来临。从一百多年前算起，AT&T 公司、IBM 公司、苹果（Apple）公司、英特 尔（Intel）公司、微软（Microsoft）公司、思科（Cisco）公司、雅虎（Yahoo） 公 司 和 Google 公 司， 也 许 还 有 接 下 来 的 Facebook 公 司， 都 先 后 被 幸 运 地推到了浪尖。虽然，它们来自不同的领域，中间有些已经衰落或正在衰 落，但是它们都极度辉煌过。它们都曾经是全球性的帝国，统治着自己所 在的产业。这些公司里面大大小小的人在外人看来都是时代的幸运儿。因为，虽然对 于一个公司来讲，赶上一次浪潮不能保证它长盛不衰；但是，对于一个人 来讲，一生赶上这样一次浪潮就足够了。对于一个弄潮的年轻人来讲，最 幸运的莫过于赶上一波大潮。加拿大作家格拉德威尔（Gradwell）在《异类》（Outliers）一书中介绍了 这样一个事实：在人类历史上最富有的 75 人中，有 1/5 出生在 1830~1840 年 的 美 国， 其 中 包 括 大 家 熟 知 的 钢 铁 大 王 卡 内 基 和 石 油 大 王 洛 克 菲 勒。 这 一 不 符 合 统 计 规 律 的 现 象 的 背 后 有 着 其 必 然 性， 他 们 都 在 自 己 年 富 力强（30~40 岁） 时， 赶 上 了 美 国 内 战 后 的 工 业 革 命 浪 潮。 这 是 人 类 历 史 上产生实业巨子的高峰年代。而第二个高峰年代就是从上世纪 50 年代末 到 70 年 代 初 的 20 年 间， 出 现 了 苹 果 公 司 创 始 人 史 蒂 夫• 乔 布 斯（Steve Jobs）、微软公司创始人比尔•盖茨(Bill Gates）、太阳公司的创始人 安迪•贝托谢姆（Andy Bechtolsheim）和比尔•乔伊（Bill Joy）、戴尔公 司的创始人迈克尔•戴尔（Michael Dell）、Google 的创始人拉里•佩奇（Larry Page）和谢尔盖•布林（Sergey  Brin）等，因为他们在自己年富 力 强 时 幸 运 地 赶 上 了 信 息 革 命 的 大 潮。 而 这 恰 恰 发 生 在 我 们 现 在 这 个 时 代，我们每一个人都有幸亲历了信息革命的历史。要预测未来是很难的，但是看看过去和现在，我们也许能悟出一些道理。 我 希 望 将 我 这 些 年 来 看 到 的 和 听 到 的 人 和 事 拿 出 来 与 大 家 分 享。 帮 助 读 者，尤其是年轻的读者，对当今世界科技产业的发展有系统的了解。我会 谈 一 谈 我 对 每 次 浪 潮 的 看 法， 对 上 述 每 个 公 司 的 看 法， 以 及 对 其 中 关 键 人 物 的 认 识。 在 极 度 商 业 化 的 今 天， 科 技 的 进 步 和 商 机 是 分 不 开 的。 因 此，我也要提到间接影响到科技浪潮的风险投资公司，诸如 KPCB 和红杉 资本（Sequoia  Capital），以及百年来为科技捧场的投资银行，例如高盛（Goldman Sachs），等等。本 书 最 初 应 崔 瑾 女 士 的 约 稿， 以 博 客 的 形 式 在 Google 黑 板 报 上 连 载。 Just-Pub 出 版 团 队 负 责 人 周 筠 女 士 读 后 一 直 在 热 情 地 向 读 者 推 荐， 并 且 后 来 和 我 约 稿 出 版 实 体 书。 这 前 前 后 后 有 三 四 年 的 跨 度， 信 息 科 技（Information Technologies，简称 IT）产业的世界格局也发生了较大的 变化，因此，这次在出版实体书时，我不仅补齐了全部的章节，也对原有 章节进行了大规模的修改。书中适当地保留了一些章 节的原貌，以帮助读 者了解我的思考过程。本书的结构有些独特，经常是介绍完一些公司后，中间穿插着一些其他的 话题。一些媒体的朋友问我为什么这样组织全书？原因是为了帮助读者理解一些公司的决策的原因和它们的商业模式，必须提前介绍和它们相关的IT 产 业 的 一 些 规 律。 比 如， 在 介 绍 英 特 尔 公 司 和 微 软 公 司 时， 就 一 定 会 谈 及 半 导 体 行 业 的 摩 尔 定 律 和 微 机 时 代 的 WinTel 体 系 1， 以 及 由 此 产 生的微机时代 IT 产业的生态链。我们在后面分析其他公司时，又会多次提到这个生态链，因此必须在英特尔 / 微软之后，其他章节之前对 IT 的一 些规律先做介绍。而对于影响科技公司业务和发展的幕后推动力，包括投 资银行和风投公司，也必须在谈及一些公司的决策前先对其做一番介绍。 因此，这本书在章节上做了一些特殊的安排。但是，全书基本还是按照时 间顺序展开，也就是从 AT&T 和 IBM 这两个百年老店讲起，到微机时代， 再到互联网时代，以及现在的云计算和互联网 2.0 时代。在我写作的过程中，得到了很多人的帮助和鼓励，包括李开复博士、崔瑾 女 士、 周 筠 女 士、Google 和 腾 讯 的 数 百 名 年 轻 人， 以 及 成 千 上 万 名 的 博 客 读 者。Google 北 京 的 工 程 师吴 根 清、 宿 华 和 单 久 龙 先 生 帮 助 我 校 对 了 部分章节，在此我对他们表示衷心的感谢。尤其需要感谢的是我的妻子张 彦女士和我的母亲朱秀珍女士，她们不仅是我博客的第一读者，而且在我写作中给予我不断的鼓励和帮助。 吴军2011 年 5 月于深圳腾讯公司==============目录i\t序言（李开复）iii\t前言　有幸见证历史1\t第 1 章　帝国的余辉—AT&T 公司AT&T 100年 来 发 展 得 非 常 健 康。 虽 然 它 一 直 受 反 垄断法的约束，但是美国政府司法部并没有真正要过它的命，每一次反垄断其实是帮助AT&T修枝剪叶，然后让它发展得更好。1　百年帝国2　几度繁荣3　利令智昏4　外来冲击 结束语15\t第 2 章　蓝色巨人—IBM 公司 郭士纳在到IBM以前也是做(芯)片的，但是，是土豆芯片1　赶上机械革命的最后一次浪潮2　领导电子技术革命的浪潮3　错过全球信息化的大潮4　他也是做（芯）片的5　保守的创新者 6   内部的优胜劣汰7　后金融危机时代 结束语38\t第 3 章　“水果”公司的复兴—乔布斯和苹果公司 在每一次技术革命中，新技术必须比老的技术有数量 级的进步才能站住脚。1　传奇小子2　迷失方向3　再创辉煌4　大难不死5　i 十年 结束语54\t第 4 章　计算机工业的生态链一 个IT公 司 如 果 今 天 和18个 月 前 卖 掉 同 样 多 的、 同样的产品，它的营业额就要降一半。1　摩尔定律2　安迪 – 比尔定律3　反摩尔定律 结束语66\t第 5 章　奔腾的芯 — 英特尔公司  英特尔的CEO格罗夫虽然是学者出身，但他同时也是微机时代最优秀的领导者和管理者，数次被评为世界上最好的CEO。1　时势造英雄2　英特尔、摩托罗拉之战3　指令集之争4　英特尔和 AMD 的关系5　举步艰难 结束语82\t第 6 章　IT 领域的罗马帝国—微软公司 当乔布斯给盖茨看了新设计的麦金托什个人电脑，以 及漂亮的基于图形界面的操作系统时，盖茨惊呆了。 那一年，乔布斯和盖茨都是 26岁。1　双雄会2　亡羊补牢3　人民战争4　帝国的诞生5　当世拿破仑6　尾大不掉7　条顿堡之战8　客厅争夺战 结束语 110\t第 7 章　互联网的金门大桥—思科公司据说斯坦福两个系的计算中心主管莱昂纳多•波萨卡 和桑迪•勒纳要在计算机上写情书，由于各自管理的 网络不同，设备又是乱七八糟，什么厂家的、什么协 议的都有，互不兼容，情书传递起来很不方便，于是 两人干脆发明了一种能支持各种网络服务器、各种网 络协议的路由器。于是思科公司赖以生存的“多协议 路由器”便诞生了。1　好风凭借力2　持续发展的绝招3　竞争者4　诺威格定律的宿命 结束语125\t第 8 章　英名不朽—杨致远、菲洛和雅虎公司 一百年后，如果人们只记得两个对互联网贡献最大的 人，那么这两个人很可能就是杨致远和菲洛。1　当世福特2　流量、流量、流量3　成也萧何，败也萧何4　既生瑜，何生亮5　红巨星6　自废武功 结束语 147\t第 9 章　硅谷的见证人—惠普公司作为硅谷最早的公司，惠普见证了硅谷发展的全过程， 从无到有，从硬件到软件，惠普的历史从某种程度上 讲就是硅谷历史的缩影。1　昔日硅谷之星2　有争议的生死抉择3　最有争议的 CEO4　亚洲制造的冲击5　峰回路转 结束语163\t第 10 章　没落的贵族—摩托罗拉公司 如果我们认为公司之中也有所谓的贵族，摩托罗拉无 疑可以算一个。曾几何时，摩托罗拉就是无线通信的 代名词，同时它还是技术和品质的结晶。1　二战的品牌2　黄金时代3　基因决定定律4　铱星计划5　全线溃败6　回天乏力 结束语 182\t第 11 章　硅谷的另一面美国的硅谷只占国土面积万分之五，却创造了无数的商 业神话。在这里，大约每10天便有一家公司上市。美 国前100强的公司中，硅谷占了四成。1　成王败寇2　嗜血的地方3　机会均等4　硅含量不断降低5　亘古而常青 结束语204\t第 12 章　短暂的春秋—与机会失之交臂的公司 在人类命运降临的伟大瞬间，市民的一切美德 — 小 心、顺从、勤勉、谨慎，都无济于事，它始终只要求 天才人物，并且将他造就成不朽的形象。命运鄙视地 把畏首畏尾的人拒之门外。命运 — 这世上的另一位 神，只愿意用热烈的双臂把勇敢者高高举起，送上英 雄们的天堂。1　太阳公司2　Novell 公司3　网景公司4　RealNetworks结束语 236\t第 13 章　幕后的英雄—风险投资对于想找投资的新创业公司，红杉资本有一些基本要 求 — 公司的业务要能几句话就讲得清楚。红杉资本 的投资人会给你一张名片，看你能不能在名片背面的 一点点地方把你想做的事情写清楚。1　风投的起源2　风投的结构3　风投的过程4　投资的决策和公司的估价5　风投的角色6　著名的风投公司 结束语265\t第 14 章　信息产业的规律性 人类的文明和技术是不断进步的，旧的不去，新的不 来，只有清除掉阻碍我们进步的那些庞大的恐龙，才 能为人类提供新的发展空间。从这个角度讲，一个昔 日跨国公司的衰亡，也许是它为我们这个社会做的最 后一次贡献。1   70–20–10 律2　诺威格定律3　基因决定定律 结束语 286\t第 15 章　硅谷的摇篮—斯坦福大学二战后，帮助斯坦福大学解决财政危机的是它的一位 教授弗里德里克•特曼，他后来被称为“硅谷之父”。 他仔细研究了斯坦福夫妇的遗嘱，发现里面没有限制 大学出租土地，于是他兴奋地声称找到了解决问题的 秘密武器 — 建立斯坦福科技园。1　充满传奇的大学2　硅谷的支柱3　纽曼加洪堡的教育模式4　创业的孵化器 结束语313\t第 16 章　科技公司的吹鼓手—投资银行华尔街的贪婪既会捧起，也会扼杀一个科技新星。1　华尔街和美国的金融体系2　著名的投资公司3　科技公司的上市过程4　成也萧何，败也萧何5　华尔街与微软、雅虎和 Google 的三国演义 结束语 341\t第 17 章　挑战者—Google 公司 Google是个奇怪的地方。也许是因为Google的年轻人太多，他们不懂得传统也不拘泥传统，只要认准了对公司对社会有用，就大胆去干了。1　历史上最轰动的 IPO2　早期岁月3　商业模式4　个人英雄主义和群众路线5　绝代双骄6　感谢上帝，今天是星期五（TGIF）7　不作恶8　不败的神话9　秘密军团10　云计算和数据中心11　Google 的新气象12　3G 时代13　进攻，永远是最好的防守 结束语396\t第 18 章　成功的转基因—诺基亚、3M、GE 公司 由于科学技术是最革命、发展最快的生产力，一家科 技公司要想在几次技术革命大潮中都能够立于浪潮之 巅是一件极不容易的事。1　从木工厂到手机之王（诺基亚公司）2　道琼斯指数中的常青树（3M 公司）3　世界最大的联合体（GE 公司） 结束语423\t第 19 章　印钞机—最佳的商业模式所有成功的大公司都有好的商业模式，很多大公司的 兴起，不是靠技术的革新而是靠商业模式的转变。1　Google 的广告系统2　eBay 和亚马逊的在线市场3　戴尔的虚拟工厂4　腾讯的虚拟物品和服务 结束语442\t第 20 章　互联网 2.0互联网2.0最重要的是提供了一个开放的平台，让用户可以在平台上开发自己的应用程序，并且提供给其 他用户使用。1　互联网的前世今生2　互联网 2.0 的特征3　著名的互联网 2.0 公司4　是革命还是泡沫 结束语467\t第 21 章　金融风暴的冲击虽然全世界在2008年的最后一个季度里陷入严重的 衰退，同时人们的恐惧心理加重了这场危机，虽然在 更 长 一 些 时 间 里 我 们 仍 将 处 于 衰 退， 但 是， 明 天 仍 然 会 好 起 来。 今 后 的44年 里 我 们 的 经 济、 我 们 的 社会 都 将 获 得 长 足 的 发 展， 就 如 同 过 去 的44年 一 样。— 沃伦•巴菲特1　金融危机的成因2　瑞雪兆丰年：优胜劣汰3　潜在的商机到处都是4　格局的变迁 结束语497\t第 22 章 　云计算 云计算保证用户可以随时随地访问和处理信息，并且 可以非常方便地与人共享信息。 它的好处是让全社会 的计算资源得到最有效的利用。1　云计算的起源2　云计算的本质3　云计算的核心技术和工程4　对 IT 产业链的颠覆 结束语 513\t第 23 章　下一个 Google 虽然我们不知道下一个Google在哪里，但是可以肯定它不在搜索领域，这就如同几年前我们寻找的“下 一个微软”不会是一家软件公司，而最终是一家互联 网公司一样。 1　千亿俱乐部2　岁岁年年人不同3　新领域4　关注亚太地区结束语537\t后记541\t索引====================吴军博士，毕业于清华大学计算机系（本科）、电子工程系（硕士）和美国约翰霍普金斯大学计算机科学系（博士）。在清华大学和约翰霍普金斯大学期间，吴军博士致力于语音识别、自然语言处理，特别是统计语言模型的研究。他曾获得1995年全国人机语音智能接口会议的最佳论文奖和2000年Eurospeech的最佳论文奖。吴军博士于2002年加入Google公司。在Google，他和Amit Singhal（Google院士，世界著名搜索专家）、Matt Cutts（Google反作弊官方发言人）等三位同事一起开创了网络搜索反作弊的研究领域，并因此获得Google工程奖。2003年，他和Google全球架构的总工程师朱会灿博士等共同成立了中日韩文搜索部门。吴军博士是当前Google中日韩文搜索算法的主要设计者。在Google其间，他还领导了许多研发项目，包括许多与中文相关的产品和自然语言处理的项目，并得到了当时公司首席执行官埃里克．施密特和创始人谢尔盖．布林的高度评价。此外，他还在谷歌黑板报上发表了《数学之美》系列博客。吴军博士在国内外发表过数十篇论文，并获得和申请了十余项美国和国际专利。他于2005年起，当选为约翰霍普金斯大学计算机系董事会董事。2007起担任风险投资基金中国世纪基金的董事。2010年，吴军博士离开Google,加盟腾讯公司，担任负责搜索业务的副总裁。并担任国家重大专项“新一代搜索引擎和浏览器”项目的总负责人。  《浪潮之巅》豆瓣主页：http://book.douban.com/subject/6709783/ 互动网预定页面：http://product.china-pub.com/198405 快书包预定页面：http://www.kuaishubao.com/62/1199.html","title":"[置顶] （转）《浪潮之巅》作者吴军前言：有幸见证历史（内有目录和作者简介）"},{"content":"SPT-下载爬虫系统和数据分析(7370)职位描述：     岗位职责： 负责搜索引擎爬虫系统的开发及相关数据的分析挖掘 岗位要求： 2年以上c 编程经验，对算法/数据结构有深刻理解 熟悉linux编程和调试环境 熟悉 socket 网络编程 具备良好的团队协作能力及沟通能力 有大型分布式系统设计开发经验优先 有搜索引擎经验者优先 具有数据挖掘、自然语言处理、信息检索、机器学习、数据统计背景的优先       我们正在创建新一代的智能网页搜索应用。我们在努力寻找优秀的，在文本挖掘、信息抽取、信息检索和自然语言处理等方面的专家级的软件开发人才，帮助我们建立产品数据库来展示我们的“长尾”策略产品。 职责： 1. 负责建立业内最大最全的原标签产品目录； 2. 在不知名的非主流网站上寻找鲜为人知的与众不同的产品； 3. 同团队一起合作为用户提供最完美的体验； 4. 汇报给技术总监； 要求： 1. 精通信息抓取和整合技术，从结构化的和非结构化的数据中获取信息。 2. 熟悉数据分析的统计方法，如PMT，HMM，Na&iuml;ve Bayes等。自然语言处理技巧和经验尤为重要。 3. 精通与搜索和个性化相关的机器学习算法，大规模网页聚类，分类和提取摘要。 4. 精通大规模推荐系统和内容过滤（large scale recommendation system, content based recommendation and collaborative filtering）。 5. 5年以上Java开发经验，超强的编程技巧。 6. 精通Java技术，如JDBC，servlet，web service，最好熟悉Ruby。 7. 精通关系型数据库，尤其是MySQL， 大数据量的。 8. 熟悉大规模网页爬取，深度网页爬取，熟悉nutch、hadoop等爬虫工具尤佳。 9. 有测试驱动和敏捷开发经验。 10. 能用英文进行日常工作沟通交流。","title":"临时笔记"},{"content":"一旦用过了这 18 种方法， Google Calendar 必可成为你居家旅行杀人越货纵横冒险打家劫舍的必备利器。为行文方便，下面将 Google Calendar 简称为 GCal ： 1.添加节假日，月相，比赛时刻表和其它常用日历：点击左边“其它日历”旁边的“＋”号，选择浏览日历，然后你就可以添加一些常用的日历了。而如果你是一个体育迷，试试搜索你所钟爱的球队名称，你可以顺利的添加他们的所有比赛时段日历。 2.定制你的视图：你可以自定义你的 GCal 视图，比如把右上角的“接下来 4 天”改为”接下来 3 周“，要做到这点只需点击右上角的设置，在”基本“分栏页的”自定义视图“项中选择你所要定义的时间，然后保存即可。（按下“ x ”键可以快速的转到你的“自定义视图”页。） # Copyright for Jandan.net(http://jandan.net/) 3.高亮你的当前事件：这个 geasemonkey script 可以在 GCal 的当前时间段加上一段红线。醒目的提醒你是该开会了还是该约会了。 4.快速添加事件：在 GCal 界面按” q “键或者点击页面上的“快速添加”链接即可以快速的添加事件。（# Cunni ：见鬼，这一条我试不出出来，按下 q 键没有反应，而“快速添加”链接也没有找到，是因为我使用的是中文版的原因？） 5.不用访问 GCal 页面即可添加事件：一个酷酷的 firefox 插件： GCal Quick Add extension 可以帮你搞定，你可以随时按下“ ctrl + ;”来呼出它并添加一个事件。 6.接受提醒和其它通知： GCal 提供了数种提醒方式，包括电子邮件， SMS 或弹出式窗口等（大陆地区 SMS 方式似乎不可用？ update by sfufoet :感谢读者 dodo 的补充：联通可以，移动不行。）。选择“设置”内的“通知”分栏页来设置。 7.发送每日日程：在“设置”内的“通知”分栏页中还有一个“每日日程:”的选项，可以每天定时为你送出当日日程。 8.随时随地得到你的 GCal 信息：发送“ next ”到 48368 ，可以得到你的下一条约会信息，发送“ nday ”则可以得到下一天的日程。 9.在 GTalk 或 MSN 中访问 GCal ： imified 是一个活跃在 IM 网络中的虚拟实体，把他添加到你的联系人名单中（ MSN 为 imified@imified.com ， GTalk/Jabber 为 imified2@gmail.com ）。然后发送一条 help 信息给它，然后发送“ 4 ”，点击他返回给你的“ add/edit service ”链接，在弹出的页面里添加右边的 GCal ，设置好正确的 GCal 信息（请参考其 Help ）。然后在 Account Setting 中设置正确的时区。随后返回你的 IM 软件界面，发送 M 重新刷新你的菜单，你会看到多了一项 GCal 的选项，随后就可以发送信息对其进行操作了。 10.学习使用快捷键：这里有一份 GCal 所有快捷键的列表。 11.在你的 GCal 列表中添加 ToDo 事件：别忘了喂奶( RememberTheMilk )是一个非常流行的个人任务管理软件，它们还贴心的提供了整合在 GCal 里面的方法。 12.得到一个更大的 GCal 视图：如果你觉得缺省的视图不够大的话，试试 fiefox 的这个插件，它可以让你自定义快捷键快速切换到 GCal 界面并且选择常规视图或者全屏视图。 13.在你的 Blog 上分享你的忙碌/空闲信息：点击“设置”－》“日历”，选择一个日历，点击“共享此日历”，选择“只共享我的空闲/忙碌信息（隐藏详细信息）”，你可以看到右边有“在您的网站上使用此按钮以使其他人可以订阅此日历。”的提示，点击“获得代码”，并把代码嵌入到你的 blog 中，即可于别人分享你的忙碌/空闲信息了。更简单的方法是：如果你的 blog 软件是 typepad 的话，你还可以使用这个插件，而如果你和煎蛋一样喜欢 Wordpress 的话，则可以使用这个插件。 14.和你的桌面日历软件同步： Calgoo 是一个基于 Java 的桌面日历软件，它内建了和 GCal 或 iCal 或 Outlook 同步的功能。 15 . 从 Gmail 中添加事件： Gmail 内建了一些自然语言处理功能，如果看到你的 mail 中有一些类似于约会的信息的话，会自动的显示一个“添加到 Calendar ”的链接。（不知道中文的好不好使，可惜没有人给我的 Gmail 邮箱发约会邀请啊 -___- ）。 16.在 Gmail 中显示日程：又一个 Greasemonkey 的 script ，然后在你的 Google Bookmarks 中添加一个 GCal 的私有链接（“设置”－》“日历”，选择一个日历，选择“私人网址”－》“ XML ”），并给其起名叫“ GMgcal ”，然后运行这个 Greasemonkey 的脚本 GMail Agenda Setup （”工具” －》 “ Greasemonkey ” －》 “ User Script Commands …” －》 “ GMail Agenda Setup ”）。你将会在你的 Gmail 界面的联系人列表和标签列表中看到一个新增的 GCal 事件列表。如果你糊涂了的话，这里有更详细的说明。 17.为你的 Firefox 状态栏添加一个弹出式日程提醒框：这个名叫 Google Calendar Notifier 的 Firefox 插件可以满足你的愿望：） 18.显示你的天气情况：在“设置”，“基本”中填入你的所在地信息，然后选择“显示天气”。（别试了，当前这一功能只有美国可用 -_____- ）。     http://jandan.net/2007/03/23/rock_your_google_calendar_in_18_ways.html ","title":"18种方法折腾你的Google Calendar"},{"content":"声明：此文转载自“我爱自然语言处理”，请注意版权，谢谢。 EM(Expectation-Maximization)算法在机器学习和自然语言处理应用非常广泛，典型的像是聚类算法K-means和高斯混 合模型以及HMM(Hidden Markov Model)。笔者觉得讲EM算法最好的就是斯坦福大学Andrew Ng机器学习课的讲课笔记和视频。本文总结性的给出普遍的EM算法的推导和证明，希望能够帮助接触过EM算法但对它不是很明白的人更好地理解这一算法。 EM算法的目标是找出有隐性变量的概率模型的最大可能性解，它分为两个过程E-step和M-step，E-step通过最初假设或上一步得出的模型参数得到后验概率，M-step重新算出模型的参数，重复这个过程直到目标函数值收敛。我们设观察到的变量所组成的向量为，所有的隐性变量所组成的向量为,模型的参数为（一个或多个参数）。在聚类的情况下，是潜在的类，而就是需要分类的数据。我们的目标就是找出模型的参数使得出现的可能性最大，也就是使下面的对数可能性最大化： 注：这里仿照Andrew Ng 的用法使用而不是，因为是模型的参数而不是随机变量。关于为什么要用EM算法而不是不直接通过得出，是因为这样可能会出现严重的overfitting (这里不详细说明，请参看Pattern Recognition and Machine Learning一书9.2.1节)。 假设是上一个概率分布，所以 最后一步是根据Jensen不等式如果是凹函数，在这个式子中就是对数函数。就是而就是。 当是严格的 凹函数的时候，中等号成立的条件是是常数，也就是说在这个特定的式子中，满足这个条件加上之前的的其实就是后验概率（参看http://www.stanford.edu/class/cs229/materials.html Lecture notes: The EM Algorithm）。这就是EM算法中E-step的由来。 M-step一般来说就是个就是二次规划的问题，通过得出。这里也就不再赘述。 EM算法其实就是coordinate ascent， E-step是将视为常数，选择一个概率分布使得目标函数最大化， M-step就是保持不变，选择使得目标函数最大化，因为每一步的目标函数都比前一步的值大，所以只要目标函数存在最大值，EM算法就会收敛。这个过程用图像表示就是： E-step找到跟（黑色弧线）交于的（蓝色弧线），M-step得到取最大值时的，这样下去直到收敛。（此图源于Andrew）","title":"52nlp：理解EM算法"},{"content":"在线直播：http://student.csdn.net/space.php?do=onlineroom&id=196 应CSDN学生给专区之邀，应广大嵌入式技术爱好者之邀，亚嵌将举办一系列在线的直播课堂，此活动为公益技术活动。11月24号在线直播课堂具体内容如下: 主讲老师：吴岳老师 吴岳，北京亚嵌教育研究中心金牌讲师。7年linux环境开发经验，3年嵌入式开发经验，2年嵌入式教学经验。 精通技术C/C++语言、Linux环境高级编程、自然语言处理技术、搜索引擎算法、Linux内核技术。实际项目经验：google和新浪联合的IASK项目；一搜公司的手机搜索引擎项目；江西省公安厅的通信产业搜索项目。   课程大纲： ——玩转static， 玩转模块化设计 ·似曾相识的static关键字 1）static关键字初探 2）局部变量也能“长生不老”？ 3）static关键字之于局部变量 4）你能回答全局变量的作用域吗？ 5）static关键字之于全局变量 6）为什么static关键字可以检验C语言程序员的成色？   ·小小的关键字蕴藏着大大的思想 1）模块——是我踩在脚下的巨人肩膀 ——某Linux社区开发者 2）我们也在推动生产力的发展——模块化设计与现代软件工业 3）C语言也可以使用面向对象思想进行设计  ·良好的模块化利人利己 1）数据和接口的关系 2）数据——封装于模块之内的灵魂 3）接口——进入模块的桥梁 4）高效使用封装良好的模块 课程时间：2010年11月24日（周三）晚上19：30——21：00 历次嵌入式在线直播课堂的课件会放在QQ群: 76977848、54181364、30332464、609309、 22188564、35514925、121896381共享中，请勿重复加群，谢谢合作！ 历次课堂回顾：   11月24日 第十次课  吴岳  玩转static， 玩转模块化设计 课程介绍：http://www.akaedu.org/page/newsdetail-1457.html  11月10日  第九次课  李强   怎么在新的硬件平台上porting android 课程简介：http://www.akaedu.org/page/newsdetail-1435.html 课程视频：http://www.akaedu.org/page/newsdetail-1437.html 11月03日  第八次课  吴岳   玩转指针，玩转C语言 课程简介：http://www.akaedu.org/page/newsdetail-1425.html 课程视频：http://www.akaedu.org/page/newsdetail-1436.html 10月27日  第七次课  朱仲涛  红黑树 课程简介：http://www.akaedu.org/page/newsdetail-1416.html 课程视频：http://www.akaedu.org/page/newsdetail-1428.html 10月20日  第六次课  翟开源  释放创意的动力 ——ARM Cortex-M3微控制器应用编程指南 课程简介：http://www.akaedu.org/page/newsdetail-1421.html 课程视频：http://www.akaedu.org/page/newsdetail-1424.html 10月13日 第五次课  程振林  C语言中的“加减乘除” 课程介绍：http://www.akaedu.org/page/newsdetail-1392.html   9月29日  第四次课  朱仲涛  多核处理器环境编程浅说---圆周率的近似计算 课程介绍：http://www.akaedu.org/page/newsdetail-1358.html  课程视频：http://www.akaedu.org/page/newsdetail-1412.html  9月15日  第三次课  吴岳  玩转指针，玩转C语言 课程介绍：http://www.akaedu.org/page/newsdetail-1341.html    9月1日   第二次课  李明  怎样从0开始写bootloader？ 课程介绍：http://www.akaedu.org/page/newsdetail-1328.html  课程视频：http://www.akaedu.org/page/newsdetail-1344.html    8月18日  第一次课  李明  怎样学习ARM嵌入式开发? 课程介绍：http://www.akaedu.org/page/newsdetail-1297.html  课程视频：http://www.akaedu.org/page/newsdetail-1324.html 近期亚嵌推出一系列免费活动，详情如下： 12月6日，嵌入式linux就业班  12月18日，嵌入式linux周末班 11月27日，Android应用及系统开发班","title":"玩转static， 玩转模块化设计"},{"content":"http://bdonline.sqe.com/一个关于网站方面的网页,对这方面感兴趣的人可以参考 http://citeseer.nj.nec.com/一个丰富的电子书库,内容很多,而且提供著作的相关文档参考和下载,是作者非常推荐的一个资料参考网站 http://groups.yahoo.com/group/LoadRunner工具的一个论坛 http://groups.yahoo.com/grorp/testing-paperannou-nce/messages提供网站上当前发布的资料列表 http://satc.gsfc.nasa.gov/homepage.html软件保证中心是美国国家航天局（NASA)投资设立的一个软件可靠性和安全性研究中心，研究包括了度量、工具、风险等各个方面 http://seg.iit.nrc.ca/English/index.html加拿大的一个研究软件工程质量方面的组织，可以提供研究论文的下载 http://sepo.nosc.mil/内容来自美国SAN DIEGO的软件工程机构（Sofrware Engineering Process Office)主页，包括软件工程知识方面的资料 http://www.asq.org/是世界上最大的一个质量团体组织之一，有着比较丰富的论文资源，不过是收费的 http://www.automated-testing.com/一个软件测试和自然语言处理研究页面，属于个人网页，上面有些资源可供下载 http://www.benchmarkresources.com/提供有关标杆方面的资料，也有一些软件测试方面的资料 http://www.betasoft.com/包含一些流行的介绍、下载和讨论，还提供测试方面的资料 http://www.brunel.ac.uk/~csstmmh2/vast/home.htmlVASTT研究组织，主要从事通过切片、和转换技术来验证和分析系统，对这方面技术感兴趣的人是可以在这里参考一些研究的项目及相关的一些主题信息 http://www.cc.gatech.edu/aristotle/Aristole研究组织，研究软件系统分析、测试和维护等方面的技术，在测试方面的研究包括了回归测试、测试套最小化、面向对象软件测试等内容，该网站有丰富的论文资源可供下载 http://www.computer.org/IEEE是世界上最悠久，也是在最大的计算机社会团体，它的电子图书馆拥有众多计算机方面的论文资料，是研究计算机方面的一个重要资源参考来源 http://www.cs.colostate.edu/testing/可靠性研究网站，有一些可靠性方面的论文资料 http://www.cs.york.ac.uk/testsig/约克大学的测试专业兴趣研究组网页，有比较丰富的资料下载，内容涵盖了测试的多个方面，包括测试自动化、测试数据生成、面向对象软件测试、验证确认过程等 http://www.csr.ncl.ac.uk/index.html学校里面的一个软件可靠性研究中心，提供有关软件可靠性研究方面的一些信息和资料，对这方面感兴趣的人可以参考 http://www.dcs.shef.ac.uk/research/groups/vt/学校里的一个验证和测试研究机构，有一些相关项目和论文可供参考 http://www.esi.es/en/main/ESI（欧洲软件组织），提供包括评估方面的各种服务 http://www.europeindia.org/cd02/index.htm一个可靠性研究网站，有可靠性方面的一些资料提供参考 http://www.fortest.org.uk/一个测试研究网站，研究包括了静态测试技术（如模型检查、理论证明）和动态测试（如测试自动化、特定缺陷的检查、测试有效性分析等） http://www.grove.co.uk/一个有关软件测试和咨询机构的网站，有一些测试方面的课程和资料供下载 http://www.hq.nasa.gov/office/codeq/relpract/prcls-23.htmNASA可靠性设计实践资料 http://www.io.com/~wazmo/Bret Pettichord的主页，他的一个热点测试页面连接非常有价值，从中可以获得相当大的测试资料，很有价值 http://www.iso.ch/iso/en/ISOOnline.frontpage国际标准化组织，提供包括ISO标准系统方面的各类参考资料 http://www.isse.gmu.edu/faculty/ofut/classes/821-ootest/papers.html 提供面向对象和基于构架的测试方面著作下载，对这方面感兴趣的读者可以参考该网站，肯定有价值 http://www.ivv.nasa.gov/NASA设立的独立验证和确认机构，该机构提出了软件开发的全面验证和确认，在此可以获得这方面的研究资料 http://www.kaner.com/著名的测试专家Cem Kanner的主页，里面有许多关于测试的专题文章，相信对大家都有用。Cem Kanner关于测试的最著名的书要算Software,这本书已成为一个测试人员的标准参考书 http://www.library.cmu.edu/Re-search/Engineer-ingAndSciences/CS+ECE/index.html 卡耐基梅陇大学网上图书馆，在这里你可以获得有关计算机方面各类论文资料，内容极其庞大，是研究软件测试不可获取的资料来源之一 http://www.loadtester.com/一个性能测试方面的网站，提供有关性能测试、性能监控等方面的资源，包括论文、论坛以及一些相关链接 http://www.mareinig.ch/mt/index.html关于软件工程和应用开发领域的各种免费的实践知识、时事信息和资料文件下载，包括了测试方面的内容 http://www.mtsu.ceu/-storm/软件测试在线资源，包括提供目前有哪些人在研究测试，测试工具列表连接，测试会议，测试新闻和讨论，软件测试文学（包括各种测试杂志，测试报告），各种测试研究组织等内容 http://www.psqtcomference.com/实用软件质量技术和实用软件测试技术国际学术会议宣传网站，每年都会举行两次 http://www.qacity.com/front.htm测试工程师资源网站，包含各种测试技术及相关资料下载 http://www.qaforums.com/关于软件质量保证方面的一个论坛，需要注册 http://www.qaiusa.com/QAI是一个提供质量保证方面咨询的国际著名机构，提供各种质量和测试方面证书认证 http://www.qualitytree.com/一个测试咨询提供商，有一些测试可供下载，有几篇关于方面的文章值得参考 http://www.rational.com/的官方网站，可以在这里寻找测试方面的工具信息。IBM Rational提供测试方面一系列的工具，比较全面 http://rexblackconsulting.com/Pages/publicat-ions.htm Rex Black的个人主页，有一些测试和测试管理方面的资料可供下载 http://www.riceconsulting.com/一个测试咨询提供商，有一些测试资料可供下载，但不多 http://www.satisfice.com/包含James Bach关于软件测试和过程方面的很多论文，尤其在启发式测试策略方面值得参考 http://www.satisfice.com/seminars.shtml 一个黑盒软件测试方面的研讨会，主要由测试专家Cem Kanar和James Bach组织，有一些值得下载的资料 http://www.sdmagazine.com/软件开发杂志，经常会有一些关于测试方面好的论文资料，同时还包括了项目和过程改进方面的课题，并且定期会有一些关于质量和测试方面的问题讨论 http://www.sei.cmu.edu/著名的软件工程组织，承担美国国防部众多软件工程研究项目，在这里你可以获俄各类关于工程质量和测试方面的资料。该网站提供强有力的搜索功能，可以快速检索到你想要的论文资料，并且可以免费下载 http://www.soft.com/Institute/HotList/提供了网上软件质量热点连接，包括：专业团体组织连接、教育机构连接、商业咨询公司连接、质量相关技术会议连接、各类测试技术专题连接等 http://www.soft.com/News/QTN-Online/质量技术时事，提供有关测试质量方面的一些时事介绍信息，对于关心测试和质量发展的人士来说是很有价值的 http://www.softwaredioxide.com/包括软件工程（CMM,CMMI,项目管理）软件测试等方面的资源 http://www.softwareqatest.com/软件质量/测试资源中心。该中心提供了常见的有关测试方面的FAQ资料，各质量/测试网站介绍，各质量/测试工具介绍，各质量/策划书籍介绍以及与测试相关的网站介绍 http://www.softwaretestinginstitute.com/一个软件测试机构，提供软件质量/测试方面的调查分析，测试计划模板，测试WWW的技术，如何获得测试证书的指导，测试方面书籍介绍，并且提供了一个测试论坛 http://www.sqatester.com/index.htm一个包含各种测试和质量保证方面的技术网站，提供咨询和培训服务，并有一些测试人员社团组织，特色内容是缺陷处理方面的技术 http://www.sqe.com/一个软件质量工程服务性网站，组织软件测试自动化、STAR-EASE、STARWEST等方面的测试学术会议，并提供一些相关信息资料和课程服务 http://www.stickyminds.com/提供关于软件测试和质量保证方面的当前发展信息资料，论文等资源 http://www.stqemagazine.com/软件策划和质量工程杂志，经常有一些好的论文供下载，不过数量较少，更多地需要通过订购获得，内容还是很有价值的 http://www.tantara.ab.ca/软件质量方面的一个咨询网站，有过程改进方面的一些资料提供 http://www.tcse.org/IEEE的一个软件工程技术委员会，提供技术论文下载，并有一个功能强大的分类下载搜索功能，可以搜索到测试类型、测试管理、 测试分析等各方面资料 http://www.testing.com/测试技术专家Brain Marick的主页，包含了Marick 研究的一些资料和论文，该网页提供了测试模式方面的资料，值得研究。总之，如果对测试实践感兴趣，该网站一定不能错过 http://www.testingcenter.com/有一些测试方面的课程体系，有一些价值 http://www.testingconferences.com/asiastar/home著名的AsiaStar测试国际学术会议官方网站，感兴趣的人一定不能错过 http://www.testingstuff.com/Kerry Zallar的个人主页，提供一些有关培训、工具、会议、论文方面的参考信息 http://www-sqi.cit.gu.edu.au/软件质量机构，有一些技术资料可以供下载，包括软件产品质量模型、再工程、软件质量改进等 工作RationalIBM缺陷管理TestingCMM测试技术技术测试工具其它自动化软件测试LoadRunner性能测试测试","title":"国外优秀测试网址"},{"content":"亚嵌将举办一系列在线的直播课堂，此活动为公益技术活动。9月15号第三期在线直播课堂具体内容如下: C语言之魂——指针初探 ·了解指针的常规与非常规用法 1）揭开指针神秘的面纱 2）拗口的问题：指针能指向指针吗？ 3）非典型指针使用法：指针使用错误汇总1 4）指针颠覆我们的概念：一个函数只能有一个返回值？ 5）指针作为函数的返回值 6）C语言中的双胞胎——指针和数组   ·迈出成为C语言顶级高手的第一步 1）everything is file, everything is memory.   ——Richard 2）控制指针访问内存3）控制指针的移动 4）指针是把双刃剑：指针使用错误汇总2    ·控制指针的人就能控制编程世界 1）指针是你的朋友，不是你的敌人 2）指针调试技巧：指针使用错误汇总3使用指针的5个原则：1）指针一定要初始化2）函数的返回值不能是局部变量的地址3）指针指向的空间不能是已经失效的 4）指针的移动取决于指针类型 5）NULL指针永远不能访问 主讲老师：吴岳老师 吴岳，北京亚嵌教育研究中心金牌讲师。7年linux环境开发经验，3年嵌入式开发经验，2年嵌入式教学经验。 精通技术C/C++语言、Linux环境高级编程、自然语言处理技术、搜索引擎算法、Linux内核技术。实际项目经验：google和新浪联合的IASK项目；一搜公司的手机搜索引擎项目；江西省公安厅的通信产业搜索项目。 第二期亚嵌嵌入式在线课堂回顾：怎样从零开始写Bootloader？http://www.akaedu.org/page/newsdetail-1342.html  第一期亚嵌嵌入式在线课堂回顾：如何学习ARM嵌入式开发？http://www.akaedu.org/page/newsdetail-1312.html","title":"9.15日在线直播课堂：玩转指针，玩转c语言"},{"content":"亚嵌将举办一系列在线的直播课堂，此活动为公益技术活动。9月15号第三期在线直播课堂具体内容如下: C语言之魂——指针初探 ·了解指针的常规与非常规用法 1）揭开指针神秘的面纱 2）拗口的问题：指针能指向指针吗？ 3）非典型指针使用法：指针使用错误汇总1 4）指针颠覆我们的概念：一个函数只能有一个返回值？ 5）指针作为函数的返回值 6）C语言中的双胞胎——指针和数组   ·迈出成为C语言顶级高手的第一步 1）everything is file, everything is memory.   ——Richard 2）控制指针访问内存3）控制指针的移动 4）指针是把双刃剑：指针使用错误汇总2    ·控制指针的人就能控制编程世界 1）指针是你的朋友，不是你的敌人 2）指针调试技巧：指针使用错误汇总3使用指针的5个原则：1）指针一定要初始化2）函数的返回值不能是局部变量的地址3）指针指向的空间不能是已经失效的 4）指针的移动取决于指针类型 5）NULL指针永远不能访问 主讲老师：吴岳老师 吴岳，北京亚嵌教育研究中心金牌讲师。7年linux环境开发经验，3年嵌入式开发经验，2年嵌入式教学经验。 精通技术C/C++语言、Linux环境高级编程、自然语言处理技术、搜索引擎算法、Linux内核技术。实际项目经验：google和新浪联合的IASK项目；一搜公司的手机搜索引擎项目；江西省公安厅的通信产业搜索项目。 第二期亚嵌嵌入式在线课堂回顾：怎样从零开始写Bootloader？http://www.akaedu.org/page/newsdetail-1342.html  第一期亚嵌嵌入式在线课堂回顾：如何学习ARM嵌入式开发？http://www.akaedu.org/page/newsdetail-1312.html","title":"9月15日在线直播课堂：玩转指针，玩转C语言"},{"content":" 转眼间工作10年了……  我是96年考入天津大学计算机系的，上大学之前仅在高一时短暂的接触过Basic，老师手里的5寸软盘只见过没摸过。高三的暑假，找了一本薄薄的C++的书硬啃，只记得了一点点点的皮毛，知道了有这么一门带class的编程语言。刚入学，老师给我们扫盲时，看到老师手里的3寸软盘，觉得好生羡慕，从贩假软盘的师兄手里买过一盒10张的软盘（当时不知是假，Maxwell，好牌子的）。和其他有一定基础的同学相比，简直是相差太远了；许多同学是用过C的，也有同学是参加过编程比赛的。仗着在小霸王学习机上突击学的指法，还不至于在TT（80后的同学们可能没机会用到了）练习上被甩下太远。 刚上大学时，觉得学习是次要的，锻炼自己的各方面能力更重要，凡是社团活动都积极参加。大学第一学期期中考试，险些挂科，终于警醒，开始上晚自习了。第一学期期末考试时，终于追赶上来，在班里排在4名女生的后面，加上其他乱七八糟的总评分，混了个班级第三。下半学年，和同寝的哥们儿凑分子，买了年级里的第一台电脑（二手的奔75，好像是6或8M内存），去取电脑时，南开的师兄给我们show了一把红警，把我们都给震住了。其他寝室的同学们也陆续买了机器，大家用同轴线缆连局域网打红警，晚上接走廊的灯泡偷电通宵打游戏。走廊里的电据说是半副的交流电，CRT屏幕接上电四角会有奇怪的彩虹纹，而且有源音箱一接就会烧。我们寝室，慢慢从最整洁寝室，变成了最脏乱差寝室。原先班里开班会，都是到我们寝室来的。后来，大家懒到晚上洗完脚都不倒洗脚水的地步了，地上堆满了报纸和烟头，呵呵 …… 在校期间，正式学习的第一门编程语言是Pascal，第二门是汇编，数据结构也是基于Pascal的，甚至大二暑假实习的时候要用Pascal写一个Pascal的小型编译器（很遗憾我当时被借调到实验室去做项目了，没赶上）。大二时，为了进学校的IBM中心实验室，我自学了点C/C++和Java。学C++时，找了本《Visual C++从入门到精通》，照着书上的例子敲进去，呼呼呼，编译器报了好几百个错，仔细地一点一点查看，有的是自己的typo，也有是书籍的印刷错误。从此，我就特别痛恨书名是从入门到精通的书籍，绝对是伪劣书刊的代名词。在实验室给老师做项目，还是学了不少东西，我们一帮同学还给实验室写了个网站，参加全国高校IBM中心实验室的比赛，拿过个一等奖呢。当时去参赛时，老师是把网站的前端后台刻了张光盘带去参赛的，呵呵…… 在实验室一开始，跟着两个高我一年的师兄搞Java+VRML，大三下半年的时候，还帮两个研究生做毕设来着，由那开始，我就特瞧不上研究生教育，决心坚决不上研究生。后来，大四上半年的时候，阴差阳错和同学参加了一个语音传输的小比赛，是一个什么日本公司赞助我们学校搞的。当时用了多线程、多播等，调用Windows ACM来压缩音频，还自己臆想了一个类似freelist的缓冲区管理。据说我们这一组总评排第三来着，后来为了鼓励本科生的参与精神，颁给我们一个一等奖。到毕业设计时，我和搭档开始弄视频传输，主要也就是用Windows VCM来压缩视频，据说后来评了个优秀毕业设计，还据说当年老师手下几个研究生在继续完善我们的系统，拿这个东西毕业呢。 我很痛恨学校里的教育方式，总体来说是，学不“知”用。还好二年级就学了基础物理，要不然我都不知道微积分有啥用。还有计算数学，虽然会用到微积分，但是还是不明白，计算数学是干啥用的。典型的还有，线性代数、概率论等等。现在回过头来看，这些数学多重要啊，真后悔当初没好好学。好不容易离散数学是真章吧，老师又忒离散，把我们这些学生搞得神经分裂，最后才考了60多分。如果当初能从一个比较前沿的应用领域入手，例如基于统计的自然语言处理，我们肯定会有很大的兴趣，把这些基础学科都学好。还有C语言，大三才开始教，我仗着自己C++/Java都很熟，满不在乎，结果考试时尽是考查错题和输出题，险些不及格。害得我找工作时，每每说自己擅长C/C++时，都被很严格的盘问…… 在学校4年，最大的收获是，交了女朋友，也是我现在的老婆。我们是同班同学，从大二开始确定恋爱关系。在那时，我们是很认真的、本着将来结婚组建家庭来谈恋爱的。当时追她的时候，在食堂凑到她身边吃饭，因为我吃得快，她吃得慢，为了不至于我吃完了不好意思不闪人，我那个时期的饭量很大，吃7两米饭，然后还喝一大盆粥。大四时，她希望考研究生的，我就帮她复习，也去陪考了。最后她如愿考上了本校的研究生。我的总分太低，英语还不及格，以至于实验室的老师本来想特招我，都没办法。毕业后，我就到北京工作了，两地分开的两年半，我的电话费花了很多，呵呵…… 现在想想，如果不是在校期间就情定终身，我们这种社交圈子小而又小的IT民工，还真很容易变成剩男剩女。现在，我们结婚已7年，小孩儿都快3岁了…… 原贴地址：http://yongsun.me/2010/05/十年码工路——大学篇/ 延伸阅读： 《编程之美》豆瓣 《编程之美》互动网 《编程之美》，IT人求职面试必读  ","title":"十年码工路——大学篇（转）"},{"content":"自然语言处理(NLP, natural language processing)及计算语言学(CL, Computational Linguistics)常见缩略词 ACL = Association for Computational Linguistics(计算语言学协会) AFNLP = Asian Federation of Natural Language Processing(亚洲自然语言处理联盟) AI = Artificial Intelligence(人工智能) ALPAC = Automated Language Processing Advisory Committee(语言自动处理咨询委员会) ASR = Automatic Speech Recognition(自动语音识别) CAT = Computer Assisted/Aided Translation（计算机辅助翻译） CBC = Clustering by Committee CCG = Combinatory Categorial Grammar（组合范畴语法） CICLing = International Conference on Intelligent text processing and Computational Linguistics（国际智能文本处理与计算语言学大会） CL = Computational Linguistics（计算语言学） COBUILD = Collins Birmingham University International Language Database（柯林斯伯明翰大学国际语言数据库） COLING = International Conference on Computational Linguistics（国际计算语言学大会） CRF = Conditional Random Fields（条件随机场） DRS = Discourse Representation Structure（篇章表述结构） DRT = Discourse Representation Theory（篇章表述理论） EACL = European chapter of the Association for Computational Linguistics EBMT = Example-based machine translation（基于实例的机器翻译） EM = Expectation Maximization（期望最大化） FAHQMT = Fully Automated High-Quality Machine Translation（全自动高质量机器翻译） FOL = First Order Logic（一阶逻辑） HAMT = Human Assisted/Aided Machine Translation（人工辅助机器翻译） HLT = Human Language Technologies（人类语言技术） HMM = Hidden Markov Model（隐马尔科夫模型） HPSG = Head-Driven Phrase Structure Grammar（中心语驱动短语结构语法） IE = Information Extraction（信息抽取） IR = Information Retrieval（信息检索） IST = Information Society Technologies（信息社会技术） KR = Knowledge Representation（知识表示） LFG = Lexical Functional Grammar（词汇功能语法） LSA = Latent Semantic Analysis（潜在语义分析）; Linguistics Society of America（美国语言学学会） LSI = Latent Semantic Indexing（潜在语义索引） MAHT = Machine Assisted/Aided Human Translation（计算机辅助人工翻译） ME = Maximum Entropy（最大熵） MI = Mutual Information（互信息） ML = Machine Learning（机器学习） MRD = Machine-Readable Dictionary（机读词典） MT = Mechanical Translation/Machine Translation （机器翻译） NAACL = North American chapter of the Association for Computational Linguistics NE = Named Entity（命名实体） NEALT = Northern European Association for Language Technology NER = Named Entity Recognition（命名实体识别） NLG = Natural Language Generation（自然语言生成） NLP = Natural Language Processing（自然语言处理） NLU = Natural Language Understanding（自然语言理解） NML = National Museum of Language PLSA = Probabilistic Latent Semantic Analysis（概率潜在语义分析） PMI = Pointwise Mutual Information（点间互信息） POS = Part of Speech（词性） RTE = Recognising Textual Entailment SLT = Spoken Language Translation（口语翻译） SVM = Support Vector Machine（支持向量机） TAG = Tree-Adjoining Grammar（树邻接语法） TINLAP = Theoretical Issues in Natural Language Processing TLA = Three-letter acronym（三字母缩略语） TMI = Theoretical and Methodological Issues (in Machine Translation) TREC = The Text REtrieval Conference（文本检索会议） VSM = Vector Space Model（向量空间模型） WSD = Word Sense Disambiguation（词义消歧） ACL会议（Annual Meeting of the Association for Computational Linguistics）是自然语言处理与计算语言学领域最高级别的学术会议，由计算语言学协会主办，每年一届。涉及 对话(Dialogue) 篇章(Discourse) 评测( Eval) 信息抽取( IE) 信息检索( IR) 语言生成(LanguageGen) 语言资源(LanguageRes) 机器翻译(MT) 多模态(Multimodal) 音韵学/ 形态学( Phon/ Morph) 自动问答(QA) 语义(Semantics) 情感(Sentiment) 语音(Speech) 统计机器学习(Stat ML) 文摘(Summarisation) 句法(Syntax) 等多个方面","title":"自然语言处理及计算语言学常见缩略词"},{"content":"坚定离开的信念，只有离开才能有所发展，不能总在这里徘徊了！徘徊不前，最后只能是杯具了！ 一、从现在开始加紧做下面几个事情：【四月份完成】 （1）数据结构重温 （2）软件设计方法学习 （3）文本挖掘方法学习 （4）软件架构学习 （5）项目管理学习 （6）自然语言处理学习 寻找自己的核心竞争力！专业/技术/计算机 二、简历编写，从5月初开始编写简历【15天内完成】 （1）核心竞争力编写 （2）项目经历编写 （3）个人优缺点分析 （4）面试策略分析 （5）礼仪学习 三、简历投递【15天内完成】 （1）分析意向公司特点 （2）分别投递简历 （3）面试技巧探析，心理学分析 四、面试阶段【6月、7月 完成】 （1）C++笔试/C笔试 （2）技术面/电话面试/英文面试 （3）最后面试 五、入职工作【争取8月完成】 六、工资目标：1W5  ","title":"我准备离开这里了"},{"content":"早晨接到了百度的电话面试，考了三道算法题，和一些自然语言处理方面的问题。   1：m路归并的时间复杂度。相当简单的题目，回答得还不错。   2： 任给n个未排序的实数，找出在实数轴上间隔最大的两个连续实数。我的第一直觉是想到Dijstra的单源最短路径算法，只不过这道题将空间距离换成了轴上 距离，而且连续。这道题讨论了很久，用最笨的方法已经能达到O(n*log(n))，更快的算法大概也只能是O(n)吧，隐隐觉得这道题动态规划可解，但是时间复杂度能到线性么？想了很久没有想出来……   3：中文分词中的最大正向匹配。真的欲哭无泪，去年在北京参加信息安全决赛时就栽 在了这里，今天……重蹈覆辙。 自然语言处理方面的问了下语义角色标注，句法分析，还有最大熵、神经网络等分类问题。技能方面提到了 linux和shell。这部分答得还好，比较自信。   总体而言，自我感觉算法偏弱，其他尚可。礼貌 么，结束时忘说谢谢了，直接“en，byebye”挂掉了电话，咳。 BS一下百度的突然袭击，一封提前的邮件通知都没有。。我可是 顶着空空的肚子上了一堂早课啊。饿晕了。Anyway，发现了自己的弱点，找到了自己的位置，不管结果如何，都是有意义有价值的一次经历。 原文链接：http://blog.163.com/jiangfeng11_24/blog/static/23940277201022423257870/ 《编程之美》，IT人求职面试必读 《编程之美》豆瓣主页 《编程之美》互动网购买链接","title":"别让算法成了你的绊脚石"},{"content":"转自：http://www.shamoxia.com/html/y2010/1384.html   Latent Dirichlet Allocation(LDA) [pdf]模型是近年来提出的一种具有文本主题表示能力的非监督学习模型。 关键所在：it posits that each document is a mixture of a small number of topics and that each word’s creation is attributable to one of the document’s topics。 将文档看成是一组主题的混合，词有分配到每个主题的概率。 Probabilistic latent semantic analysis（PLSA） LDA可以看成是服 从贝叶斯分布的PLSA LDA，就是将原来向量空间的词的维度转变为Topic的维度，这一点是十分有意义的。 例如，如果一个文档A，包含电脑这个关键词，那么A向量化后可能是,比如电脑这个词是 100万词汇中的第2维（便于举例），微机这个词是100万词汇中的第3维，维上的投影简单看作是tf，即文档中出现的次数。 A={x,2,0,…,x} 表示文档A中电脑出现了2次.x表示出现次数不care B={x,0,3,…,x} 表示文档B中微机次数出现了3次。 如果是用词做维度的向量空间，做聚类也好，分类也好，A和B在电脑和微机上的这种向量表示，机器理解为A和B完全在表示不同的意义。而事实上，如果在词的 高维空间上看，电脑和微机的维是很近似的，正交性是很低的。 如果能够将高维空间上，近义词或者表示接近的词的维度“捏“成一个维度，比如电脑和微机这两个词被捏成了第2维，但是每个词在这个维上的权重给与不同的度 量（比如概率）。 这样上诉例子变为 A={x,2*pi,x,…x}，pi表示电脑这个词到Topic2的转移概率。 B={x,3*pj,x,…x} 这样，A和B看上去在第二个Topic上显示了一定的相关性。 由于Topic是被捏后的产物，每个Topic的正交性直观上看都很强，LDA开源的工具做出的结果可以把转移到TOpic最Top的那些词提取出来，都 是十分相关或近似的词。而Topic与Topic之间显示出很大的差异性。 短文本分类的商业价值是很大的，在视频分类，广告分类上都可以看作是短文本分类问题，我有幸做了一些这方面的工作，其中提到的短文本的扩展是很好的思路。 问答系统商业价值也很巨大，特别是封闭领域的问答系统，可以拦截投诉，用户提问，降低人工成本。开放领域的问答系统商业上感觉前途有限，当然把搜索引擎的 搜索结果进一步精化的思路肯定是搜索引擎的一个方向，用户会越来越懒，搜索引擎已经让用户懒了一些，还需要让用户继续懒下去。 下面是baidu知道中有人对LDA的解释 lda是一个集合概率模型，主要用于处理离散的数据集合，目前主要用在数据挖掘（dm）中的text mining和自然语言处理中，主要是用来降低维度的。据说效果不错。 以下是在tm中对lda的定义： Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. 其实它还可以用在别的方面，早期是被用在自然语言处理的文本表示方面。因为他提供了一个理解相关词为什么在同一文档出现的框架解释模型。 ========================================= Latent Dirichlet allocation This is a C implementation of variational EM for latent Dirichlet allocation (LDA), a topic model for text or other discrete data. LDA allows you to analyze of corpus, and extract the topics that combined to form its documents. For example, click here to see the topics estimated from a small corpus of Associated Press documents. LDA is fully described in Blei et al. (2003) . This code contains: an implementation of variational inference for the per-document topic proportions and per-word topic assignments a variational EM procedure for estimating the topics and exchangeable Dirichlet hyperparameter Downloads Download the readme.txt . Download the code: lda-c.tgz . Sample data 2246 documents from the Associated Press [ download ]. Top 20 words from 100 topics estimated from the AP corpus [pdf]. 还有一个 普林斯顿大学 的主题模型的邮件列表，牛人们： https://lists.cs.princeton.edu/mailman/listinfo/topic-models ============================================= [论文导读][教程][介绍]文本处理、图像标注中的一篇重要论文Latent Dirichlet Allocation 原文信息 Latent Dirichlet Allocation David.M.Blei Andrew.Y.Ng Michael.I.Jordan JMLR2003 （可google到） 原文的主要内容 有两种方法设计分类器： 1. discriminative model，就是由样本直接设计判别函数，例如SVM； 2. generative model，就是先从样本恢复概率模型——例如我们熟悉的参数方法：混合高斯模型GMM;非参数方法Parzen 窗。然后再充分挖掘模型，用以分类。例如Bayes最大后验概率准则；或者将模型中的参数当作提取的特征（参数一般都比较少，所以这么做实际上是在降维），在这些新特征上设计分类器（例如又用SVM）。 恢复的模型可生成新的样本，所以得名generative。 原文就是讲了一种建立generative model的方法，用于文本处理。 对文本（document）中各单词（word）的出现频率（简称词频）建立概率模型通常是文本处理的第一步。 开始讨论前，先做如下约定： - 仅考虑文本的词频，而不考虑单词在文本中出现的先后顺序及其约束关系 - 文本中的单词来自大小为|V|的词汇表。例如： V = {FILM, MUSIC, TAX, MILLION, STUDENT, TEACHER, SCHOOL}. |V| = 7 - 每篇文本有N个单词 - 文本来自k个主题（topic）。例如: T = {Arts, Budgets, Education}. k = 3 一种简单直观的词频概率模型——unigram model（原文Figure 3(a)）这样描述某一文本中单词的“发生方式”： For each of the N words w_n: Choose a word w_n ～ p(w); 其中，w是离散随机变量，在词汇表V中取|V|个离散的值。p(w)是w的分布，可由训练样本通过机器学习或其它方法获得。这个模型就是每个单词的词频，没有考虑文本的主题，过于简单。于是我们引出考虑了文本主题的模型—— Mixture of unigram(原文中Figure 3(b)). 它这样描述某一文本中单词的“发生方式”： Choose a topic z ～ p(z); For each of the N words w_n: Choose a word w_n ～ p(w|z); 其中，z是离散随机变量，在主题T中取k个离散值，p(z)是z的分布；w_n是离散随机变量,在词汇表V中取|V|个离散值，p(w|z)是给定z时w的条件分布。z可取k个值，w可取|V|个值，p(w|z)可看作一个k×|V|的矩阵，可由训练样本通过机器学习或其它方法获得。 对照我们在前面的约定中给出的V和T的具体示例，p(w|z)是3×7矩阵。若p(w|z)的第1行表示主题{Education}——可以想象这个主题的文本中{STUDENT, TEACHER, SCHOOL}的词频会高些，其它单词的词频会低些——因此该行的行向量所表示的分布 p(w|z)会在{STUDENT, TEACHER, SCHOOL}附近出现峰值；若第2行表示主题{Budgets}，p(w|z)就会在 {TAX,MILLION}附近出现峰值… 在“发生”一篇文本前先随机选出p(w|z)的第z行（根据分布p(z)）；再依次随机选出第z行的w_1,w_2,…,w_N列（每次选取都根据分布p(w|z)）,这就“发生”出了文本中的所有单词。 但是这个模型只允许一篇文本有一个主题，这是不够妥当的。一篇关于北邮科研经费的文本，可能 {STUDENT, TEACHER, SCHOOL, TAX, MILLION}的词频都很高——这个文本同时具有两个主题 {Education,Budgets}。如何模拟一篇文本多个主题的情形呢？在此，我们跳过pLSI模型，直接引入原文探讨的—— Latent Dirichlet Allocation (LDA, 原文中Figure 1). 它这样描述某一文本中单词的“发生方式”： Choose parameter θ ～ p(θ); For each of the N words w_n: Choose a topic z_n ～ p(z|θ); Choose a word w_n ～ p(w|z); 其中θ是一个1×k的随机行向量，p(θ)是θ的分布，它的具体函数形式就是Dirichlet分布，这一分布保证θ的k个分量 θ_1,θ_2,…,θ_k都取连续的非负值，且θ_1 + θ_2 + … + θ_k = 1；z_n是离散随机变量，在主题T中取k个离散值，p(z|θ)是给定θ时z的条件分布，它的具体函数形式很简单，就是把θ直接拿来作为概率值：p(z = i|θ) = θ_i,也就是说z取第 1,2,…k个主题的概率分别是θ_1,θ_2,…,θ_k；w_n是离散随机变量,在词汇表V中取|V|个离散值，p(w|z)是给定z_n时 w的条件分布。和前面一样，可以把它看作k×|V|的矩阵。 LDA在“发生”一篇文本前,先随机生成一个1×k的向量θ（根据Dirichlet分布p(θ)）,生成的这个θ非负且归一化，可以看作某个随机变量的分布(也就是说，Dirichlet可以看作是分布的分布…)；然后随机选取p(w|z)的第z_1行（根据分布p(z|θ)）,接着随机选取z_1行的w_1列（根据分布p(w|z = z_1)）,同样的方法依次选出z_2,w_2,…z_N,w_N,这就“发生”出了文本中的所有单词。 剩下的任务就是如何根据训练样本学习出LDA模型的具体形式。模型无非是含有控制参数的表达式，学习出了参数就确定了模型。我们看看LDA有哪些控制参数呢？ - 分布p(θ)的表达式需要一个1×k行向量的参数，记为α - p(w|z)可看作k×|V|的矩阵，记该矩阵为β 把w看作观察变量，θ和z看作隐藏变量，就可以通过著名的EM算法学习出α和β，但这一过程中后验概率p(θ,z|w)无法计算出解析表达式，因此需要近似解，原文中使用了基于分解（factorization）假设的变分法(Variational Methods)，其实也就是先假设θ和z在给定w时条件独立：p(θ,z|w) ≈ p(θ|w)*p(z|w)，然后进行后续推导（参考本导读“预备知识 ” – Variational Inference）。这一套方法原文叫做variational EM，推导过程确实有些复杂，但最后总结出的不过是几个可以通过迭代求解的表达式（参见原文5.3节的1.2.两步）。 最后理一下原文的结构 1 Introduction 综述其它方法 2 Notation and terminology 符号约定 3 Latent Dirichlet allocation 详细介绍 4 Relationship with other latent variable models LDA与 unigram, mixture of unigram, pLSI的区别。前两个本导读已经提到了，建议读者再仔细比较pLSI和LDA的区别，理解为什么作者说pLSI不是well-defined graphic model 5 Inference and Parameter Estimation Inference部分介绍了 variational inference，就是本导读前面提到的如何近似计算隐藏变量的后验概率。Parameter Estimation就是用 EM法估计α和β 6 Examples和7 Applications and Empirical Results 给出的具体例子、应用和实验结果 预备知识 如果牢固掌握这些预备知识，理解原文会更容易些。 - p(X|Y)的记法。注意|右边的Y既可以表示随机变量（已经取定了某具体值），也可以表示普通的非随机变量。这样我们可以在最大似然估计和 Bayes方法间方便的“切换”，而不会让符号记法影响我们的表述。例如，考虑具有确定但未知参数μ，Σ的高斯分布p(x),可以记为p(x|μ,Σ); 若按照Bayes学派观点，可以将μ和Σ也看作随机变量，x的分布就能记为随机变量μ，Σ取定某值后的条件分布p(x|μ,Σ)——统一的记法。 - k取1分布/多项式分布(Multinomial)。考虑取3个离散值的随机变量x ～ p(x)。这个看似很平庸的分布…就是所谓的k 取1分布或称多项式分布。一般我们习惯的把它记为p(x_i) = u_i, i = 1,2,3,且u_1 + u_2 + u_3 = 1. 但在有些数学推导中，将它记为指数形式会更方便些.将x看作3维的随机向量，各分量是“互斥”的，即它只能取(1,0,0),(0,1,0),(0,0,1)三组值。于是可将分布重新记为 p(x) = (u_1^x_1)*(u_2^x_2)*(u_3^x_3).注意论文原文中Multinomial就是这儿说的k取1分布，与一些概率教科书中的定义不同。一般的k维情况依次类推。具体参[Bishop]的2.2节. - 共轭先验分布(Conjugate Prior)。考虑某概率密度函数，要估计其中的参数t。按照Bayes学派的观点，参数 t ～ p(t).我们有p(t|X) ∝ p(X|t)p(t),这个式子说：在没有做任何观测时，我们对t的知识用先验分布p(t)表示。当观察到X 后，就通过该式将先验概率p(t)更新（计算）为后验概率p(t|X)，使我们对t的知识增加。仔细观察，若p(t)与p(X|t)有相同的函数形式，那么后验概率p(t|X)就与先验概率p(t)有相同的函数形式——这使得t的后验概率与先验概率具有相同的表达式，只是参数被更新了！ 更妙的是，本次后验概率可以作为下次观测时的先验概率，于是当继续进行观测X_2,X_3…时，只是不断的在更新先验概率p(t)的参数,p(t)的函数形式不变。具体参见[Bishop]的2.2节。 这也是Bayes学派饱受批评的地方：先验概率的选取有时只是方便数学推导，而非准确的反映我们的先验知识。 - Dirichlet分布。现在我们可以说，Dirichlet分布就是k取1分布的Conjugate Prior。若k维随机向量 θ ～ Drichlet分布，则θ的k个分量θ_1,θ_2,…,θ_k都取连续的非负值，且 θ_1 + θ_2 + … + θ_k = 1。Dirichlet分布的具体表达式参见[Bishop]的2.2节。 - Simplex。考虑2维的例子：以(0,1)与(1,0)为端点的线段就是simplex。考虑3维的例子，以(0,0,1), (0,1,0),(0,0,1)为端点的三角形内部就是simplex。更高维的情况可依次类推。考虑θ ～ Drichlet分布。注意到θ的k个分量 θ_1,θ_2,…,θ_k都取连续的非负值，且θ_1 + θ_2 + … + θ_k = 1，可知Dirichlet分布的定义域是一个 simplex.这也就是原文中Figure 2那个三角形的含义（k = 3的示意图，让这个simplex三角形平躺在水平面上）。参见 [Bishop]的2.2节 - Graphical Models. 就是用图来表示随机变量中的依赖关系。这个tutorial一google一大把。建议参考 [Bishop]的8.1节，了解几个符号（空心圆圈——隐藏(latent)变量,实心圆圈——观察(observed)变量，方框——重复次数）就足够看懂原文中的Figure 1和Figure 3了。最多再看看[Bishop]的8.2节 - EM.关于这个的tutorial很多，但我觉得[Bishop]的9.2节是数学处理最为简洁，最容易看懂的(有个tutorial在关键的几步中用了大量∑和∏，让人抓狂) 。另外[Bishop]的9.4节也值得看，为理解其它内容如variational inference有好处。 - Variational Inference. 就是计算后验概率的近似方法。考虑随机变量{X,Z}，其中X是观察变量，Z = {Z_1,Z_2}是隐藏变量。用EM法或做Bayes推理的关键一步,就是要求后验概率p(Z|X).不巧的是,在一些复杂问题中 p(Z|X)没有解析表达式,需要近似求解.相关的方法很多,一种经常使用的是基于可分解(factorization)假设的方法：p(Z|X) ≈ p(Z_1|X)p(Z_2|X)——就是说强行假设Z_1和Z_2条件独立——然后进行后续推导。 这一假设当然会产生误差，考虑二维高斯分布p(Z|X) = p(Z_1,Z_2|X)，Z_1与Z_2不独立，所以p(Z_1,Z_2|X)的等高图是同心椭圆，椭圆可任意倾斜（例如，若Z_1与Z_2的线性相关系数是1，则椭圆倾斜45°）。现简记 p(Z_1|X) = q_1(Z_1), p(Z_2|X) = q_2(Z_2)，我们想改变q_1与q_2，用q_1*q_2去拟合 p(Z_1,Z_2|X).但无论如何改变q_1与q_2的形式，q_1*q_2的椭圆等高线都是长轴、短轴分别与Z_1轴、Z_2轴平行！不过，合适的 q_1与q_2保证q_1*q_2与p(Z|X)的峰值点重合，一般这就足以解决实际问题了。详细讲解可以参见[Bishop]的第10章。也可参考 [Winn]的1.8节。 另外，[Winn]提出了通用的计算框架，你不必每次都先用Variational Inference推导出公式，再手工编写代码；你只用在一个GUI里编辑好Graphical Model，再点start…（作为类比，考虑离散的线性系统转移函数H(z)，你可以由此推导出差分方程的表达式，然后用Matlab编程求解；也可以在Simulink中编辑好框图，再点start…） 参考文献 [Bishop] Pattern Recognition And Machine Learning. C.M.Bishop. Springer, 2006（cryppie在本版曾发过电子版） [Winn] Variational Message Passing and its Applications. John M. Winn. Ph.D. dissertation, 2004(可google到) 网上资源 可google到LDA的Matlab和C实现 http://vibes.sourceforge.net/index.shtml john winn的通用框架,Java实现（里面有文档指导怎样在Matlab中调Java）。","title":"latent Dirichlet allocation (LDA)"},{"content":"我经常在 TopLanguage 讨论组上推荐一些书籍，也经常问里面的牛人们搜罗一些有关的资料，人工智能、机器学习、自然语言处理、知识发现（特别地，数据挖掘）、信息检索这些无疑是 CS 领域最好玩的分支了（也是互相紧密联系的），这里将最近有关机器学习和人工智能相关的一些学习资源归一个类： 首先是两个非常棒的 Wikipedia 条目，我也算是 wikipedia 的重度用户了，学习一门东西的时候常常发现是始于 wikipedia 中间经过若干次 google ，然后止于某一本或几本著作。 第一个是“人工智能的历史”（History of Artificial Intelligence），我在讨论组上写道： 而今天看到的这篇文章是我在 wikipedia 浏览至今觉得最好的。文章名为《人工智能的历史》，顺着 AI 发展时间线娓娓道来，中间穿插无数牛人故事，且一波三折大气磅礴，可谓\"事实比想象更令人惊讶\"。人工智能始于哲学思辨，中间经历了一个没有心理学（尤其是认知神经科学的）的帮助的阶段，仅通过牛人对人类思维的外在表现的归纳、内省，以及数学工具进行探索，其间最令人激动的是 Herbert Simon （决策理论之父，诺奖，跨领域牛人）写的一个自动证明机，证明了罗素的数学原理中的二十几个定理，其中有一个定理比原书中的还要优雅，Simon 的程序用的是启发式搜索，因为公理系统中的证明可以简化为从条件到结论的树状搜索（但由于组合爆炸，所以必须使用启发式剪枝）。后来 Simon 又写了 GPS （General Problem Solver），据说能解决一些能良好形式化的问题，如汉诺塔。但说到底 Simon 的研究毕竟只触及了人类思维的一个很小很小的方面 —— Formal Logic，甚至更狭义一点 Deductive Reasoning （即不包含 Inductive Reasoning , Transductive Reasoning (俗称 analogic thinking）。还有诸多比如 Common Sense、Vision、尤其是最为复杂的 Language 、Consciousness 都还谜团未解。还有一个比较有趣的就是有人认为 AI 问题必须要以一个物理的 Body 为支撑，一个能够感受这个世界的物理规则的身体本身就是一个强大的信息来源，基于这个信息来源，人类能够自身与时俱进地总结所谓的 Common-Sense Knowledge （这个就是所谓的 Emboddied Mind 理论。 ），否则像一些老兄直接手动构建 Common-Sense Knowledge Base ，就很傻很天真了，须知人根据感知系统从自然界获取知识是一个动态的自动更新的系统，而手动构建常识库则无异于古老的 Expert System 的做法。当然，以上只总结了很小一部分我个人觉得比较有趣或新颖的，每个人看到的有趣的地方不一样，比如里面相当详细地介绍了神经网络理论的兴衰。所以我强烈建议你看自己一遍，别忘了里面链接到其他地方的链接。 顺便一说，徐宥同学打算找时间把这个条目翻译出来，这是一个相当长的条目，看不动 E 文的等着看翻译吧:) 第二个则是“人工智能”（Artificial Intelligence）。当然，还有机器学习等等。从这些条目出发能够找到许多非常有用和靠谱的深入参考资料。 然后是一些书籍 : 1. 《Programming Collective Intelligence》，近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的:P 2. Peter Norvig 的《AI, Modern Approach 2nd》（无争议的领域经典）。 3. 《The Elements of Statistical Learning》，数学性比较强，可以做参考了。 4. 《Foundations of Statistical Natural Language Processing》，自然语言处理领域公认经典。 5. 《Data Mining, Concepts and Techniques》，华裔科学家写的书，相当深入浅出。 6. 《Managing Gigabytes》，信息检索好书。 7. 《Information Theory：Inference and Learning Algorithms》，参考书吧，比较深。 相关数学基础（参考书，不适合拿来通读）： 1. 线性代数：这个参考书就不列了，很多。 2. 矩阵数学：《矩阵分析》，Roger Horn。矩阵分析领域无争议的经典。 3. 概率论与统计：《概率论及其应用》，威廉·费勒。也是极牛的书，可数学味道太重，不适合做机器学习的。于是讨论组里的 Du Lei 同学推荐了《All Of Statistics》并说到 机器学习这个方向，统计学也一样非常重要。推荐All of statistics，这是CMU的一本很简洁的教科书，注重概念，简化计算，简化与Machine Learning无关的概念和统计内容，可以说是很好的快速入门材料。 4. 最优化方法：《Nonlinear Programming, 2nd》非线性规划的参考书。《Convex Optimization》凸优化的参考书。此外还有一些书可以参考 wikipedia 上的最优化方法条目。要深入理解机器学习方法的技术细节很多时候（如SVM）需要最优化方法作为铺垫。 王宁同学推荐了好几本书： 《Machine Learning, Tom Michell》, 1997. 老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能\"新\"到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。 《Modern Information Retrieval, Ricardo Baeza-Yates et al》. 1999 老书，牛人。貌似第一本完整讲述IR的书。可惜IR这些年进展迅猛，这本书略有些过时了。翻翻做参考还是不错的。另外，Ricardo同学现在是Yahoo Research for Europe and Latin Ameria的头头。 《Pattern Classification (2ed)》, Richard O. Duda, Peter E. Hart, David G. Stork 大约也是01年左右的大块头，有影印版，彩色。没读完，但如果想深入学习ML和IR，前三章（介绍，贝叶斯学习，线性分类器）必修。 还有些经典与我只有一面之缘，没有资格评价。另外还有两本小册子，论文集性质的，倒是讲到了了不少前沿和细节，诸如索引如何压缩之类。可惜忘了名字，又被我压在箱底，下次搬家前怕是难见天日了。 （呵呵，想起来一本：《Mining the Web - Discovering Knowledge from Hypertext Data》 ） 说一本名气很大的书：《Data Mining: Practical Machine Learning Tools and Techniques》。Weka 的作者写的。可惜内容一般。理论部分太单薄，而实践部分也很脱离实际。DM的入门书已经不少，这一本应该可以不看了。如果要学习了解 Weka ，看文档就好。第二版已经出了，没读过，不清楚。 信息检索方面，Du Lei 同学再次推荐： 信息检索方面的书现在建议看Stanford的那本《Introduction to Information Retrieval》，这书刚刚正式出版，内容当然up to date。另外信息检索第一大牛Croft老爷也正在写教科书，应该很快就要面世了。据说是非常pratical的一本书。 对信息检索有兴趣的同学，强烈推荐翟成祥博士在北大的暑期学校课程，这里有全slides和阅读材料：http://net.pku.edu.cn/~course/cs410/schedule.html maximzhao 同学推荐了一本机器学习： 加一本书：Bishop, 《Pattern Recognition and Machine Learning》. 没有影印的，但是网上能下到。经典中的经典。Pattern Classification 和这本书是两本必读之书。《Pattern Recognition and Machine Learning》是很新（07年），深入浅出，手不释卷。 最后，关于人工智能方面（特别地，决策与判断），再推荐两本有意思的书， 一本是《Simple Heuristics that Makes Us Smart》 另一本是《Bounded Rationality: The Adaptive Toolbox》 不同于计算机学界所采用的统计机器学习方法，这两本书更多地着眼于人类实际上所采用的认知方式，以下是我在讨论组上写的简介： 这两本都是德国ABC研究小组（一个由计算机科学家、认知科学家、神经科学家、经济学家、数学家、统计学家等组成的跨学科研究团体）集体写的，都是引起领域内广泛关注的书，尤其是前一本，後一本则是对 Herbert Simon （决策科学之父，诺奖获得者）提出的人类理性模型的扩充研究），可以说是把什么是真正的人类智能这个问题提上了台面。核心思想是，我们的大脑根本不能做大量的统计计算，使用fancy的数学手法去解释和预测这个世界，而是通过简单而鲁棒的启发法来面对不确定的世界（比如第一本书中提到的两个后来非常著名的启发法：再认启发法（cognition heuristics）和选择最佳（Take the Best）。当然，这两本书并没有排斥统计方法就是了，数据量大的时候统计优势就出来了，而数据量小的时候统计方法就变得非常糟糕；人类简单的启发法则充分利用生态环境中的规律性（regularities），都做到计算复杂性小且鲁棒。 关于第二本书的简介： 1. 谁是 Herbert Simon 2. 什么是 Bounded Rationality 3. 这本书讲啥的： 我一直觉得人类的决策与判断是一个非常迷人的问题。这本书简单地说可以看作是《决策与判断》的更全面更理论的版本。系统且理论化地介绍人类决策与判断过程中的各种启发式方法（heuristics）及其利弊（为什么他们是最优化方法在信息不足情况下的快捷且鲁棒的逼近，以及为什么在一些情况下会带来糟糕的后果等，比如学过机器学习的都知道朴素贝叶斯方法在许多情况下往往并不比贝叶斯网络效果差，而且还速度快；比如多项式插值的维数越高越容易overfit，而基于低阶多项式的分段样条插值却被证明是一个非常鲁棒的方案）。 在此提一个书中提到的例子，非常有意思：两个团队被派去设计一个能够在场上接住抛过来的棒球的机器人。第一组做了详细的数学分析，建立了一个相当复杂的抛物线近似模型（因为还要考虑空气阻力之类的原因，所以并非严格抛物线），用于计算球的落点，以便正确地接到球。显然这个方案耗资巨大，而且实际运算也需要时间，大家都知道生物的神经网络中生物电流传输只有百米每秒之内，所以 computational complexity 对于生物来说是个宝贵资源，所以这个方案虽然可行，但不够好。第二组则采访了真正的运动员，听取他们总结自己到底是如何接球的感受，然后他们做了这样一个机器人：这个机器人在球抛出的一开始一半路程啥也不做，等到比较近了才开始跑动，并在跑动中一直保持眼睛于球之间的视角不变，后者就保证了机器人的跑动路线一定会和球的轨迹有交点；整个过程中这个机器人只做非常粗糙的轨迹估算。体会一下你接球的时候是不是眼睛一直都盯着球，然后根据视线角度来调整跑动方向？实际上人类就是这么干的，这就是 heuristics 的力量。 相对于偏向于心理学以及科普的《决策与判断》来说，这本书的理论性更强，引用文献也很多而经典，而且与人工智能和机器学习都有交叉，里面也有不少数学内容，全书由十几个章节构成，每个章节都是由不同的作者写的，类似于 paper 一样的，很严谨，也没啥废话，跟《Psychology of Problem Solving》类似。比较适合 geeks 阅读哈。 另外，对理论的技术细节看不下去的也建议看看《决策与判断》这类书（以及像《别做正常的傻瓜》这样的傻瓜科普读本），对自己在生活中做决策有莫大的好处。人类决策与判断中使用了很多的 heuristics ，很不幸的是，其中许多都是在适应几十万年前的社会环境中建立起来的，并不适合于现代社会，所以了解这些思维中的缺点、盲点，对自己成为一个良好的决策者有很大的好处，而且这本身也是一个非常有趣的领域。 （完）  ","title":"机器学习与人工智能学习资源导引"},{"content":"说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可 以给到1+, 也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比 IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉, 所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的 介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字 “Advances in Neural Information Processing Systems”, 所以, 与ICML/ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事, 但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML/ECML/COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短, 毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示/推理/学习等很多方面, AUAI (Association of UAI) 主办, 每年开. tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念, 几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说).","title":"偶尔转帖：AI会议的总结（by南大周志华）"},{"content":"    随着信息数字化和网络化的进程不断加快，人们的行为轨迹越来越多地被记录下来，这使得利用计算技术观察和研究社会成为可能。2009年2月，美国哈佛大学大卫·拉泽（David Lazer）等15位美国学者在《Science》上联合发表了一篇具有里程碑意义的文章“Computational Social Science”，该文指出：“计算社会科学”这一研究领域正在兴起，人们将在前所未有的深度和广度上自动地收集和利用数据，为社会科学的研究服务。     正如“计算语言学”又称为“自然语言处理”一样，“计算社会科学”又可以叫作“社会计算”，字面不同的术语其侧重点略有不同，但概念基本一致。本文遵从近年来国内学者的习惯，使用 “社会计算”一词。     那么，到底什么是“社会计算”呢？对于一个新兴的跨学科的研究领域往往是仁者见仁，智者见智，很难给出一个公认的定义。一般而言，社会计算是指社会行为和计算系统交叉融合而成的一个研究领域，研究的是如何利用计算系统帮助人们进行沟通与协作，如何利用计算技术研究社会运行的规律与发展趋势。     所谓“利用计算系统帮助人们进行沟通与协作”是指帮助人们在互联网上建设虚拟社会，对现实社会中人与人的关系进行复制和重构，使人们更紧密地联系在一起，随时随地互相通讯，以协作的方式生产知识。这方面的研究包括社会网络服务、群体智慧等。     所谓“利用计算技术研究社会运行的规律与发展趋势”，是指以社交网络和社会媒体为研究对象，从中发现社会关系、社会行为的规律，预测政策实施的可行性。社会学鼻祖奥古斯特·孔德最初定义社会学时，希望社会学能够使用类似物理学的方法，成为经得起科学规则考验的一门学科，互联网背景下的社会计算使这一理念具有了现实可行性。这方面的研究包括社会网络分析、内容计算、人工社会等。       研究内容     1。社交网络服务（Social Network Service，SNS）     谈到社交网络服务，就会让人想起时下最热门的Facebook。社交网络服务研究的是利用信息技术构建虚拟空间，实现社会性的交互和通信。SNS还有一种解释是社会网络软件（Social Network Software），电子邮件、网络论坛等许多传统网络工具都可以视为一种社会软件。     在社交网络服务的网站上，人们以认识朋友的朋友的方式，扩展自己的人脉。国内最有名的社交网络服务网站是“人人网”，他们从实践中总结出以下值得重点关注的研究点：社会关系强度、信息的绝对价值和相对价值、新鲜事排序算法、隐私性以及社会化搜索。      2. 群体智慧（Collective Intelligence）     群体智慧的典型应用是“维基百科”和“百度知道”。这些互联网平台系统不仅帮助用户相互沟通联系，更重要的是将用户组织起来，发挥他们的群体智慧，以协作的方式一起创造、加工和分享知识。     2005年，美国卡耐基梅隆大学的路易斯·冯·安（Luis Von Ahn）提出“人本计算（Human Computation）”的思想，用验证码、游戏等方式调动网民的热情，使众多的人脑自觉不自觉地参与到计算任务中，轻松地解决了本来非常耗时耗力的问题。这也是群体智慧的体现。     知识获取是一切智能系统的瓶颈，传统的依靠专家编辑知识的方式效率太低，无法满足大规模真实信息处理的需求。在网络社会的大背景下，群体智慧的出现为知识获取提供了一条崭新的充满希望的道路。如何巧妙地设计用户界面以激发用户的参与热情，如何克服人脑计算的不精确性，如何将人脑和电脑最佳地结合起来，都是值得深入研究的问题。     3. 社会网络分析（Social Network Analysis）     社会网络分析依据网络理论看待社会，节点是网络中的独立角色，边是社会关系，社会网络就是由节点和边构成的一张图，这张图往往非常复杂，节点之间的关系类型多种多样。     社会网络分析的典型例子是社区计算。中科院计算技术研究所的研究工作指出：社区是社会信息网络的普遍现象，大规模信息网络中的一些社会化特征在全局层面往往具有稳定的统计规律。如何度量、发现和利用这些规律是大规模社会信息网络分析与处理的一个基础问题。一般而言，社区结构是度量和利用这些特性的基本单元。因此，发现一个网络中有意义的、自然的、相对稳态的社区结构，对网络信息的搜索与挖掘、信息的推荐以及网络演化与扩散的预测具有重要价值。     4. 内容计算（Content Computing）     除社会网络外，社会媒体也是分析理解社会的重要素材，如新闻、论坛、博客、微博等。由于它们都以语言文字为主要展示形式，因此从事内容计算研究的学者需要掌握语言分析技术。当前内容计算的热点包括舆情分析、人际关系挖掘、微博应用等。     舆情分析：传统上，对舆情的研究主要有两种方法：一是观察思辨，，二是问卷调查。前者缺乏数据支持，后者采集的数据量亦有限。互联网技术为舆情分析提供了全新的技术路线，通过对各种社会媒体的跟踪与挖掘，结合传统的舆论分析理论，可以有效地观察社会的状态，并能辅助决策，及时发出预警。     基于内容的人际关系挖掘：互联网中蕴含着大量公开的人名实体和人际关系信息。利用文本信息抽取技术可以自动地抽取人名，识别重名，自动计算出人物之间的关系，进而找出关系描述词，形成一个互联网世界的社会关系网。微软亚洲研究院的“人立方”就是一个典型系统。     微博应用：如果说“人人网”是中国的Facebook，那么“新浪微博”则是中国的Twitter。近来“新浪微博”迅猛发展，2010年11月，其用户数为5000万，2011年3月，其用户数突破1亿，在四个月内翻了一倍。“微博”同时具有“社会网络”和“媒体平台”的属性，它催生了信息生产和传播方式的革命，对社会事件和人们的意识已然产生了很大影响。“微博”明确地定位为平台，它提供开放的API接口，积极支持第三方应用的发展，基于“微博”的研究与开发必将成为未来一段时期互联网学术界和产业界的热点。     5. 人工社会（Artificial Society）     社会计算的一个重要使命是对复杂社会问题建立计算模型，进行实验分析并提供决策支持。利用计算机模拟手段测试和验证社会经济政策的效果，已成为一个公共政策领域的迫切需求，这些需求催生了“人工社会”、“平行社会”等诸多相关领域的研究。     通过建立各种人工社会，构造相应的平行系统，为“全面、综合、可持续的科学发展观”提供了一种可行的分析和评估方法，并应用于复杂社会系统的管理与控制，可以为将要到来的数字化社会和数字化政府管理奠定基础。中科院自动化所是“人工社会”这一研究课题的积极倡导者和实践者。     社会计算面临的挑战     社会计算方兴未艾，生机勃勃，却也面临诸多挑战：     1. 个人数据整合：同一个人在不同场合、不同终端上留下的各种行为记录目前都散落在不同的存储节点上，要整合这些信息，不但涉及技术问题，更涉及复杂的管理问题。     2. 巨量数据存储：为大规模人类行为进行全面实时的记录，需要巨大的数据存储和管理能力，当前的计算机系统还无法满足这一需求。     3. 个人隐私保护：出于隐私保护方面的考虑，大型互联网公司往往不敢向学术界公开用户日志，致使学术界对社会计算的研究遇到用户数据采集方面的严重困难。匿名化处理是一种解决方案，但好事者依然可以从匿名后的数据中发现个人行为的轮廓，使匿名化处理失效。     4. 研究成果保密：如果揭示出某种社会关系或某个组织的运行规律，是否会受到相关个人和组织的质疑或反对呢？对研究成果要达到何等保密程度呢？     5. 学术队伍组织：由于背景各异，如何使计算机专家和社会科学领域的专家相互理解，密切配合，一道推进社会计算的研究，并非易事。同时，如何建设社会计算学科，培养既懂计算科学，又懂社会科学的人才，也是亟待探索的话题。       结语     社会计算是一个方兴未艾的多学科交叉领域，网络科学、复杂系统、数据挖掘、社会学、管理科学、语言处理、信息检索等不同背景的学者从不同的角度对社会计算进行了研究。社会计算的研究横跨文理，为社会科学提供了一条革命性的计算之路，其研究成果对于社会管理、社会生活都将产生重大影响。随着学术界、产业界和政府对社会计算的认识不断加深，关注度不断提高，社会计算正逐步进入蓬勃发展的阶段。   转自http://blog.sina.com.cn/s/blog_4cbec5e90100pyvz.html      ","title":"方兴未艾的社会计算"},{"content":"六月IBM将庆祝100周年，eWEEK的决定浏览一下系统巨头也许是最好的产品和技术，以及一些已经有了技术，但没有为公司创造那么好的经济效果的技 术。凭借IBM在研究和开发的丰富遗产，以及600亿美元的年度研发预算蓝色巨人开发了很多创新技术。事实上，在1月，IBM宣布，它的发明者在2010 年获得创纪录的5896项美国专利，这标志着它已连续第18年荣登世界最具创意的公司名单。 IBM成为美国第一家在一年内获得多达5000项专利的公司。自1911年成立后, IBM花了50年的时间首次获得年发明超过5000项专利。从交付的微调打孔卡和计算机主机登场，为磁带存储，个人电脑和新的沃森电脑,IBM公司获得了 超过其份额的更多的成就。但该公司也出现了一些失误。然而，即使IBM的技术没能商业化，并不是单纯的技术错误或只是简单的哑弹。许多要么超前自己时代， 也影响深远，有的没有足够的支持或市场，或处于激烈竞争的领域。 eWEEK的列举的20个IBM技术，取得了巨大的成就。随着行业分析师Judith Hurwitz, Rob Enderle, Amy Wohl and Al Hilwa的帮助, eWEEK的列举了其他10个IBM技术项目，虽然创新，但没有获得IBM公司所希望的商业上的成功。 IBM的打卡机 也许信息时代的最早的标志是IBM公司生产的被称为“IBM卡片机\"的一个简单的穿孔打卡机。尺寸仅为7-3/8和3-1/4英寸，张纸使连续的。 IBM卡片机收集了世界上近半个世纪几乎所有的的已知信息。在经济大萧条时期得到普及，并成为世界上无处不在的数据处理装置。卡片机还提供了一个重要的利 润流，这有助于IBM在20世纪中期的快速增长。选择IBM工程师Clair D. IBM成为美国第一家在一年内获得多达5000项专利的公司。自1911年成立后, IBM花了50年的时间首次获得年发明超过5000项专利。从交付的微调打孔卡和计算机主机登场，为磁带存储，个人电脑和新的沃森电脑,IBM公司获得了 超过其份额的更多的成就。但该公司也出现了一些失误。然而，即使IBM的技术没能商业化，并不是单纯的技术错误或只是简单的哑弹。许多要么超前自己时代， 也影响深远，有的没有足够的支持或市场，或处于激烈竞争的领域。 eWEEK的列举的20个IBM技术，取得了巨大的成就。随着行业分析师Judith Hurwitz, Rob Enderle, Amy Wohl and Al Hilwa的帮助, eWEEK的列举了其他10个IBM技术项目，虽然创新，但没有获得IBM公司所希望的商业上的成功。 IBM的打卡机 也许信息时代的最早的标志是IBM公司生产的被称为“IBM卡片机\"的一个简单的穿孔打卡机。尺寸仅为7-3/8和3-1/4英寸，张纸使连续的。 IBM卡片机收集了世界上近半个世纪几乎所有的的已知信息。在经济大萧条时期得到普及，并成为世界上无处不在的数据处理装置。卡片机还提供了一个重要的利 润流，这有助于IBM在20世纪中期的快速增长。选择IBM工程师Clair D. Lake的突破性的80列矩形孔设计，IBM公司有意识地作出决定，在装备中使用专有方式，不仅会加倍可存储的数据量卡，而且也只与IBM制造的机器保持 兼容。 系统主机 1959年，IBM推出了1401，第一次大批量生产， 使用存储程序和核心内存晶体管的主机。,它能运行各种企业应用软件的通用性使之成为在60年代初世界上最流行的计算机。 IBM还推出了1403链式打印机，有高速，高容量的打印效果。 直到20世纪70年代出现的激光打印机,1403年打印质量都是顶级。 1401是世界上第一个达到销量10000的系统。 IBM System/360 没多少产品能象IBM System/360那样的对技术，对世界的运作方式，或对创建它组织有如此巨大影响。本的System/360迎来了电脑兼容时代，首次允许跨产品线模 式，甚至允许与其他公司彼此合作。这是信息科学在新兴领域的转折点。System/360之后，该行业不再谈论有自动化与特殊任务的电脑。现在，技术提供 商谈论通过复杂的管理流程的计算机系统。System/360 IBM将主宰了未来20年的计算机行业。System/360处理了休斯敦美国宇航局载人航天中心首次登月资料的75%。后来也计算了宇航员返回地球所需 的数据。 磁带存储 20世纪40年代后期,受到克劳斯贝率先使用磁带记录他的广播节目，IBM的工程师开始为寻找数据存储打孔卡的继承者进行磁带试验。 1952年，IBM公司宣布推出第一款磁带存储单元IBM 726。 IBM推出了磁带驱动器减压塔，使脆弱的磁带可能成为一个可行的数据存储介质。真空列在IBM 701电子数据处理系统的使用标志着磁存储时代的开始。 个人电脑 1981年8月12日，在纽约市的新闻发布会上，Phillip \"Don\" Estridge 宣布了1565美元标价的IBM个人电脑（IBM 5150）。二十年前，IBM电脑成本经常高达900万美元，并要求四分之一英亩的有空调的空间和员工60人。新的IBM个人电脑不仅速度要快，而且还提 出可以每个家庭拥有一台电脑。 软盘 IBM开发软盘的工程师无法想象，它将很快成为消费者生活的很大一部分。它最初是设计用于为IBM的System/370主机大型系统加载数据。但 是，磁盘的小尺寸和不断增长的存储能力，导致其在较小的系统的用途。实用，耐用，灵活，软盘成为了新兴的个人电脑行业的首选存储介质。90年代中期每年在 世界范围内超过5亿元的销售达到高峰。 硅锗芯片 1994年，IBM研究中心发明了一种用于从低成本的半导体芯片的硅锗方法。锗比其他稀有昂贵的的材料更容易获得，并提高了集成电路的速度和多功能 性。引入全硅锗基础层的芯片提高了工作频率，电流，噪音和功率的能力。更小，更节能的芯片扩大了无线行业，从雷达到太空探索。今天，硅锗技术用在新一代的 移动设备和智能技术。 RAMAC 世界上第一个硬盘驱动器是两个厨房冰箱并排大小。它包含50个磁盘,每分钟1200转旋转，提供每秒10万位的数据。IBM RAMAC（随机存取统计和控制的方法）使企业能够以新的方式组合和匹配有关的资料，让每个比特的信息随意读取或变更。在IBM的磁带机之后 ，1956年发布的RAMAC推动了数据存储行业。 内存 60年代中期，IBM公司研究员鲍勃丹纳德研制出世界上第一个晶体管存储器，称之为“动态随机存取记忆体”，或DRAM。大型机现在可以配备内存作 为磁盘驱动器上存储的数据的短期缓冲区。内存芯片将保存计算机即将使用的信息，所以系统只在它需要新的东西才会访问磁盘驱动器。这加快了访问和使用存储信 息的过程。DRAM使计算机内存更小，更密集，更便宜，同时要求所有更省电。此外，简单、成本低、低功耗的DRAM，与首家低成本的微处理器相结合，开启 了小型个人电脑时代。今天，所有的个人电脑，笔记本电脑，游戏机和其他计算设备都有DRAM芯片。 DRAM也提升了运行互联网上大型机，服务器和数据中心的处理能力。 UPC码 UPC（通用产品代码）条形码系统是一个人在时间紧迫工作下的想法的结果。这个1973年发明变成了工业技术重要贡献。UPC是一个普遍的标准，但 却是历史上最被认可的设计，而且使IBM公司使用优雅简单的信息，就可以定制几乎所有的交易类型，可以尽可能多的处理所需数据。UPC对零售商来说，可以 更好地为客户服务，控制库存精确数据，丰富营销数据。 电子商务 初创的网络公司给消费者带来蓬勃发展的电子商务，大公司却不知道该怎么做。在90年代后期，IBM认识到大型机在联网交易的使用趋势和优势，帮助企 业建立所需的“e-business”战略。这是美国企业在互联网时代的关键点，这表明大公司，而不仅仅是硅谷的暴发户，有一个基于Web协作和商业的企 业重要未来。 WebSphere IBM在面对互联网高度泡沫时，IBM软件小组组长Steve Mills，叫他的三个顶级助手进他的办公室，商讨如何发展公司的顶尖企业软件工具。该谈话使得IBM的WebSphere应用服务器在1998年初发 布。最初时，WebSphere团队专注于迅速发展和支持HTTP，Servlet和Java Server Pages应用程序的Web应用程序的部署。 IBM公司迅速扩展WebSphere到交易应用程序以及更多。今天，WebSphere套件产品和服务帮助企业建立，运作和整合跨多个计算平台的电子应 用。 蓝色基因 上世纪90年代IBM公司花费1亿美元，五年时间开发的项目，充分利用可扩展并行处理于实际用途：气象预报，石油勘探和复杂的制造工艺。 IBM的工程师开始了大大提高计算机的速度和效率，同时减少其大小的追求。与劳伦斯利弗莫尔国家实验室合作研制的第一个蓝色基因帮助生物学家观察蛋白质折 叠和以前看不见的基因发展的进程。 打破petaflop的障碍 IBM于2008年在洛斯阿拉莫斯国家实验室建成的“Roadrunner的项目”，是世界上第一个运行在超过每秒1千万亿(petaflop)次 计算速度的计算机。也是世界上第一个“混合”超级计算机（使用两种不同的处理器架构），Roadrunner的能源效率是以前的两倍，使用一半的电力，就 可以维持同一水平的计算能力。 电动打字机 1961年，IBM推出电动打字机。在此之前，传统的打字机的字库必然趋于复杂，减缓了打字员的速度。电动打字机配有高尔夫球球头形打字，取代马车 式的字库驱动模式，减少了打字机上在桌面上空间量。银色的打字头部也消除了干扰问题：由于没有字库启动纠缠，打字速度和工作效率猛增。 FORTRAN语言 早期的计算机编程意味着使用一个神秘的针对每台计算机“机器码”。 IBM的程序员john Backus发现了一个更好的解决方案。 1957年，他和他的团队开发了第一个高层次的语言，Fortran语言（公式翻译系统）。一个FORTRAN程序可以运行在任何一个FORTRAN编译 器上，它有效地转化Backus的代码为机器码系统。这是第一次，代码是不是程序员的人理解，使数学家和科学家编写的程序，可以在不同的系统共享。 FORTRAN帮助软件从基础硬件中解放出来。 磁条技术 1969年，IBM工程师Forrest Parry遇到一个问题。他试图用一个塑料贴上磁带为中央情报局创建一个身份证件，但两者结合起来很困难。当他在他的妻子熨烫衣服的时候提到了这个问 题，，她建议使用铁来融合磁带。他试了一下，成功了。磁条，与各地的销售网络设备和数据相结合，有助于加快信贷卡在世界各地的使用和扩散。 准分子激光手术 1981年，三个IBM的科学家，Rangaswamy Srinivasan, James Wynne 和 Samuel Blum，发现了新发明的准分子激光能够清除微量特定人体组织，而不伤害周围地区。它成为改变角膜的形状和改善视力的LASIK和PRK无痛手术基础。 美国宇航局的航天任务 IBM的参与空间探索的种子在上世纪30年代哥伦比亚大学天文计算局成立之时就根植在Thomas J. Watson心中了，比美国航空航天局成立还早了几十年。 IBM公司参与了美国载人航天事业各个部分的工作 ，水星，泰坦火箭和阿波罗土星系统的工作任务,以及1969年阿波罗11号月球任务。 IBM还帮助泰坦火箭开发团，太空实验室和美苏阿波罗联盟项目的控制，以及航天飞机计划。 沃森 IBM的“沃森”利用先进的计算机自然语言处理和理解实现自动问答技术。它采用大规模并行的分析能力来模拟人类思维的能力，理解其背后的含义的话， 区分相关和不相关数据，并展示提供确切的答案的信心指数。今年二月，沃森不仅成为第一台在人类智力竞赛电视节目上一争高下的计算机，还以压倒性的优势赢了 前冠军肯詹宁斯和布拉德鲁特。 ---------------------------------------------------------------------------------------------------------------------------------------- OS / 2的 在OS / 2操作系统最初是由微软和IBM于1985年创建，后来由IBM专门开发。在首次发布两年后，1990和微软合作伙伴关系解体，微软接着开始提供 Windows与OS / 2操作系统竞争 。虽然OS / 2的功能有很多技术优势，但各种各样的原因使它从来没有真正流行起来。Windows获得了胜利。 PCjr IBM的PCjr（即“电脑少年”）是IBM的第一次尝试进入廉价的教育和家用电脑市场。该PCjr，IBM的型号4860，保留了IBM个人电脑的8088的CPU和BIOS兼容的接口，但不同的设计和实现决策导致PCjr商业上的失败。 Simon智能手机 Simon是IBM的超越时代的技术的一个例子。它提供了一个在今天的很多智能手机都提供的触摸屏界面和功能，但在1993年发布的时候，有没有足 够的带宽来支持所有电话的功能。SimonIBM个人通信手机是那个时代先进的技术，促成了IBM和南方贝尔联手。Simon1992年首次亮相是作为 Comdex贸易展的一个概念产品。在1993年推出时，它结合了移动电话，寻呼机，PDA和传真机的功能。经过一些延迟，在1994年由南方贝尔销售， 最初售价为899美元。除了移动电话，主要的应用是日历，地址簿，世界时钟，计算器，记事本，电子邮件和游戏。它没有物理按键来拨打电话,顾客使用手指在 触摸屏上选择电话号码或创建手写备忘录。文字输入是通过显示在屏幕上的 智能预测键盘或QWERTY键盘进行。 AD/Cycle 据SDLC的博客，IBM公司的AD/Cycle是基于IBM的系统应用架构的计算机辅助软件工程的倡议，它试图以集中的大型机应用程序开发的管 理，通过一致的用户界面，工作站服务，广告信息模型，工具服务，知识库服务的工具的集成，以及 软件库服务，提供定义和共享应用程序开发的控制数据。然而，以库为基础的发展战略并没有做成。 IBM的SSA 系统应用架构是20世纪80年代IBM的操作系统开发，包括的MVS，OS/400和OS / 2的实施的软件标准。 SAA的定义的三个层次的服务：一个普通用户接入层，一个共同的编程接口，通用的通信支持。SSA没有做成，由IBM的开放蓝图替代，这是他本身的扩展与 取代。 IBM的Home Director IBM的Home Director家庭网络解决方案是一种由一个其主要的技术公司第一次努力提供一个完整的家庭网络从一台PC或电视屏幕控制。它由IBM的家庭网络控制器 和IBM的家庭网络连接中心组成，是一个智能家居网络和控制解决方案，使用家庭网络的控制器电源和家庭网络的连接中心连接，以方便普通百姓家庭的各系统之 间的通信。 IBM公司后来于2000年剥离出来Home Director业务，作为一个独立的公司。 IBM现在似乎通过其智能家庭系统返回到家庭自动化业务。 IBM公司OfficeVision OfficeVision是IBM专有的办公室应用程序支持，主要基于IBM的VM操作系统和CMS用户界面（内容管理系统）运行。PC和客户服务 器模式的出现改变了办公自动化的组织。特别是办公用户想要图形用户界面。因此，随着PC客户端的电子邮件应用程序变得越来越流行。 OfficeVision未能获得市场认可.IBM的抵抗措施就是通过收购莲花，获取新的办公生产套件。 IBM的TopView TopView是IBM在1984年推出的一个文本模式的MS - DOS的多任务环境。为了与各种其他图形环境竞争，IBM承诺TopView将有一个用户友好的GUI，但从来没有。 Windows是从一开始就基于图形的，是市场首选，IBM公司1990在发布TopView1.1.2版本后正式停止生产。 微通道架构 微通道架构，或MCA，是上世纪80年代由IBM创建的PS / 2并行计算机一个专有的16 位或32位总线。在1987年首次推出高端的PS / 2系列MCA机器蔓延到IBM的整个电脑线。MCA相对ISA是一个巨大的技术进步，但IBM的引进和推广处理不当。 IBM没有象PC那样开发MCA外围卡市场。它没有提供采用先进的数字总线控制和I / O处理能力的MCA的周边设备卡。 只有少量周边卡制造商自行开发了这种技术。因此，没有为客户提供更多先进的功能，购买mac系统也较为昂贵的.IBM的竞争对手提供廉价的ISA多元化的设计选择。MCA消失几年后的PCI总线，引入了更广泛的行业支持。 IBM笔记本的创新特点 市场似乎没有准备好接受IBM ThinkPad笔记本的两个创新功能。一个是在ThinkPad TransNote，类似平板和iPad，但当时该技术还没有应用的地方。另一个是ThinkPad的蝴蝶，这是一个十年前的折叠式键盘的上网本，但它的 价格让其退出市场。ThinkPad TransNote不使用纸张和墨水。凭借其突破性的设计和能力，使用手写信息采集和管理，ThinkPad TransNote建立了灵活性和易用性的新模式。它有一个触摸屏或数字键盘和手写笔用于输入与检索信息。而且它有一个简易的264.2毫米（10.4英 寸）FlipTouch屏幕用于显示导航和检索关键信息。 原文连接 http://www.eweek.com/c/a/IT-Infrastructure/IBM-at-100-20-Technologies-That-Soared-and-10-That-Failed-348925/ 译文连接 http://www.chinashare.net/?/4013-1-0-1-1.html 欢迎访问中国开发论坛(http://www.chinashare.net ),论坛提供delphi/java/.net/php/c/python等开发语言交流平台，并有大量软件开发招聘资料可供参考，欢迎大家前来灌水. Lake的突破性的80列矩形孔设计，IBM公司有意识地作出决定，在装备中使用专有方式，不仅会加倍可存储的数据量卡，而且也只与IBM制造的机器保持 兼容。 系统主机 1959年，IBM推出了1401，第一次大批量生产， 使用存储程序和核心内存晶体管的主机。,它能运行各种企业应用软件的通用性使之成为在60年代初世界上最流行的计算机。 IBM还推出了1403链式打印机，有高速，高容量的打印效果。 直到20世纪70年代出现的激光打印机,1403年打印质量都是顶级。 1401是世界上第一个达到销量10000的系统。 IBM System/360 没多少产品能象IBM System/360那样的对技术，对世界的运作方式，或对创建它组织有如此巨大影响。本的System/360迎来了电脑兼容时代，首次允许跨产品线模 式，甚至允许与其他公司彼此合作。这是信息科学在新兴领域的转折点。System/360之后，该行业不再谈论有自动化与特殊任务的电脑。现在，技术提供 商谈论通过复杂的管理流程的计算机系统。System/360 IBM将主宰了未来20年的计算机行业。System/360处理了休斯敦美国宇航局载人航天中心首次登月资料的75%。后来也计算了宇航员返回地球所需 的数据。 磁带存储 20世纪40年代后期,受到克劳斯贝率先使用磁带记录他的广播节目，IBM的工程师开始为寻找数据存储打孔卡的继承者进行磁带试验。 1952年，IBM公司宣布推出第一款磁带存储单元IBM 726。 IBM推出了磁带驱动器减压塔，使脆弱的磁带可能成为一个可行的数据存储介质。真空列在IBM 701电子数据处理系统的使用标志着磁存储时代的开始。 个人电脑 1981年8月12日，在纽约市的新闻发布会上，Phillip \"Don\" Estridge 宣布了1565美元标价的IBM个人电脑（IBM 5150）。二十年前，IBM电脑成本经常高达900万美元，并要求四分之一英亩的有空调的空间和员工60人。新的IBM个人电脑不仅速度要快，而且还提 出可以每个家庭拥有一台电脑。 软盘 IBM开发软盘的工程师无法想象，它将很快成为消费者生活的很大一部分。它最初是设计用于为IBM的System/370主机大型系统加载数据。但 是，磁盘的小尺寸和不断增长的存储能力，导致其在较小的系统的用途。实用，耐用，灵活，软盘成为了新兴的个人电脑行业的首选存储介质。90年代中期每年在 世界范围内超过5亿元的销售达到高峰。 硅锗芯片 1994年，IBM研究中心发明了一种用于从低成本的半导体芯片的硅锗方法。锗比其他稀有昂贵的的材料更容易获得，并提高了集成电路的速度和多功能 性。引入全硅锗基础层的芯片提高了工作频率，电流，噪音和功率的能力。更小，更节能的芯片扩大了无线行业，从雷达到太空探索。今天，硅锗技术用在新一代的 移动设备和智能技术。 RAMAC 世界上第一个硬盘驱动器是两个厨房冰箱并排大小。它包含50个磁盘,每分钟1200转旋转，提供每秒10万位的数据。IBM RAMAC（随机存取统计和控制的方法）使企业能够以新的方式组合和匹配有关的资料，让每个比特的信息随意读取或变更。在IBM的磁带机之后 ，1956年发布的RAMAC推动了数据存储行业。 内存 60年代中期，IBM公司研究员鲍勃丹纳德研制出世界上第一个晶体管存储器，称之为“动态随机存取记忆体”，或DRAM。大型机现在可以配备内存作 为磁盘驱动器上存储的数据的短期缓冲区。内存芯片将保存计算机即将使用的信息，所以系统只在它需要新的东西才会访问磁盘驱动器。这加快了访问和使用存储信 息的过程。DRAM使计算机内存更小，更密集，更便宜，同时要求所有更省电。此外，简单、成本低、低功耗的DRAM，与首家低成本的微处理器相结合，开启 了小型个人电脑时代。今天，所有的个人电脑，笔记本电脑，游戏机和其他计算设备都有DRAM芯片。 DRAM也提升了运行互联网上大型机，服务器和数据中心的处理能力。 UPC码 UPC（通用产品代码）条形码系统是一个人在时间紧迫工作下的想法的结果。这个1973年发明变成了工业技术重要贡献。UPC是一个普遍的标准，但 却是历史上最被认可的设计，而且使IBM公司使用优雅简单的信息，就可以定制几乎所有的交易类型，可以尽可能多的处理所需数据。UPC对零售商来说，可以 更好地为客户服务，控制库存精确数据，丰富营销数据。 电子商务 初创的网络公司给消费者带来蓬勃发展的电子商务，大公司却不知道该怎么做。在90年代后期，IBM认识到大型机在联网交易的使用趋势和优势，帮助企 业建立所需的“e-business”战略。这是美国企业在互联网时代的关键点，这表明大公司，而不仅仅是硅谷的暴发户，有一个基于Web协作和商业的企 业重要未来。 WebSphere IBM在面对互联网高度泡沫时，IBM软件小组组长Steve Mills，叫他的三个顶级助手进他的办公室，商讨如何发展公司的顶尖企业软件工具。该谈话使得IBM的WebSphere应用服务器在1998年初发 布。最初时，WebSphere团队专注于迅速发展和支持HTTP，Servlet和Java Server Pages应用程序的Web应用程序的部署。 IBM公司迅速扩展WebSphere到交易应用程序以及更多。今天，WebSphere套件产品和服务帮助企业建立，运作和整合跨多个计算平台的电子应 用。 蓝色基因 上世纪90年代IBM公司花费1亿美元，五年时间开发的项目，充分利用可扩展并行处理于实际用途：气象预报，石油勘探和复杂的制造工艺。 IBM的工程师开始了大大提高计算机的速度和效率，同时减少其大小的追求。与劳伦斯利弗莫尔国家实验室合作研制的第一个蓝色基因帮助生物学家观察蛋白质折 叠和以前看不见的基因发展的进程。 打破petaflop的障碍 IBM于2008年在洛斯阿拉莫斯国家实验室建成的“Roadrunner的项目”，是世界上第一个运行在超过每秒1千万亿(petaflop)次 计算速度的计算机。也是世界上第一个“混合”超级计算机（使用两种不同的处理器架构），Roadrunner的能源效率是以前的两倍，使用一半的电力，就 可以维持同一水平的计算能力。 电动打字机 1961年，IBM推出电动打字机。在此之前，传统的打字机的字库必然趋于复杂，减缓了打字员的速度。电动打字机配有高尔夫球球头形打字，取代马车 式的字库驱动模式，减少了打字机上在桌面上空间量。银色的打字头部也消除了干扰问题：由于没有字库启动纠缠，打字速度和工作效率猛增。 FORTRAN语言 早期的计算机编程意味着使用一个神秘的针对每台计算机“机器码”。 IBM的程序员john Backus发现了一个更好的解决方案。 1957年，他和他的团队开发了第一个高层次的语言，Fortran语言（公式翻译系统）。一个FORTRAN程序可以运行在任何一个FORTRAN编译 器上，它有效地转化Backus的代码为机器码系统。这是第一次，代码是不是程序员的人理解，使数学家和科学家编写的程序，可以在不同的系统共享。 FORTRAN帮助软件从基础硬件中解放出来。 磁条技术 1969年，IBM工程师Forrest Parry遇到一个问题。他试图用一个塑料贴上磁带为中央情报局创建一个身份证件，但两者结合起来很困难。当他在他的妻子熨烫衣服的时候提到了这个问 题，，她建议使用铁来融合磁带。他试了一下，成功了。磁条，与各地的销售网络设备和数据相结合，有助于加快信贷卡在世界各地的使用和扩散。 准分子激光手术 1981年，三个IBM的科学家，Rangaswamy Srinivasan, James Wynne 和 Samuel Blum，发现了新发明的准分子激光能够清除微量特定人体组织，而不伤害周围地区。它成为改变角膜的形状和改善视力的LASIK和PRK无痛手术基础。 美国宇航局的航天任务 IBM的参与空间探索的种子在上世纪30年代哥伦比亚大学天文计算局成立之时就根植在Thomas J. Watson心中了，比美国航空航天局成立还早了几十年。 IBM公司参与了美国载人航天事业各个部分的工作 ，水星，泰坦火箭和阿波罗土星系统的工作任务,以及1969年阿波罗11号月球任务。 IBM还帮助泰坦火箭开发团，太空实验室和美苏阿波罗联盟项目的控制，以及航天飞机计划。 沃森 IBM的“沃森”利用先进的计算机自然语言处理和理解实现自动问答技术。它采用大规模并行的分析能力来模拟人类思维的能力，理解其背后的含义的话， 区分相关和不相关数据，并展示提供确切的答案的信心指数。今年二月，沃森不仅成为第一台在人类智力竞赛电视节目上一争高下的计算机，还以压倒性的优势赢了 前冠军肯詹宁斯和布拉德鲁特。 ---------------------------------------------------------------------------------------------------------------------------------------- OS / 2的 在OS / 2操作系统最初是由微软和IBM于1985年创建，后来由IBM专门开发。在首次发布两年后，1990和微软合作伙伴关系解体，微软接着开始提供 Windows与OS / 2操作系统竞争 。虽然OS / 2的功能有很多技术优势，但各种各样的原因使它从来没有真正流行起来。Windows获得了胜利。 PCjr IBM的PCjr（即“电脑少年”）是IBM的第一次尝试进入廉价的教育和家用电脑市场。该PCjr，IBM的型号4860，保留了IBM个人电脑的8088的CPU和BIOS兼容的接口，但不同的设计和实现决策导致PCjr商业上的失败。 Simon智能手机 Simon是IBM的超越时代的技术的一个例子。它提供了一个在今天的很多智能手机都提供的触摸屏界面和功能，但在1993年发布的时候，有没有足 够的带宽来支持所有电话的功能。SimonIBM个人通信手机是那个时代先进的技术，促成了IBM和南方贝尔联手。Simon1992年首次亮相是作为 Comdex贸易展的一个概念产品。在1993年推出时，它结合了移动电话，寻呼机，PDA和传真机的功能。经过一些延迟，在1994年由南方贝尔销售， 最初售价为899美元。除了移动电话，主要的应用是日历，地址簿，世界时钟，计算器，记事本，电子邮件和游戏。它没有物理按键来拨打电话,顾客使用手指在 触摸屏上选择电话号码或创建手写备忘录。文字输入是通过显示在屏幕上的 智能预测键盘或QWERTY键盘进行。 AD/Cycle 据SDLC的博客，IBM公司的AD/Cycle是基于IBM的系统应用架构的计算机辅助软件工程的倡议，它试图以集中的大型机应用程序开发的管 理，通过一致的用户界面，工作站服务，广告信息模型，工具服务，知识库服务的工具的集成，以及 软件库服务，提供定义和共享应用程序开发的控制数据。然而，以库为基础的发展战略并没有做成。 IBM的SSA 系统应用架构是20世纪80年代IBM的操作系统开发，包括的MVS，OS/400和OS / 2的实施的软件标准。 SAA的定义的三个层次的服务：一个普通用户接入层，一个共同的编程接口，通用的通信支持。SSA没有做成，由IBM的开放蓝图替代，这是他本身的扩展与 取代。 IBM的Home Director IBM的Home Director家庭网络解决方案是一种由一个其主要的技术公司第一次努力提供一个完整的家庭网络从一台PC或电视屏幕控制。它由IBM的家庭网络控制器 和IBM的家庭网络连接中心组成，是一个智能家居网络和控制解决方案，使用家庭网络的控制器电源和家庭网络的连接中心连接，以方便普通百姓家庭的各系统之 间的通信。 IBM公司后来于2000年剥离出来Home Director业务，作为一个独立的公司。 IBM现在似乎通过其智能家庭系统返回到家庭自动化业务。 IBM公司OfficeVision OfficeVision是IBM专有的办公室应用程序支持，主要基于IBM的VM操作系统和CMS用户界面（内容管理系统）运行。PC和客户服务 器模式的出现改变了办公自动化的组织。特别是办公用户想要图形用户界面。因此，随着PC客户端的电子邮件应用程序变得越来越流行。 OfficeVision未能获得市场认可.IBM的抵抗措施就是通过收购莲花，获取新的办公生产套件。 IBM的TopView TopView是IBM在1984年推出的一个文本模式的MS - DOS的多任务环境。为了与各种其他图形环境竞争，IBM承诺TopView将有一个用户友好的GUI，但从来没有。 Windows是从一开始就基于图形的，是市场首选，IBM公司1990在发布TopView1.1.2版本后正式停止生产。 微通道架构 微通道架构，或MCA，是上世纪80年代由IBM创建的PS / 2并行计算机一个专有的16 位或32位总线。在1987年首次推出高端的PS / 2系列MCA机器蔓延到IBM的整个电脑线。MCA相对ISA是一个巨大的技术进步，但IBM的引进和推广处理不当。 IBM没有象PC那样开发MCA外围卡市场。它没有提供采用先进的数字总线控制和I / O处理能力的MCA的周边设备卡。 只有少量周边卡制造商自行开发了这种技术。因此，没有为客户提供更多先进的功能，购买mac系统也较为昂贵的.IBM的竞争对手提供廉价的ISA多元化的设计选择。MCA消失几年后的PCI总线，引入了更广泛的行业支持。 IBM笔记本的创新特点 市场似乎没有准备好接受IBM ThinkPad笔记本的两个创新功能。一个是在ThinkPad TransNote，类似平板和iPad，但当时该技术还没有应用的地方。另一个是ThinkPad的蝴蝶，这是一个十年前的折叠式键盘的上网本，但它的 价格让其退出市场。ThinkPad TransNote不使用纸张和墨水。凭借其突破性的设计和能力，使用手写信息采集和管理，ThinkPad TransNote建立了灵活性和易用性的新模式。它有一个触摸屏或数字键盘和手写笔用于输入与检索信息。而且它有一个简易的264.2毫米（10.4英 寸）FlipTouch屏幕用于显示导航和检索关键信息。","title":"IBM百年庆典标志:20个成功的项目 10个失败的项目"},{"content":"转自：http://www.matrix67.com/blog/archives/333    除了字符串匹配、查找回文串、查找重复子串等经典问题以外，日常生活中我们还会遇到其它一些怪异的字符串问题。比如，有时我们需要知道给定的两个 字符串“有多像”，换句话说两个字符串的相似度是多少。1965年，俄国科学家Vladimir Levenshtein给字符串相似度做出了一个明确的定义叫做Levenshtein距离，我们通常叫它“编辑距离”。字符串A到B的编辑距离是指，只 用插入、删除和替换三种操作，最少需要多少步可以把A变成B。例如，从FAME到GATE需要两步（两次替换），从GAME到ACM则需要三步（删除G和 E再添加C）。Levenshtein给出了编辑距离的一般求法，就是大家都非常熟悉的经典动态规划问题。     在自然语言处理中，这个概念非 常重要，例如我们可以根据这个定义开发出一套半自动的校对系统：查找出一篇文章里所有不在字典里的单词，然后对于每个单词，列出字典里与它的 Levenshtein距离小于某个数n的单词，让用户选择正确的那一个。n通常取到2或者3，或者更好地，取该单词长度的1/4等等。这个想法倒不错， 但算法的效率成了新的难题：查字典好办，建一个Trie树即可；但怎样才能快速在字典里找出最相近的单词呢？这个问题难就难在，Levenshtein的 定义可以是单词任意位置上的操作，似乎不遍历字典是不可能完成的。现在很多软件都有拼写检查的功能，提出更正建议的速度是很快的。它们到底是怎么做的 呢？1973年，Burkhard和Keller提出的BK树有效地解决了这个问题。这个数据结构强就强在，它初步解决了一个看似不可能的问题，而其原理 非常简单。     首先，我们观察Levenshtein距离的性质。令d(x,y)表示字符串x到y的Levenshtein距离，那么显然： 1. d(x,y) = 0 当且仅当 x=y  （Levenshtein距离为0 <==> 字符串相等） 2. d(x,y) = d(y,x)     （从x变到y的最少步数就是从y变到x的最少步数） 3. d(x,y) + d(y,z) >= d(x,z)  （从x变到z所需的步数不会超过x先变成y再变成z的步数）     最 后这一个性质叫做三角形不等式。就好像一个三角形一样，两边之和必然大于第三边。给某个集合内的元素定义一个二元的“距离函数”，如果这个距离函数同时满 足上面说的三个性质，我们就称它为“度量空间”。我们的三维空间就是一个典型的度量空间，它的距离函数就是点对的直线距离。度量空间还有很多，比如 Manhattan距离，图论中的最短路，当然还有这里提到的Levenshtein距离。就好像并查集对所有等价关系都适用一样，BK树可以用于任何一 个度量空间。     建树的过程有些类似于Trie。首先我们随便找一个单词作为根（比如GAME）。以后插入一个单词时首先计算单词与 根的Levenshtein距离：如果这个距离值是该节点处头一次出现，建立一个新的儿子节点；否则沿着对应的边递归下去。例如，我们插入单词FAME， 它与GAME的距离为1，于是新建一个儿子，连一条标号为1的边；下一次插入GAIN，算得它与GAME的距离为2，于是放在编号为2的边下。再下次我们 插入GATE，它与GAME距离为1，于是沿着那条编号为1的边下去，递归地插入到FAME所在子树；GATE与FAME的距离为2，于是把GATE放在 FAME节点下，边的编号为2。           查询操作异常方便。如果我们需要返回与错误单词距离不超过n的单词，这个错误单词与树根所对应的单词距离为d，那么接下来我们只需要递归地考虑编号在d-n到d+n范围内的边所连接的子树。由于n通常很小，因此每次与某个节点进行比较时都可以排除很多子树。     举 个例子，假如我们输入一个GAIE，程序发现它不在字典中。现在，我们想返回字典中所有与GAIE距离为1的单词。我们首先将GAIE与树根进行比较，得 到的距离d=1。由于Levenshtein距离满足三角形不等式，因此现在所有离GAME距离超过2的单词全部可以排除了。比如，以AIM为根的子树到 GAME的距离都是3，而GAME和GAIE之间的距离是1，那么AIM及其子树到GAIE的距离至少都是2。于是，现在程序只需要沿着标号范围在1-1 到1+1里的边继续走下去。我们继续计算GAIE和FAME的距离，发现它为2，于是继续沿标号在1和3之间的边前进。遍历结束后回到GAME的第二个节 点，发现GAIE和GAIN距离为1，输出GAIN并继续沿编号为1或2的边递归下去（那条编号为4的边连接的子树又被排除掉了）……     实践表明，一次查询所遍历的节点不会超过所有节点的5%到8%，两次查询则一般不会17-25%，效率远远超过暴力枚举。适当进行缓存，减小Levenshtein距离常数n可以使算法效率更高。","title":"编辑距离、拼写检查与度量空间：一个有趣的数据结构"},{"content":"了解nutch的人基本上对这个开源的系统都是比较欣赏的，起码在国内是这样的，也很有多搜索网站是基于这个系统修改过来的，不过要做得好，做得真正是一个商业化的搜索，这个修改就不是一朝一夕的事情，也不是修修剪剪那么简单了。 作为一个通用的全网级别的搜索引擎架构，nutch(lucene)确实为广大人民群众提供了一块大大的蛋糕，为进入搜索这个行业大大降低了门槛。那么它距商业的搜索到底有多远呢？以我的个人观点来谈一下。   一、总体功能 一个专业的网络搜索引擎至少包含3部分即抓取、处理和搜索。下面是它们的一般功能： 抓取：抓取（蜘蛛、爬虫、crawler、spider等）程序负责爬行特定网络（也可能是整个网络），把网络上的页面和其它需要的文件下载到本地来。目前的难点是web2.0的普及导致的js分析和身份认证等问题。 处理：处理（分类、信息抽取、数据挖掘、classify、information extraction、data mining等）程序对抓回来的页面进行分析，比如，对网站的内容进行分类、对新闻页面的新闻信息进行提取、页面模版生成、对各个网站之间的关系进行计算等等。 搜索：搜索（information retrieve）程序负责把文档填充到数据库，然后根据查询字符串到数据库中寻找最相关的文档展现给用户。   二、信息抓取 网络信息抓取包含了见面抓取、文本文件抓取和其它文件地抓取。普通的信息抓取利用基本的html页面分析器(htmlParser、NeckoHtml、 JTidy等)来解析页面，得到其中的信息。基本上两点，一个是抓取，一个是分析。 抓取这一步要处理身份论证、要支持多种协议等等的，nutch在这里默认的插件使用的是nekohtml，效果还可以。但是nutch对html分析的结果的文本是把页面里所有的文本都合在一起(其中有一个开关来控制内层锚文本是否加上)作为总文本输出，所以这样页面上所有的噪音都没有去除 。 另一个是分析，分析一个html，最强的要数ie、firefox等浏览器了，在这一步上，nutch默认的htmlParser的处理能力是不可同日而语的。现在ajax盛行，对于js的处理也是一个重大的问题，现在nutch对js是视而不见的。   三、信息处理 对于信息的处理是nutch最薄弱的环节了，同时也是这个行业里的“宝地”，胜败决定就在这里。这里包括分类、信息抽取、数据挖掘、classify、information extraction、data mining等等。 在默认的nutch组件里有cluster这个包，是用来为搜索结果进行聚类用的， nutch默认的聚类是用开源Carrot2 的后缀树算法做Web文本聚类 还有ontology (本体),是人工智能范畴内的概念。 Ontology研究热点的出现与Semantic Web的提出和发展直接相关，借助Ontology中的推理规则，使应用系统具有一定的推理能力。默认的nutch也带了一个简单的ontology的应 用系统--HP的jena。但是对于一个商业应用来说这些仅仅是一个模具性质的。 nutch为这方面准备了最最基本的接口，其它的就得自己搞定了，比如机器学习(ML)、自然语言处理(NLP)、数据分析(DA)。。 而   四、搜索 nutch其实从功能上来讲是由爬虫和搜索两大部分组成的，搜索是lucene来挑梁的。所以这部分的局限其实就是lucene的局限了。lucene也可能从功能上分为两大部分，一个是索引，一个是查询。对于这部分的研究已经很久了，就是把用户最想要的文档返回给用户，对于搜索引擎而言，速度是非常重要的。索引，专业点说，包含2种：前向索引和反向索引（倒排索引，inverted index）。前者表示的是某个文档里面的所有词语，后者表示的是包含某个词语的所有文档。对应到Lucene上面，它的前向索引可以认为是Term Vectors（词语向量）相关文件，包含.tvx、.tvd和.tvf这3种文件。前向索引没有什么好评论的，它一般只是做为重组原始数据时候的依据，其构建十分简单明了。反向索引对应到Lucene上就是index（索引）。 Lucene把索引划分成一个一个的segment（块，其实是一个小索引），直观的说，当有一批新数据到达的时候，我们一般给其构建成一个新的segment，这是因为修改原来的segment的代价很高（并不是说一定很高，只是lucene采用的文件结构无法简单的加入新的文档）。 当一个index包含的segment太多的时候，查找性能就很差了（因为一次查询需要查询多个segment），需要进行segment的合并。在搜索方面nutch对lucene作了外部的处理，一是可以进行分布式搜索，每个节点只返回最高分值的结果，最近再合并；另一方面是对查询进行缓冲，不过只有一级缓冲--LRU(nutch的cache策略及cache策略研究)。   五、结论 仅仅从搜索引擎的构架来看，Lucene(Nutch)缺失的一环是信息的处理。信息的处理恰恰是整个搜索引擎中最核心的技术。所以说对于现在这个行业化、垂直化的搜索时代，nutch 的先天不足就已经是致命的了，但是这并不是不可挽回的，nutch的插件式架构，开放的系统逻辑等等特点，已经为开发者打开了窗户。nutch比较通用的处理逻辑，加上灵活的插件式架构，给我们定制它插上了翅膀。但是它也仅仅是一个框架，里面的任何一个细节都会让你头痛不已(比如ML，NLP)!所以真正的难点也就是这些让人头痛的地方。 Generated by Bo-blog 2.1.1 Release  ","title":"Nutch距离一个商业应用的搜索引擎还有多远收藏"},{"content":"昨天在四号线上手机上网时发现百度的状态已经变成了“三面已通过”，在感叹百度做决定的效率之高之余，心里的一块大石头也落地了，毕竟这是第一个实习offer（虽然offer还没到手-。-），而我打主意去的公司也就剩下百度的和有道了，关于公司的选择一会儿在三面的时候我会提到，所以还是有一定压力的。好在从笔试到三面一路下来，除了时间上跨度比较远——大概横跨了快一个月，其他方面还是比较顺利的，甚至可以说是幸运的。 7号笔试的时候是在清华管院，我大概扫了下名单，发现最差的学校也是北京理工大学，果然百度的要求还是挺高的。然后看了下自然语言处理方向的人数，总共大概才60人，不知道其他考场还有没有，但是相对于其他方向肯定算是比较少的，最后录取的人数应该也蛮少的，心里难免有点担心，不过自己确实对这个方向比较感兴趣，这学期也做了很多这方面的学习，即使最后真的去不成，好歹也可以了解下自己目前的水平和需要改进的地方。笔试的题目没什么好说的，比腾讯的难一些，全是大题：第一部分是C的语法、计算机网络和设计模式，除了设计模式比较熟悉外，前两个都不太确定了，按照印象写的，回去却发现网络那个居然写对了，确实运气不错；第二部分是两个设计题，一个给出算法思想，一个给出准确代码，也不算太难；第三部分有点难度，涉及到了海量数据的存储和查询的系统的设计，要考虑很多因素，这个我考虑了很久，最后想的也不是很清楚，只是尽量的把自己觉得合理的一些策略或者机制写了下，但是距离一个比较完善的系统还很遥远，所以考完之后觉得不是那么理想，也考虑过笔试都通不过的可能，不过觉得前面还做得不错，而且一般来说笔试不会涮很多人，所以对进一面还是比较有信心的。 从笔试到一面这段时间是最难熬的一段时间，虽然对自己很有信心，但是眼看周围的人都通知了，而自己却完全没消息，难免会有一些动摇和沮丧，好在我知道的其他自然语言处理方向的人也都没有通知（不过后来发现北邮已经有一些同学获得了通知），略感欣慰。17号，一面的通知终于来了，这时候距离笔试已经10天了，因为第二天有考试，所以定在了19号——星期四，从一面开始的三场面试就比较有规律的，一星期一场，都是周四下午。          19号一面的面试官是一个很阳光的帅哥，大概只比我大三到五岁，整个面试的风格都很nice。面试开始的时候问了很多关于我们学院的情况，应该是为了让我放松情绪吧，然后让我介绍下自己。自我介绍我准备的还是比较充分的，先提了下本科的一些基本情况，核心课程什么的，然后重点突出了这学期的信息检索和数据挖掘课程，并说明自己对相关领域有很强的兴趣，在课下也做了一些工作，当我提到我最近在翻译隐马尔科夫模型的一些国外的入门介绍的时候，帅哥面试官也表现了很强的兴趣，首先问了一句“你知道隐马尔科夫模型？”，可能更多的是确定刚才没听错，我只好说自己目前是入门水平，尽量回答他的问题吧，这个绝对不是谦虚，确实是因为是刚开始接触。然后他就问了如果用隐马尔科夫模型来进行词性标注，即如何知道“百度大厦”这个百度是名词还是动词，这里显然是名词，但是在“百度一下”中百度就是一个动词，然后我就结合刚学习的隐马尔科夫模型，把百度这个词作为一个可观察的状态，百度的词性作为一个隐藏的状态，和他讲了在这个应用下转移矩阵和混淆矩阵以及马尔科夫假设等，虽然说到最后我都觉得词性标注可能用隐马尔科夫不是很合适，尤其是混淆的概念不是很合适，但是基本上还是把隐马尔科夫模型说的比较清楚了。他也比较满意，然后问了下一个变形，即把马尔科夫假设进行了扩展，然后让我考虑，我说了很多，思路也不是很清楚，最后他提示我讲的可能太深了，他需要的只是一个条件概率公式，我果断写了下，他点点头，然后看了下时间，发现已经过了四十分钟，于是决定再给我出两个问题，一个是海量数据处理，这个我之前准备的也很充分，很快的就给出了一个解答，他想了想就说了两个字“可行”。第二个是两个树的结构的比较，我先给出了一个递归的朴素的解法，然后提出了从树的结构信息方面进行优化的思路，他觉得还行，然后又把问题引入了怎么比较两个很长的字符串是否相同的问题上，我一看，果断说了哈希。然后他说你这里说的应该是全文哈希吧，我一听，自然说，如果数据比较大并且允许一些误差，可以使用采样哈希，n-gram等等，然后他笑着说“我发现你的思维跳跃性很强啊，我一否定一个思路，你就能立马给出一个完全不同的想法”，我不好意思的说“可能我的想法比较多，但是思维不是很连贯”，他大笑道“没事，你懂我懂就行了”，然后我知道一面基本上可以确定过了，后面的就是一些不是很重要的问题啦，实习时间啊什么的。 一面当天晚上就出结果了，不出意外的通过了，这对自己的自信心的提升也有很大的帮助，至少感觉不是那么遥远了。二面定在了26号，刚好距离一面一个星期。二面的面试官看上去就比较犀利，上来就直接出了几个自然语言处理方面的问题。第一个是多音字注音的问题，这个刚好和我之前在信息检索课上面听到的多意字词意消歧的思路是一模一样的，于是我就给出了一个分类的机器学习的方法，分类的特征主要是词的上下文，也就是窗口内的其他词，这个方法的思路应该是没问题的，最后他让我给出一个比较好的窗口大小，我给出了正负2的范围，稍微解释下，就过了。第二个问题是让我介绍下我在简历中说的分类，聚类和频繁模式挖掘的项目，这个自然是我的长项，因为项目确实是很认真的做了，而且之前刚好数据挖掘考试，也把整个思路又重新理顺了一遍，所以这时候思路就非常清晰了，包括朴素贝叶斯的公式的意义、频繁模式中支持度和置信度的统计方法、关联模式的确定。第三个问题和刚才的频繁模式有点关系，要求在海量的query中找到某些经常出现的模式，也就是找到某些经常同时出现在一个query中的term，这个和刚才的频繁模式有点像，但是先要做一步频繁term的排序，取top k的频繁词，然后再按照刚才的思路做，海量数据找top k的思路我也有准备，所以答得也还算顺利。最后一个问题有点波折，是问怎么求两个query的相似度，我开始的思路局限在了VSM模型上，提出了很多改善的方法，但是都被他一一否定，最后他提示了一句“求query的相似度不一定非要用query本身啊”，我恍然大悟，想到了之前在Croft的书上看到的搜索结果的反馈机制，于是遍给出了利用query结果文档来计算相似度的思路，他直接说没什么问题了，以后会有人联系我，然后我回到寝室一看，状态已经改了，效率比上次还高。 三面是在2号，这次是自然语言处理部门的经理，看上去也很年轻，当我介绍到自己是保送的时候，他立马说了句你应该很强吧，我当时脑袋上就有面条了=。=。后面的话他主要是问了我一些对自然语言处理领域的了解，为什么选择百度这个公司等等，这些问题自然不在话下。我觉得有点意外的是他问我投了几个公司，我说三个，腾讯投的技术研究，今年就招了一个悲剧了，还一个网易没通知，然后我又附带的说了下没打算投国外的公司，因为还是想做一些创造性的工作，在外企可能很难接触到核心的东西，永远是流水线的一颗螺丝钉，他对我想做创造性的工作的想法貌似很满意，然后问我具体想做自然语言处理的哪块，我自然说是搜索引擎和自然语言处理相结合的领域，毕竟这个学期的做的一些东西，看的一些论文也都是和这个相关的。然后又问了我的职业规划，我说三年之内注重积累吧，三年之后希望能在团队中起到比较关键的一个作用，然后就扯到了北京的压力上去了，这个当然也很好回答。最后，他说没问题了，我一看表才二十多分钟，就问了一下自己进去之后可能会做的工作，然后又聊了会。在四号线上快到的时候，我突然想起来可以手机上网查下状态，然后就查了下，发现已经改了，确实是一次比一次效率高，然后果断分享了这个好消息。 总结了这持续了一个月的面试经历，最重要的应该还是兴趣吧，确实是对这方面感兴趣，所以才敢于尝试，想知道自己目前是个什么水平，还需要做哪些工作；因为感兴趣，所以才会去坚持旁听信息检索的课，正是这个课给我打好了这个领域相关的基础；因为感兴趣，所以才会自己课下买一些英文的原版书、看一些国外的论文，去了解这个领域最新的一些进展，这些都在我回答问题的时候给了我很多的思考角度的帮助。当然除了兴趣，还要有充分准备，尤其是一些常见的面试题，比如说海量数据的处理啊，就是高命中的题型，另外自己在做项目的时候，一定要认真，因为面试官会对你简历上写的项目深度挖掘，如果你在某个项目中只是打打酱油，最好不要写上。 最后想说一句，这只是一个新的开始，我十分期待在百度的实习，因为我就像一个海绵，需要汲取各种知识，在自然语言处理这个领域，我还刚刚上路。","title":"你好 百度——你好 NLP"},{"content":"招聘标题：同方知网（北京）研究院长期招聘WEB/自然语言处理/人工智能研发工程师/编辑 招聘单位：（中国知网）同方知网研究院 招聘时间：长期招聘 联系邮箱：sj3493@cnki.net  联系人：孙老师 联系电话：010-62969002-8004 注意：应聘邮件主题请标明应聘的职位   web开发工程师 1.计算机相关专业硕士及以上学历，理解各种常用的数据结构和算法，数学基础扎实，思维严谨灵活，具有较强的学习能力，良好的沟通和团队合作能力； 2.具有1年以上的工作经验者或应届生（应届毕业生毕业前须在公司实习）； 3.熟练掌握C#编程语言，asp.net编程，熟悉 javascript脚本语言； 4.至少熟练一种数据库应用及有关数据库方面应用编程， 如 sql server， oracle，db2，mysql 等； 5.有检索产品开发经验的优先考虑。   自然语言处理方向软件工程师 1.计算机相关专业硕士及以上学历，理解各种常用的数据结构和算法，数学基础扎实，思维严谨灵活，具有较强的学习能力，良好的沟通和团队合作能力； 2.有3年以上的工作经验者或应届生（应届毕业生毕业前须在公司实习）； 3.熟悉自然语言常用算法； 4.熟练掌握C/C++编程语言。   人工智能研发工程师 1.计算机相关专业硕士及以上学历，理解各种常用的数据结构和算法，数学基础扎实，思维严谨灵活，具有较强的学习能力，良好的沟通和团队合作能力； 2.有3年以上的工作经验者或应届生（应届毕业生毕业前须在公司实习）； 3.熟悉人工智能各基本算法； 4.熟悉Prolog语言；   编辑 1.现代汉语、应用语言学、图书情报相关专业硕士或双学士，思维严谨灵活，具有较强的学习能力，良好的沟通和团队合作能力； 2. 有3年以上的工作经验者或应届生（应届毕业生毕业前须在公司实习）； 3.能熟练使用WORD/EXCEL等办公软件；","title":"同方知网（北京）研究院长期招聘WEB/自然语言处理工程师"},{"content":" 搜索引擎分为搜索器，索引器，检索器，人机接口四部分（如下图）。建立索引是个复杂的过程，索引数据库是搜索引擎前端和后端的联系桥梁，可以说起到了管理器的作用。 下面对整个过程做个简要介绍： (1)搜索器（俗称的网络蜘蛛Robot）从互联网上抓取网页，把网页送入网页数据库，从网页中“提取URL”，把URL送入URL数据库，网络蜘蛛根据得到网页的URL，继续抓取其它网页，反复循环直到把所有的网页抓取完成。 (2)系统对抓取的网页进行分类过滤后存入网页数据库，再对网页内容进行分析，送入索引器模块建立索引，形成“索引数据库”。同时进行链接信息提取，把链接信息（包括锚文本、链接本身等信息）送入索引数据库相关表（链接数据库），为网页评级提供依据。 (3)用户通过查询界面提交查询请求给查询服务器，服务器在“索引数据库”中进行相关网页的查找，同时“网页评级”把查询请求和链接信息结合起来对搜索结果进行相关度的评价，通过查询服务器按照相关度进行排序，并提取关键词的内容摘要，组织最后的页面返回给用户。 在上面的几个环节中，每个部分都可采用不同的技术和模型实现，下面分别做出分析和比较： 一、搜索器-信息采集技术 详细内容见我的另一篇文章：NetSpider初探，这里主要强调下信息过滤的重要性，由于在互联网中，存在有大量的无用信息，一个好的搜索引擎应当是尽量减少垃圾内容的数量。这是信息过滤要着重解决的问题。 二、索引器-信息索引技术 信息索引就是创建文档信息的特征记录（如标题，作者，关键字，时间等），以使用户能够快速地检索到所需信息。建立索引一般有如下几个处理步骤： (1)识别文档中的词 (2)删除停用词(stop words) (3)提取词干(stemming) (4)用索引项的标号代替词干(stems) (5)统计词干的数量( tf词频率) (6)计算所有单个词项、短语和语义类的权重 建立索引的问题： (1)信息语词切分和语词词法分析 语词是信息表达的最小单位。由于语词切分中存在切分歧意，切分需要利用各种上下文知识。语词词法分析是指识别出各个语词的词干，以便根据词千建立信息索引。 (2)进行词性标注及相关的自然语言处理 词性标注是指利用基于规则和统计(马尔科夫链)的数学方法对语词进行标注。基于马尔科夫链随机过程的n元语法统计分析方法在词性标注中能达到较高的精度。利用各种语法规则，识别出重要的短语结构。自然语言处理是指将自然语言理解应用在信息检索中，可以提高信息检索的精度和相关性。 (3)建立检索项索引 使用倒排文件的方式建立检索项索引。一般应包括“检索项”、“检索项所在文件位置信息”以及“检索项权重”等信息。 三、检索器-信息检索技术 信息检索过程大致有如下几个步骤： (1)给定查询 (2)对查询进行词干提取，算法和对文档的处理相同 (3)用索引编号代替词干 (4)计算查询词干的权重 (5)形成查询向量(VSM) (6)计算查询向量和文档向量之间的相似度 (7)返回排序后的文档集合给用户 搜索引擎所使用的信息检索模型主要有布尔逻辑模型、模糊逻辑模型、向量空间模型以及概率模型等。 (1)布尔逻辑模型 布尔型信息检索模型是最简单的信息检索模型，用户利用布尔逻辑关系构造查询式并提交，搜索引擎根据事先建立的倒排文件确定查询结果。标准布尔逻辑模型为二元逻辑，并可用逻辑符(\"and\". \"or\". \"not\")来组织关键词表达式。布尔型信息检索模型的查全率高，查准率低，为目前大多数搜索引擎所使用。 (2)模糊逻辑模型 这种模型在查询结果处理中加入模糊逻辑运算，将所检索的数据库文档信息与用户的查询要求进行模糊逻辑比较，按照相关的优先次序排列查询结果。模糊逻辑模型可以克服布尔型信息检索模型查询中结果的无序性问题。例如，查询“搜索引擎”，则关键词“搜索引擎”出现次数多的文档将排列在较前的位置上。 (3)向量空间模型 向量空间模型用检索项的向量空间来表示用户的查询要求和数据库文档信息。查询结果是根据向量空间的相似性而排列的。向量空间模型可方便地产生有效的查询结果，能提供相关文档的文摘，并对查询结果进行分类，为用户提供准确的信息。 (4)概率模型 基干贝叶斯概率论原理的概率模型利用相关反馈的归纳学习方法，获取匹配函数，这是一种较复杂的检索模型。目前，商用信息检索系统主要以布尔模糊逻辑加向量空间模型为主，辅以部分自然语言处理技术来构造自己的检索算法。 四、人机接口-查询组合和结果处理技术 查询界面如何更人性化，符合大多数用户的查询习惯是个有待研究的问题。能否提供自然语言的检索，这部分处理过程实际上也会涉及到建立索引过程中的一些技术，比如分词，自然语言处理。还有问答式的搜索引擎，这些都是第三代智能化搜索引擎必须解决的问题。 再就是搜索引擎的检索结果通常包含大量文件，用户不可能一一浏览。搜索引擎一般应按与查询的相关程度对检索结果进行排列，最相关的文件通常排在最前面。搜索引擎确定相关性的方法有概率方法、位置方法、摘要方法、分类或聚类方法等。 还有就是对用户行为的分析，利用数据挖掘技术对搜索引擎的日志进行分析，得出用户搜索行为的模式，是提高搜索引擎个性化，人性化的必要手段。下面对最常用也是最重要的技术确定检索网页相关性的方法做个介绍： (1)概率方法 概率方法根据关键词在文中出现的频率来判定文件的相关性。这种方法对关键词出现的次数进行统计，关键词出现的次数越多，该文件与查询的相关程度就越高。 (2)位置方法 位置方法根据关键词在文中出现的位置来判定文件的相关性。认为关键词在文件中出现得越早，文件的相关程度就越高。 (3)摘要方法 摘要方法是指搜索引擎自动地为每个文件生成一份摘要，让用户自己判断结果的相关性，以便用户进行选择。 (4)分类或聚类方法 分类或聚类方法是指搜索引擎采用分类或聚类技术，自动把查询结果归入到不同的类别中。 (5)用户反馈方法 对收集的用户反馈信息进行分析，实际上是个自适应的过程，通过对检索行为模型的反复验证，一定会让客户体验的满意度越来越高","title":"搜索引擎（三）"},{"content":"最大熵马尔科夫，看了N遍了。终于有点眉目。   最大熵马尔科夫在建模的时候考虑的问题和ME（最大熵模型）其实是一样的。   同样是马尔科夫的三个基本问题。   问题1：给定观察序列O=O1,O2,…OT,以及模型  λ=(π, A, B),  如何计算P(O|λ)？  问题2：给定观察序列O=O1,O2,…OT以及模型λ,如何选择一个对应的状态序列     S = q1,q2,…qT，使得S能够最为合理的解释观察序列O？即argmaxss∈Q|T|P(T,S|λ). 问题3：如何调整模型参数 λ=(π, A, B)  ,  使得P(O|λ)最大？   1. 对于第一个问题使用类似于Forward-Backward的过程来解决。 2. 对于第二个问题使用类似于Viterbi的算法来解决。 3. 对于第三个问题使用和ME模型训练相同的方法GIS来解决。   在建模的时候MEMM想优化的问题和ME很像。而在运用模型的时候和HMM很像。","title":"自然语言处理之三：最大熵马尔科夫模型"},{"content":"在谈最大熵马尔科夫模型之前，先熟悉一下隐马尔科夫模型     一个隐马尔可夫模型 (HMM) 是一个五元组：   (ΩX , ΩO, A, B, π )   其中：     ΩX = {q1,...qN}：状态的有限集合     ΩO = {v1,...,vM}：观察值的有限集合     A = {aij}，aij = p(Xt+1 = qj |Xt = qi)：转移概率     B = {bik}，bik = p(Ot = vk | Xt = qi)：输出概率     π = {πi}， πi = p(X1 = qi)：初始状态分布     问题1：给定观察序列O=O1,O2,…OT,以及模型  λ=(π, A, B),  如何计算P(O|λ)？  问题2：给定观察序列O=O1,O2,…OT以及模型λ,如何选择一个对应的状态序列     S = q1,q2,…qT，使得S能够最为合理的解释观察序列O？即argmaxss∈Q|T|P(T,S|λ). 问题3：如何调整模型参数 λ=(π, A, B)  ,  使得P(O|λ)最大？     1. 对于第一个问题使用Forward-Backward过程来解决。 2. 对于第二个问题使用Viterbi算法来解决 3. 对于第三个问题使用Baum-Welsh来解决。","title":"自然语言处理之二：隐马尔科夫模型"},{"content":"                                                            最大熵模型：读书笔记                                                                     胡江堂，北京大学软件学院 1. 物理学的熵 2. 信息论的熵 3. 熵和主观概率（一个简单注释 4. 熵的性质 4.1. 当所有概率相等时，熵取得最大值 4.2. 小概率事件发生时携带的信息量比大概率事件发生时携带的信息量多 5. 最大熵原理：直觉讨论 6. 最大熵原理：一个手工例子 7. 最大熵原理：正式表述 8. 最大熵模型的训练：GIS算法 9. 最大熵模型：金融领域内的应用 参考文献   这篇读书笔记主要写了对熵的理解、对最大熵原则的理解，还有一个手工计算的例子。在处理一般化的最大熵模型时，我采用了我偏爱的连续随机变量形式，而不是一般有助于计算机理解的离散形式。连续而非离散的处理方式的一个好处就是，它能非常方便地推出最大熵模型的解是一个指数形式。如果使用离散形式，一样的结论，那符号就看着复杂多了。 所有的东西都来自篇末的参考资料。 1. 物理学的熵 熵是一个物理学概念，它是描述事物无序性的参数，熵越大则无序性越强。从宏观方面讲（根据热力学定律），一个体系的熵等于其可逆过程吸收或耗散的热量除以它的绝对温度；从微观讲，熵是大量微观粒子的位置和速度的分布概率的函数。自然界的一个基本规律就是熵递增原理，即，一个孤立系统的熵，自发性地趋于极大，随着熵的增加，有序状态逐步变为混沌状态，不可能自发地产生新的有序结构，这意味着自然界越变越无序。 2. 信息论的熵 在物理学中，熵是描述客观事物无序性的参数。信息论的开创者香农认为，信息（知识）是人们对事物了解的不确定性的消除或减少。他把不确定的程度称为信息熵。假设每种可能的状态都有概率，我们用关于被占据状态的未知信息来量化不确定性，这个信息熵即为： 其中是以2为底的对数，所以这个信息用位衡量。前面说过，在物理学的背景下，这个不确定性被称为熵（在通讯系统中，关于传输的实际信息的不确定性也被称为数据源的熵）。 扩展到连续情形。假设连续变量的概率密度函数是，与离散随机变量的熵的定义类似，信息熵的连续定义为： 上式就是我们定义的随机变量的微分熵。当被解释为一个随机连续向量时，就是的联合概率密度函数。 3. 熵和主观概率（一个简单注释） 因为熵用概率表示，所以这涉及到主观概率。概率用于处理知识的缺乏（概率值为1表明对知识的完全掌握，这就不需要概率了），而一个人可能比另一个人有着更多的知识，所以两个观察者可能会使用不同的概率分布，也就是说，概率（以及所有基于概率的物理量）都是主观的。在现代的主流概率论教材中，都采用这种主观概率的处理方法。 4. 熵的性质 4.1. 当所有概率相等时，熵取得最大值 上面关于熵的公式有一个性质：假设可能状态的数量有限，当所有概率相等时，熵取得最大值。证明如下：   在只有两个状态的例子中，要使熵最大，每个状态发生的概率都是1/2，如下图所示： 4.2. 小概率事件发生时携带的信息量比大概率事件发生时携带的信息量多 证明略，可以简要说明一下，也挺直观的。如果事件发生的概率为1，在这种情况下，事件发生就没有什么“惊奇”了，并且不传达任何“信息”，因为我们已经知道这“信息”是什么，没有任何的“不确定”；反之，如果事件发生的概率很小，这就有更大的“惊奇”和有“信息”了。这里，“不确定”、“惊奇”和“信息”是相关的，信息量与事件发生的概率成反比。 5. 最大熵原理：直觉讨论 最大熵原理是根据样本信息对某个未知分布做出推断的一种方法。日常生活中，很多事情的发生表现出一定的随机性，试验的结果往往是不确定的，而且也不知道这个随机现象所服从的概率分布，所有的只有一些试验样本或样本特征，统计学常常关心的一个问题，在这种情况下如何对分布作出一个合理的推断？最大熵采取的原则就是：保留全部的不确定性，将风险降到最小。在金融理论中，一个类似的教训是，为了降低风险，投资应该多样化，不要把所有的鸡蛋都放在一个篮子里。 吴军（2006）举了一个例子。对一个均匀的骰子，问它每个面朝上的概率分别是多少。所有人都会说是1/6。这种“猜测”当然是对的，因为对这个“一无所知”的色子，假定它每一个朝上概率均等是最安全的做法，你不应该假设它被做了手脚。从信息论的角度讲，就是保留了最大的不确定性，让熵达到最大（从投资的角度来看，这就是风险最小的做法）。但是，如果这个骰子被灌过铅，已知四点朝上的概率是1/3，在这种情况下，每个面朝上的概率是多少？当然，根据简单的条件概率计算，除去四点的概率是 1/3外，其余的概率都是 2/15。也就是说，除已知的条件（四点概率为 1/3）必须满足外，对其它各点的概率，我们仍然无从知道，也只好认为它们相等。这种基于直觉的猜测之所以准确，是因为它恰好符合了最大熵原理。 回到物理学例子中。在涉及物理系统的情形中，一般要确定该系统可以存在的多种状态，需要了解约束下的所有参数。比如能量、电荷和其他与每个状态相关的物理量都假设为已知。为了完成这个任务常常需要量子力学。我们不假设在这个步骤系统处于特定状态；事实上我们假定我们不知道也不可能知道这一点，所以我们反而可以处理被占据的每个状态的概率。这样把概率当作应对知识缺乏的一种方法。我们很自然地想避免假定了比我们实际有的更多的知识，最大熵原理就是完成这个的方法。 这里可以总结出最大熵对待已知事物和未知事物的原则：承认已知事物（知识）；对未知事物不做任何假设，没有任何偏见。最大熵原理指出，当我们需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设（不做主观假设，这点很重要。）在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。 6. 最大熵原理：一个手工例子 举个例子，一个快餐店提供3种食品：汉堡(B)、鸡肉(C)、鱼(F)。价格分别是1元、2元、3元。已知人们在这家店的平均消费是1.75元，求顾客购买这3种食品的概率。如果你假设一半人买鱼另一半人买鸡肉，那么根据熵公式，这不确定性就是1位（熵等于1）。但是这个假设很不合适，因为它超过了你所知道的事情。我们已知的信息是： 以及关于对概率分布的不确定性度量，熵： 对前两个约束，两个未知概率可以由第三个量来表示，可以得到： 把上式代入熵的表达式中，熵就可以用单个概率来表示： 对这个单变量优化问题，很容易求出时熵最大，有，和。 总结一下。以上，我们根据未知的概率分布表示了约束条件，又用这些约束条件消去了两个变量，用剩下的变量表示熵，最后求出了熵最大时剩余变量的值，结果就求出了一个符合约束条件的概率分布，它有最大不确定性，我们在概率估计中没有引入任何偏差。 7. 最大熵原理：正式表述 假 设有一个随机系统，已知一组状态，但不知道其概率，而且我们知道这些状态的概率分布的一些限制条件。这些限制条件或者是已知一定的总体平均值，或者是它们 的一些界限。在给定关于模型的先验知识的条件下，问题是选择一个在某种意义下最佳的概率模型。Jaynes(1957)提出了一个最大熵原则：当根据不完 整的信息作为依据进行推断时，应该由满足分布限制条件的具有最大熵的概率分布推得。也就是说，熵的概念在概率分布空间定义一种度量，使得具有较高熵的分布 比其它的分布具有更大的值。显然，“最大熵问题”是一个带约束的最优化问题。 为方便叙述，考虑最大微分熵 对所有随机变量的概率密度函数，满足以下约束条件： 其中，是的一个函数。约束1和约束2描述的是概率密度函数的基本属性，约束3定义变量的矩，它随函数的表达式不同而发生变化，它综合了随机变量的所有可用的先验知识。为了解这个约束最优化问题，利用拉格朗日乘子法，目标函数为： 其中， 是拉格朗日乘子。对被积函数求的微分，并令其为0，有： 解得： 我 们看到这个概率密度函数具有指数形式。匈牙利数学家Csiszar曾经证明，对任何一组不自相矛盾的信息，最大熵模型不仅存在，而且是唯一的。而且它们都 有同一个非常简单的形式 -- 指数函数。我们还可以得到，在所有零均值随机向量可达到的微分熵中，多元正态分布具有最大的微分熵。最大熵的解，同时是最吻合样本数据分布的解。 8. 最大熵模型的训练：GIS算法和其他 上 节我们得到，一个最大熵模型可以有效地把各种信息综合在一起（无偏见地对待不确定性），而且具有指数函数的形式，下面模型的训练就要确定这个指数函数的各 个参数。最原始的最大熵模型的训练方法是一种称为通用迭代算法 GIS(generalized iterative scaling) 的迭代算法，由 Darroch 和 Ratcliff 在七十年代提出，大致可以概括为以下几个步骤： 1. 假定第零次迭代的初始模型为等概率的均匀分布。 2. 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，如果超过了实际 的，就把相应的模型参数变小；否则，将它们便大。 3. 重复步骤 2 直到收敛。 Darroch 和 Ratcliff没有能对这种算法的物理含义进行很好地解释，后来是由Csiszar解释清楚的，因此，人们在谈到这个算法 时，总是同时引用 Darroch 和Ratcliff 以及希萨的两篇论文。GIS 算法每次迭代的时间都很长，需要迭代很多次才能收敛，而且不太稳定，即使在 64 位计算机上都会出现溢出。因此，在实际应用中很少有人真正使用，大家只是通过它来了解最大熵模型的算法。 八十年代，Della Pietra在IBM对GIS算法进行了两方面的改进，提出了改进迭代算法IIS（improved iterative scaling）。这使得最大熵模型的训练时间缩短了一到两个数量级。这样最大熵模型才有可能变得实用。即使如此，在当时也只有 IBM 有条件是用最大熵模型。 由于最大熵模型在数学上十分完美，对科学家们有很大的诱惑力，因此不少研究者试图把自己的问题用一个类似最大熵 的近似模型去套。谁知这一近似，最大熵模型就变得不完美了，结果可想而知，比打补丁的凑合的方法也好不了多少。于是，不少热心人又放弃了这种方法。第一个 在实际信息处理应用中验证了最大熵模型的优势的，是原IBM现微软的研究员Adwait Ratnaparkhi。Ratnaparkhi的聪明之处在于他没有对最大熵模型进行近似，而是找到了几个最适合用最大熵模型、而计算量相对不太大的自 然语言处理问 题，比如词性标注和句法分析。拉纳帕提成功地将上下文信息、词性（名词、动词和形容词等）、句子成分（主谓宾）通过最大熵模型结合起来，做出了当时世界上 最好的词性标识系统和句法分析器。 9. 最大熵模型：金融领域内的应用 最大熵模型在自然语言处理领域 内得到了广泛的应用，在金融界，也能见到它的影子。当年最早改进最大熵模型算法的Della Pietra在九十年代初退出了学术界，而到在金融界大显身手。他和很多IBM语音识别的同事一同到了一家当时还不大，但现在是世界上最成功对冲基金公司 ----(Renaissance Technologies。我们知道，决定股票涨落的因素可能有几十甚至上百种，而最大熵方法恰恰能找到一个同时满足成千上万种不同条件的模型。 Della Pietra等科学家在那里，用于最大熵模型和其他一些先进的数学工具对股票预测，获得了巨大的成功。从该基金1988 年创立至今，它的净回报率高达平均每年34%。也就是说，如果1988年你在该基金投入一块钱，今天你能得到200块钱。这个业绩，远远超过股神巴菲特的 旗舰公司Berkshire Hathaway（同期，Berkshire Hathaway的总回报是16倍）。   参考文献 1. 吴军《数学之美系列十六（上）-不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型》，http://googlechinablog.com/2006/10/blog-post.html 2. 吴军《数学之美系列十六（下）-不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型》，http://googlechinablog.com/2006/11/blog-post.html 3. Jaynes, E.T., 1957. ”Information Theory and Statistical Mechanics”, Physical Review, vol.106, pp.620-630. http://bayes.wustl.edu/etj/articles/theory.1.pdf 4. Haykin, Simon《神经网络原理》（第10章 信息论模型，叶世伟等译，北京：机械工业出版社，2004） 5. 王厚峰. 机器学习课程讲义之六MEM (Maximum Entropy Model).北京大学软件与微电子学院，2007年春季学期 6. Penfield, Paul. Information and Entrop. MIT Open Course, Spring 2003. http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-050JInformation-and-EntropySpring2003/CourseHome/index.htm 7. Wei, Xiaoliang《最大熵模型与自然语言处理》www.cs.caltech.edu/~weixl/research/read/summary/MaxEnt2.ppt 8. 常宝宝《自然语言处理的最大熵模型》www.icl.pku.cn/WebData_http-dir-listable/ICLseminars/2003spring/最大熵模型.pdf 9. 廖先桃《最大熵理论及其应用》http://ir.hit.edu.cn/phpwebsite/index.php?module=documents&JAS_DocumentManager_op=downloadFile&JAS_File_id=196","title":"最大熵模型"},{"content":"一直对自然语言处理中的各种模型一知半解。总是抓不住它们的思想。 今天看了一下这个“最大熵模型”（A  Maximum Entropy Approach to  Natural  Language  Processing），写写自己的想法吧。呵呵。   就像论文中所说的：希望找到一个最佳的uniform。也就是在模型的建立时，将所有已知的事实建入模型中，而对于未知的则尽量的使它们一致。 比如我们目前只知道的事实是某个随机变量取值的概率分布具有约束条件： (1) P(A+B)=0.2, (2) P(C+D+E+F)=0.8。 那么我们在预测P(A),P(B),P(C),P(D),P(E),P(F)的各个值时，按照直觉应该对（A，B）和（C，D，E，F）平均分配概率。 P(A)=0.1; P(B)=0.1; P(C)=0.2; P(D)=0.2; P(E)=0.2; P(F)=0.2。   那么这样的一种uniform应该在公式上面怎样体现呢，文章随之就引入了最大熵的思想。 最大化条件熵： H(p)=-∑{x,y}{~p(x)p(y|x)log p(y|x)}即能够满足我们所需要的uniform条件。   而我们的事实应该怎么表达呢？ 原文文章中的公式1,2,3的综合给了我们他的表示方法。   《文本挖掘》中所写的式子感觉更加容易理解。   p(f)=∑{x,y}p(x,y)f(x,y)=P(f)=∑{x,y}P(x,y)f(x,y) 在实际中由于不好计算真实的期望值，因此使用经验值，只在训练样本熵进行求和，则有： pE(f)=∑{i=1...N}∑{y∈Y}p(y|xi)f(xi,y)/N=PE(f)=∑{i=1...N}f(xi,yi)/N   由于对于每个f都有一个公式，这组公式很像上面例子中给出的（1），（2）两个公式。公式的右边已知，而我们希望公式左边的概率p（y|x）能够服从这个约束条件。该公式可以理解为特征函数f生成的约束条件。在这个特征函数下的概率和满足右边式子的值。 其中引入了一个特征函数f（x,y）。这个特征函数的取值要么为1，要么为0。表示了我们希望考虑的一些条件，而我们不想考虑的条件则由于=0的原因而没有考虑到等式中。因此被叫做特征函数，他表示我们感兴趣的特征。   最大上模型的大概思想我感觉就是这个样子了。当然，具体的寻找最优解的算法还是很复杂的。","title":"自然语言处理之一：最大熵模型"},{"content":"说明: 纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全. 同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令 人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的 tier-1: IJCAI (1+): International Joint Conference on Artificial Intelligence AAAI (1): National Conference on Artificial Intelligence COLT (1): Annual Conference on Computational Learning Theory CVPR (1): IEEE International Conference on Computer Vision and Pattern   Recognition ICCV (1): IEEE International Conference on Computer Vision ICML (1): International Conference on Machine Learning NIPS (1): Annual Conference on Neural Information Processing Systems ACL (1-): Annual Meeting of the Association for Computational Linguistics KR (1-): International Conference on Principles of Knowledge Representation   and Reasoning SIGIR (1-): Annual International ACM SIGIR Conference on Research and Development in Information Retrieval SIGKDD (1-): ACM SIGKDD International Conference on Knowledge Discovery and Data Mining UAI (1-): International Conference on Uncertainty in Artificial Intelligence *Impact factor (According to Citeseer 03): IJCAI ：1.82 (top 4.09 %) AAAI ：1.49 (top 9.17%) COLT：1.49 (top 9.25%) ICCV ：1.78 (top 4.75%) ICML ：2.12 (top 1.88%) NIPS ：1.06 (top 20.96%) ACL ：1.44 (top 10.07%) KR ：1.76 (top 4.99%) SIGIR ：1.10 (top 19.08%) Average：1.56 (top 8.02%) IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个 领域就没几篇了，象achine learning、computer vision这么大的领域每次大概也 就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会 议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在 complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年 国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了 减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司 的”IJCAI Inc.”主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer & Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的 青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约 这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找 3位reviewer, second PC member 找一位. AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可   以给到1+, 也可以给到1-或者2+, 总的来说我给它”1″. 这是因为它的开法完全受 IJCAI制约: 每年开, 但如果这一年的 IJCAI在北美举行, 那么就停开. 所以, 偶数年 里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些, 特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱 一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比   IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协 调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章 可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI 那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了. COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉,   所以这个会被一些人看成是理论计算 机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: “一小群数 学家在开会”. 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便 提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出 论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的 会议, 例如COLT. CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题 目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识 别最好的会议, 而在计算机视觉方面, 还有ICCV 与之相当. IEEE一直有个倾向, 要把 会办成”盛会”, 历史上已经有些会被它从quality很好的会办成”盛会”了. CVPR搞不好 也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信 说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减 少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了. ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的   介绍. NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会 每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS’05的论文集是06年出. 会议的名字   “Advances in Neural Information Processing Systems”, 所以, 与ICML/ECML这样 的”标准的”机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有 一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在Michael Jordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很 强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给”外 人”的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有 些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,   但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选 理事, 有资格提名的人包括近三年在ICML/ECML/COLT发过文章的人, NIPS则被排除在 外了. 无论如何, 这是一个非常好的会. ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association of   Computational Linguistics) 主办, 每年开. KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来 越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至 有点机器学习应用会议的味道了, 所以把它也列进来. SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,   毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列 在tier-1里面, 一方面是名声远不及其他的 top conference响亮, 另一方面是相对容易 被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. UAI (1-): 名字叫”人工智能中的不确定性”, 涉及表示/推理/学习等很多方面, AUAI   (Association of UAI) 主办, 每年开. -------------------------------------------------------------------------------- tier-2: AAMAS (2+): International Joint Conference on Autonomous Agents and   Multiagent Systems ECCV (2+): European Conference on Computer Vision ECML (2+): European Conference on Machine Learning ICDM (2+): IEEE International Conference on Data Mining SDM (2+): SIAM International Conference on Data Mining ICAPS (2): International Conference on Automated Planning and Scheduling ICCBR (2): International Conference on Case-Based Reasoning COLLING (2): International Conference on Computational Linguistics ECAI (2): European Conference on Artificial Intelligence ALT (2-): International Conference on Algorithmic Learning Theory EMNLP (2-): Conference on Empirical Methods in Natural Language Processing ILP (2-): International Conference on Inductive Logic Programming PKDD (2-): European Conference on Principles and Practice of Knowledge   Discovery in Databases *Impact factor (According to Citeseer 03): ECCV ：1.58 (top 7.20 %) ECML ：0.83 (top 30.63 %) ICDM ：0.35 (top 59.86 %) ICCBR ：0.72 (top 36.69 %) ECAI ：0.69 (top 38.49 %) ALT ：0.63 (top 42.91 %) ILP ：1.06 (top 20.80 %) PKDD ：0.50 (top 51.26 %) Average：0.80 (top 32.02%) AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,   几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能 升级到1-去. ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了. ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上. COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容. EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了. PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的 PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被 PKDD接受). -------------------------------------------------------------------------------- tier-3: ACCV (3+): Asian Conference on Computer Vision DS (3+): International Conference on Discovery Science ECIR (3+): European Conference on IR Research ICTAI (3+): IEEE International Conference on Tools with Artificial Intelligence PAKDD (3+): Pacific-Asia Conference on Knowledge Discovery and Data Mining ICANN (3+): International Conference on Artificial Neural Networks AJCAI (3): Australian Joint Conference on Artificial Intelligence CAI (3): Canadian Conference on Artificial Intelligence CEC (3): IEEE Congress on Evolutionary Computation FUZZ-IEEE (3): IEEE International Conference on Fuzzy Systems GECCO (3): Genetic and Evolutionary Computation Conference ICASSP (3): International Conference on Acoustics, Speech, and Signal   Processing ICIP (3): International Conference on Image Processing ICPR (3): International Conference on Pattern Recognition IEA/AIE (3): International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IJCNN (3): International Joint Conference on Neural Networks IJNLP (3): International Joint Conference on Natural Language Processing PRICAI (3): Pacific-Rim International Conference on Artificial Intelligence *Impact factor (According to Citeseer 03): ACCV ：0.42 (top 55.61%) ICTAI ：0.25 (top 69.86 %) PAKDD ：0.30(top 65.60 %) ICANN ：0.27 (top 67.73 %) AJCAI ：0.16 (top 79.44 %) CAI ：0.26 (top 68.87 %) ICIP ：0.50 (top 50.20 %) IEA/AIE ：0.09 (top 87.79 %) PRICAI ：0.19 (top 76.33 %) Average：0.27 (top 68.30%) ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. DS (3+): 日本人发起的一个接近数据挖掘的会议. ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议. ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了. PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5. ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN. AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了. CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC /FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作. FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍. GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型. ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议. ICIP (3): 图像处理方面最著名的会议之一, 盛会型. ICPR (3): 模式识别方面最著名的会议之一, 盛会型. IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹. IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议. PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升. -------------------------------------------------------------------------------- 列list只是为了帮助新人熟悉领域, 给出的评分或等级都是个人意见, 仅供参考. 特别要说明的是: 1. tier-1 conference上的文章并不一定比tier-3的好, 只能说前者的平均水准更高. 2. 研究工作的好坏不是以它发表在哪儿来决定的, 发表在高档次的地方只是为了让工作更容易被同行注意到. tier-3会议上发表1篇被引用10次的文章可能比在tier-1会议上发表10篇被引用0次的文章更有价值. 所以, 数top会议文章数并没有太大意义, 重要的是同行的评价和认可程度. 3. 很多经典工作并不是发表在高档次的发表源上, 有不少经典工作甚至是发表在很低档的发表源上. 原因很多, 就不细说了. 4. 会议毕竟是会议, 由于审稿时间紧, 错杀好人和漏过坏人的情况比比皆是, 更何况还要考虑到有不少刚开始做研究的学生在代老板审稿. 5. 会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑. 6. 只有计算机科学才重视会议论文, 其他学科并不把会议当回事. 但在计算机科学中也有不太重视会议的分支. 7. Politics无所不在. 你老板是谁, 你在哪个研究组, 你在哪个单位, 这些简单的因素都可能造成决定性的影响. 换言之, 不同环境的人发表的难度是不一样的. 了解到这一点后, 你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学报>上发表了平顶山铁道电子信息科技学院的作者的文章,我一定会仔细读). 8. 评价体系有巨大的影响. 不管是在哪儿谋生的学者, 都需要在一定程度上去迎合评价体系, 否则连生路都没有了, 还谈什么做研究. 以国内来说, 由于评价体系只重视journal, 有一些工作做得很出色的学者甚至从来不投会议. 另外, 经费也有巨大的制约作用. 国外很多好的研究组往往是重要会议都有文章. 但国内是不行的, 档次低一些的会议还可以投了只交注册费不开会, 档次高的会议不去做报告会有很大的负面影响, 所以只能投很少的会议. 这是在国内做CS研究最不利的地方. 我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然, 人民币升值对整个中国来说利大于弊还是弊大于利很难说","title":"AI的相关会议"},{"content":"闲着无事，想写点一些我所了解的machine learning大家。由于学识浅薄，见识有限，并且仅局限于某些领域，一些在NLP及最近很热的生物信息领域活跃的学者我就浅陋无知，所以不对的地方大家仅当一笑。  Machine Learning 大家(1)：M. I. Jordan (http://www.cs.berkeley.edu/~jordan/)  在我的眼里，M Jordan无疑是武林中的泰山北斗。他师出MIT，现在在berkeley坐镇一方，在附近的两所名校（加stanford）中都可以说无出其右者， stanford的Daphne Koller虽然也声名遐迩，但是和Jordan比还是有一段距离。  Jordan身兼stat和cs两个系的教授，从他身上可以看出Stat和ML的融合。  Jordan最先专注于mixtures of experts，并迅速奠定了自己的地位，我们哈尔滨工业大学的校友徐雷跟他做博后期间，也在这个方向上沾光不少。Jordan和他的弟子在很多方面作出 了开创性的成果，如spectral clustering， Graphical model和nonparametric Bayesian。现在后两者在ML领域是非常炙手可热的两个方向，可以说很大程度上是Jordan的lab一手推动的。  更难能可贵的是，Jordan不仅自己武艺高强，并且揽钱有法，教育有方，手下门徒众多且很多人成了大器，隐然成为江湖大帮派。他的弟子中有10多人任教 授，个人认为他现在的弟子中最出色的是stanford的Andrew Ng，不过由于资历原因，现在还是assistant professor，不过成为大教授指日可待；另外Tommi Jaakkola和David Blei也非常厉害，其中Tommi Jaakkola在mit任教而David Blei在cmu做博后，数次获得NIPS最佳论文奖，把SVM的最大间隔方法和Markov network的structure结构结合起来，赫赫有名。还有一个博后是来自于toronto的Yee Whye Teh，非常不错，有幸跟他打过几次交道，人非常nice。另外还有一个博后居然在做生物信息方面的东西，看来jordan在这方面也捞了钱。这方面他有 一个中国学生Eric P. Xing(清华大学校友)，现在在cmu做assistant professor。  总的说来，我觉得Jordan现在做的主要还是graphical model和Bayesian learning，他去年写了一本关于graphical model的书，今年由mit press出版，应该是这个领域里程碑式的著作。3月份曾经有人答应给我一本打印本看看，因为Jordan不让他传播电子版，但后来好像没放在心上（可见 美国人也不是很守信的），人不熟我也不好意思问着要，可以说是一大遗憾. 另外发现一个有趣的现象就是Jordan对hierarchical情有独钟，相当多的文章都是关于hierarchical的，所以能 hierarchical大家赶快hierarchical，否则就让他给抢了。  用我朋友话说看jordan牛不牛，看他主页下面的Past students and postdocs就知道了。  Machine Learning大家（2）：D. Koller (http://ai.stanford.edu/~koller/)  D. Koller是1999年美国青年科学家总统奖(PECASE)得主，IJCAI 2001 Computers and Thought Award(IJCAI计算机与思维奖，这是国际人工智能界35岁以下青年学者的最高奖)得主，2004 World Technology Award得主。  最先知道D koller是因为她得了一个大奖，2001年IJCAI计算机与思维奖。Koller因她在概率推理的理论和实践、机器学习、计算博弈论等领域的重要贡 献，成为继Terry Winograd、David Marr、Tom Mitchell、Rodney Brooks等人之后的第18位获奖者。说起这个奖挺有意思的，IJCAI终身成就奖（IJCAI Award for Research Excellence），是国际人工智能界的最高荣誉; IJCAI计算机与思维奖是国际人工智能界35岁以下青年学者的最高荣誉。早期AI研究将推理置于至高无上的地位; 但是1991年牛人Rodney Brooks对推理全面否定，指出机器只能独立学习而得到了IJCAI计算机与思维奖; 但是koller却因提出了Probabilistic Relational Models 而证明机器可以推理论知而又得到了这个奖，可见世事无绝对，科学有轮回。  D koller的Probabilistic Relational Models在nips和icml等各种牛会上活跃了相当长的一段时间，并且至少在实验室里证明了它在信息搜索上的价值，这也导致了她的很多学生进入了 google。虽然进入google可能没有在牛校当faculty名声响亮，但要知道google的很多员工现在可都是百万富翁，在全美大肆买房买车的 主。  Koller的研究主要都集中在probabilistic graphical model，如Bayesian网络，但这玩意我没有接触过，我只看过几篇他们的markov network的文章，但看了也就看了，一点想法都没有，这滩水有点深，不是我这种非科班出身的能趟的，并且感觉难以应用到我现在这个领域中。  Koller才从教10年，所以学生还没有涌现出太多的牛人，这也是她不能跟Jordan比拟的地方，并且由于在stanford的关系，很多学生直接去 硅谷赚大钱去了，而没有在学术界开江湖大帮派的影响，但在stanford这可能太难以办到，因为金钱的诱惑实在太大了。不过Koller的一个学生我非 常崇拜，叫Ben Taskar，就是我在（1）中所提到的Jordan的博后，是好几个牛会的最佳论文奖，他把SVM的最大间隔方法和Markov network结合起来，可以说是对structure data处理的一种标准工具，也把最大间隔方法带入了一个新的热潮，近几年很多牛会都有这样的workshop。 我最开始上Ben Taskar的在stanford的个人网页时，正赶上他刚毕业，他的顶上有这么一句话：流言变成了现实，我终于毕业了！ 可见Koller是很变态的，把自己的学生关得这么郁闷，这恐怕也是大多数女faculty的通病吧，并且估计还非常的push！  Machine learning 大家（3）: J. D. Lafferty  大家都知道NIPS和ICML向来都是由大大小小的山头所割据，而John Lafferty无疑是里面相当高的一座高山，这一点可从他的publication list里的NIPS和ICML数目得到明证。虽然江湖传说计算机重镇CMU现在在走向衰落，但这无碍Lafferty拥有越来越大的影响力，翻开AI兵 器谱排名第一的journal of machine learning research的很多文章，我们都能发现author或者editor中赫然有Lafferty的名字。  Lafferty给人留下的最大的印象似乎是他2001年的conditional random fields，这篇文章后来被疯狂引用，广泛地应用在语言和图像处理，并随之出现了很多的变体，如Kumar的discriminative random fields等。虽然大家都知道discriminative learning好，但很久没有找到好的discriminative方法去处理这些具有丰富的contextual inxxxxation的数据，直到Lafferty的出现。  而现在Lafferty做的东西好像很杂，semi－supervised learning， kernel learning，graphical models甚至manifold learning都有涉及，可能就是像武侠里一样只要学会了九阳神功，那么其它的武功就可以一窥而知其精髓了。这里面我最喜欢的是semi－ supervised learning，因为随着要处理的数据越来越多，进行全部label过于困难，而完全unsupervised的方法又让人不太放心，在这种情况下 semi－supervised learning就成了最好的。这没有一个比较清晰的认识，不过这也给了江湖后辈成名的可乘之机。到现在为止，我觉得cmu的semi－ supervised是做得最好的，以前是KAMAL NIGAM做了开创性的工作，而现在Lafferty和他的弟子作出了很多总结和创新。  Lafferty的弟子好像不是很多，并且好像都不是很有名。不过今年毕业了一个中国人，Xiaojin Zhu(上海交通大学校友)，就是做semi－supervised的那个人，现在在wisconsin-madison做assistant professor。他做了迄今为止最全面的Semi-supervised learning literature survey，大家可以从他的个人主页中找到。这人看着很憨厚，估计是很好的陶瓷对象。另外我在（1）中所说的Jordan的牛弟子D Blei今年也投奔Lafferty做博后，就足见Lafferty的牛了。  Lafferty做NLP是很好的，著名的Link Grammar Parser还有很多别的应用。其中language model在IR中应用，这方面他的另一个中国学生ChengXiang Zhai(南京大学校友，2004年美国青年科学家总统奖(PECASE)得主)，现在在uiuc做assistant professor。  Machine learning 大家（4): Peter L. Bartlett  鄙人浅薄之见，Jordan比起同在berkeley的Peter Bartlett还是要差一个层次。Bartlett主要的成就都是在learning theory方面，也就是ML最本质的东西。他的几篇开创性理论分析的论文，当然还有他的书Neural Network Learning: Theoretical Foundations。  UC Berkeley的统计系在强手如林的北美高校中一直是top3， 这就足以证明其肯定是群星荟萃，而其中，Peter L. Bartlett是相当亮的一颗星。关于他的研究，我想可以从他的一本书里得到答案：Neural Network Learning: Theoretical Foundations。也就是说，他主要做的是Theoretical Foundations。基础理论虽然没有一些直接可面向应用的算法那样引人注目，但对科学的发展实际上起着更大的作用。试想vapnik要不是在VC维 的理论上辛苦了这么多年，怎么可能有SVM的问世。不过阳春白雪固是高雅，但大多数人只能听懂下里巴人，所以Bartlett的文章大多只能在做理论的那 个圈子里产生影响，而不能为大多数人所广泛引用。  Bartlett在最近两年做了大量的Large margin classifiers方面的工作，如其convergence rate和generalization bound等。并且很多是与jordan合作，足见两人的工作有很多相通之处。不过我发现Bartlett的大多数文章都是自己为第一作者，估计是在教育 上存在问题吧，没带出特别牛的学生出来。  Bartlett的个人主页的talk里有很多值得一看的slides,如Large Margin Classifiers: Convexity and Classification；Large Margin Methods for Structured Classification: Exponentiated Gradient Algorithms。大家有兴趣的话可以去下来看看。  Machine learning 大家（5): Michael Collins  Michael Collins (http://people.csail.mit.edu/mcollins/  自然语言处理(NLP)江湖的第一高人。出身Upenn，靠一身叫做Collins Parser的武功在江湖上展露头脚。当然除了资质好之外，其出身也帮了不少忙。早年一个叫做Mitchell P. Marcus的师傅传授了他一本葵花宝典-Penn Treebank。从此，Collins整日沉迷于此，终于练成盖世神功。  学成之后，Collins告别师傅开始闯荡江湖，投入了一个叫AT&T Labs Research的帮会，并有幸结识了Robert Schapire、Yoram Singer等众多高手。大家不要小瞧这个叫AT&T Labs Research的帮会，如果谁没有听过它的大名总该知道它的同父异母的兄弟Bell Labs吧。  言归正传，话说Collins在这里度过了3年快乐的时光。其间也奠定了其NLP江湖老大的地位。并且练就了Discriminative Reranking， Convolution Kernels，Discriminative Training Methods for Hidden Markov Models等多种绝技。然而，世事难料，怎奈由于帮会经营不善，这帮大牛又不会为帮会拼杀，终于被一脚踢开，大家如鸟兽散了。Schapire去了 Princeton， Singer 也回老家以色列了。Collins来到了MIT，成为了武林第一大帮的六袋长老，并教授一门叫做Machine Learning Approaches for NLP (http://www.ai.mit.edu/courses/6.891-nlp/) 的功夫。虽然这一地位与其功力极不相符，但是这并没有打消Collins的积极性，通过其刻苦打拼，终于得到了一个叫Sloan Research Fellow的头衔，并于今年7月，光荣的升任7袋Associate Professor。  在其下山短短7年时间内，Collins共获得了4次世界级武道大会冠军(EMNLP2002, 2004, UAI2004, 2005)。相信年轻的他，总有一天会一统丐帮，甚至整个江湖。  看过Collins和别人合作的一篇文章，用conditional random fields 做object recogntion。还这么年轻，admire to death  本文转自  http://hi.baidu.com/gavindge/blog/item/23f286976af1c56e55fb9697.html","title":"机器学习的牛人们_遥远的星空"},{"content":"做算法和作技术哪个难? 都很难, 没有可比性. 但是算法做得好的可以转行做技术, 技术做得好的想转行做算法却很困难. 我是08年下半年将近期末的时候加入华理ACM队的. 我高中的时候没有编程经验, 数学也不好, 高考数学刚及格. 因为第二工业大学的网络工程专业的分数是最低的, 所以就比较巧合地步入了计算机行业. 大一有一门C++课程, 当时我在第一次上机的时候就深深的被C++迷住了. C++是一门极其优美的语言, 相比于高中计算机课上一笔带过的VB, 我最喜欢C++的花括号. 因为学校比较差, 周围的学生没几个专心读书的, 所以对于一个稍微想要学点东西的学生来说, 这样的学校里的硬件资源就显得异常丰富了, 图书馆里很多计算机经典书都是新的, 而且自修室人也很少, 整个大一除了谈谈恋爱解解闷以及陪室友打打魔兽以外, 我大多数时间都在图书馆或者自修室, 虽然在那里的时候不总是做些学习方面的事情...我也会经常无聊的在网上乱逛或者到处下载资料做收藏家. 但正是这个时候我接触到了C++的圣经[The C++ Programming Language], 当时我看了一段时间的电子版, 因为不方便作笔记, 很不爽, 于是在08年1月的时候买了中译本. 之后的大一下学期我又陆陆续续看了部分[Effective C++], [More Effective C++], 还有一些数据结构方面的东西. 然后学期中期的时候我集中精力做了一段时间的数学建模竞赛. 大一结束的时候我通过插班生考试从第二工业大学转来华理, 很不巧, 插班生考试数学又是刚及格. 在暑假期间我去上了CCNA和CCNP的部分课程, 因为感觉这种纯操作工一样的\"背诵\"活很不适合我, 于是半途而废了, 暑假在家的时候做了很多windows下的网络编程, 自己写发包程序, 写SYN攻击很有意思. 大二刚来到华理的时候自我感觉很牛B, 因为我自学过的东西几乎都没有人能来跟我做做讨论. 于是我自以为很了不起, 也没怎么在意学校的课程, 除了上课, 平时我几乎都在图书馆或者自修室. 这期间我接触了设计模式, 以及.NET开发. 大约在开学不久, 08年的Regional还没有开始的时候, 我听说了ACM这种东西. 当时对这个东西我是完全没概念的, 在某个晚上我去奉贤311找了罗老师, 当时咏天也在, 是ACM队准备召新的时候, 罗老师跟每个同学谈了话, 我是最后一个, 跟他哗啦啦地说了很多以前我看过的书, 做过的东西. 被罗老师赞赏了一番之后我更加自以为是了, 并以非正式的身份进了ACM队. 但是直到学期结束, 我也没做什么题. 学期将尽的时候我跟室友展开了一个旧书交 易网站的建设, 然后我就一股脑地完全投入进去了, 用了将近五个月的时间, 其中占用了我整个寒假, 我用ASP.NET+Spring.NET框架写了一个自以为有很高技术含量的简单的旧书信息发布平台, 全站单页面, 无刷新, 所有操作都用AJAX技术完成, 并且我自己设计了一个貌似很牛B的内存数据库. 结果在09年的上海市计算机应用能力大赛中, 我的作品只获了一个优胜奖. 貌似还有很多同学沉迷于这些, 以为这就是编程. 备受打击的我在迷茫了一阵子之后想起来ACM, 于是在大概3月底的时候开始在PKU切题. 以上都是之前的一些学习经历, 关于ACM的部分从这里开始. ACM是个很有意思的东西. 起初, 我有点看不起这个东西, 相信很多过早接触技术开发方面的同学都有这样类似的想法, 于是就有了诸如\"做算法和做技术哪个难\"的问题. 今天再让我来回答这个问题的话, 我只能说, 都很难, 并且没有可比性. 但是我要补充一点, 大学生可以做技术, 不上大学同样可以做技术. 再要我说得明白一点的话就是, 一年前我用了将近一个星期的时间, 看着MSDN和CSDN写了一个windows下的扫雷程序, 并且在之前我用了若干个星期积累了SDK编程的一些知识, 而今天虽然我已经忘记了几乎所有SDK编程的细节, 不知道怎么去画一个自定义的按钮, 但是给我一台能上网的电脑和一天的时间, 我能把当时的那个扫雷程序写出来, 并且让它的代码量精简掉一半以上. 另外一个事实就是, 绝大多数的ACMer以及几乎所有成功的ACMer的数学都非常好, 不论他们是原来就很好还是后来变得很好. 以下说一些实质性的东西. 编码能力 在ACM的世界里, 编码能力和数学功底是绝对的王道, 硬要分个先后的话, 编码能力就是王道中的王道. ben在学校的时候几乎不看算法类书籍, 最多也就是看看网上零散的知识点, 但是他的成绩是大家有目共睹的, 其中最重要的一点就是他的编码能力至少在华理, 那是令人发指的强. 写一个容斥原理你要多久? 或者给你入射光和球的相关参数, 要你编码求出反射光的运动参数, 你又要写多久? 更有甚者, 在今年暑期的个人赛第一场最后一题, 给你一个扫雷过程中的状态, 要你计算出所有能够确定的雷, 这题ben用了不到半个小时就AC了. 今年的上海赛区同样对编码能力要求很高, 其中我们没有过的I题, 当时ben打印出来的代码有4张纸, 也正是因为这么长的代码, 我跟sky连去纠错的信心都没有, 结果ben自己也没有能够在赛场上把错误找出来. 编码能力的培养的捷径就是做题, 尤其是搜索, 模拟, 计算几何等类型的题目尤其锻炼编码能力. 我能够很快适应ACM, 很大程度上也是得益于大一期间积累的编码能力和C++语言基础. 在锻炼编码能力时, 特别要注意做题要限制时间, 这点跟自己做一些小项目, 小程序完全不同, 不能拖拖拉拉一做就是几天. 最好的方法是在写代码的同时把自己做这题的开始和结束时间都标注上去, 这点在后面关于解题报告的部分我还会说到. 如果觉得有时自己会记不住记录开始时间, 那就养成一个习惯, 每看到一题就先随便submit几个字母, 让它CE一次, 这样以后再回过头来看历史记录就可以知道自己做这题用了多少时间了. 限制做题时间又一个很明显的作用就是可以自我认识, 即了解自己对于某一方面的知识掌握程度, 如果平时遇到某一类型的一般题目(没有特别恶心的trick, 没有特别恶心的输入), 自己需要花超过2个小时去AC, 那么就几乎可以肯定你在赛场上是不太可能把这题做出来的, 换句话说, 如果你在赛场上遇到这种类型的题目就可以暂时先放一放了. 编码能力的一个很关键的地方就是编码风格问题, 一些普遍适应的原则诸如\"不要把复杂的语句写在[]里\", \"不要在if里写复杂的语句\", \"把if里复杂的逻辑拆分开\", \"全局变量要特殊命名\"等等, 这里不可能一一列举, 唯一的办法就是每次自己在这里吃到苦头的时候把它记录下来. 具体的可以参考ben的方法, 例如把PKU的每次提交情况都粘贴进一个文本文件里, 然后在每次提交记录后面都加上一些注释, 比如\"++i写成i++\", 或者\"dis数组忘记初始化\"之类的东西, 这样即使不能保证此类错误以后不会再犯, 也至少可以降低你犯此类错误的概率. 关于降低犯错的概率还有一个生理学上的解释, 即记忆的编码并不是纯粹的把东西原封不动地放进脑子里, 而是会被我们的神经系统先分解, 储存在大脑各处, 并且其中夹杂了大量记忆的场景, 即记忆发生时的环境. 当我们再次处于类似的环境时就有更大的概率把相关的事情回忆起来, 所谓的联想记忆也是这么一回事. 所以当我们犯错误时就应该尽可能地增加可以勾起我们回忆的材料, 比如写下一些箴言形式的语句加深记忆. 解题报告 上面说到错误记录可以降低我们犯错的概率, 同样的, 解题报告也是基于这一原理, 并且它往往比错误记录更加有效. 我们常常可以看见一些大牛blog上的学术类文章写得非常风趣, 有时里面会参杂一些很搞笑的例子, 于是他们的文章较之纯理论的文章更容易被我们记住. 这仍然是基于上述的生理学原理, 我们用来会议的材料越丰富, 或者这些材料越投我们所好, 我们就越容易联想起来与这些材料相关的东西. 这个原理应该用到些解题报告上来, 那就是尽量把自己关于这题的, 一些有意思的想法记录下来. 另外, 记录自己在解决这题时的思维过程也是非常有益的. 如果你还不知道思维过程的重要性, 那么请去看看波利亚的[How To Solve It](中文译名\"怎样解题\"), 这是每一个理工科学生的必读读物之一, 甚至你可能会在看完此书的若干年以后才会突然体会到其中的一些条款的极端正确性, 所以, 越早看这本书越好. 关于思维过程, 并不是十分容易被记录下来, 并且, 在你能够明明白白把你对于某题的思维过程叙述出来之前, 你对于这题的解决始终都是不完整的, 即下次遇到此类题目的变种, 十有八九还是无从下手(某些大牛可能因为语文水平不足而无法很好用语言表达, 这是特例...). 写解题报告的另一个好处当然是有助于自己复习. 尤其是在比赛之前看看解题报告是非常有益的, 一般情况下, 就拿我来说, 在两个月内做过的题目, 只要扫一眼解题报告, 一般我能马上把它原样地用代码实现出来. 如果不是很确定自己能坚持写解题报告, 或者担心自己的解法的正确性(很多时候AC了的代码不一定是正确的), 那么就去写blog吧, 把解题报告贴到blog上吧, 也许会有人用很尖锐的言语指出你的错误, 但, 那不正是你想要的吗? 另外贴到blog上也方便了自己日后的搜索. 关于图论 论资格, 我绝对在ACM队是派不上号的, 切题数也十分寒酸, 于是当你处于这种情况的时候就应该考虑主攻某一方面, 毕竟ACM是一项团队比赛, 一把瑞士军刀总也没有几把专业的刀具功能强大. 我在队里主要负责的是图论, 所以我在这里着重说一下图论方面的东西. ACM题目类型主要分为DP, 图论, 搜索, 数据结构, 模拟, 计算几何, 字符串, 组合数学, 数论等, 其中前两个是重头戏, 我做过的每场比赛里, 前两种题目都是必然会出现的, DP很大程度上需要依靠YY, 即它与IQ的关系很大, 这几乎是一个毋庸置疑的事实...并且DP的某些思想贯穿大部分ACM题目, 很容易于其它类型题目融合起来, 想要掌握它非一朝一夕之事. 而图论相对来说并没有DP那么可怕, 比较容易入门, 并且很多图论类题目可以套模板, 但是相对的, 图论题目也可以出得令人发指的难, 并且其数学模型往往隐藏在搜索, 计算几何, 字符串等类型的题目后面, 即表面上看起来不是图论, 但实际上这题考的却是图论原理. 图论的变形繁多, ACM题目中尤以Dijkstra最多, 看似简单的Dijkstra, 其变形程度是相当可怕的, 比如会消耗汽油的车的最短路问题, 这就是一个相对简单的二维Dijkstra, 而更加复杂的, 例如08年哈尔滨赛区的H, 一道隐藏在搜索背后的三维Dijkstra, 全场没有队伍出. 解决这类问题的一个根本性方法就是充分理解Dijkstra的定标技术, 以及规范的状态表示. 何为规范? 即当状态维数增高时, 需要对应的结构定义其状态, 并且此结构体切不可与存入堆的结构相混淆, 只要明确了Dijkstra的状态表示以及在某些限制条件下的状态转移(即图中的\"边\"), 则高维Dijkstra就不再是无法攻克的拦路虎了. 图论的另一个大头就是网络流, 这是图论中最容易套模板的一部分, 也是极其困难的一部分. 说容易套模板是因为往往这类题目在赛场上本身就是中等题或难题, 如果再不能套模板, 八成就变成了无人能出的大自然题了...网络流的变形数量几乎可以赶的上Dijkstra了, 如果算上匹配类的题目则就是有过之而无不及.网络流基本可以分为最大流, 限制流, 费用流三种, 其中最大流可以变形为二分匹配, 费用流可以变形为带权匹配. 其中最大流的算法是其它几种流算法的基础, 主流算法可以分类增广路类和预流推进类, 这两类算法几乎没有联系, 对于ACM, 只要学习前一种就足够了. 网络流类题目, 包括匹配类题目的核心思想就是增广路, 在匹配类题目中特化为交错轨, 这也是增广路类算法的核心. 各种增广路类算法的区别大都在于如何快速找到一条增广路, 其中比较简单的EK就是每次BFS找到一条增广路, 我就不多说了, 而高级一点的ISAP和Dinic都属于SAP, 两者都是基于分层图的思想, 实现分层图的方法只要为每个顶点标记所在层号即可, 其中Dinic是通过一次BFS建立分层图, 然后按照建立好的分层图进行多次DFS找到多条增广路, 从而不用像EK那样每条增广路都做一次BFS. 而ISAP则是在DFS的同时建立分层图, 即遇到DFS前进不了的时候对下一顶点重新标号, 于是这张分层图是逐步建立的, 建立后可以被后面的DFS所利用, 从而降低寻找增广路的消耗. Dinic和ISAP都可以用\"当前弧\"技术进行优化, 而ISAP还有一个进需要添加一行代码的GAP优化, 具体实现很容易在网上搜到. 这里要特别说一下Dinic和ISAP的适用范围, ISAP是万金油, 几乎可以应付绝大部分最大流和限制流的题目, 而Dinic特别适用于有向无环图, 即画在纸上可以分层的题目, 此时Dinic往往只总共需要做一次BFS(因为没有可以回退的边), 这时Dinic往往会比ISAP快很多. 要知道网络流是一个极度悲观的世界, 任何已知的算法都没有能突破O(n^3), 所以最大流算法写得不好很容易超时. 至于费用流, 可以基于EK算法, 只要将BFS改为SPFA就可以应付几乎所有的费用流问题了, 少量需要用到消负环算法的题目只要用SPFA找负环即可. 而限制流则只是一个定式的建图, 没什么特别的地方. 最后还有一种限制费用流, 如果遇到, 几乎肯定就是大自然题, 可以直接无视之. 关于而分图匹配的算法, 网上资料很多, 核心思想还是基于最大流, 只是可能不用最大流来解释也是可以的. 其中用于带权匹配的KM算法只要准备好模板即可, 一般不会有太大变形. 上面一段说的是既有的算法, 大部分都可以模板化, 其中要注意准备模板的时候最要准备针对整数和浮点数的两种, 对于C++程序员来说, 相应的只要写成函数模板, 然后传入比较函数对象即可, 代码量几乎不会有什么增加. 下面要说说网络流类题目真正的难点之一, 建图. 网络流类题目难在它往往伴随着一个不太直观的建图, 其中有些利用到最大流最小割定理的建图已经有了套路, 比如说限制流的建图, 最大权闭合图等, 具体可以参见国家IO集训队2007年胡伯涛的论文. 另外一些建图则很有技巧性. 比如\"比赛\"类题目, 有很多场比赛, 要你求得分能否达到某种条件等, 这时需要把每场比赛看成顶点, 然后两条流进来, 只有一条可以出去, 这种题目的特色是存在大量容量1的边. 还有拆点建图, 这类题目往往一个顶点上包含了2种\"属性\", 而网络流算法中, 属性是体现在连在边上的, 尽量要使每个顶点表示的属性单一化, 于是就把一个点拆分成两个点, 然后把属性分配出去, 比如PKU有一题说企鹅在冰块上跳来跳去, 冰块就有两个属性, 一个是冰上的企鹅数量, 另一个是冰块再被起跳多少次后会碎掉, 这时就需要把两个属性拆分开来, 拆点的另一个应用就是求最小点隔集, 其中的思想就是像这题冰上企鹅一样, 把出度和入度两个属性分离开. 另外一些更加巧妙的建图可能涉及逆向思维等技高技巧性的思维方法, 这里就不一一列举了. 网络流的难点之二是算法变形, 特别是有的题目在增广路上做文章. 有一种叫做\"连续增广路\"的技术, 需要深入理解增广路的原理, 今年的上海赛区F题就是基于连续增广路的二分匹配加搜索, 虽然之前做过一个基于最大流算法的连续增广路, 但是很遗憾, 当时没有能想到这个算法. 另外, 与字典序相关的最大流题目往往需要枚举删边, 如字典序最小边割集. 与其它类型题目相结合的算法变形就多不胜数了, 最常见的当属二分答案判可行性, 很多貌似运输类的问题很多都可以归结到这种方法上. 刚才说道了二分答案, 这也是所有图论类(应该说不仅仅是图论)题目最常见的变形, \"最(大)小值最大(小)\"是这类题目的最一般特征. 这类题目常常跟分数规划联系在一起, 比如最优比率生成树, 最优比率环等. 另外图论的一些经典算法可以衍生出很多强大的应用, 比如差分约束就是Bellman-Ford或SPFA的一个最好的应用, 这类题目的建图关键是找出差分式子. 一个需要特别注意的地方是[算法导论]上对于不连通图上添加顶点的讨论, 最好的方法是不用添加顶点, 开始时直接将dis数组清0, 然后所有顶点入队即可. 生成树类的题目也有很多变形, 如欧几里得生成树, 往往需要利用平面图上的一些几何性质建图, 如曼哈顿距离生成树, 限制度生成树等等, 此类题目套路不多, 且知识点比较散乱, 需要多做题来熟练. 图的连通性也是一个常见的考点. 割点, 桥, 强连通, 双连通问题的求解不仅要准备模板, 还要充分理解模板. 一些地图上的题目(二维的方格阵), 往往有些特殊的性质, 不仅需要你一些建图的敏感度, 偶尔需要在模板中添加一些巧妙的代码. 图论还可以与组合数学和计算几何等产生紧密的联系, 比如生成树和图的计数, 最简单的诸如n个顶点可以产生多少个不同的无向连通图, 这时一些组合数学中的基础原理往往十分有用, 最典型的就是容斥原理. 图论中还有茫茫多的定理, 性质等. 我所知道的最雷人要数08年哈尔滨赛区那道赤裸裸的Havel定理了. 这些定理和性质可能会在一些意想不到的地方发挥作用. 最后图论中的一些NP问题或非NP但是代码不好写的问题, 如旅行商, 同构, 支配集, 最大团, 以及由树所引申出的一大堆树形DP问题不好准备模板, 但可以考虑准备一些具有代表性的解题报告的纸版材料. 其中树形DP是一个很容易写错的重头戏, 它往往还和背包问题联系在一起, 尤其是泛化物品的背包, 关于泛化物品, 请参见DD牛的[背包九讲]. 关于工具 ACM的学习, 对于我们一个弱校来说, 没有牛B的教练, 没有严格的教学, 最主要的学习方式莫过于收集和记录, 不论是外部知识的搜索收集, 还是内部知识的记录整理, 最离不开的当属Google. 搜索我就不用说了, 这里关于知识的记录和整理, 我要推荐如下几个工具: Google Docs, Google Desktop, Google Calender, Everything. 前两个都是辅助你记录和复习你的解题报告, 第三个用于时间管理以及任务计划, 详情请自己去用Google搜索去. 最后一个Everything是windows平台下特别针对NTFS文件系统的一个神奇的搜索工具, 它可以仅仅使用约5MB的内存在1秒内搜索出你的所有NTFS盘上任何一个文件名跟输入关键字匹配的文件, 其原理据说是基于NTFS文件系统的一个特殊的记录表的. 这个东西的一个最有趣的应用可以是这样: 把你硬盘上的所有文件和文件夹都加上标签吧, 用\"[]\"括起来, 然后用\",\"隔开, 然后任何时候你都可以通过标签搜索到你想要的东西, 一切图片, 论文, 软件等等, 比如PKU的1001题我就可以加上\"[java,BigDecimal,浮点数,高精度]\"之类的字样. 甚至更精细一点, 定义一些特殊的tag, 比如\"tag.r\"表示\"需要review\", \"tag.h\"表示\"我是在别人的help下做出的题目\", \"tag.p\"表示\"这题我还有未解决的problem\"等等. 关于学习 我所知道的关于学习, 90%来自于[www.xiaolai.net](李笑来的blog), 和[www.mindhacks.cn](刘未鹏的blog), 把里面关于学习方法的文章看完, 你就升华了. 在此特别感谢上海第二工业大学的王学长告诉我Google Reader的存在, 大二开始的时候我因为这个接触到blog这种东西, 让我对学习方法有了一个相对清醒的认识. 也许很多所谓的方法我们早就知道了, 可就是\"很多道理我们其实我们老早就知道, 只是需要一个权威人士告诉我们它确实是正确的\". 这里特别要说一些关于学习效率和顺序的问题. 大二就去看什么人工智能, 自然语言处理, 对于绝大多数人来说, 那就是低效, 甚至你在看这些书的时候连笔记都不知道该怎么记, 更何况还有那么多人看书都没有记笔记的习惯. 但是即使你用心做了笔记, 也不一定能学到什么东西, 比如我当时就将近记了一大本的关于人工智能, 心理学之类的东西, 可现在回头看看一恰的笔记几乎已经不敢相信那是我自己写的了, 完全不认识. 究其原因有二, 一是先导知识储备不足, 二是没有及时回顾笔记. 这两点在很多人的学习过程中都在不断重复上演着, 其结果就是浪费了大量时间却学不到什么东西. 所谓的\"做开发\"也是如此, 连关系数据库的几个范式都不知道是什么就去弄什么SQL Server, 搞什么ORM, 全都是空谈, 充其量也就是会用用工具而已. 就我目前的经历来看, 我认为作为一个软件方向的CS学生, 相对高效的学习方案应该是学基础课时学好高数和英语(我现在就特别想再去重学高数), 闲暇时间搞搞C++, Java, 等各种语言, 至少做到一个了解的程度, 个别的要熟练, 与计算机软件关系不是很大的理科课程成绩搞到90+就行, 比如物理, 比如电路, 线性代数很简单, 概率论要特别认真学, 特别是经常要拿起来复习复习, 这是一门终身受用的学科. 然后其余的时间可以用来啃算法导论, 相信我, 这本书普通人大学四年根本不可能啃完. 总之数学类的书看的再多都嫌少. 另外可以扩展出去接触一些人文类书籍, 特别是传记类, 励志类的东西. 关于修养 我刚上大学的时候有这么一个习惯, 拿一些自己心中已经知道答案的问题去请教老师, 以显得自己很好学, 很牛B. 记得我有过在软件工程选修课上跑去问我们学校有没有关于\"设计模式\"的课程, 或者问ben一些明显Google一下就能知道答案的问题...等等等等都是内心浮躁的表现. 自我修养是一种很要紧的东西, 对于一个性格内向的ACMer来说, 这往往不会有什么问题, 这是由性格决定的, 但不是每个人都性格内向, 不是每个人都能保持内敛. 一旦你有装B(一切骄傲, 或者因为自己的某方面才能而沾沾自喜, 或者想要显露自己, 不管和不合适, 暂且用\"装B\"来代替吧)的行为或者想法, 或者仅仅是潜意识, 那就预示着你会被大多数人看不起, 会大量浪费自己的时间, 会降低自己的学习效率, 会使自己成为井底之蛙. 不过完全不装B倒也不太好做到, 比如我以为自己写blog多少有些显露自己的意思, 自我分析一下, 动机大概有如下几条: 想让日后身边的某人突然发现我说:\"啊, 你就是Answeror啊!\", 想让我以前女朋友在若干年后突然发现我的blog说:\"原来我以前男朋友这么好学, 这么有修养有文化!\", 想让我被某些大学教授或者公 司里的人认识, 想结交志同道合朋友, 想让别人给我的解题报告挑错... 写blog这件事情也就算了, 毕竟是件利人利己的好事, 偶尔装装B也就算了. 但是像我在本文开头说的那样, 刚进华理仅仅因为自己有点C++和网络工程知识的皮毛就装出一副很牛B的样子, 那种装任谁都无法忍受. 尤其是第一次跟罗老师的谈话, 简直就是装到了极点, 以至于我后来就有这么一个想法: 反正我可以做技术活, 干嘛非要搞ACM, 我可以自由地去学心理学, 学人工智能, 学.NET, 我牛B着呢, ACM算个屁. 然后我就傻BB地搞了5个月的.NET, 然后到大三上数据库原理的时候发现当时花了一个星期学的种种东西现在只是一节课的事情, 直到那时我还在装B, 跟旁边的同学说我上述的体会, 仿彿就是在告诉他说:\"你看我牛B吧, 我老早就知道这是什么东西了, 还做过它的开发\", 时至今日, 我仍然有时会忍不住在同学讨论C++种种语言特性时上去插插嘴, 装装B. 高中的一个好友曾这样形容我: \"他这个人啊, 只要看了什么书或者电影之类的, 就要马上在他的文章里体现出来\". 听起来很好笑吧, 请正在看这些文字的你不妨停下来反思一下自己最近一个星期有没有类似的行为? 作为一个ACMer, 改掉这些吧. 我们做ACM的, 本来在这个学风不正的计算机学院里就是一群特殊人群, 我们需要做的仅仅是不停地补充知识, 完善自己, 没必要费尽心机去博得他人的一两句赞美之辞. 我是谁? 我就是华理的一个ACMer. 我每天窝在机房里为的不是哪天去比赛有小MM来找我们签名, 而是让自己精通各种算法, 提升自己的数学修养, 增强自己解决问题的能力, 培养自己内敛的人格, 然后在某日某公 司的面试时让面试官打心地里认为我是个人才, 然后用我以前长期积累的知识写出卓越的软件. 希望你我都能早日问心无愧地说出上面的话. if(musicUrl!='') { document.write(' [增加到媒体播放列表]'); musicUrl=''; }","title":"华东理工某ACMer的总结"},{"content":"4.3  汇总范式——一张五味俱全的大烙饼 形者神之质，神者形之用。 ——《范缜·神灭论》 关键词：编程范式；设计模式；编程语言 摘  要：总结编程范式 预览 设计模式好比组合套路，能在一些特定场合下克敌制胜；编程范式则好比武功门派，博大精深且自成体系。 一种编程范式之所以能独树一帜，关键在于它突破了原有的编程方式的某些限制，带来革命性的新思维和新方法，进一步解放了程序员的劳动力。 因其长而容己，因其短而容他，此万物之理也。 语言为形，范式为神。若能以神导形、以形传神，则看似平白无趣的程序也能写出诗画般的意境。 提问 编程范式与设计模式有什么区别？ 编程范式的核心价值是什么？ 总结前面介绍的编程范式，它们各自有哪些代表语言？核心概念和运行机制是什么？针对的问题和主要的目的是什么？实现原理是什么？常见的应用有哪些？有什么不足之处？ 讲解 稍事休整后，大家重新团结在以冒号为中心的周围。 问号再度发问：“编程范式与设计模式都是一种抽象的软件思想，都有一套具体的实现方法。单从字面上看，‘编程’与‘设计’、‘范式’与‘模式’的区别似乎也不太大。它们究竟有什么不同呢？” “这个问题有点意思。”冒号颔言，“设计模式一般针对某一特定场景的问题，而编程范式针对的是广泛得多的问题领域，通常有一整套的思想和理论体系，具有全局性、系统性和渗透性，这一点在5大重要范式中显得尤为突出。因此，编程范式更普适更抽象，涉及的深度和广度也是设计模式难以比拟的。” 引号不免有些疑问：“但事件驱动式不是也能作为设计模式吗？” 冒号解疑：“这倒并不矛盾。同样的思想用在整体系统的结构设计上，则称为架构模式；用在局部模块的细节实现上，则称为设计模式[1]；用在引导编程实践上，则称为编程范式。” 句号的武侠瘾又犯了：“设计模式好比组合套路，能在一些特定场合下克敌制胜；编程范式则好比武功门派，博大精深且自成体系。” “很形象的比喻。”冒号赞赏道，“设计模式是遵循设计原则的一些具体技巧，以保证代码的可维护性、扩展性和可重用性为目的。它重在设计，对语言一般没有要求[2]。编程范式则不同，对语言往往有专门的要求。通常所说的某某范式的语言，即指该语言对该范式在语法上有明确充分的支持，不须要借助其他的范式或工具。事实上，语言本来就是围绕其所倡导的核心范式来设计的[3]。” 逗号询问：“如果一种语言不支持某种范式，那么还能用这种范式编程吗？” “语言不直接支持范式，只是说明它不属于该范式的语言，但还是可能求助工具来应用该范式。比如元编程可以借助Yacc或ANTLR来完成，AOP可以借助一些库或框架来实现。”冒号道，“正是依靠语言和工具的支持，编程范式得以建立起一套独特而完善的抽象机制和方法体系，从而为所倡导的世界观与方法论奠定基石。” 叹号请求：“能不能帮我们理清一下思路，把学过的范式一并汇总比较？” 不一会儿，众人面前呈现出一张表格，地毯似的覆盖了整个投影屏（如表4-1所示）——   表4-1  常见的编程范式 编程范式：命令式/过程式（Imperative/Procedural） 代表语言 Fortran/Pascal/C 核心概念 命令/过程（Command/Procedure） 运行机制 命令执行 关键突破 突破单一主程序和非结构化程序的限制 实现原理 引入逻辑控制和子程序 主要目的 模拟机器思维，实现自顶向下的模块设计 常见应用 交互式、事件驱动型系统、数值计算等 编程范式：函数式/应用式（Functional/Applicative） 代表语言 Scheme/Haskell 核心概念 函数（Function） 运行机制 表达式计算 关键突破 突破机器思维的限制 实现原理 引入高阶函数，将函数作为数据处理 主要目的 模拟数学思维，简化代码，减少副作用 常见应用 微积分计算、数学逻辑、博弈等 编程范式：逻辑式（Logic） 代表语言 Prolog/Mercury 核心概念 断言（Predicate） 运行机制 逻辑推理 关键突破 突破逻辑与控制粘合的限制 实现原理 利用推理引擎在已知的事实和规则的基础上进行逻辑推断 主要目的 专注逻辑分析，减少控制代码 常见应用 机器证明、专家系统、自然语言处理、语义网（semantic web）、决策分析、业务规则管理等 编程范式：对象式（Object-Oriented） 代表语言 Smalltalk/Java 核心概念 对象（Object） 运行机制 对象间信息交换 关键突破 突破数据与代码分离的限制 实现原理 引入封装、继承和多态机制 主要目的 迎合人类认知模式，提高软件的易用性和重用性 常见应用 大型复杂交互式系统等 编程范式：并发式/并行式（Concurrent/Parallel） 代表语言 Erlang/Oz 核心概念 进程/线程（Process/Thread） 运行机制 进程/线程间通信与同步 关键突破 突破串行的限制 实现原理 引入并行的线程模块及模块间的通信与同步机制 主要目的 充分利用资源、提高运行效率、提高软件的响应能力 常见应用 图形用户界面，I/O处理，多任务系统如操作系统、网络服务器等，实时系统，嵌入式系统，计算密集型系统如科学计算、人工智能等 编程范式：泛型式（Generic） 代表语言 Ada/Eiffel/C++ 核心概念 算法（Algorithm） 运行机制 算法实例化（多发生于编译期） 关键突破 突破静态类型语言的限制 实现原理 利用模板推迟类型指定 主要目的 提高算法的普适性 常见应用 普适性算法如排序、搜索等，集合类容器等 编程范式：元编程（Metaprogramming） 代表语言 Lisp/Ruby/JavaScript 核心概念 元程序（Metaprogram） 运行机制 动态生成代码或自动修改执行指令 关键突破 突破语言的常规语法限制 实现原理 利用代码生成或语言内建的反射（reflection）、动态等机制，将程序语言作为数据来处理 主要目的 减少手工编码，提升语言级别 常见应用 自动代码生成、定义结构化配置文件、IDE、编译器、解释器、人工智能、模型驱动架构（MDA）、领域特定语言（DSL）等 编程范式：切面式（Aspect-Oriented） 代表语言 AspectJ/AspectC++ 核心概念 切面（Aspect） 运行机制 在接入点处执行建议 关键突破 突破横切关注点无法模块化的限制 实现原理 通过编织（weaving）将附加行为嵌入主体程序 主要目的 实现横切关注点分离 常见应用 日志输出、代码跟踪、性能监控、异常处理、安全检查、事务管理等 编程范式：事件驱动（Event-Driven） 代表语言 C#/VB.NET 核心概念 事件（Event） 运行机制 监听器收到事件通知后做出响应 关键突破 突破顺序、同步的流程限制 实现原理 引入控制反转和异步机制 主要目的 调用者与被调用者在代码和时间上双重解耦 常见应用 图形用户界面、网络应用、服务器、操作系统、IoC框架、异步输入、DOM等 叹号怔了怔，好似被一张巨大的烙饼给噎住了。 冒号并不急于讲解，欲以静制动。 果然，逗号沉不住气了，问道：“在第1栏的编程范式及其代表语言中，为什么并发式的代表语言没有Java和C#，只有Erlang和Oz？” “Java和C#虽然在语法和核心库中为并发编程提供了不少支持，但真正将并发范式融入基本设计理念的语言还得数Erlang、Oz这些较为冷门的语言。”冒号解释，“类似地，比起Java、JavaScript等语言来，C#和VB.NET在语言设计上对事件驱动式编程给予了更多的关注[4]，因而更具代表性。” 问号发现：“‘关键突破’的提法很特别啊。” 冒号轻捶桌面以示强调：“一种编程范式之所以能独树一帜，关键在于它突破了原有的编程方式的某些限制，带来革命性的新思维和新方法，进一步解放了程序员的劳动力。这便是范式的核心价值所在。” 引号如获至宝：“这张表格浓缩了范式的精华，既是对此前知识的总结，也是对今后编程的指导，实在太有用了！” 句号显得更为冷静：“有其长必有其短。我们了解了每种范式的长处，是不是还应该了解它们各自的短处？” 冒号开始对各个范式逐一数落：“过程式编程的数据与代码脱节，不方便维护；函数式和逻辑式的开发效率一般比过程式高，但运行效率和语言表现力则有所不如；对象式编程用于数学计算、符号处理等对象特征淡薄的领域，在心理上缺乏认知基础，在运行效率上不如纯过程式，在开发效率上不如函数式；并发式编程增加了代码的复杂度，加重了程序员的负担；泛型式编程影响了代码的可读性，过度使用模块还可能造成代码膨胀（code bloat）[5]；元编程过于强大，运用不当会超出程序员的控制，宜谨慎使用；切面式编程减少了程序的可预测性和可控性，同时给代码的跟踪调试带来一定困难，还可能造成性能上的损失；事件驱动式编程虽然也能用于同步的流程应用，但毕竟机制更复杂，没有普通的流程式编程那么自然易懂。” 叹号看上去有点泄气：“您可真够绝的，先把这些编程范式一个个捧到天上，又几杆子它们一个个打下云端。” “因其长而容己，因其短而容他，此万物之理也。”冒号忽然惜言如金，一番之乎者也地予以回应。 句号借用了一句俗话：“不怕有缺点，就怕没特点。” 冒号本欲多言，却恐众人食多伤胃，遂作结案陈词：“尽管只是管中窥豹，相信大家多少见识了编程范式的魅力之处。它们各擅胜场，有风格之别而无高下之分。作文绘画讲究形神兼备，编程也不例外。语言为形，范式为神。若能以神导形、以形传神，则看似平白无趣的程序也能写出诗画般的意境。” 一席话说得众人皆觉虽不能至，然心向往之。 总结 Ø  相比设计模式，编程范式针对的问题领域更广泛，提出的思想和方法更普适、更抽象、更系统。此外，设计模式重在设计，对语言和工具的要求不高，而编程范式须要建立一套抽象机制和方法体系，离不开语言或工具的支持。 Ø  编程范式的核心价值在于：突破原有编程方式的某些限制，带来新思维和新方法，从而进一步解放程序员的劳动力。 Ø  正文中编程范式的汇总表格既是对此前知识的总结，也是对今后编程的指导。 Ø  既要了解编程范式的长处，也要了解它们的短处。 Ø  编程范式为神，编程语言为形，应以神导形、以形传神。 参考 [1] Elena Bolshakova．PROGRAMMING PARADIGMS IN COMPUTER SCIENCE EDUCATION．International Journal \"Information Theories & Applications\"，2005，Vol.12：285-290 [2] Amnon H. Eden，Rick Kazman．Architecture, design, implementation．Proceedings of the 25th International Conference on Software engineering，2003：149–159 插语 [1]因此设计模式有时被称为微架构（microarchi- tecture）模式。 [2]设计模式的应用范围主要集中于静态的OOP语言，但也不排斥动态的或非OOP的语言。 [3]当然随着语言的演进，也可能支持新的范式。比如，C++、Java和C#一开始都不支持泛型编程，C#对函数范式的支持也是逐渐加大的。 [4]C#和VB.NET专门为事件驱动式设计了event、delegate等关键字，以及一些配套的便利机制。 [5]这里的代码不是指程序员写的源代码，而是指编译器生成的代码。     欢迎转载，转载时请注明： 本文出自电子工业出版社博文视点（武汉）新书《冒号课堂——编程范式与OOP思想》。 http://www.china-pub.com/196068&ref=ps http://www.douban.com/subject/4031906/","title":"《冒号课堂》连载之十八——汇总范式"},{"content":" http://www.cnblogs.com/herso/archive/2009/12/07/1618562.html   大一： 其实大一，没必要学习各种新鲜的技术…..把高等数学学好吧….这才是正事，是决定了着将来你是否能称为一个大牛还是一个编程语言的熟练操作工人的因素…. 也许这时候的你还不知道高等数学有什么作用… 但我要告诉你的是如果你的悟性高…. 工作一两年也许就能体会到数学的做用…. 学高数..不是简简单单的学习微积分…. 在掌握这些知识的时候….锻炼自己的逻辑思维….. 锻炼自己的思考问题解决问题的方法和能力。作用在将来一定大大的….. 等将来如果你涉足密码学…你会发现各种积分方程和矩阵变化…. 将来在计算一个算法的复杂性和证明算法的可靠性时，也离不开数学知识…. 如果你涉足人工智能和语音识别，各种统计模型就会呈现在你面前。在你毕业找工作时，这个才是你和专业培训机构培训出来的学生的差异能力。这才是企业更看重的能力。如果你还有时间的话，学习C语言… 但是不要再用谭浩强的书了…. 看 The C program langue 吧… 如果能真正领悟书中70%的例子话，那就足够了. 如果能把这两门课程学到十分优秀，恭喜你，你已经成功了一半了…..   大二： 如果你在大一学习了C之后，这个时候大学的课程就要涉及操作系统和数据结构、还有汇编语言了…… 这也是大二一定要学好的两门课了……大学的操作系统太失败了，上完课后，很多的学生不知道所云，更加感觉操作系统的神秘了，课程设计也就是什么银行家算法的，然后大家在网上一顿搜索，然后交给老师就算完事了… 其实，我的建议是自己写一个操作系统内核，实现内存管理，进程管理和切换 等一些基础的东西了就可以了，《自己动手写操作系统》就是很好的教材…… 如果还有时间，学习《Linux内核设计与实现》，看看现实商用的操作系统是怎么实现的？ 当然最好和原码结合的一起看，效果最好。还有赵炯博士的 “.012Linux内核完全剖析”什么的。如果能仔细阅读，收获一定不少。当然还有数据结构，这个也是重中之中，这也是和非科班出身的学生的差别，关键是你学的好坏，这个的实践主要在ACM上，当学习完数据结构后，最重要的是使用，不断的在Acm上做各种各样的题目，不断的提升自己算法设计的能力。从大二开始，如果能坚持两年下来，那么一般的算法设计肯定是难不住的了，也许这时候高数打下的基础就会起作用了。 当毕业的时候，进入一家好的公司应该不是太难的事情了。再说说汇编语言，本质上这也是一门编程语言，可能刚入门的时候比较困难，但是程序写多了，和C也没有差别了。我还想说一点，就是现在Windows内核也逐步开放了，至少有很多的逆向的资源可以学习。如果对Windows有兴趣，一样可以学习操作系统的实现原理。   大三： 离散数学和编译原理是个重头戏，离散数学虽然我现在还没体会到他的作用，但是和高数一样，这中内在的东西才是最重要的，代表着内功，如果没有学好，这些债迟早还要要还的。 编译原理，学习完以后一样会让你云里雾里，整天做那些无聊的题目。还是说实践吧，网上有开源的C编译器的源码，下载下来然后好好学习下，结合编译原理书中讲的东西，好好的消化一些这些知识，最后，自己如果能写出来一个C编译器的话，那你的编译原理也就通过了。当然这个时候可以学习一些C++或Java之类语言，但是学到够平时用的就可以了，没有学非常深。选择一本教材学习两三个月就行了。     当然，这个时候，可能你的同学已经能做出来各种漂亮的网页，也可能熟练的使用MFC类库做出各种各样的漂亮的软件，这些没什么，如果三年下来，如果你能够按照上面我写的那样坚持学习。也许他们用三年学习的这些东西，你用三个月就能熟练。   大四： 到了找工作的时候，如果你按照上面一步一个脚印的学习，我相信你会收到很多大公司的offer。因为大公司更看重的是你的内功的深厚，而小公司才会看重那些花拳绣腿的技术。但是这个时候，千万不要忘记继续学习，很多的学生大四一年都浪费掉了，真实太可惜了，在前面三年的基础上，到了厚积薄发的时候了， 开始要思考自己的职业规划了，你要选择Linux方向还是Windows方向，要选择底层方向还是应用方向， 要选择网页方向还是桌面应用方向。是选择自然语言处理还是人工智能。这个时候你要选择自己的一个方向，当然你可以向你的导师求助，然后确定自己的发展方向，大四一年就可以专心的学习了。     4. 附上我认为计算机学习比较好辅助教材：   C语言： the C Program Language 操作系统; 于渊：《自己动手写操作系统》                    《Linux内核设计与实现》                    《Linux内核完全剖析》                    《Linux内核情景分析》                    《Windows内核情景分析》 编译原理： 龙书《编译原理》 汇编： 王爽老师《汇编第二版》   5. 后记   以上都是自己在工作后对大学四年的反思，可能很多人有不一样的看法，我没有任何异议。毕竟每个人经历是不一样的，但是如果你向想做真正的计算机科班出身的学生，学好上面介绍的课程吧。在以后的职业生涯中，你会终身受益的。当然上面很多的课程我没有提到，并不代表他们不需要学习，只是分量没有那么重而已。因为你还是要毕业的，每门功课还是要过的。zds   当然，我现在认为，计算机的本科四年真是一个打基础的四年，之后才是学习各种招式，如果基础打好了，招式的学习会事半功倍的。当进入公司后，一样要持续不断的学习，才能让你不断的进步。自己文采不好，写的比较乱，但都是肺腑之言，各位将就看吧。zds","title":"（转贴）计算机系学生大学四年应该这样过"},{"content":"3. 四年后，我能骄傲的说我是计算机系的学生 上面发了那么多的牢骚，其实都是有感而发….下面在结合自己的工作的感受具体谈谈计算机学生应该如何规划自己的大学四年 大一： 一个新兵蛋子，刚走进象牙塔的大门，什么都是新鲜的，不断听着学长们说着天书般的技术术语… 天天争论C++和java哪个好，.net是否比Vc更智能先进…. 还有什么Asp.net …. 一堆的技术摆在自己面前了… 然后自己就糊涂了….去问学长吧…学长告诉你..好好学习java吧…将来有钱途….. 其实大一，没必要学习各种新鲜的技术…..把高等数学学好吧….这才是正事，是决定了着将来你是否能称为一个大牛还是一个编程语言的熟练操作工人的因素…. 也许这时候的你还不知道高等数学有什么作用… 但我要告诉你的是如果你的悟性高…. 工作一两年也许就能体会到数学的做用…. 学高数..不是简简单单的学习微积分…. 在掌握这些知识的时候….锻炼自己的逻辑思维….. 锻炼自己的思考问题解决问题的方法和能力。作用在将来一定大大的….. 等将来如果你涉足密码学…你会发现各种积分方程和矩阵变化…. 将来在计算一个算法的复杂性和证明算法的可靠性时，也离不开数学知识…. 如果你涉足人工智能和语音识别，各种统计模型就会呈现在你面前。在你毕业找工作时，这个才是你和专业培训机构培训出来的学生的差异能力。这才是企业更看重的能力。如果你还有时间的话，学习C语言… 但是不要再用谭浩强的书了…. 看 The C program langue 吧… 如果能真正领悟书中70%的例子话，那就足够了. 如果能把这两门课程学到十分优秀，恭喜你，你已经成功了一半了….. 大二： 如果你在大一学习了C之后，这个时候大学的课程就要涉及操作系统和数据结构、还有汇编语言了…… 这也是大二一定要学好的两门课了……大学的操作系统太失败了，上完课后，很多的学生不知道所云，更加感觉操作系统的神秘了，课程设计也就是什么银行家算法的，然后大家在网上一顿搜索，然后交给老师就算完事了… 其实，我的建议是自己写一个操作系统内核，实现内存管理，进程管理和切换 等一些基础的东西了就可以了，《自己动手写操作系统》就是很好的教材…… 如果还有时间，学习《Linux内核设计与实现》，看看现实商用的操作系统是怎么实现的？ 当然最好和原码结合的一起看，效果最好。还有赵炯博士的 “.012Linux内核完全剖析”什么的。如果能仔细阅读，收获一定不少。当然还有数据结构，这个也是重中之中，这也是和非科班出身的学生的差别，关键是你学的好坏，这个的实践主要在ACM上，当学习完数据结构后，最重要的是使用，不断的在Acm上做各种各样的题目，不断的提升自己算法设计的能力。从大二开始，如果能坚持两年下来，那么一般的算法设计肯定是难不住的了，也许这时候高数打下的基础就会起作用了。 当毕业的时候，进入一家好的公司应该不是太难的事情了。再说说汇编语言，本质上这也是一门编程语言，可能刚入门的时候比较困难，但是程序写多了，和C也没有差别了。我还想说一点，就是现在Windows内核也逐步开放了，至少有很多的逆向的资源可以学习。如果对Windows有兴趣，一样可以学习操作系统的实现原理。 大三： 离散数学和编译原理是个重头戏，离散数学虽然我现在还没体会到他的作用，但是和高数一样，这中内在的东西才是最重要的，代表着内功，如果没有学好，这些债迟早还要要还的。 编译原理，学习完以后一样会让你云里雾里，整天做那些无聊的题目。还是说实践吧，网上有开源的C编译器的源码，下载下来然后好好学习下，结合编译原理书中讲的东西，好好的消化一些这些知识，最后，自己如果能写出来一个C编译器的话，那你的编译原理也就通过了。当然这个时候可以学习一些C++或Java之类语言，但是学到够平时用的就可以了，没有学非常深。选择一本教材学习两三个月就行了。 当然，这个时候，可能你的同学已经能做出来各种漂亮的网页，也可能熟练的使用MFC类库做出各种各样的漂亮的软件，这些没什么，如果三年下来，如果你能够按照上面我写的那样坚持学习。也许他们用三年学习的这些东西，你用三个月就能熟练。 大四： 到了找工作的时候，如果你按照上面一步一个脚印的学习，我相信你会收到很多大公司的offer。因为大公司更看重的是你的内功的深厚，而小公司才会看重那些花拳绣腿的技术。但是这个时候，千万不要忘记继续学习，很多的学生大四一年都浪费掉了，真实太可惜了，在前面三年的基础上，到了厚积薄发的时候了， 开始要思考自己的职业规划了，你要选择Linux方向还是Windows方向，要选择底层方向还是应用方向， 要选择网页方向还是桌面应用方向。是选择自然语言处理还是人工智能。这个时候你要选择自己的一个方向，当然你可以向你的导师求助，然后确定自己的发展方向，大四一年就可以专心的学习了。   感觉虽然说的不全对，但却也感触不少。。。更明白自己的路了。。不想让自己的日子那么浑浑噩噩地过下去……","title":"看到的帖子"},{"content":"隐马尔科夫模型HMM 来自：http://hi.baidu.com/549800946/blog/item/45a62cc4010cd7ae8326ac76.html 2009年01月07日 星期三 15:41 原文网址：http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html 算法：Viterbi algorithm 和 Forward-Backward Algorithm。 小实验程序：http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg3.html 隐马尔科夫模型HMM自学 （1） 介绍 崔晓源 翻译 我们通常都习惯寻找一个事物在一段时间里的变化规律。在很多领域我们都希望找到这个规律，比如计算机中的指令顺序，句子中的词顺序和语音中的词顺序等等。一个最适用的例子就是天气的预测。 首先，本文会介绍声称概率模式的系统，用来预测天气的变化 然后，我们会分析这样一个系统，我们希望预测的状态是隐藏在表象之后的，并不是我们观察到的现象。比如，我们会根据观察到的植物海藻的表象来预测天气的状态变化。 最后，我们会利用已经建立的模型解决一些实际的问题，比如根据一些列海藻的观察记录，分析出这几天的天气状态。 Generating Patterns 有两种生成模式：确定性的和非确定性的。 确定性的生成模式：就好比日常生活中的红绿灯，我们知道每个灯的变化规律是固定的。我们可以轻松的根据当前的灯的状态，判断出下一状态。   非确定性的生成模式：比如说天气晴、多云、和雨。与红绿灯不同，我们不能确 定下一时刻的天气状态，但是我们希望能够生成一个模式来得出天气的变化规律。我们可以简单的假设当前的天气只与以前的天气情况有关，这被称为马尔科夫假 设。虽然这是一个大概的估计，会丢失一些信息。但是这个方法非常适于分析。 马尔科夫过程就是当前的状态只与前n个状态有关。这被称作n阶马尔科夫模型。最简单的模型就当n=1时的一阶模型。就当前的状态只与前一状态有关。（这里要注意它和确定性生成模式的区别，这里我们得到的是一个概率模型）。下图是所有可能的天气转变情况：   对于有M个状态的一阶马尔科夫模型，共有M*M个状态转移。每一个状态转移都有其一定的概率，我们叫做转移概率，所有的转移概率可以用一个矩阵表示。在整个建模的过程中，我们假设这个转移矩阵是不变的。   该矩阵的意义是：如果昨天是晴，那么今天是晴的概率为0.5，多云的概率是0.25，雨的概率是0.25。注意每一行和每一列的概率之和为1。 另外，在一个系统开始的时候，我们需要知道一个初始概率，称为 向量。   到现在，我们定义了一个一阶马尔科夫模型，包括如下概念： 状态：晴、多云、雨 状态转移概率 初始概率 隐马尔科夫模型HMM自学 （2） 马尔科夫模型也需要改进！ 崔晓源 翻译 当一个隐士不能通过直接观察天气状态来预测天气时，但他有一些水藻。民间的传说告诉我们水藻的状态与天气有一定的概率关系。也就是说，水藻 的状态与天气时紧密相关的。此时，我们就有两组状态：观察状态（水藻的状态）和隐含状态（天气状态）。因此，我们希望得到一个算法可以为隐士通过水藻和马 尔科夫过程，在没有直接观察天气的情况下得到天气的变化情况。 更容易理解的一个应用就是语音识别，我们的问题定义就是如何通过给出的语音信号预测出原来的文字信息。在这里，语音信号就是观察状态，识别出的文字就是隐含状态。 这里需要注意的是，在任何一种应用中，观察状态的个数与隐含状态的个数有可能不一样的。下面我们就用隐马尔科夫模型HMM来解决这类问题。 HMM 下图是天气例子中两类状态的转移图，我们假设隐状态是由一阶马尔科夫过程描述，因此他们相互连接。 隐状态和观察状态之间的连线表示：在给定的马尔科夫过程中，一个特定的隐状态对应的观察状态的概率。我们同样可以得到一个矩阵： 注意每一行（隐状态对应的所有观察状态）之和为1。 到此，我们可以得到HMM的所有要素：两类状态和三组概率 两类状态：观察状态和隐状态； 三组概率：初始概率、状态转移概率和两态对应概率（confusion matrix） 隐马尔科夫模型HMM自学 （3） HMM 定义 崔晓源 翻译 HMM是一个三元组 (,A,B).  the vector of the initial state probabilities;  the state transition matrix;  the confusion matrix; 这其中，所有的状态转移概率和混淆概率在整个系统中都是一成不变的。这也是HMM中最不切实际的假设。 HMM的应用 有三个主要的应用：前两个是模式识别后一个作为参数估计 (1) 评估 根据已知的HMM找出一个观察序列的概率。 这类问题是假设我们有一系列的HMM模型，来描述不同的系统（比如夏天的天气变化规律和冬天的天气变化规律）， 我们想知道哪个系统生成观察状态序列的概率最大。反过来说，把不同季节的天气系统应用到一个给定的观察状态序列上，得到概率最大的哪个系统所对应的季节就 是最有可能出现的季节。（也就是根据观察状态序列，如何判断季节）。在语音识别中也有同样的应用。 我们会用forward algorithm 算法来得到观察状态序列对应于一个HMM的概率。 (2) 解码 根据观察序列找到最有可能出现的隐状态序列 回想水藻和天气的例子，一个盲人隐士只能通过感受水藻的状态来判断天气状况，这就显得尤为重要。我们使用viterbi algorithm来解决这类问题。 viterbi算法也被广泛的应用在自然语言处理领域。比如词性标注。字面上的文字信息就是观察状态，而词性就是隐状态。通过HMM我们就可以找到一句话上下文中最有可能出现的句法结构。 (3) 学习 从观察序列中得出HMM 这是最难的HMM应用。也就是根据观察序列和其代表的隐状态，生成一个三元组HMM (,A,B)。使这个三元组能够最好的描述我们所见的一个现象规律。 我们用forward-backward algorithm来解决在现实中经常出现的问题--转移矩阵和混淆矩阵不能直接得到的情况。 总结 HMM可以解决的三类问题 Matching the most likely system to a sequence of observations -evaluation, solved using the forward algorithm; determining the hidden sequence most likely to have generated a sequence of observations - decoding, solved using the Viterbi algorithm; determining the model parameters most likely to have generated a sequence of observations - learning, solved using the forward-backward algorithm. 隐马尔科夫模型HMM自学 （4-1）Forward Algorithm 找到观察序列的概率 崔晓源 翻译 Finding the probability of an observed sequence 1、穷举搜索方法 对于水藻和天气的关系，我们可以用穷举搜索方法的到下面的状态转移图（trellis）： 图中，每一列于相邻列的连线由状态转移概率决定，而观察状态和每一列的隐状态则由混淆矩阵决定。如果用穷举的方法的到某一观察状态序列的概率，就要求所有可能的天气状态序列下的概率之和，这个trellis中共有3*3=27个可能的序列。 Pr(dry,damp,soggy | HMM) = Pr(dry,damp,soggy | sunny,sunny,sunny) + Pr(dry,damp,soggy | sunny,sunny ,cloudy) + Pr(dry,damp,soggy | sunny,sunny ,rainy) + . . . . Pr(dry,damp,soggy | rainy,rainy ,rainy) 可见计算复杂度是很大，特别是当状态空间很大，观察序列很长时。我们可以利用概率的时间不变性解决复杂度。 2、采用递归方法降低复杂度 我们采用递归的方式计算观察序列的概率，首先定义部分概率为到达trellis中某一中间状态的概率。在后面的文章里，我们把长度为T的观察状态序列表示为： 2a. Partial probabilities, ('s) 在计算trellis中某一中间状态的概率时，用所有可能到达该状态的路径之和表示。 比如在t=2时间，状态为cloudy的概率可以用下面的路径计算： 用t ( j ) 表示在时间t时 状态j的部分概率。计算方法如下： t ( j )= Pr( observation | hidden state is j ) * Pr(all paths to state j at time t) 最后的观察状态的部分概率表示，这些状态所经过的所有可能路径的概率。比如： 这表示最后的部分概率的和即为trellis中所有可能路径的和，也就是当前HMM下观察序列的概率。 Section 3 会给出一个动态效果介绍如何计算概率。 2b.计算初始状态的部分概率 我们计算部分概率的公式为:  t ( j )= Pr( observation | hidden state is j ) x Pr(all paths to state j at time t) 但是在初始状态，没有路径到达这些状态。那么我们就用probability乘以associated observation probability计算： 这样初始时刻的状态的部分概率就只与其自身的概率和该时刻观察状态的概率有关。 隐马尔科夫模型HMM自学 （4-2）Forward Algorithm 崔晓源 翻译 书接上文，前一话我们讲到了Forward Algorithm中初始状态的部分概率的计算方法。这次我们继续介绍。 2c.如何计算t>1时刻的部分概率 回忆一下我们如何计算部分概率： t ( j )= Pr( observation | hidden state is j ) * Pr(all paths to state j at time t) 我们可知（通过递归）乘积中第一项是可用的。那么如何得到Pr(all paths to state j at time t) 呢？ 为了计算到达一个状态的所有路径的概率，就等于每一个到达这个状态的路径之和： 随着序列数的增长，所要计算的路径数呈指数增长。但是在t时刻我们已经计算出所有到达某一状态的部分概率，因此在计算t+1时刻的某一状态的部分概率时只和t时刻有关。 这个式子的含义就是恰当的观察概率（状态j下，时刻t+1所真正看到的观察状态的概率）乘以此时所有到达该状态的概 率和（前一时刻所有状态的概率与相应的转移概率的积）。因此，我们说在计算t+1时刻的概率时，只用到了t时刻的概率。这样我们就可以计算出整个观察序列 的概率。 2d.复杂度比较 对于观察序列长度T，穷举法的复杂度为T的指数级；而Forward Algorithm的复杂度为T的线性。 ======================================================= 最后我们给出Forward Algorithm的完整定义 We use the forward algorithm to calculate the probability of a T long observation sequence; where each of the y is one of the observable set. Intermediate probabilities ('s) are calculated recursively by first calculating  for all states at t=1.   Then for each time step, t = 2, ..., T, the partial probability  is calculated for each state; that is, the product of the appropriate observation probability and the sum over all possible routes to that state, exploiting recursion by knowing these values already for the previous time step. Finally the sum of all partial probabilities gives the probability of the observation, given the HMM, .  ======================================================= 我们还用天气的例子来说明如何计算t=2时刻，状态CLOUDY的部分概率   怎么样？看到这里豁然开朗了吧。要是还不明白，我就.....................还有办法，看个动画效果： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/forward_algorithm/s3_pg3.html 参数定义： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/forward_algorithm/s3_pg4.html http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/forward_algorithm/s3_pg5.html 最后记住我们使用这个算法的目的（没有应用任何算法都是垃圾），从若干个HMM模型中选出一个最能够体现给定的观察状态序列的模型（概率最大的那个）。 Forward Algorithm （Done） 隐马尔科夫模型HMM自学 （5-1）Viterbi Algorithm 本来想明天再把后面的部分写好，可是睡觉今天是节日呢？一时情不自禁就有打开电脑.......... 找到可能性最大的隐含状态序列 崔晓源 翻译 多数情况下，我们都希望能够根据一个给定的HMM模型，根据观察状态序列找到产生这一序列的潜在的隐含状态序列。 1、穷举搜索方法   我们可以通过穷举的方式列出所有可能隐含状态序列，并算出每一种隐状态序列组合对应的观察状态序列的概率。概率最大的那个组合对应的就是最可能的隐状态序列组合。 Pr(observed sequence | hidden state combination). 比如说上图中的trellis中，最有可能的隐状态序列是使得概率： Pr(dry,damp,soggy | sunny,sunny,sunny), Pr(dry,damp,soggy | sunny,sunny,cloudy), Pr(dry,damp,soggy | sunny,sunny,rainy), . . . . Pr(dry,damp,soggy | rainy,rainy,rainy) 得到最大值的序列。 同样这种穷举法的计算量太大了。为了解决这个问题，我们可以利用和Forward algorithm一样的原理--概率的时间不变性来减少计算量。 2.用递归方式减少复杂度 在给定的观察序列和HMM模型下，我们用一种递归的方式找到最有可能的隐状态序列。同样我们滴定部分概率，即在trellis中到达某一中间状态的概率。然后介绍如何在初始时刻t=1和t>1的时刻分别求解这个部分概率。但要注意，这里的部分概率是到达某一中间状态的概率最大的路径而不是所有概率之和。 2.1部分概率和部分最优路径 看如下trellis   对于trellis中的每个中间状态和结束状态，都存在一条到达它的最优路径。他可能是下图这样：   我们这些路径为部分最优路径，每一条 部分最优路径都对应一个关联概率--部分概率。与Forward algorithm不同是最有可能到达该状态的一条路径的概率。  (i,t)是所有序列中在t时刻以状态i终止的最大概率。当然它所对应那条路径就是部分最优路径。  (i,t)对于每个i,t都是存在的。这样我们就可以在时间T（序列的最后一个状态）找到整个序列的最优路径。 2b. 计算  's 在t = 1的初始值 由于在t=1不存在任何部分最优路径，因此可以用初始状态 向量协助计算。   这一点与Forward Algorithm相同 2c. 计算  's 在t > 1 的部分概率 同样我们只用t-1时刻的信息来得到t时刻的部分概率。   由此图可以看出到达X的最优路径是下面中的一条： (sequence of states), . . ., A, X                                (sequence of states), . . ., B, X or (sequence of states), . . ., C, X 我们希望找到一条概率最大的。回想马尔科夫一阶模型的假设，一个状态之和它前一时刻的状态有关。 Pr (most probable path to A) . Pr (X | A) . Pr (observation | X) 因此到达X的最大概率就是：   其中第一部分由t-1时刻的部分概率得到，第二部分是状态转移概率，第三部分是混淆矩阵中对应的概率。 隐马尔科夫模型HMM自学 （5-2）Viterbi Algorithm 书接前文，viterbi算法已经基本成形...... 崔晓源 翻译 一般化上一篇最后得到的公式我们可以把概率的求解写成：   2d. 反向指针, 's 考虑下面trellis   现在我们可以得到到达每一个中间或者终点状态的概率最大的路径。但是我们需要采取一些方法来记录这条路径。这就需要在每个状态记录得到该状态最优路径的前一状态。记为：   这样argmax操作符就会选择使得括号中式子最大的索引j。 如果有人问，为什么没有乘以混淆矩阵中的观察概率因子。这是因为我们关心的是在到达当前状态的最优路径中，前一状态的信息，而与他对应的观察状态无关。 2e. viterbi算法的两个优点 1）与Forward算法一样，它极大的降低了计算复杂度 2）viterbi会根据输入的观察序列，“自左向右”的根据上下文给出最优的理解。由于viterbi会在给出最终选择前考虑所有的观察序列因素，这样就避免了由于突然的噪声使得决策原理正确答案。这种情况在真实的数据中经常出现。 ================================================== 下面给出viterbi算法完整的定义 1. Formal definition of algorithm The algorithm may be summarised formally as: For each i,, i = 1, ... , n, let :   - this intialises the probability calculations by taking the product of the intitial hidden state probabilities with the associated observation probabilities. For t = 2, ..., T, and i = 1, ... , n let :   - thus determining the most probable route to the next state, and remembering how to get there. This is done by considering all products of transition probabilities with the maximal probabilities already derived for the preceding step. The largest such is remembered, together with what provoked it. Let :   - thus determining which state at system completion (t=T) is the most probable. For t = T - 1, ..., 1 Let :   - thus backtracking through the trellis, following the most probable route. On completion, the sequence i1 ... iT will hold the most probable sequence of hidden states for the observation sequence in hand. ================================================== 我们还用天气的例子来说明如何计算状态CLOUDY的部分概率，注意它与Forward算法的区别 还是那句话： 怎么样？看到这里豁然开朗了吧。要是还不明白，我就.....................还有办法，看个动画效果： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg3.html 参数定义： http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg4.html http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/viterbi_algorithm/s3_pg5.html 别忘了，viterbi算法的目的是根据给定的观察状态序列找出最有可能的隐含状态序列，别忘了viterbi算法不会被中间的噪音所干扰。 隐马尔科夫模型HMM自学 （6）尾声 崔晓源 翻译 HMM的第三个应用就是learning，这个算法就不再这里详述了，并不是因为他难于理解，而是它比前两个算法要复杂很多。这个方向在语 音处理数据库上有重要的地位。因为它可以帮助我们在状态空间很大，观察序列很长的环境下找到合适HMM模型参数：初始状态、转移概率、混淆矩阵等。  ","title":"[转]隐马尔科夫模型HMM"},{"content":" 美剧“数字追凶”(Numb3rs) 是一部描写将数学方法用于侦破的系列电视剧，其中有几集都用到了同一种数学方法，即将组织（譬如黑社会，反战组织）、人物或者事件间的联系，描绘成图形，探索其中的特征，最终发现重大线索。这类方法便是“社会网络分析”（Social Network Analysis，以下简称SNA）。     电视剧中的情节设置并非毫无根据，在真实世界里，SNA确实被应用于安全领域。譬如，据今日美国2006年的报道，“911事件”以后，美国国家安全局从AT&T、Verizon、BellSouth等三家美国主要电信公司搜集电话记录，从中分析和查找潜在的恐怖分子网络。     当然，SNA的应用远不局限在研究犯罪集团，实质上它是一门研究组织中不同实体间联系，以及信息在其间传播模式的方法。SNA也不是一门最新的学科，它的成型和发展在互联网和电脑时代来临之前就开始了。     早在上世纪30年代，美国的一批社会学家，如J.L.Moreno, W.Lloyd Warner等便开始尝试用节点和连线，来分别反映个人及人际关系，并且从中发现了人际关系最为密切的角色以及子群体的存在。之后随着图论等数学工具的引入，这一方法得到进一步巩固和发展，社会学家和数学家们联手建立了一套能够阐释社会学意义的基本计算指标体系，用于评估一个社区组织的结构形态。     随着互联网的兴起，社会网络分析和各类网络应用彼此也有着，或者说，潜在有着巨大的互相推动作用。     一方面，在线的人际数据非常便于获取，大量的Blog, 或者新型的SNS上能够直接展现人际的交往关系，而早期的研究者则不得不依赖于访谈和问卷。如果今天我们需要研究Blog圈时，只要顺着超链往下找，很短时间内就能积累数百万的样本。相比之下，1968年时，加拿大社会学家Barry Wellman在多伦多近郊研究当地社群，第一轮就访谈了845个人，让他们提供和自己关系最为紧密的人员，再继续约见那些人员，以此类推，可想工作量之巨大。     而另一方面，愈加廉价的计算和存储资源，更先进的算法和模型，也让SNA有机会在更多的领域大展身手。譬如Google的网页级别（Pagerank）算法，也可以看作是其应用之一。     从这一点上来说，SNA和网络口碑研究的结合是有相当应用潜力的。以BBS为例，虽然其同Blog或者SNS不同，用户之间没有显式的联系，但是直觉上我们认为论坛上的用户也像现实当中的人群一样，因为长期的交流或者是偏好，能够形成一定的稳定交流形态。由于BBS上交流形式是发贴和回帖，很自然我们可以将彼此间回帖的关系来作为用户之间的联系，建立网络模型。 更有意思的是，针对相同的论坛，如果生成不同月份的数据及相应的网络图形，就会发现对应的网络形态，以及相同指标排名下的关键用户，是具有相当的延续性的。这笔者更有信心社会网络分析以及相应的展示方法，是有助于进一步在IWOM领域探索和研究在线社群的性质，以及信息传播的模式的。     除了在线社区，我们也尝试了通过文本挖掘分析的公司和品牌作为研究对象，分析它们在的关联关系，也取得了不错的效果。     在CIC的正式产品IWOM Master里，用户不仅能查看不同产品间是否存在联系，还可以进一步通过点击连线来显示印证这些联系的文章，来深入探索这些联系的实质，我们在研究的过程中，发现这些联系可以包括竞争，合作，OEM，仿制，共用部件等。     最后，SNA方法结合IWOM研究的应用只是刚刚开始，有待进一步发展的地方还很多，譬如SNA的大量指标和计算方法虽然已经可以用来阐释社会现象，但是在IWOM领域具备什么样的阐释力还需要探索；还有对于研究各种领域内不同对象之间的关系，也需要更加严格和形式化的定义；同时背后的文本挖掘和自然语言处理技术，需要相应进一步发展以期能够更加准确得定位文本中不同实体间的联系。  ","title":"社会网络分析方法解析"},{"content":"c++学习团队（ 张进昌 杜万智 王锴）第二次作业 一、一所大学，做一个学生信息管理系统，你觉得要怎么考虑？ 学生管理系统需要处理的信息如学生的基本信息、每学年成绩 进行的主要操作时添加、删除和查找。 每个学生入学都会被分配一个学号，而且该学号是一组连续的值，所以可以用student类保存每个学生的信息和成绩。对学生的查询操作以按学号查询为主。而学号的编排是按照学校、学院、系、班级和个人组织的。所以可以按班级为单位建立一个student类动态数组，而把学院、系、班级用父子兄弟建立三级索引。另外，可以把学院、系、班级建立成索引存储到hash表中方便查询。 二、中学生英文作文，你怎么判断有多少错误单词？ 判断单词是否错误，就必须参照字典。中学生英文作文，词汇量一般不会超过中学生课本的词汇表，大概有一万个单词。可以用平衡二叉树把这一万个单词组织起来，比如把第5000个单词作二叉树的根，把第2500个单词作为根的左子树，第7500个单词作为根的右子树，以次类推。另用hash表保存如系动词、助动词、语气词、代词等几百个常用单词。这样每从学生作文中读入一个单词，先在hash表中查询，如果没有则从字典的根开始比较，知道配备。否则，为错误单词。 三、对于两段中文文字，如果让你判断它们的相似程度，你觉得应该怎么判断？ 分析问题： 语义的判定，经我们讨论还是要从语法结构的相似性来判断。 1、而两段中文文字的语义，可以分解为句子，在这两段中找到相关度最大的句子，判断它们的相似程度。 2、句子的语义相似性，需要把句子划分为主干。 3、最后，分析句子主干，就必须分析清楚词汇的相似性。 算法设计： 1、分词程序： 按照字典讲一句话的词汇按照内涵，分析清楚。比如“北京大学”不能被分解成“北京” “大学”两个词汇，另外像“非典型性肺炎”也不能分解为否定副词“非”和“典型性肺炎”。在字典中的词条要把同义词、反义词设为相关，比如“29届奥运会”和“北京奥运会为”同义词相关，“好”“坏”为反义相关。 2、分析句子相似性 先按照最简单的“主-谓-宾”为例。 a在第一段中，按顺序选定一句话 b按相关度最大原则，在第二段中找出对应的句子。 c分析这两句的相似性。 d重复a b c，直到第一段结束。 最大相关度，就是两句话的主语、谓语、宾语对应位置的词汇为同义、反义词。 最大相似性，就是两句话的主语同褒贬，谓语的否定副词个数加上宾语的否定修饰词个数奇偶性相同。 “我参加29届奥运会”和“我不能错过北京奥运会”语义一致。 “奇丑无比的她干坏事”与“美丽无比的她不做好事”语义相关，但相似性较差。 另外，像语法中的“把”字句和“被”字句；“杨贵妃是美女”和“美女是杨贵妃”这样集体名词、个体名词；“西施、杨贵妃、王昭君和貂蝉是四大美女” 和 “四大美女是西施、杨贵妃、王昭君和貂蝉”这样的定义与解释；语气词“吧” “呢” “吗” 应按照语义分配键值，来决定相似程度。 最后，按照相似性得出这两句话的相似值，0为最佳，反义最差。 3、分析段落 段落的相似度要依靠句子的相似度，将第2步中每句话的相似度取绝对值累加，如果与0接近就相似，差距很大就不想死。 总结 第三题分析效果的好坏，有几个决定因素： 1、分词词典的容量及精确度，需要大量的统计词汇使用习惯 2、对语法的深刻理解，各种句法、比喻、语气等 3、对文章组织结构的理解，如总分式等文章结构 本小组给出答案的不足： 算法考虑的是用语法结构的相似性来分析语义的相似性，这是一种方法，但明显很机械，而且没有考虑到文章的组织结构，一方面我们的语言学知识很有限，另一方面对自然语言处理得研究还不都。不足之处，希望老师不吝赐教！  ","title":"什么是程序设计第二小组答案"},{"content":"中文分词简介  一、分词方法分类    至于分词在搜索引擎中的作用，不用多说。以下主要对搜索引擎做简单的介绍。      现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。   1、基于字符串匹配的分词方法        这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下： 1）正向最大匹配法（由左到右的方向）； 2）逆向最大匹配法（由右到左的方向）； 3）最少切分（使每一句中切出的词数最小）。       还可以将上述各种方法相互组合，例如，可以将正向最大匹配方法和逆向最大匹配方法结合起来构成双向匹配法。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。       一种方法是改进扫描方式，称为特征扫描或标志切分，优先在待分析字符串中识别和切分出一些带有明显特征的词，以这些词作为断点，可将原字符串分为较小的串再来进机械分词，从而减少匹配的错误率。另一种方法是将分词和词类标注结合起来，利用丰富的词类信息对分词决策提供帮助，并且在标注过程中又反过来对分词结果进行检验、调整，从而极大地提高切分的准确率。       对于机械分词方法，可以建立一个一般的模型，在这方面有专业的学术论文，这里不做详细论述。   2、基于理解的分词方法       这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。   3、基于统计的分词方法(毕业设计)       从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组，例如“这一”、“之一”、“有的”、“我的”、“许多的”等，并且对常用词的识别精度差，时空开销大。实际应用的统计分词系统都要使用一部基本的分词词典（常用词词典）进行串匹配分词，同时使用统计方法识别一些新的词，即将串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。          到底哪种分词算法的准确度更高，目前并无定论。对于任何一个成熟的分词系统来说，不可能单独依靠某一种算法来实现，都需要综合不同的算法。笔者了解，海量科技的分词算法就采用“复方分词法”，所谓复方，相当于用中药中的复方概念，即用不同的药才综合起来去医治疾病，同样，对于中文词的识别，需要多种算法来处理不同的问题。 4、三种分词算法的比较 各种分词方法的优劣对比 分词方法 基于字符串匹配分词 基于理解的分词 基于统计的分词 歧义识别 差 强 强 新词识别 差 强 强 需要词典 需要 不需要 不需要 需要语料库 否 否 是 需要规则库 否 是 否 算法复杂性 容易 很难 一般 技术成熟度 成熟 不成熟 成熟 实施难度 容易 很难 一般 分词准确性 一般 准确 较准 分词速度 快 慢 一般 二、分词中的难题       有了成熟的分词算法，是否就能容易的解决中文分词的问题呢？事实远非如此。中文是一种十分复杂的语言，让计算机理解中文语言更是困难。在中文分词过程中，有两大难题一直没有完全突破。   1、歧义识别(交叉歧义、组合歧义、真歧义)        歧义是指同样的一句话，可能有两种或者更多的切分方法。例如：表面的，因为“表面”和“面的”都是词，那么这个短语就可以分成“表面的”和“表面的”。这种称为交叉歧义。像这种交叉歧义十分常见，前面举的“和服”的例子，其实就是因为交叉歧义引起的错误。“化妆和服装”可以分成“化妆和服装”或者“化妆 和服 装”。由于没有人的知识去理解，计算机很难知道到底哪个方案正确。        交叉歧义相对组合歧义来说是还算比较容易处理，组合歧义就必需根据整个句子来判断了。例如，在句子“这个门把手坏了”中，“把手”是个词，但在句子“请把手拿开”中，“把手”就不是一个词；在句子“将军任命了一名中将”中，“中将”是个词，但在句子“产量三年中将增长两倍”中，“中将”就不再是词。这些词计算机又如何去识别?如果交叉歧义和组合歧义计算机都能解决的话，在歧义中还有一个难题，是真歧义。真歧义意思是给出一句话，由人去判断也不知道哪个应该是词，哪个应该不是词。例如：“乒乓球拍卖完了”，可以切分成“乒乓球拍卖 完 了”、也可切分成“乒乓球 拍卖 完 了”，如果没有上下文其他的句子，恐怕谁也不知道“拍卖”在这里算不算一个词。   2、新词识别        新词，专业术语称为未登录词。也就是那些在字典中都没有收录过，但又确实能称为词的那些词。最典型的是人名，人可以很容易理解句子“王军虎去广州了”中，“王军虎”是个词，因为是一个人的名字，但要是让计算机去识别就困难了。如果把“王军虎”做为一个词收录到字典中去，全世界有那么多名字，而且每时每刻都有新增的人名，收录这些人名本身就是一项巨大的工程。即使这项工作可以完成，还是会存在问题，例如：在句子“王军虎头虎脑的”中，“王军虎”还能不能算词？       新词中除了人名以外，还有机构名、地名、产品名、商标名、简称、省略语等都是很难处理的问题，而且这些又正好是人们经常使用的词，因此对于搜索引擎来说，分词系统中的新词识别十分重要。目前新词识别准确率已经成为评价一个分词系统好坏的重要标志之一。   三、中文分词的应用         目前在自然语言处理技术中，中文处理技术比西文处理技术要落后很大一段距离，许多西文的处理方法中文不能直接采用，就是因为中文必需有分词这道工序。中文分词是其他中文信息处理的基础，搜索引擎只是中文分词的一个应用。其他的比如机器翻译（MT）、语音合成、自动分类、自动摘要、自动校对等等，都需要用到分词。因为中文需要分词，可能会影响一些研究，但同时也为一些企业带来机会，因为国外的计算机处理技术要想进入中国市场，首先也是要解决中文分词问题。在中文研究方面，相比外国人来说，中国人有十分明显的优势。         分词准确性对搜索引擎来说十分重要，但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分词耗用的时间过长，会严重影响搜索引擎内容更新的速度。因此对于搜索引擎来说，分词的准确性和速度，二者都需要达到很高的要求。目前研究中文分词的大多是科研院校，清华、北大、中科院、北京语言学院、东北大学、IBM研究院、微软中国研究院等都有自己的研究队伍，而真正专业研究中文分词的商业公司除了海量科技以外，几乎没有了。科研院校研究的技术，大部分不能很快产品化，而一个专业公司的力量毕竟有限，看来中文分词技术要想更好的服务于更多的产品，还有很长一段路。  ","title":"中文分词简介"},{"content":"文章的算法很简 单（前提是你对Machine Learning或者Computer Vision有一点了解），把8页的pdf文档并排成一张长的image，然后就在上面抽feature。做自然语言处理的朋友们请不要激动，这是 Vision的paper，自然用的是Vision圈子自己的方法。好了，抽什么feature呢？主要是HOG(Histogram of Gradients)，这是一种纯粹用于描述视觉观感的feature。显然，大段的文本，曲线图，图像，表格，数学公式，它们的feature应该是不 太一样的。然后作者用AdaBoost做feature selection训练得到一个分类器：纯粹根据paper的视觉观感来判断paper的好坏。 说到训练分类器，自然需要一个训练集。这篇文章的作者收集了CVPR 2008, ICCV 2009和CVPR 2009的全部1196篇paper构成正样本。那么负样本从何而来呢？被拒的paper显然作者是拿不到的。于是他很聪明的利用了一个众所周知但是大家 却不会公开明言的事实：workshop接纳的很多是在主要会议被拒收的paper。这样，很不幸的，workshop上发表的文章被用作负样本。在 Workshop上发表了论文的同志们不要打我——我只是讲述一篇别人的文章，这个主意不是我出的。 最有趣的部分要数实验结果了。从ROC曲线来看，结果其实还是不错的——以拒绝15%的正样本为代价，可以滤除一半的负样本。作者对于正负样本的特征做了 一些总结，也许对于大家以后投paper还是有点指导意义呢。。。 正样本的“视觉”特点： 1. 里面有几段公式，看上去文章显得似乎很专业，也显得作者似乎数学不错； 2. 实验部分里面多少要有几个曲线图，即使那几个曲线图说明不了什么。但是，只要有几个曲线图在那里，起码表示我做的是“科学实验”； 3. 最好在文章开头或者最后一页排列一堆图像。其实，我也注意到很多作者喜欢排列很多dataset里面的图像到paper上——即使那是一个 publically available的standard dataset——我不知道这样做的意义何在——除了审美效果。 4. 最好写满8页，代表分量足够。 负样本的特点： 1. 不够页数。在submission阶段，写不满6页的文章被录用的机会很小。虽然最后很多本来8页的文章还是能很神奇地被压缩到6页，如果作者想省掉 200美元的附加页费。题外话，我也一直不明白为什么多一页要多交100美元注册费。 2. 有很大的数字表，就是m行n列，排满数字那种。这篇文章表明，排列了很多曲线图和柱状图的文章比排列了很多数字表的文章有更大概率被接收。 3. 没有漂亮插图。 这篇文章的结果，我也做一些补充评论。 1. CVPR和ICCV的录用结果，对于文章的视觉观感，有着显著的统计相关。从我自己做Reviewer的经验，以及和其它reviewer的交谈来说，这 个确实在一定程度上影响了reviewer的第一印象，甚至是评价基调。一篇文章在first glance给人以专业和有内涵的感觉，会有利于它在reviewer心中树立良好印象。这与学术无关，但是，很不幸，却是一个普遍存在的事实。 对于NIPS这种理论取向的会议，虽然不需要那么多漂亮的图表，但是，文章要“长得”像这些会议的文章。让人觉得写文章的是一个有经验的研究者，而不是一 个打酱油的。 2. Workshop的文章和CVPR/ICCV主会似乎存在明显差距，以至于一个如此简单的分类器都能够在区分它们的任务中取得不俗的成绩。另外，作者使用 workshop paper作为负样本的做法虽然是个人选择，但是，起码在一定程度上反映了这个community对于workshop的态度。 3. 近年来CV paper的投稿量的高速增长，已经严重影响了review的质量。一方面，会议不得不邀请许多没有很多经验的学生参与到review的过程，即使文章是 发到senior researcher的手中，最终还是会被传递到他的某个刚入行的学生那里作为学习reviewing的“牺牲品”。我甚至听说过有reviewer为了 应付due date，把文章交给秘书或者亲戚来审，其结果可想而知了。另外，reviewer也没有足够的时间来仔细的审读paper。很多情况下，读完 abstract和intro，大概翻翻实验结果以及文章的插图，已经基本形成对文章的定性。如果reviewer喜欢这篇paper，它会根据作者的 claim对文章表示赞赏；如果不喜欢这篇文章，就会找一些似是而非的理由把文章拒掉。 当然了，最后我们还有rebuttal，然后由AC meeting来确定文章的生死。Area Chair大多是成名学者，个人的学术水平还是由一定保证的。但是，他们非常繁忙，AC meeting虽然目的是给每篇文章一个decision，但是在一些AC的心目里，这主要是一个旅游和social的机会。大部分的final decision就是根据review结果照本宣科（一个不成文的规矩是review rating的中位数是2作为录取划线标准）。另外，AC大概会看看abstract和rebuttal，然后酌情裁量。","title":"paper writing"},{"content":"计算机相关国际会议排名 计算语言学相关的学术会议,在计算机相关的国际会议中的排名都名列前茅,可以提供作大家在单位内考评的参考.另外,我们在\"IEEE/WIC Intl Joint Conf on Web Intelligence and Intelligent Agent Technology (0.82)\"会议(排名十五)中有个关于\"自然语言处理与本体工程\"的 workshop , 在ICICIC2009中有个同名的Invited Sessions希望大家继续关注.   Artificial Intelligence / Machine Learning Although we will attempt to keep this information accurate, we cannot guarantee the accuracy of the information provided. The numbers in brackets correspond to the EIC value (Estimated Impact of Conference). The numbers are normalized to be in the range 0.00-1.00 (the closer the number to 1.00, the better the conference). Only conferences with EIC above 0.50 have been included. The ranking lists will be updated every three months (end of January, April, July, and October). Top 65 conferences are listed (620 considered): AAAI: American Association for AI National Conference (0.99) NIPS: Neural Information Processing Systems (0.98) IJCAI: Intl Joint Conf on AI (0.96) ICCV: Intl Conf on Computer Vision (0.96) CVPR: IEEE Conf on Comp Vision and Pattern Recognition (0.96) ICML: Intl Conf on Machine Learning (0.95) CSSAC: Cognitive Science Society Annual Conference (0.92) UAI: Conference on Uncertainty in AI (0.91) ACL: Annual Meeting of the ACL (Association of Computational Linguistics) (0.90) NAACL: North American Chapter of the ACL (0.88) GP/GECCO: Genetic and Evolutionary Computation Conference (0.85) AID: Intl Conf on AI in Design (0.84) CAIP: Intl Conf on Comp. Analysis of Images and Patterns (0.84) IPCV: Intl Conf on Image Processing, Computer Vision, and Pattern Recognition (0.83) IEEE/WIC Intl Joint Conf on Web Intelligence and Intelligent Agent Technology (0.82) CoNLL: Conference on Natural Language Learning (0.82) EMNLP: Empirical Methods in Natural Language Processing (0.79) AIME: AI in Medicine in Europe (0.76) AAMAS: Intl Conf on Autonomous Agents and Multi-Agent Systems (0.76) ICAI: International Conference on Artificial Intelligence (0.76) IAAI: Innovative Applications in AI (0.76) ICNN/IJCNN: Intl (Joint) Conference on Neural Networks (0.76) ECML: European Conf on Machine Learning (0.76) ICPR: Intl Conf on Pattern Recognition (0.76) ECAI: European Conf on AI (0.76) ICDAR: International Conference on Document Analysis and Recognition (0.75) ICTAI: IEEE conference on Tools with AI (0.74) ANNIE: Artificial Neural Networks in Engineering (0.72) AI-ED: World Conference on AI in Education (0.72) DAS: International Workshop on Document Analysis Systems (0.71) ICIP: Intl Conf on Image Processing (0.71) EA: International Conference on Artificial Evolution (0.70) WACV: IEEE Workshop on Apps of Computer Vision (0.65) COLING: International Conference on Computational Linguistics (0.64) ECCV: European Conference on Computer Vision (0.63) EACL: Annual Meeting of European Association Computational Linguistics (0.62) DocEng: ACM Symposium on Document Engineering (0.61) CAAI: Canadian Artificial Intelligence Conference (0.60) AMAI: Artificial Intelligence and Maths (0.60) ICRA: IEEE Intl Conf on Robotics and Automation (0.60) WCES: World Congress on Expert Systems (0.60) ACCV: Asian Conference on Computer Vision (0.59) CAIA: Conf on AI for Applications (0.57) IEA/AIE: Intl Conf on Ind. and Eng. Apps of AI and Expert Sys (0.57) MLMTA: Intl Conf on Machine Learning; Models, Technologies and Applications (0.57) ICCBR: International Conference on Case-Based Reasoning (0.57) ICASSP: IEEE Intl Conf on Acoustics, Speech and SP (0.57) ASC: Intl Conf on AI and Soft Computing (0.57) PACLIC: Pacific Asia Conference on Language, Information and Computation (0.56) ICONIP: Intl Conf on Neural Information Processing (0.56) IWPAAMS: Intl Workshop on Practical Applications of Agents and Multiagent Systems (0.56) SMC: IEEE Intl Conf on Systems, Man and Cybernetics (0.55) CAPEPIA: Conference of the Spanish Association for Artificial Intelligence (0.55) IWANN: Intl Work-Conf on Art and Natural Neural Networks (0.55) CIA: Cooperative Information Agents (0.55) ICGA: International Conference on Genetic Algorithms (0.54) RANLP: Recent Advances in Natural Language Processing (0.54) ICANN: International Conf on Artificial Neural Networks (0.54) NLPRS: Natural Language Pacific Rim Symposium (0.54) ACIVS: International Conference on Advanced Concepts For Intelligent Vision Systems (0.53) ICAPS/AIPS: Conference on Artificial Intelligence Planning Systems (0.53) ECAL: European Conference on Artificial Life (0.53) MAAMAW: Modelling Autonomous Agents in a Multi-Agent World (0.52) ANTS: Ant Colony Optimization and Swarm Intelligence (0.51) NC: ICSC Symposium on Neural Computation (0.51)         原文地址:http://www.cs-conference-ranking.org/conferencerankings/topicsii.html","title":"计算机相关国际会议排名"}]